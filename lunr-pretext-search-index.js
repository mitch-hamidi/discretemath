var ptx_lunr_search_style = "textbook";
var ptx_lunr_docs = [
{
  "id": "front-colophon",
  "level": "1",
  "url": "front-colophon.html",
  "type": "Colophon",
  "number": "",
  "title": "Colophon",
  "body": "  "
},
{
  "id": "frontmatter-4",
  "level": "1",
  "url": "frontmatter-4.html",
  "type": "Acknowledgements",
  "number": "",
  "title": "Acknowledgements",
  "body": " This text is a remix of the following textbooks:     Understanding Linear Algebra by David Austin licensed under the CC BY 4.0 license . ©2023 David Austin.     Applied Discrete Structures by Al Doerr and Ken Levasseur licensed under the CC BY-NC-SA 3.0 US license . ©2024 Al Doerr, Ken Levasseur.    "
},
{
  "id": "s-set-Notation-and-Relations",
  "level": "1",
  "url": "s-set-Notation-and-Relations.html",
  "type": "Section",
  "number": "1.1",
  "title": "Set Notation and Relations",
  "body": " Set Notation and Relations   The notion of a set  The term set is intuitively understood by most people to mean a collection of objects that are called elements (of the set). This concept is the starting point on which we will build more complex ideas, much as in geometry where the concepts of point and line are left undefined. Because a set is such a simple notion, you may be surprised to learn that it is one of the most difficult concepts for mathematicians to define to their own liking. For example, the description above is not a proper definition because it requires the definition of a collection. (How would you define collection ?) Even deeper problems arise when you consider the possibility that a set could contain itself. Although these problems are of real concern to some mathematicians, they will not be of any concern to us. Our first concern will be how to describe a set; that is, how do we most conveniently describe a set and the elements that are in it? If we are going to discuss a set for any length of time, we usually give it a name in the form of a capital letter (or occasionally some other symbol). In discussing set , if is an element of , then we will write .   is an element of  On the other hand, if is not an element of , we write .   is not an element of  The most convenient way of describing the elements of a set will vary depending on the specific set.  Enumeration Enumeration . When the elements of a set are enumerated (or listed) it is traditional to enclose them in braces. For example, the set of binary digits is and the set of decimal digits is . The choice of a name for these sets would be arbitrary; but it would be logical to call them and , respectively. The choice of a set name is much like the choice of an identifier name in programming.  Some large sets can be enumerated without actually listing all the elements. For example, the letters of the alphabet and the integers from 1 to 100 could be described as , and . The three consecutive dots are called an ellipsis. We use them when it is clear what elements are included but not listed. An ellipsis is used in two other situations. To enumerate the positive integers, we would write , indicating that the list goes on infinitely. If we want to list a more general set such as the integers between 1 and , where is some undetermined positive integer, we might write .  Standard Symbols . Sets that are frequently encountered are usually given symbols that are reserved for them alone. For example, since we will be referring to the positive integers throughout this book, we will use the symbol instead of writing . A few of the other sets of numbers that we will use are:  : the natural numbers,  : the integers,  : the rational numbers  : the real numbers  : the complex numbers   Set-Builder Notation Set-Builder Notation . Another way of describing sets is to use set-builder notation. For example, we could define the rational numbers as . Note that in the set-builder description for the rational numbers:  indicates that a typical element of the set is a fraction.   The vertical line, , is read such that or where, and is used interchangeably with a colon.    is an abbreviated way of saying and are integers.  Commas in mathematics are read as and.    The important fact to keep in mind in set notation, or in any mathematical notation, is that it is meant to be a help, not a hindrance. We hope that notation will assist us in a more complete understanding of the collection of objects under consideration and will enable us to describe it in a concise manner. However, brevity of notation is not the aim of sets. If you prefer to write and instead of , you should do so. Also, there are frequently many different, and equally good, ways of describing sets. For example, and both describe the solution set .  A proper definition of the real numbers is beyond the scope of this text. It is sufficient to think of the real numbers as the set of points on a number line. The complex numbers can be defined using set-builder notation as , where .  In the following definition we will leave the word finite undefined.   Finite Set  A set is a finite set if it has a finite number of elements. Any set that is not finite is an infinite set.     Cardinality    The number of elements in a finite set .   Let be a finite set. The number of different elements in is called its cardinality. The cardinality of a finite set is denoted .    As we will see later, there are different infinite cardinalities. We can't make this distinction until Chapter 7, so we will restrict cardinality to finite sets for now.    Subsets   Subset     is a subset of .   Let and be sets. We say that is a subset of if and only if every element of is an element of . We write to denote the fact that is a subset of .    Some Subsets   If and , then .     If and , then and .     Set Equality  Let and be sets. We say that is equal to (notation ) if and only if every element of is an element of and conversely every element of is an element of ; that is, and .     Examples illustrating set equality   In , . Note that the ordering of the elements is unimportant.  The number of times that an element appears in an enumeration doesn't affect a set. For example, if and , then . Warning to readers of other texts: Some books introduce the concept of a multiset, in which the number of occurrences of an element matters.    A few comments are in order about the expression if and only if as used in our definitions. This expression means is equivalent to saying, or more exactly, that the word (or concept) being defined can at any time be replaced by the defining expression. Conversely, the expression that defines the word (or concept) can be replaced by the word.  Occasionally there is need to discuss the set that contains no elements, namely the empty set, which is denoted by  the empty set  the empty set  Empty set . This set is also called the null set. Another acceptable way to denote the empty set is .  It is clear, we hope, from the definition of a subset, that given any set we have and . If is nonempty, then is called an improper subset Improper subset of . All other subsets of , including the empty set, are called proper subsets Proper subset of . The empty set is an improper subset of itself.  Not everyone is in agreement on whether the empty set is a proper subset of any set. In fact earlier editions of this book sided with those who considered the empty set an improper subset. However, we bow to the emerging consensus at this time.    Exercises for Section 1.1   List four elements of each of the following sets:              These answers are not unique.                List all elements of the following sets:                            Describe the following sets using set-builder notation.     the rational numbers that are strictly between and   the even integers                  Use set-builder notation to describe the following sets:              These answers are far from unique.              Let , , and . Determine which of the following statements are true. Give reasons for your answers.                  True  False  True  True  False  True  False  True   (From ) Explain why there is no set which satisfies .   If \\(A\\) has cardinality 1, then \\(A\\=\\{2,1\\}\\). However, that would mean that \\(\\mid A\\mid =2\\) and \\(A\\=\\{2,2\\}=\\{2\\}\\). This is clearly a contradiction.    We introduced the empty set in this section and pointed out the two standard ways to denote this set, and . Explain why is not a correct way to denote the empty set.  is not the empty set - it contains something which happens to be the empty set.   One reason that we left the definition of a set vague is Russell's Paradox. Many mathematics and logic books contain an account of this paradox. Two references are and . Find one such reference and read it. If you don't have access to a book, you could Google \"Russell's Paradox\" and there are several sites that describe it.    "
},
{
  "id": "s-set-Notation-and-Relations-2-3",
  "level": "2",
  "url": "s-set-Notation-and-Relations.html#s-set-Notation-and-Relations-2-3",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Enumeration "
},
{
  "id": "s-set-Notation-and-Relations-2-5",
  "level": "2",
  "url": "s-set-Notation-and-Relations.html#s-set-Notation-and-Relations-2-5",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Standard Symbols "
},
{
  "id": "s-set-Notation-and-Relations-2-6",
  "level": "2",
  "url": "s-set-Notation-and-Relations.html#s-set-Notation-and-Relations-2-6",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Set-Builder Notation "
},
{
  "id": "finite-set",
  "level": "2",
  "url": "s-set-Notation-and-Relations.html#finite-set",
  "type": "Definition",
  "number": "1.1.1",
  "title": "Finite Set.",
  "body": " Finite Set  A set is a finite set if it has a finite number of elements. Any set that is not finite is an infinite set.   "
},
{
  "id": "cardinality",
  "level": "2",
  "url": "s-set-Notation-and-Relations.html#cardinality",
  "type": "Definition",
  "number": "1.1.2",
  "title": "Cardinality.",
  "body": " Cardinality    The number of elements in a finite set .   Let be a finite set. The number of different elements in is called its cardinality. The cardinality of a finite set is denoted .   "
},
{
  "id": "def-subset",
  "level": "2",
  "url": "s-set-Notation-and-Relations.html#def-subset",
  "type": "Definition",
  "number": "1.1.3",
  "title": "Subset.",
  "body": " Subset     is a subset of .   Let and be sets. We say that is a subset of if and only if every element of is an element of . We write to denote the fact that is a subset of .   "
},
{
  "id": "some-subsets",
  "level": "2",
  "url": "s-set-Notation-and-Relations.html#some-subsets",
  "type": "Example",
  "number": "1.1.4",
  "title": "Some Subsets.",
  "body": "Some Subsets   If and , then .     If and , then and .   "
},
{
  "id": "set-equality",
  "level": "2",
  "url": "s-set-Notation-and-Relations.html#set-equality",
  "type": "Definition",
  "number": "1.1.5",
  "title": "Set Equality.",
  "body": " Set Equality  Let and be sets. We say that is equal to (notation ) if and only if every element of is an element of and conversely every element of is an element of ; that is, and .   "
},
{
  "id": "set-equality-examples",
  "level": "2",
  "url": "s-set-Notation-and-Relations.html#set-equality-examples",
  "type": "Example",
  "number": "1.1.6",
  "title": "Examples illustrating set equality.",
  "body": " Examples illustrating set equality   In , . Note that the ordering of the elements is unimportant.  The number of times that an element appears in an enumeration doesn't affect a set. For example, if and , then . Warning to readers of other texts: Some books introduce the concept of a multiset, in which the number of occurrences of an element matters.   "
},
{
  "id": "ss-subsets-8",
  "level": "2",
  "url": "s-set-Notation-and-Relations.html#ss-subsets-8",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "improper subset proper subsets "
},
{
  "id": "ss-subsets-9",
  "level": "2",
  "url": "s-set-Notation-and-Relations.html#ss-subsets-9",
  "type": "Note",
  "number": "1.1.7",
  "title": "",
  "body": "Not everyone is in agreement on whether the empty set is a proper subset of any set. In fact earlier editions of this book sided with those who considered the empty set an improper subset. However, we bow to the emerging consensus at this time. "
},
{
  "id": "s-set-Notation-and-Relations-4-2",
  "level": "2",
  "url": "s-set-Notation-and-Relations.html#s-set-Notation-and-Relations-4-2",
  "type": "Exercise",
  "number": "1.1.3.1",
  "title": "",
  "body": " List four elements of each of the following sets:              These answers are not unique.              "
},
{
  "id": "s-set-Notation-and-Relations-4-3",
  "level": "2",
  "url": "s-set-Notation-and-Relations.html#s-set-Notation-and-Relations-4-3",
  "type": "Exercise",
  "number": "1.1.3.2",
  "title": "",
  "body": " List all elements of the following sets:                           "
},
{
  "id": "s-set-Notation-and-Relations-4-4",
  "level": "2",
  "url": "s-set-Notation-and-Relations.html#s-set-Notation-and-Relations-4-4",
  "type": "Exercise",
  "number": "1.1.3.3",
  "title": "",
  "body": "Describe the following sets using set-builder notation.     the rational numbers that are strictly between and   the even integers                 "
},
{
  "id": "s-set-Notation-and-Relations-4-5",
  "level": "2",
  "url": "s-set-Notation-and-Relations.html#s-set-Notation-and-Relations-4-5",
  "type": "Exercise",
  "number": "1.1.3.4",
  "title": "",
  "body": "Use set-builder notation to describe the following sets:              These answers are far from unique.             "
},
{
  "id": "s-set-Notation-and-Relations-4-6",
  "level": "2",
  "url": "s-set-Notation-and-Relations.html#s-set-Notation-and-Relations-4-6",
  "type": "Exercise",
  "number": "1.1.3.5",
  "title": "",
  "body": "Let , , and . Determine which of the following statements are true. Give reasons for your answers.                  True  False  True  True  False  True  False  True "
},
{
  "id": "s-set-Notation-and-Relations-4-7",
  "level": "2",
  "url": "s-set-Notation-and-Relations.html#s-set-Notation-and-Relations-4-7",
  "type": "Exercise",
  "number": "1.1.3.6",
  "title": "",
  "body": " (From ) Explain why there is no set which satisfies .   If \\(A\\) has cardinality 1, then \\(A\\=\\{2,1\\}\\). However, that would mean that \\(\\mid A\\mid =2\\) and \\(A\\=\\{2,2\\}=\\{2\\}\\). This is clearly a contradiction.  "
},
{
  "id": "s-set-Notation-and-Relations-4-8",
  "level": "2",
  "url": "s-set-Notation-and-Relations.html#s-set-Notation-and-Relations-4-8",
  "type": "Exercise",
  "number": "1.1.3.7",
  "title": "",
  "body": " We introduced the empty set in this section and pointed out the two standard ways to denote this set, and . Explain why is not a correct way to denote the empty set.  is not the empty set - it contains something which happens to be the empty set. "
},
{
  "id": "s-set-Notation-and-Relations-4-9",
  "level": "2",
  "url": "s-set-Notation-and-Relations.html#s-set-Notation-and-Relations-4-9",
  "type": "Exercise",
  "number": "1.1.3.8",
  "title": "",
  "body": " One reason that we left the definition of a set vague is Russell's Paradox. Many mathematics and logic books contain an account of this paradox. Two references are and . Find one such reference and read it. If you don't have access to a book, you could Google \"Russell's Paradox\" and there are several sites that describe it.  "
},
{
  "id": "s-basic_Set_Operations",
  "level": "1",
  "url": "s-basic_Set_Operations.html",
  "type": "Section",
  "number": "1.2",
  "title": "Basic Set Operations",
  "body": " Basic Set Operations Basic Set Operations   Definitions   Intersection  Intersection    The intersection of and .   Let and be sets. The intersection of and (denoted by ) is the set of all elements that are in both and . That is, .     Some Intersections   Let and . Then .  Solving a system of simultaneous equations such as and can be viewed as an intersection. Let and . These two sets are lines in the plane and their intersection, , is the solution to the system.   .  If and , then .     Disjoint Sets Disjoint Sets  Two sets are disjoint if they have no elements in common. That is, and are disjoint if .     Union Union   The union of and .   Let and be sets. The union of and (denoted by ) is the set of all elements that are in or in or in both A and B. That is, .    It is important to note in the set-builder notation for , the word or is used in the inclusive sense; it includes the case where is in both and .   Some Unions   If and , then .      for any set .    Frequently, when doing mathematics, we need to establish a universe or set of elements under discussion. For example, the set contains different elements depending on what kinds of numbers we allow ourselves to use in solving the equation . This set of numbers would be our universe. For example, if the universe is the integers, then is empty. If our universe is the rational numbers, then is and if the universe is the complex numbers, then is .   Universe Universe The universe, or universal set, is the set of all elements under discussion for possible membership in a set. We normally reserve the letter for a universe in general discussions.      Set Operations and their Venn Diagrams  When working with sets, as in other branches of mathematics, it is often quite useful to be able to draw a picture or diagram of the situation under consideration. A diagram of a set is called a Venn diagram. The universal set is represented by the interior of a rectangle and the sets by disks inside the rectangle.   Venn Diagram Examples   is illustrated in by shading the appropriate region.   Venn Diagram for the Intersection of Two Sets   A two set Venn Diagram for intersection consisting of two overlapping circles with the part of the plane that is contained in both circles shaded to represent the intersection.     The union is illustrated in .   Venn Diagram for the Union    A two set Venn Diagram for union consisting of two overlapping circles with the part of the plane that is contained in either of the circles shaded to represent the union.     In a Venn diagram, the region representing does not appear empty; however, in some instances it will represent the empty set. The same is true for any other region in a Venn diagram.    Complement of a set Complement of a set   The complement of relative to      The complement of set relative to the universe.   Let and be sets. The complement of relative to (notation ) is the set of elements that are in and not in . That is, . If is the universal set, then is denoted by and is called simply the complement of . .    Venn Diagram for   A two set Venn Diagram for intersection consisting of two overlapping circles with the part of the plane that is contained in the circle labeled A but not in the circle labeled B shaded to represent the set difference A minus B.      Some Complements   Let and . Then and .  If , then the complement of the set of rational numbers is the set of irrational numbers.   and .  The Venn diagram of is represented in .  The Venn diagram of is represented in .   If , then the Venn diagram of is as shown in .   In the universe of integers, the set of even integers, , has the set of odd integers as its complement.     Venn Diagram for   A one set Venn Diagram for the complement of a set A consisting of a circle with the part of the plane that is not contained in the circle shaded to represent the set complement.      Venn Diagram for when is a subset of   A Venn Diagram for the complement relative to a superset       Symmetric Difference Symmetric Difference   The symmetric difference of and .   Let and be sets. The symmetric difference of and (denoted by ) is the set of all elements that are in and but not in both. That is, .     Some Symmetric Differences   Let and . Then .  and for any set .  is the set of irrational numbers.  The Venn diagram of is represented in .     Venn Diagram for the symmetric difference   A two set Venn Diagram for the symmetric difference of two sets.      Why Venn?  Venn, John Euler, Leonhard Venn diagrams are named after the logician John Venn, who introduced them in a paper in 1880. In his paper, he acknowledged that they were not new. In fact he referred to them as Euler Circles, because the famous mathematician Leonhard Euler (pronounced Oy-ler) introduced them in the 1700's. Don't feel bad for Euler though. He has plenty of other things named after him, including some we see later in this book.     SageMath Note: Sets SageMath Note Sets  To work with sets in Sage, a set is an expression of the form Set( list ). By wrapping a list with Set( ) , the order of elements appearing in the list and their duplication are ignored. For example, L1 and L2 are two different lists, but notice how as sets they are considered equal:   The standard set operations are all methods and\/or functions that can act on Sage sets. You need to evaluate the following cell to use the subsequent cell.   We can test membership, asking whether 10 is in each of the sets:   The ampersand is used for the intersection of sets. Change it to the vertical bar, |, for union.   Symmetric difference and set complement are defined as methods in Sage. Here is how to compute the symmetric difference of with , followed by their differences.     Exercises  Let , , , and let the universal set be . Determine:                                                   Let , , and be as in Exercise 1, let , and let . Determine which of the following are true. Give reasons for your decisions.                            False, but not in .  False, the two sets have no elements in common.  True, the order in which elements are listed is not important.  True, the fact that 2 appears twice in the definition of doesn't change the fact that it has just two elements.  True, both intesections contain what is common to both and .  True, both unions contain numbers that appear in either or .  False, and   True,     Let . Give examples of sets , , and for which:                  These are all true for any sets , , and .    Let . Give examples to illustrate the following facts:  If and , then .  There are sets and such that   If and , it always follows that .    What can you say about if , , and (separately)                    Suppose that is an infinite universal set, and and are infinite subsets of . Answer the following questions with a brief explanation.   Must be finite?  Must be infinite?  Must be infinite?      No, need not be finite. For example if the universe is all positive integer and is the set of even positive integers, the its complement is the odd positive integers, which is also infinite.  Yes, since everything that is in is in the union, must be infinite.  No, for example, if and are the even and odd positive integer, respectively, then the intersection is empty.    Given that = all students at a university, = day students, = mathematics majors, and = graduate students. Draw Venn diagrams illustrating this situation and shade in the following sets:   evening students  undergraduate mathematics majors  non-math graduate students  non-math undergraduate students        Solution to #7 of section 1.2     Let the sets , , , and be as in exercise 7. Let , , , and . Also assume that the number of day students who are mathematics majors is 250, 50 of whom are graduate students, that there are 95 graduate mathematics majors, and that the total number of day graduate students is 700. Determine the number of students who are:  evening students  nonmathematics majors  undergraduates (day or evening)  day graduate nonmathematics majors  evening graduate students  evening graduate mathematics majors  evening undergraduate nonmathematics majors      7,000 evening students  17,700 nonmathematics majors  15,000 undergraduates (day or evening)  650 day graduate nonmathematics majors  355 evening graduate students  45 evening graduate mathematics majors  6,695 evening undergraduate nonmathematics majors     "
},
{
  "id": "def-intersection",
  "level": "2",
  "url": "s-basic_Set_Operations.html#def-intersection",
  "type": "Definition",
  "number": "1.2.1",
  "title": "Intersection.",
  "body": " Intersection  Intersection    The intersection of and .   Let and be sets. The intersection of and (denoted by ) is the set of all elements that are in both and . That is, .   "
},
{
  "id": "some_intersections",
  "level": "2",
  "url": "s-basic_Set_Operations.html#some_intersections",
  "type": "Example",
  "number": "1.2.2",
  "title": "Some Intersections.",
  "body": " Some Intersections   Let and . Then .  Solving a system of simultaneous equations such as and can be viewed as an intersection. Let and . These two sets are lines in the plane and their intersection, , is the solution to the system.   .  If and , then .   "
},
{
  "id": "def-disjoint-sets",
  "level": "2",
  "url": "s-basic_Set_Operations.html#def-disjoint-sets",
  "type": "Definition",
  "number": "1.2.3",
  "title": "Disjoint Sets.",
  "body": " Disjoint Sets Disjoint Sets  Two sets are disjoint if they have no elements in common. That is, and are disjoint if .   "
},
{
  "id": "def-union",
  "level": "2",
  "url": "s-basic_Set_Operations.html#def-union",
  "type": "Definition",
  "number": "1.2.4",
  "title": "Union.",
  "body": " Union Union   The union of and .   Let and be sets. The union of and (denoted by ) is the set of all elements that are in or in or in both A and B. That is, .   "
},
{
  "id": "some_unions",
  "level": "2",
  "url": "s-basic_Set_Operations.html#some_unions",
  "type": "Example",
  "number": "1.2.5",
  "title": "Some Unions.",
  "body": " Some Unions   If and , then .      for any set .   "
},
{
  "id": "universe",
  "level": "2",
  "url": "s-basic_Set_Operations.html#universe",
  "type": "Definition",
  "number": "1.2.6",
  "title": "Universe.",
  "body": " Universe Universe The universe, or universal set, is the set of all elements under discussion for possible membership in a set. We normally reserve the letter for a universe in general discussions.   "
},
{
  "id": "venn_diagram_examples",
  "level": "2",
  "url": "s-basic_Set_Operations.html#venn_diagram_examples",
  "type": "Example",
  "number": "1.2.7",
  "title": "Venn Diagram Examples.",
  "body": " Venn Diagram Examples   is illustrated in by shading the appropriate region.   Venn Diagram for the Intersection of Two Sets   A two set Venn Diagram for intersection consisting of two overlapping circles with the part of the plane that is contained in both circles shaded to represent the intersection.     The union is illustrated in .   Venn Diagram for the Union    A two set Venn Diagram for union consisting of two overlapping circles with the part of the plane that is contained in either of the circles shaded to represent the union.     In a Venn diagram, the region representing does not appear empty; however, in some instances it will represent the empty set. The same is true for any other region in a Venn diagram.  "
},
{
  "id": "set_complement",
  "level": "2",
  "url": "s-basic_Set_Operations.html#set_complement",
  "type": "Definition",
  "number": "1.2.10",
  "title": "Complement of a set.",
  "body": " Complement of a set Complement of a set   The complement of relative to      The complement of set relative to the universe.   Let and be sets. The complement of relative to (notation ) is the set of elements that are in and not in . That is, . If is the universal set, then is denoted by and is called simply the complement of . .  "
},
{
  "id": "venn_diagram_complement1",
  "level": "2",
  "url": "s-basic_Set_Operations.html#venn_diagram_complement1",
  "type": "Figure",
  "number": "1.2.11",
  "title": "",
  "body": " Venn Diagram for   A two set Venn Diagram for intersection consisting of two overlapping circles with the part of the plane that is contained in the circle labeled A but not in the circle labeled B shaded to represent the set difference A minus B.    "
},
{
  "id": "complements",
  "level": "2",
  "url": "s-basic_Set_Operations.html#complements",
  "type": "Example",
  "number": "1.2.12",
  "title": "Some Complements.",
  "body": " Some Complements   Let and . Then and .  If , then the complement of the set of rational numbers is the set of irrational numbers.   and .  The Venn diagram of is represented in .  The Venn diagram of is represented in .   If , then the Venn diagram of is as shown in .   In the universe of integers, the set of even integers, , has the set of odd integers as its complement.     Venn Diagram for   A one set Venn Diagram for the complement of a set A consisting of a circle with the part of the plane that is not contained in the circle shaded to represent the set complement.      Venn Diagram for when is a subset of   A Venn Diagram for the complement relative to a superset     "
},
{
  "id": "symmetric-difference",
  "level": "2",
  "url": "s-basic_Set_Operations.html#symmetric-difference",
  "type": "Definition",
  "number": "1.2.15",
  "title": "Symmetric Difference.",
  "body": " Symmetric Difference Symmetric Difference   The symmetric difference of and .   Let and be sets. The symmetric difference of and (denoted by ) is the set of all elements that are in and but not in both. That is, .   "
},
{
  "id": "some_symmetric_differences",
  "level": "2",
  "url": "s-basic_Set_Operations.html#some_symmetric_differences",
  "type": "Example",
  "number": "1.2.16",
  "title": "Some Symmetric Differences.",
  "body": " Some Symmetric Differences   Let and . Then .  and for any set .  is the set of irrational numbers.  The Venn diagram of is represented in .     Venn Diagram for the symmetric difference   A two set Venn Diagram for the symmetric difference of two sets.     "
},
{
  "id": "exercises-1-2-2",
  "level": "2",
  "url": "s-basic_Set_Operations.html#exercises-1-2-2",
  "type": "Exercise",
  "number": "1.2.4.1",
  "title": "",
  "body": "Let , , , and let the universal set be . Determine:                                                  "
},
{
  "id": "exercises-1-2-3",
  "level": "2",
  "url": "s-basic_Set_Operations.html#exercises-1-2-3",
  "type": "Exercise",
  "number": "1.2.4.2",
  "title": "",
  "body": "Let , , and be as in Exercise 1, let , and let . Determine which of the following are true. Give reasons for your decisions.                            False, but not in .  False, the two sets have no elements in common.  True, the order in which elements are listed is not important.  True, the fact that 2 appears twice in the definition of doesn't change the fact that it has just two elements.  True, both intesections contain what is common to both and .  True, both unions contain numbers that appear in either or .  False, and   True,    "
},
{
  "id": "exercises-1-2-4",
  "level": "2",
  "url": "s-basic_Set_Operations.html#exercises-1-2-4",
  "type": "Exercise",
  "number": "1.2.4.3",
  "title": "",
  "body": "Let . Give examples of sets , , and for which:                  These are all true for any sets , , and .  "
},
{
  "id": "exercises-1-2-5",
  "level": "2",
  "url": "s-basic_Set_Operations.html#exercises-1-2-5",
  "type": "Exercise",
  "number": "1.2.4.4",
  "title": "",
  "body": " Let . Give examples to illustrate the following facts:  If and , then .  There are sets and such that   If and , it always follows that .   "
},
{
  "id": "exercises-1-2-6",
  "level": "2",
  "url": "s-basic_Set_Operations.html#exercises-1-2-6",
  "type": "Exercise",
  "number": "1.2.4.5",
  "title": "",
  "body": "What can you say about if , , and (separately)                   "
},
{
  "id": "exercises-1-2-7",
  "level": "2",
  "url": "s-basic_Set_Operations.html#exercises-1-2-7",
  "type": "Exercise",
  "number": "1.2.4.6",
  "title": "",
  "body": "Suppose that is an infinite universal set, and and are infinite subsets of . Answer the following questions with a brief explanation.   Must be finite?  Must be infinite?  Must be infinite?      No, need not be finite. For example if the universe is all positive integer and is the set of even positive integers, the its complement is the odd positive integers, which is also infinite.  Yes, since everything that is in is in the union, must be infinite.  No, for example, if and are the even and odd positive integer, respectively, then the intersection is empty.   "
},
{
  "id": "exercises-1-2-8",
  "level": "2",
  "url": "s-basic_Set_Operations.html#exercises-1-2-8",
  "type": "Exercise",
  "number": "1.2.4.7",
  "title": "",
  "body": "Given that = all students at a university, = day students, = mathematics majors, and = graduate students. Draw Venn diagrams illustrating this situation and shade in the following sets:   evening students  undergraduate mathematics majors  non-math graduate students  non-math undergraduate students        Solution to #7 of section 1.2    "
},
{
  "id": "exercises-1-2-9",
  "level": "2",
  "url": "s-basic_Set_Operations.html#exercises-1-2-9",
  "type": "Exercise",
  "number": "1.2.4.8",
  "title": "",
  "body": "Let the sets , , , and be as in exercise 7. Let , , , and . Also assume that the number of day students who are mathematics majors is 250, 50 of whom are graduate students, that there are 95 graduate mathematics majors, and that the total number of day graduate students is 700. Determine the number of students who are:  evening students  nonmathematics majors  undergraduates (day or evening)  day graduate nonmathematics majors  evening graduate students  evening graduate mathematics majors  evening undergraduate nonmathematics majors      7,000 evening students  17,700 nonmathematics majors  15,000 undergraduates (day or evening)  650 day graduate nonmathematics majors  355 evening graduate students  45 evening graduate mathematics majors  6,695 evening undergraduate nonmathematics majors   "
},
{
  "id": "s-cartesian_Products_and_Power_Sets",
  "level": "1",
  "url": "s-cartesian_Products_and_Power_Sets.html",
  "type": "Section",
  "number": "1.3",
  "title": "Cartesian Products and Power Sets",
  "body": " Cartesian Products and Power Sets   Cartesian Products   Cartesian Product Cartesian Product    The cartesian product of with .   Let and be sets. The Cartesian product of and , denoted by , is defined as follows: , that is, is the set of all possible ordered pairs whose first component comes from and whose second component comes from .    Some Cartesian Products  Notation in mathematics is often developed for good reason. In this case, a few examples will make clear why the symbol is used for Cartesian products.   Let and . Then . Note that .   . Note that .    These two examples illustrate the general rule that if and are finite sets, then .  We can define the Cartesian product of three (or more) sets similarly. For example, .  It is common to use exponents if the sets in a Cartesian product are the same:  and in general, .    Power Sets   Power Set Power Set    The power set of , the set of all subsets of .   If is any set, the power set of is the set of all subsets of , denoted .   The two extreme cases, the empty set and all of , are both included in .   Some Power Sets         .   We will leave it to you to guess at a general formula for the number of elements in the power set of a finite set. In Chapter 2, we will discuss counting rules that will help us derive this formula.     SageMath Note: Cartesian Products and Power Sets SageMath Note Cartesian Products and Power Sets  Here is a simple example of a cartesian product of two sets:   Here is the cardinality of the cartesian product.   The power set of a set is an iterable, as you can see from the output of this next cell   You can iterate over a powerset. Here is a trivial example.     Exercises   Let , , , and let the universal set be . List the elements of                                         Suppose that you are about to flip a coin and then roll a die. Let and .   What is ?  How could you interpret the set ?       .  Each element of the set can be thought of a possible outcome of flipping a coin and rolling a die.     List all two-element sets in     List all three-element sets in .   , , and .    How many singleton (one-element) sets are there in if ?  There are singleton subsets, one for each element.    A person has four coins in his pocket: a penny, a nickel, a dime, and a quarter. How many different sums of money can he take out if he removes 3 coins at a time?   There are four sums one for each way you can leave a coin in your pocket.    Let and .   List the elements of   How many elements do and have?           Let and .   List the elements of and . The parentheses and comma in an ordered pair are not necessary in cases such as this where the elements of each set are individual symbols.  Identify the intersection of and for the case above, and then guess at a general rule for the intersection of and , where and are any two sets.       and    The intersection of and is . In general the, intersection of and will be .      Let and be nonempty sets. When are and equal?  They are equal when .    "
},
{
  "id": "cartesian-product",
  "level": "2",
  "url": "s-cartesian_Products_and_Power_Sets.html#cartesian-product",
  "type": "Definition",
  "number": "1.3.1",
  "title": "Cartesian Product.",
  "body": " Cartesian Product Cartesian Product    The cartesian product of with .   Let and be sets. The Cartesian product of and , denoted by , is defined as follows: , that is, is the set of all possible ordered pairs whose first component comes from and whose second component comes from .   "
},
{
  "id": "Cartesian-products-3",
  "level": "2",
  "url": "s-cartesian_Products_and_Power_Sets.html#Cartesian-products-3",
  "type": "Example",
  "number": "1.3.2",
  "title": "Some Cartesian Products.",
  "body": "Some Cartesian Products  Notation in mathematics is often developed for good reason. In this case, a few examples will make clear why the symbol is used for Cartesian products.   Let and . Then . Note that .   . Note that .   "
},
{
  "id": "power-set",
  "level": "2",
  "url": "s-cartesian_Products_and_Power_Sets.html#power-set",
  "type": "Definition",
  "number": "1.3.3",
  "title": "Power Set.",
  "body": " Power Set Power Set    The power set of , the set of all subsets of .   If is any set, the power set of is the set of all subsets of , denoted .  "
},
{
  "id": "Some_Power_Sets",
  "level": "2",
  "url": "s-cartesian_Products_and_Power_Sets.html#Some_Power_Sets",
  "type": "Example",
  "number": "1.3.4",
  "title": "Some Power Sets.",
  "body": " Some Power Sets         .   We will leave it to you to guess at a general formula for the number of elements in the power set of a finite set. In Chapter 2, we will discuss counting rules that will help us derive this formula.  "
},
{
  "id": "exercises-1-3-2",
  "level": "2",
  "url": "s-cartesian_Products_and_Power_Sets.html#exercises-1-3-2",
  "type": "Exercise",
  "number": "1.3.4.1",
  "title": "",
  "body": " Let , , , and let the universal set be . List the elements of                                        "
},
{
  "id": "exercises-1-3-3",
  "level": "2",
  "url": "s-cartesian_Products_and_Power_Sets.html#exercises-1-3-3",
  "type": "Exercise",
  "number": "1.3.4.2",
  "title": "",
  "body": "Suppose that you are about to flip a coin and then roll a die. Let and .   What is ?  How could you interpret the set ?       .  Each element of the set can be thought of a possible outcome of flipping a coin and rolling a die.    "
},
{
  "id": "exercises-1-3-4",
  "level": "2",
  "url": "s-cartesian_Products_and_Power_Sets.html#exercises-1-3-4",
  "type": "Exercise",
  "number": "1.3.4.3",
  "title": "",
  "body": "List all two-element sets in   "
},
{
  "id": "exercises-1-3-5",
  "level": "2",
  "url": "s-cartesian_Products_and_Power_Sets.html#exercises-1-3-5",
  "type": "Exercise",
  "number": "1.3.4.4",
  "title": "",
  "body": " List all three-element sets in .   , , and .  "
},
{
  "id": "exercises-1-3-6",
  "level": "2",
  "url": "s-cartesian_Products_and_Power_Sets.html#exercises-1-3-6",
  "type": "Exercise",
  "number": "1.3.4.5",
  "title": "",
  "body": " How many singleton (one-element) sets are there in if ?  There are singleton subsets, one for each element.  "
},
{
  "id": "exercises-1-3-7",
  "level": "2",
  "url": "s-cartesian_Products_and_Power_Sets.html#exercises-1-3-7",
  "type": "Exercise",
  "number": "1.3.4.6",
  "title": "",
  "body": " A person has four coins in his pocket: a penny, a nickel, a dime, and a quarter. How many different sums of money can he take out if he removes 3 coins at a time?   There are four sums one for each way you can leave a coin in your pocket.  "
},
{
  "id": "exercises-1-3-8",
  "level": "2",
  "url": "s-cartesian_Products_and_Power_Sets.html#exercises-1-3-8",
  "type": "Exercise",
  "number": "1.3.4.7",
  "title": "",
  "body": " Let and .   List the elements of   How many elements do and have?         "
},
{
  "id": "exercises-1-3-9",
  "level": "2",
  "url": "s-cartesian_Products_and_Power_Sets.html#exercises-1-3-9",
  "type": "Exercise",
  "number": "1.3.4.8",
  "title": "",
  "body": " Let and .   List the elements of and . The parentheses and comma in an ordered pair are not necessary in cases such as this where the elements of each set are individual symbols.  Identify the intersection of and for the case above, and then guess at a general rule for the intersection of and , where and are any two sets.       and    The intersection of and is . In general the, intersection of and will be .    "
},
{
  "id": "exercises-1-3-10",
  "level": "2",
  "url": "s-cartesian_Products_and_Power_Sets.html#exercises-1-3-10",
  "type": "Exercise",
  "number": "1.3.4.9",
  "title": "",
  "body": " Let and be nonempty sets. When are and equal?  They are equal when .  "
},
{
  "id": "s-binary_Representation_of_Positive_Integers",
  "level": "1",
  "url": "s-binary_Representation_of_Positive_Integers.html",
  "type": "Section",
  "number": "1.4",
  "title": "Binary Representation of Positive Integers",
  "body": " Binary Representation of Positive Integers Binary Representation   Grouping by Twos  Recall that the set of positive integers, , is . Positive integers are naturally used to count things. There are many ways to count and many ways to record, or represent, the results of counting. For example, if we wanted to count five hundred twenty-three apples, we might group the apples by tens. There would be fifty-two groups of ten with three single apples left over. The fifty-two groups of ten could be put into five groups of ten tens (hundreds), with two tens left over. The five hundreds, two tens, and three units is recorded as 523. This system of counting is called the base ten positional system, or decimal system. It is quite natural for us to do grouping by tens, hundreds, thousands, since it is the method that all of us use in everyday life.  The term positional refers to the fact that each digit in the decimal representation of a number has a significance based on its position. Of course this means that rearranging digits will change the number being described. You may have learned of numeration systems in which the position of symbols does not have any significance (e.g., the ancient Egyptian system). Most of these systems are merely curiosities to us now.  The binary number system differs from the decimal number system in that units are grouped by twos, fours, eights, etc. That is, the group sizes are powers of two instead of powers of ten. For example, twenty-three can be grouped into eleven groups of two with one left over. The eleven twos can be grouped into five groups of four with one group of two left over. Continuing along the same lines, we find that twenty-three can be described as one sixteen, zero eights, one four, one two, and one one, which is abbreviated , or simply if the context is clear.    A Conversion Algorithm  The process that we used to determine the binary representation of can be described in general terms to determine the binary representation of any positive integer . A general description of a process such as this one is called an algorithm. Since this is the first algorithm in the book, we will first write it out using less formal language than usual, and then introduce some algorithmic notation. If you are unfamiliar with algorithms, we refer you to   Start with an empty list of bits.  Assign the variable the value .  While 's value is positive, continue performing the following three steps until becomes zero and then stop.   divide by 2, obtaining a quotient (often denoted ) and a remainder (denoted ).  attach to the left-hand side of the list of bits.  assign the variable the value .     An example of conversion to binary  To determine the binary representation of 41 we take the following steps:              Therefore,   The notation that we will use to describe this algorithm and all others is called pseudocode, an informal variation of the instructions that are commonly used in many computer languages. Read the following description carefully, comparing it with the informal description above. Appendix B, which contains a general discussion of the components of the algorithms in this book, should clear up any lingering questions. Anything after \/\/ are comments.   Binary Conversion Algorithm Binary Conversion Algorithm  An algorithm for determining the binary representation of a positive integer.  Input: a positive integer n.  Output: the binary representation of n in the form of a list of bits, with units bit last, twos bit next to last, etc.   k := n \/\/initialize k  L := { } \/\/initialize L to an empty list  While k > 0 do   q := k div 2 \/\/divide k by 2  r:= k mod 2  L: = prepend r to L \/\/add r to the front of L  k:= q \/\/reassign k      Here is a Sage version of the algorithm with two alterations. It outputs the binary representation as a string, and it handles all integers, not just positive ones.   Now that you've read this section, you should get this joke.   With permission from Randall Munroe, http:\/\/xkcd.com.   Comic from http:\/\/xkcd.com. Two individuals, Megan and Cueball, are conversing. Transcript follows. Megan: On a scale of 1 to 10, how likely is it that this question is using Binary? Cueball: ...4? Megan: What's a 4?      Exercises   Find the binary representation of each of the following positive integers by working through the algorithm by hand. You can check your answer using the sage cell above.  31 32 10 100             Find the binary representation of each of the following positive integers by working through the algorithm by hand. You can check your answer using the sage cell above.  64 67 28 256   1000000 1000011 11100 100000000    What positive integers have the following binary representations?  10010 10011 101010 10011110000             What positive integers have the following binary representations? 100001 1001001 1000000000 1001110000   33 73 512 624    The number of bits in the binary representations of integers increases by one as the numbers double. Using this fact, determine how many bits the binary representations of the following decimal numbers have without actually doing the full conversion. 2017 4000 4500  There is a bit for each power of 2 up to the largest one needed to represent an integer, and you start counting with the zeroth power. For example, 2017 is between and , and so the largest power needed is . Therefore there are bits in binary 2017.         51     Let be a positive integer with -bit binary representation: with What are the smallest and largest values that could have?   The smallest positive value is while the largest is .    If a positive integer is a multiple of 100, we can identify this fact from its decimal representation, since it will end with two zeros. What can you say about a positive integer if its binary representation ends with two zeros? What if it ends in zeros?  A number must be a multiple of four if its binary representation ends in two zeros. If it ends in zeros, it must be a multiple of .    Can a multiple of ten be easily identified from its binary representation?   No, there is no easy way to identify a multiple of 10. The last (units) bit must be 0, but the rest of the bits for multiples of 10 have no single pattern that we can easily identify. Here are the first four multiples of 10 in binary: , , and . There are a few patterns though. Notice that 10, 20 and 40 are related, but 30 has a different pattern. Since 60 is two times 30, we know that 60 will have a binary representation of 111100. Given a random positive integer in binary form that ends with 0, it's likely that we couldn't say whether it is a multiple of 10 without converting to base 10.    "
},
{
  "id": "An_example_of_conversion_to_binary",
  "level": "2",
  "url": "s-binary_Representation_of_Positive_Integers.html#An_example_of_conversion_to_binary",
  "type": "Example",
  "number": "1.4.1",
  "title": "An example of conversion to binary.",
  "body": " An example of conversion to binary  To determine the binary representation of 41 we take the following steps:              Therefore,  "
},
{
  "id": "binary-conversion-algorithm",
  "level": "2",
  "url": "s-binary_Representation_of_Positive_Integers.html#binary-conversion-algorithm",
  "type": "Algorithm",
  "number": "1.4.2",
  "title": "Binary Conversion Algorithm.",
  "body": " Binary Conversion Algorithm Binary Conversion Algorithm  An algorithm for determining the binary representation of a positive integer.  Input: a positive integer n.  Output: the binary representation of n in the form of a list of bits, with units bit last, twos bit next to last, etc.   k := n \/\/initialize k  L := { } \/\/initialize L to an empty list  While k > 0 do   q := k div 2 \/\/divide k by 2  r:= k mod 2  L: = prepend r to L \/\/add r to the front of L  k:= q \/\/reassign k     "
},
{
  "id": "onetoten",
  "level": "2",
  "url": "s-binary_Representation_of_Positive_Integers.html#onetoten",
  "type": "Figure",
  "number": "1.4.3",
  "title": "",
  "body": " With permission from Randall Munroe, http:\/\/xkcd.com.   Comic from http:\/\/xkcd.com. Two individuals, Megan and Cueball, are conversing. Transcript follows. Megan: On a scale of 1 to 10, how likely is it that this question is using Binary? Cueball: ...4? Megan: What's a 4?   "
},
{
  "id": "exercises-1-4-2",
  "level": "2",
  "url": "s-binary_Representation_of_Positive_Integers.html#exercises-1-4-2",
  "type": "Exercise",
  "number": "1.4.3.1",
  "title": "",
  "body": " Find the binary representation of each of the following positive integers by working through the algorithm by hand. You can check your answer using the sage cell above.  31 32 10 100           "
},
{
  "id": "exercises-1-4-3",
  "level": "2",
  "url": "s-binary_Representation_of_Positive_Integers.html#exercises-1-4-3",
  "type": "Exercise",
  "number": "1.4.3.2",
  "title": "",
  "body": " Find the binary representation of each of the following positive integers by working through the algorithm by hand. You can check your answer using the sage cell above.  64 67 28 256   1000000 1000011 11100 100000000  "
},
{
  "id": "exercises-1-4-4",
  "level": "2",
  "url": "s-binary_Representation_of_Positive_Integers.html#exercises-1-4-4",
  "type": "Exercise",
  "number": "1.4.3.3",
  "title": "",
  "body": " What positive integers have the following binary representations?  10010 10011 101010 10011110000           "
},
{
  "id": "exercises-1-4-5",
  "level": "2",
  "url": "s-binary_Representation_of_Positive_Integers.html#exercises-1-4-5",
  "type": "Exercise",
  "number": "1.4.3.4",
  "title": "",
  "body": " What positive integers have the following binary representations? 100001 1001001 1000000000 1001110000   33 73 512 624  "
},
{
  "id": "exercises-1-4-6",
  "level": "2",
  "url": "s-binary_Representation_of_Positive_Integers.html#exercises-1-4-6",
  "type": "Exercise",
  "number": "1.4.3.5",
  "title": "",
  "body": " The number of bits in the binary representations of integers increases by one as the numbers double. Using this fact, determine how many bits the binary representations of the following decimal numbers have without actually doing the full conversion. 2017 4000 4500  There is a bit for each power of 2 up to the largest one needed to represent an integer, and you start counting with the zeroth power. For example, 2017 is between and , and so the largest power needed is . Therefore there are bits in binary 2017.         51   "
},
{
  "id": "exercises-1-4-7",
  "level": "2",
  "url": "s-binary_Representation_of_Positive_Integers.html#exercises-1-4-7",
  "type": "Exercise",
  "number": "1.4.3.6",
  "title": "",
  "body": " Let be a positive integer with -bit binary representation: with What are the smallest and largest values that could have?   The smallest positive value is while the largest is .  "
},
{
  "id": "exercises-1-4-8",
  "level": "2",
  "url": "s-binary_Representation_of_Positive_Integers.html#exercises-1-4-8",
  "type": "Exercise",
  "number": "1.4.3.7",
  "title": "",
  "body": " If a positive integer is a multiple of 100, we can identify this fact from its decimal representation, since it will end with two zeros. What can you say about a positive integer if its binary representation ends with two zeros? What if it ends in zeros?  A number must be a multiple of four if its binary representation ends in two zeros. If it ends in zeros, it must be a multiple of .  "
},
{
  "id": "exercises-1-4-9",
  "level": "2",
  "url": "s-binary_Representation_of_Positive_Integers.html#exercises-1-4-9",
  "type": "Exercise",
  "number": "1.4.3.8",
  "title": "",
  "body": " Can a multiple of ten be easily identified from its binary representation?   No, there is no easy way to identify a multiple of 10. The last (units) bit must be 0, but the rest of the bits for multiples of 10 have no single pattern that we can easily identify. Here are the first four multiples of 10 in binary: , , and . There are a few patterns though. Notice that 10, 20 and 40 are related, but 30 has a different pattern. Since 60 is two times 30, we know that 60 will have a binary representation of 111100. Given a random positive integer in binary form that ends with 0, it's likely that we couldn't say whether it is a multiple of 10 without converting to base 10.  "
},
{
  "id": "s-summation_Notation_and_Generalizations",
  "level": "1",
  "url": "s-summation_Notation_and_Generalizations.html",
  "type": "Section",
  "number": "1.5",
  "title": "Summation Notation and Generalizations",
  "body": " Summation Notation and Generalizations Summation Notation and Generalizations   Sums  Most operations such as addition of numbers are introduced as binary operations. That is, we are taught that two numbers may be added together to give us a single number. Before long, we run into situations where more than two numbers are to be added. For example, if four numbers, , , , and are to be added, their sum may be written down in several ways, such as  or . In the first expression, the first two numbers are added, the result is added to the third number, and that result is added to the fourth number. In the second expression the first two numbers and the last two numbers are added and the results of these additions are added. Of course, we know that the final results will be the same. This is due to the fact that addition of numbers is an associative operation. For such operations, there is no need to describe how more than two objects will be operated on. A sum of numbers such as is called a series and is often written in what is called summation notation .  We first recall some basic facts about series that you probably have seen before. A more formal treatment of sequences and series is covered in Chapter 8. The purpose here is to give the reader a working knowledge of summation notation and to carry this notation through to intersection and union of sets and other mathematical operations.  A finite series is an expression such as  In the expression :   The variable is referred to as the index , or the index of summation.  The expression is the general term of the series. It defines the numbers that are being added together in the series.  The value of below the summation symbol is the initial index and the value above the summation symbol is the terminal index .  It is understood that the series is a sum of the general terms where the index starts with the initial index and increases by one up to and including the terminal index.    Some finite series            More finite series  If the general terms in a series are more specific, the sum can often be simplified. For example,        .      Generalizations  Summation notation can be generalized to many mathematical operations, for example,    Generalized Set Operations  Generalized Set Operations Let be sets. Then:              Some generalized operations  If , , and , then and . With this notation it is quite easy to write lengthy expressions in a fairly compact form. For example, the statement becomes .     Exercises   Calculate the following series:         for   for                Calculate the following series:    for       for    for              The sum is zero of all .      Express the formula without using summation notation.  Verify this formula for .  Repeat parts (a) and (b) for            Verify the following properties for .           Rewrite the following without summation sign for . It is not necessary that you understand or expand the notation at this point. .      Draw the Venn diagram for .  Express in expanded format : .     For any positive integer , let and . What are the following sets?                            For any positive integer , let and . What are the following sets?              The symbol is used for the product of numbers in the same way that is used for sums. For example, . Evaluate the following:               Evaluate           "
},
{
  "id": "some_finite_series",
  "level": "2",
  "url": "s-summation_Notation_and_Generalizations.html#some_finite_series",
  "type": "Example",
  "number": "1.5.1",
  "title": "Some finite series.",
  "body": " Some finite series          "
},
{
  "id": "more_finite_series",
  "level": "2",
  "url": "s-summation_Notation_and_Generalizations.html#more_finite_series",
  "type": "Example",
  "number": "1.5.2",
  "title": "More finite series.",
  "body": " More finite series  If the general terms in a series are more specific, the sum can often be simplified. For example,        .   "
},
{
  "id": "generalized-set-operations",
  "level": "2",
  "url": "s-summation_Notation_and_Generalizations.html#generalized-set-operations",
  "type": "Definition",
  "number": "1.5.3",
  "title": "Generalized Set Operations.",
  "body": " Generalized Set Operations  Generalized Set Operations Let be sets. Then:            "
},
{
  "id": "some_generalized_operations",
  "level": "2",
  "url": "s-summation_Notation_and_Generalizations.html#some_generalized_operations",
  "type": "Example",
  "number": "1.5.4",
  "title": "Some generalized operations.",
  "body": " Some generalized operations  If , , and , then and . With this notation it is quite easy to write lengthy expressions in a fairly compact form. For example, the statement becomes .  "
},
{
  "id": "exercises-1-5-2",
  "level": "2",
  "url": "s-summation_Notation_and_Generalizations.html#exercises-1-5-2",
  "type": "Exercise",
  "number": "1.5.3.1",
  "title": "",
  "body": " Calculate the following series:         for   for               "
},
{
  "id": "exercises-1-5-3",
  "level": "2",
  "url": "s-summation_Notation_and_Generalizations.html#exercises-1-5-3",
  "type": "Exercise",
  "number": "1.5.3.2",
  "title": "",
  "body": "Calculate the following series:    for       for    for              The sum is zero of all .   "
},
{
  "id": "exercises-1-5-4",
  "level": "2",
  "url": "s-summation_Notation_and_Generalizations.html#exercises-1-5-4",
  "type": "Exercise",
  "number": "1.5.3.3",
  "title": "",
  "body": "  Express the formula without using summation notation.  Verify this formula for .  Repeat parts (a) and (b) for           "
},
{
  "id": "exercises-1-5-5",
  "level": "2",
  "url": "s-summation_Notation_and_Generalizations.html#exercises-1-5-5",
  "type": "Exercise",
  "number": "1.5.3.4",
  "title": "",
  "body": "Verify the following properties for .         "
},
{
  "id": "exercises-1-5-6",
  "level": "2",
  "url": "s-summation_Notation_and_Generalizations.html#exercises-1-5-6",
  "type": "Exercise",
  "number": "1.5.3.5",
  "title": "",
  "body": " Rewrite the following without summation sign for . It is not necessary that you understand or expand the notation at this point. .   "
},
{
  "id": "exercises-1-5-7",
  "level": "2",
  "url": "s-summation_Notation_and_Generalizations.html#exercises-1-5-7",
  "type": "Exercise",
  "number": "1.5.3.6",
  "title": "",
  "body": "  Draw the Venn diagram for .  Express in expanded format : .   "
},
{
  "id": "exercises-1-5-8",
  "level": "2",
  "url": "s-summation_Notation_and_Generalizations.html#exercises-1-5-8",
  "type": "Exercise",
  "number": "1.5.3.7",
  "title": "",
  "body": " For any positive integer , let and . What are the following sets?                          "
},
{
  "id": "exercises-1-5-9",
  "level": "2",
  "url": "s-summation_Notation_and_Generalizations.html#exercises-1-5-9",
  "type": "Exercise",
  "number": "1.5.3.8",
  "title": "",
  "body": " For any positive integer , let and . What are the following sets?            "
},
{
  "id": "exercises-1-5-10",
  "level": "2",
  "url": "s-summation_Notation_and_Generalizations.html#exercises-1-5-10",
  "type": "Exercise",
  "number": "1.5.3.9",
  "title": "",
  "body": " The symbol is used for the product of numbers in the same way that is used for sums. For example, . Evaluate the following:             "
},
{
  "id": "exercises-1-5-11",
  "level": "2",
  "url": "s-summation_Notation_and_Generalizations.html#exercises-1-5-11",
  "type": "Exercise",
  "number": "1.5.3.10",
  "title": "",
  "body": " Evaluate        "
},
{
  "id": "s-the-rule-of-products",
  "level": "1",
  "url": "s-the-rule-of-products.html",
  "type": "Section",
  "number": "2.1",
  "title": "Basic Counting Techniques - The Rule of Products",
  "body": " Basic Counting Techniques - The Rule of Products  What is Combinatorics?  One of the first concepts our parents taught us was the art of counting. We were taught to raise three fingers to indicate that we were three years old. The question of how many is a natural and frequently asked question. Combinatorics is the art of counting. It is the study of techniques that will help us to count the number of objects in a set quickly. Highly sophisticated results can be obtained with this simple concept. The following examples will illustrate that many questions concerned with counting involve the same process.  How many lunches can you have? A snack bar serves five different sandwiches and three different beverages. How many different lunches can a person order? One way of determining the number of possible lunches is by listing or enumerating all the possibilities. One systematic way of doing this is by means of a tree, as in the following figure.   Tree diagram to enumerate the number of possible lunches.   Tree diagram to enumerate the number of possible lunches. Starting at a node labeled Start, there are five branches, one for each possible sandwich that can be ordered. For each sandwich node there are three branches emanating from it, one for each possible beverage. The result is fifteen end nodes, one for each possible lunch.    Every path that begins at the position labeled START and goes to the right can be interpreted as a choice of one of the five sandwiches followed by a choice of one of the three beverages. Note that considerable work is required to arrive at the number fifteen this way; but we also get more than just a number. The result is a complete list of all possible lunches. If we need to answer a question that starts with How many . . . , enumeration would be done only as a last resort. In a later chapter we will examine more enumeration techniques.  An alternative method of solution for this example is to make the simple observation that there are five different choices for sandwiches and three different choices for beverages, so there are different lunches that can be ordered.    Counting elements in a cartesian product  Let and . From Chapter 1 we know how to list the elements in . Since the first entry of each pair can be any one of the five elements , and , and since the second can be any one of the three numbers 1, 2, and 3, it is quite clear there are different elements in .   A True-False Questionnaire A person is to complete a true-false questionnaire consisting of ten questions. How many different ways are there to answer the questionnaire? Since each question can be answered in either of two ways (true or false), and there are ten questions, there are different ways of answering the questionnaire. The reader is encouraged to visualize the tree diagram of this example, but not to draw it!   We formalize the procedures developed in the previous examples with the following rule and its extension.    The Rule Of Products   The Rule Of Products  Rule Of Products, The  Products Rule of  If two operations must be performed, and if the first operation can always be performed different ways and the second operation can always be performed different ways, then there are different ways that the two operations can be performed.   Note: It is important that does not depend on the option that is chosen in the first operation. Another way of saying this is that is independent of the first operation. If is dependent on the first operation, then the rule of products does not apply.  Reduced Lunch Possibilities Assume in , coffee is not served with a beef or chicken sandwiches. Then by inspection of we see that there are only thirteen different choices for lunch. The rule of products does not apply, since the choice of beverage depends on one's choice of a sandwich.   The rule of products can be extended to include sequences of more than two operations.   Extended Rule Of Products  Extended Rule Of Products, The  Products Extended Rule of  If operations must be performed, and the number of options for each operation is , respectively, with each independent of previous choices, then the operations can be performed different ways.   A Multiple Choice Questionnaire A questionnaire contains four questions that have two possible answers and three questions with five possible answers. Since the answer to each question is independent of the answers to the other questions, the extended rule of products applies and there are different ways to answer the questionnaire.   In Chapter 1 we introduced the power set of a set , , which is the set of all subsets of . Can we predict how many elements are in for a given finite set ? The answer is yes, and in fact if , then . The ease with which we can prove this fact demonstrates the power and usefulness of the rule of products. Do not underestimate the usefulness of simple ideas.  Power Set Cardinality Theorem Power Set Cardinality Theorem  If is a finite set, then .  Proof: Consider how we might determine any , where . For each element there are two choices, either or . Since there are elements of we have, by the rule of products, different subsets of . Therefore, .     In horse racing, to bet the daily double is to select the winners of the first two races of the day. You win only if both selections are correct. In terms of the number of horses that are entered in the first two races, how many different daily double bets could be made?  If there are horses in race 1 and horses in race 2 then there are possible daily doubles.    Professor Shortcut records his grades using only his students' first and last initials. What is the smallest class size that will definitely force Prof. S. to use a different system? There are possible first initial\/last initial pairs. If there are at least students then surely, there will be at least two students with the same initials. Of course, a realistic number is much lower since many initial pairs are very rare and some are quite common.   A certain shirt comes in four sizes and six colors. One also has the choice of a dragon, an alligator, or no emblem on the pocket. How many different shirts could you order?    A builder of modular homes would like to impress his potential customers with the variety of styles of his houses. For each house there are blueprints for three different living rooms, four different bedroom configurations, and two different garage styles. In addition, the outside can be finished in cedar shingles or brick. How many different houses can be designed from these plans? There are different designs.   The Pi Mu Epsilon mathematics honorary society of Outstanding University wishes to have a picture taken of its six officers. There will be two rows of three people. How many different way can the six officers be arranged?      An automobile dealer has several options available for each of three different packages of a particular model car: a choice of two styles of seats in three different colors, a choice of four different radios, and five different exteriors. How many choices of automobile does a customer have? There are different choices for a customer.   A clothing manufacturer has put out a mix-and-match collection consisting of two blouses, two pairs of pants, a skirt, and a blazer. How many outfits can you make? Did you consider that the blazer is optional? How many outfits can you make if the manufacturer adds a sweater to the collection?  If we always include the blazer in the outfit we would have 6 outfits. If we consider the blazer optional then there would be 12 outfits. When we add a sweater we have the same type of choice. Considering the sweater optional produces 24 outfits.   As a freshman, suppose you had to take two of four lab science courses, one of two literature courses, two of three math courses, and one of seven physical education courses. Disregarding possible time conflicts, how many different schedules do you have to choose from? The only tricky thing about this problem is that in choosing the two lab science courses, there are just six possible choices. This is because the order in which the courses are selected isn't indicated as being significant. For example, if chemistry and biology are among the courses, choosing chamistry and then biology is the same as choosing biology and then chemistry, so although it may appear that there are ways of picking lab science courses, there are only half that many. The number of ways to select other courses is more obvous. For the math courses, there are three choices because we need only decide which course to not take. So the number of ways to pick courses (not taking into account times) is .   (a) Suppose each single character stored in a computer uses eight bits. Then each character is represented by a different sequence of eight 0's and 1's called a bit pattern. How many different bit patterns are there? (That is, how many different characters could be represented?)  (b) How many bit patterns are palindromes (the same backwards as forwards)?  (c) How many different bit patterns have an even number of 1's?    . Here we are concerned only with the first four bits, since the last four are a mirror image of the first four.   , you have no choice in the last bit.     Automobile license plates in Massachusetts usually consist of three digits followed by three letters. The first digit is never zero. How many different plates of this type could be made? The number of possible license plates is is .    Let . Determine the number of different subsets of .   Let . Determine the number of proper subsets of .       In the second part we can arrive at the answer by counting all subsets and subtracting one since one of the sets (the whole set) is an improper subsets.   How many integers from 100 to 999 can be written in base ten without using the digit 7? The hundreths digit can be any of 8 different values while we have 9 choices for the other two digits, so the number of possibilities is    Consider three persons, A, B, and C, who are to be seated in a row of three chairs. Suppose A and B are identical twins. How many seating arrangements of these persons can there be   If you are a total stranger?  If you are A and B's mother?   This problem is designed to show you that different people can have different correct answers to the same problem.       How many ways can a student do a ten-question true-false exam if he or she can choose not to answer any number of questions?  Each question can be answered three ways: true, false or blank. There are ways to answer the whole exam.   Suppose you have a choice of fish, lamb, or beef for a main course, a choice of peas or carrots for a vegetable, and a choice of pie, cake, or ice cream for dessert. If you must order one item from each category, how many different dinners are possible?   Suppose you have a choice of vanilla, chocolate, maple walnut or strawberry for ice cream, a choice of peanuts or walnuts for chopped nuts, and a choice of hot fudge or marshmallow for topping. You don't have to order nuts. How many different sundaes are possible? There are ways to order a sundae. The factor of three accounts for the possible choices for nuts, one of which is to not order them.   A questionnaire contains six questions each having yes-no answers. For each yes response, there is a follow-up question with four possible responses.   Draw a tree diagram that illustrates how many ways a single question in the questionnaire can be answered.  How many ways can the questionnaire be answered?     Solution to 17(a)   solution to exercise 17a of section 2.1. From a start node, there are two branches. The first branch, labeled yes, has four branches coming from it, one for each of the possible follow-up responses. The second branch from start is an end branch labeled no.      See     Six people are invited to a dinner party. How many ways are there of seating them at a round table? If the six people consist of three who identify as male and three who identify as female, how many ways are there of seating them if each male must be surrounded by two females? Assume we are only concerned with the relative positions around the table. So if we rotate everyone we wouldn't consider this a change in the seating. With no restrictions on seating, we can select any person in the party and seat them at the table. Then to their left we have five choices for seating someone and next to the left there are 4 choices, etc. The number of choices in all is then . With the alternate male\/female scheme, if we seat any male, there are three choices for the female to the left, two choices for the male who is further left, two choices for the female to the right and the other two individuals have their seats determined at this point. So the number of possibilities is .   How many ways can you separate a set with elements into two nonempty subsets if the order of the subsets is immaterial? What if the order of the subsets is important? and   A gardener has three flowering shrubs and four nonflowering shrubs, where all shrubs are distinguishable from one another. He must plant these shrubs in a row using an alternating pattern, that is, a shrub must be of a different type from that on either side. How many ways can he plant these shrubs? If he has to plant these shrubs in a circle using the same pattern, how many ways can he plant this circle? Note that one nonflowering shrub will be left out at the end. If you arrange the shrubs in a row, the nonflowering shrubs must be in positions 1, 3, 5 and 7. We assume that reversing the order would be considered different. So the number of orderings of the nonflowering shubs would be . The flowering shrubs in positions 22, 4 and 6 can be arranged ways and so the number of possible row arrangements is . For the circular arrangement, we can decide which nonflowering shrub to not include in the arrangement four ways. Once we have done this, we have the same situation as the dinner party above where we specified male\/female alternation. So there are 12 ways to arrange the shrubs. Therefore we have possible arrangements of the shrubs in a circle. Let's put the remaining one in the center!    "
},
{
  "id": "lunch-possibilies1",
  "level": "2",
  "url": "s-the-rule-of-products.html#lunch-possibilies1",
  "type": "Example",
  "number": "2.1.1",
  "title": "How many lunches can you have?",
  "body": "How many lunches can you have? A snack bar serves five different sandwiches and three different beverages. How many different lunches can a person order? One way of determining the number of possible lunches is by listing or enumerating all the possibilities. One systematic way of doing this is by means of a tree, as in the following figure.   Tree diagram to enumerate the number of possible lunches.   Tree diagram to enumerate the number of possible lunches. Starting at a node labeled Start, there are five branches, one for each possible sandwich that can be ordered. For each sandwich node there are three branches emanating from it, one for each possible beverage. The result is fifteen end nodes, one for each possible lunch.    Every path that begins at the position labeled START and goes to the right can be interpreted as a choice of one of the five sandwiches followed by a choice of one of the three beverages. Note that considerable work is required to arrive at the number fifteen this way; but we also get more than just a number. The result is a complete list of all possible lunches. If we need to answer a question that starts with How many . . . , enumeration would be done only as a last resort. In a later chapter we will examine more enumeration techniques.  An alternative method of solution for this example is to make the simple observation that there are five different choices for sandwiches and three different choices for beverages, so there are different lunches that can be ordered.  "
},
{
  "id": "cartesian-cardinality",
  "level": "2",
  "url": "s-the-rule-of-products.html#cartesian-cardinality",
  "type": "Example",
  "number": "2.1.3",
  "title": "Counting elements in a cartesian product.",
  "body": " Counting elements in a cartesian product  Let and . From Chapter 1 we know how to list the elements in . Since the first entry of each pair can be any one of the five elements , and , and since the second can be any one of the three numbers 1, 2, and 3, it is quite clear there are different elements in .  "
},
{
  "id": "questionnaire",
  "level": "2",
  "url": "s-the-rule-of-products.html#questionnaire",
  "type": "Example",
  "number": "2.1.4",
  "title": "A True-False Questionnaire.",
  "body": "A True-False Questionnaire A person is to complete a true-false questionnaire consisting of ten questions. How many different ways are there to answer the questionnaire? Since each question can be answered in either of two ways (true or false), and there are ten questions, there are different ways of answering the questionnaire. The reader is encouraged to visualize the tree diagram of this example, but not to draw it!  "
},
{
  "id": "prop-rule-of-products",
  "level": "2",
  "url": "s-the-rule-of-products.html#prop-rule-of-products",
  "type": "Theorem",
  "number": "2.1.5",
  "title": "The Rule Of Products.",
  "body": " The Rule Of Products  Rule Of Products, The  Products Rule of  If two operations must be performed, and if the first operation can always be performed different ways and the second operation can always be performed different ways, then there are different ways that the two operations can be performed.  "
},
{
  "id": "lunch-possibilites2",
  "level": "2",
  "url": "s-the-rule-of-products.html#lunch-possibilites2",
  "type": "Example",
  "number": "2.1.6",
  "title": "Reduced Lunch Possibilities.",
  "body": "Reduced Lunch Possibilities Assume in , coffee is not served with a beef or chicken sandwiches. Then by inspection of we see that there are only thirteen different choices for lunch. The rule of products does not apply, since the choice of beverage depends on one's choice of a sandwich.  "
},
{
  "id": "prop-extended-rule-of-products",
  "level": "2",
  "url": "s-the-rule-of-products.html#prop-extended-rule-of-products",
  "type": "Theorem",
  "number": "2.1.7",
  "title": "Extended Rule Of Products.",
  "body": " Extended Rule Of Products  Extended Rule Of Products, The  Products Extended Rule of  If operations must be performed, and the number of options for each operation is , respectively, with each independent of previous choices, then the operations can be performed different ways.  "
},
{
  "id": "another_questionnaire",
  "level": "2",
  "url": "s-the-rule-of-products.html#another_questionnaire",
  "type": "Example",
  "number": "2.1.8",
  "title": "A Multiple Choice Questionnaire.",
  "body": "A Multiple Choice Questionnaire A questionnaire contains four questions that have two possible answers and three questions with five possible answers. Since the answer to each question is independent of the answers to the other questions, the extended rule of products applies and there are different ways to answer the questionnaire.  "
},
{
  "id": "power-set-cardinality-theorem",
  "level": "2",
  "url": "s-the-rule-of-products.html#power-set-cardinality-theorem",
  "type": "Theorem",
  "number": "2.1.9",
  "title": "Power Set Cardinality Theorem.",
  "body": "Power Set Cardinality Theorem Power Set Cardinality Theorem  If is a finite set, then .  Proof: Consider how we might determine any , where . For each element there are two choices, either or . Since there are elements of we have, by the rule of products, different subsets of . Therefore, .  "
},
{
  "id": "EXERCISES-FOR-SECTION-2-1-1",
  "level": "2",
  "url": "s-the-rule-of-products.html#EXERCISES-FOR-SECTION-2-1-1",
  "type": "Exercise",
  "number": "2.1.3.1",
  "title": "",
  "body": "In horse racing, to bet the daily double is to select the winners of the first two races of the day. You win only if both selections are correct. In terms of the number of horses that are entered in the first two races, how many different daily double bets could be made?  If there are horses in race 1 and horses in race 2 then there are possible daily doubles.  "
},
{
  "id": "exercise-shortcut",
  "level": "2",
  "url": "s-the-rule-of-products.html#exercise-shortcut",
  "type": "Exercise",
  "number": "2.1.3.2",
  "title": "",
  "body": " Professor Shortcut records his grades using only his students' first and last initials. What is the smallest class size that will definitely force Prof. S. to use a different system? There are possible first initial\/last initial pairs. If there are at least students then surely, there will be at least two students with the same initials. Of course, a realistic number is much lower since many initial pairs are very rare and some are quite common.  "
},
{
  "id": "EXERCISES-FOR-SECTION-2-1-3",
  "level": "2",
  "url": "s-the-rule-of-products.html#EXERCISES-FOR-SECTION-2-1-3",
  "type": "Exercise",
  "number": "2.1.3.3",
  "title": "",
  "body": "A certain shirt comes in four sizes and six colors. One also has the choice of a dragon, an alligator, or no emblem on the pocket. How many different shirts could you order?   "
},
{
  "id": "EXERCISES-FOR-SECTION-2-1-4",
  "level": "2",
  "url": "s-the-rule-of-products.html#EXERCISES-FOR-SECTION-2-1-4",
  "type": "Exercise",
  "number": "2.1.3.4",
  "title": "",
  "body": "A builder of modular homes would like to impress his potential customers with the variety of styles of his houses. For each house there are blueprints for three different living rooms, four different bedroom configurations, and two different garage styles. In addition, the outside can be finished in cedar shingles or brick. How many different houses can be designed from these plans? There are different designs.  "
},
{
  "id": "EXERCISES-FOR-SECTION-2-1-5",
  "level": "2",
  "url": "s-the-rule-of-products.html#EXERCISES-FOR-SECTION-2-1-5",
  "type": "Exercise",
  "number": "2.1.3.5",
  "title": "",
  "body": "The Pi Mu Epsilon mathematics honorary society of Outstanding University wishes to have a picture taken of its six officers. There will be two rows of three people. How many different way can the six officers be arranged?    "
},
{
  "id": "EXERCISES-FOR-SECTION-2-1-6",
  "level": "2",
  "url": "s-the-rule-of-products.html#EXERCISES-FOR-SECTION-2-1-6",
  "type": "Exercise",
  "number": "2.1.3.6",
  "title": "",
  "body": " An automobile dealer has several options available for each of three different packages of a particular model car: a choice of two styles of seats in three different colors, a choice of four different radios, and five different exteriors. How many choices of automobile does a customer have? There are different choices for a customer.  "
},
{
  "id": "EXERCISES-FOR-SECTION-2-1-7",
  "level": "2",
  "url": "s-the-rule-of-products.html#EXERCISES-FOR-SECTION-2-1-7",
  "type": "Exercise",
  "number": "2.1.3.7",
  "title": "",
  "body": "A clothing manufacturer has put out a mix-and-match collection consisting of two blouses, two pairs of pants, a skirt, and a blazer. How many outfits can you make? Did you consider that the blazer is optional? How many outfits can you make if the manufacturer adds a sweater to the collection?  If we always include the blazer in the outfit we would have 6 outfits. If we consider the blazer optional then there would be 12 outfits. When we add a sweater we have the same type of choice. Considering the sweater optional produces 24 outfits.  "
},
{
  "id": "EXERCISES-FOR-SECTION-2-1-8",
  "level": "2",
  "url": "s-the-rule-of-products.html#EXERCISES-FOR-SECTION-2-1-8",
  "type": "Exercise",
  "number": "2.1.3.8",
  "title": "",
  "body": "As a freshman, suppose you had to take two of four lab science courses, one of two literature courses, two of three math courses, and one of seven physical education courses. Disregarding possible time conflicts, how many different schedules do you have to choose from? The only tricky thing about this problem is that in choosing the two lab science courses, there are just six possible choices. This is because the order in which the courses are selected isn't indicated as being significant. For example, if chemistry and biology are among the courses, choosing chamistry and then biology is the same as choosing biology and then chemistry, so although it may appear that there are ways of picking lab science courses, there are only half that many. The number of ways to select other courses is more obvous. For the math courses, there are three choices because we need only decide which course to not take. So the number of ways to pick courses (not taking into account times) is .  "
},
{
  "id": "EXERCISES-FOR-SECTION-2-1-9",
  "level": "2",
  "url": "s-the-rule-of-products.html#EXERCISES-FOR-SECTION-2-1-9",
  "type": "Exercise",
  "number": "2.1.3.9",
  "title": "",
  "body": "(a) Suppose each single character stored in a computer uses eight bits. Then each character is represented by a different sequence of eight 0's and 1's called a bit pattern. How many different bit patterns are there? (That is, how many different characters could be represented?)  (b) How many bit patterns are palindromes (the same backwards as forwards)?  (c) How many different bit patterns have an even number of 1's?    . Here we are concerned only with the first four bits, since the last four are a mirror image of the first four.   , you have no choice in the last bit.    "
},
{
  "id": "EXERCISES-FOR-SECTION-2-1-10",
  "level": "2",
  "url": "s-the-rule-of-products.html#EXERCISES-FOR-SECTION-2-1-10",
  "type": "Exercise",
  "number": "2.1.3.10",
  "title": "",
  "body": "Automobile license plates in Massachusetts usually consist of three digits followed by three letters. The first digit is never zero. How many different plates of this type could be made? The number of possible license plates is is .  "
},
{
  "id": "EXERCISES-FOR-SECTION-2-1-11",
  "level": "2",
  "url": "s-the-rule-of-products.html#EXERCISES-FOR-SECTION-2-1-11",
  "type": "Exercise",
  "number": "2.1.3.11",
  "title": "",
  "body": " Let . Determine the number of different subsets of .   Let . Determine the number of proper subsets of .       In the second part we can arrive at the answer by counting all subsets and subtracting one since one of the sets (the whole set) is an improper subsets.  "
},
{
  "id": "EXERCISES-FOR-SECTION-2-1-12",
  "level": "2",
  "url": "s-the-rule-of-products.html#EXERCISES-FOR-SECTION-2-1-12",
  "type": "Exercise",
  "number": "2.1.3.12",
  "title": "",
  "body": "How many integers from 100 to 999 can be written in base ten without using the digit 7? The hundreths digit can be any of 8 different values while we have 9 choices for the other two digits, so the number of possibilities is   "
},
{
  "id": "EXERCISES-FOR-SECTION-2-1-13",
  "level": "2",
  "url": "s-the-rule-of-products.html#EXERCISES-FOR-SECTION-2-1-13",
  "type": "Exercise",
  "number": "2.1.3.13",
  "title": "",
  "body": "Consider three persons, A, B, and C, who are to be seated in a row of three chairs. Suppose A and B are identical twins. How many seating arrangements of these persons can there be   If you are a total stranger?  If you are A and B's mother?   This problem is designed to show you that different people can have different correct answers to the same problem.     "
},
{
  "id": "EXERCISES-FOR-SECTION-2-1-14",
  "level": "2",
  "url": "s-the-rule-of-products.html#EXERCISES-FOR-SECTION-2-1-14",
  "type": "Exercise",
  "number": "2.1.3.14",
  "title": "",
  "body": " How many ways can a student do a ten-question true-false exam if he or she can choose not to answer any number of questions?  Each question can be answered three ways: true, false or blank. There are ways to answer the whole exam.  "
},
{
  "id": "EXERCISES-FOR-SECTION-2-1-15",
  "level": "2",
  "url": "s-the-rule-of-products.html#EXERCISES-FOR-SECTION-2-1-15",
  "type": "Exercise",
  "number": "2.1.3.15",
  "title": "",
  "body": "Suppose you have a choice of fish, lamb, or beef for a main course, a choice of peas or carrots for a vegetable, and a choice of pie, cake, or ice cream for dessert. If you must order one item from each category, how many different dinners are possible?  "
},
{
  "id": "EXERCISES-FOR-SECTION-2-1-16",
  "level": "2",
  "url": "s-the-rule-of-products.html#EXERCISES-FOR-SECTION-2-1-16",
  "type": "Exercise",
  "number": "2.1.3.16",
  "title": "",
  "body": "Suppose you have a choice of vanilla, chocolate, maple walnut or strawberry for ice cream, a choice of peanuts or walnuts for chopped nuts, and a choice of hot fudge or marshmallow for topping. You don't have to order nuts. How many different sundaes are possible? There are ways to order a sundae. The factor of three accounts for the possible choices for nuts, one of which is to not order them.  "
},
{
  "id": "EXERCISES-FOR-SECTION-2-1-17",
  "level": "2",
  "url": "s-the-rule-of-products.html#EXERCISES-FOR-SECTION-2-1-17",
  "type": "Exercise",
  "number": "2.1.3.17",
  "title": "",
  "body": "A questionnaire contains six questions each having yes-no answers. For each yes response, there is a follow-up question with four possible responses.   Draw a tree diagram that illustrates how many ways a single question in the questionnaire can be answered.  How many ways can the questionnaire be answered?     Solution to 17(a)   solution to exercise 17a of section 2.1. From a start node, there are two branches. The first branch, labeled yes, has four branches coming from it, one for each of the possible follow-up responses. The second branch from start is an end branch labeled no.      See    "
},
{
  "id": "EXERCISES-FOR-SECTION-2-1-18",
  "level": "2",
  "url": "s-the-rule-of-products.html#EXERCISES-FOR-SECTION-2-1-18",
  "type": "Exercise",
  "number": "2.1.3.18",
  "title": "",
  "body": "Six people are invited to a dinner party. How many ways are there of seating them at a round table? If the six people consist of three who identify as male and three who identify as female, how many ways are there of seating them if each male must be surrounded by two females? Assume we are only concerned with the relative positions around the table. So if we rotate everyone we wouldn't consider this a change in the seating. With no restrictions on seating, we can select any person in the party and seat them at the table. Then to their left we have five choices for seating someone and next to the left there are 4 choices, etc. The number of choices in all is then . With the alternate male\/female scheme, if we seat any male, there are three choices for the female to the left, two choices for the male who is further left, two choices for the female to the right and the other two individuals have their seats determined at this point. So the number of possibilities is .  "
},
{
  "id": "EXERCISES-FOR-SECTION-2-1-19",
  "level": "2",
  "url": "s-the-rule-of-products.html#EXERCISES-FOR-SECTION-2-1-19",
  "type": "Exercise",
  "number": "2.1.3.19",
  "title": "",
  "body": "How many ways can you separate a set with elements into two nonempty subsets if the order of the subsets is immaterial? What if the order of the subsets is important? and  "
},
{
  "id": "EXERCISES-FOR-SECTION-2-1-20",
  "level": "2",
  "url": "s-the-rule-of-products.html#EXERCISES-FOR-SECTION-2-1-20",
  "type": "Exercise",
  "number": "2.1.3.20",
  "title": "",
  "body": "A gardener has three flowering shrubs and four nonflowering shrubs, where all shrubs are distinguishable from one another. He must plant these shrubs in a row using an alternating pattern, that is, a shrub must be of a different type from that on either side. How many ways can he plant these shrubs? If he has to plant these shrubs in a circle using the same pattern, how many ways can he plant this circle? Note that one nonflowering shrub will be left out at the end. If you arrange the shrubs in a row, the nonflowering shrubs must be in positions 1, 3, 5 and 7. We assume that reversing the order would be considered different. So the number of orderings of the nonflowering shubs would be . The flowering shrubs in positions 22, 4 and 6 can be arranged ways and so the number of possible row arrangements is . For the circular arrangement, we can decide which nonflowering shrub to not include in the arrangement four ways. Once we have done this, we have the same situation as the dinner party above where we specified male\/female alternation. So there are 12 ways to arrange the shrubs. Therefore we have possible arrangements of the shrubs in a circle. Let's put the remaining one in the center!  "
},
{
  "id": "s-permutations",
  "level": "1",
  "url": "s-permutations.html",
  "type": "Section",
  "number": "2.2",
  "title": "Permutations",
  "body": " Permutations   Ordering Things  A number of applications of the rule of products are of a specific type, and because of their frequent appearance they are given their own designation, permutations. Consider the following examples.  Ordering the elements of a set How many different ways can we order the three different elements of the set ? Since we have three choices for position one, two choices for position two, and one choice for the third position, we have, by the rule of products, different ways of ordering the three letters. We illustrate through a tree diagram.   A tree to enumerate permutations of a three element set.   A tree to enumerate permutations of a three element set.    Each of the six orderings is called a permutation of the set .   Preference Voting In an election that uses preference voting, voters rank candidates etc. in order of their preference instead of just selecting their first preference. How many different ways can a voter cast a ballot with five candidates? Each way to vote is a permutation of the five candidates. There are different ways to vote.   In each of the above examples of the rule of products we observe that:   We are asked to order or arrange elements from a single set.  Each element is listed exactly once in each list (permutation). So if there are choices for position one in a list, there are choices for position two, choices for position three, etc.   Some orderings of a baseball team The alphabetical ordering of the players of a baseball team is one permutation of the set of players. Other orderings of the players' names might be done by batting average, age, or height. The information that determines the ordering is called the key. We would expect that each key would give a different permutation of the names. If there are twenty-five players on the team, there are different permutations of the players.  This number of permutations is huge. In fact it is 15511210043330985984000000, but writing it like this isn't all that instructive, while leaving it as a product as we originally had makes it easier to see where the number comes from. We just need to find a more compact way of writing these products.   We now develop notation that will be useful for permutation problems.  Factorial  Factorial factorial, the product of the first positive integers  If is a positive integer then factorial is the product of the first positive integers and is denoted . Additionally, we define zero factorial, , to be 1.   The first few factorials are .  Note that is 4 times , or 24, and is 5 times , or 120. In addition, note that as grows in size, grows extremely quickly. For example, . If the answer to a problem happens to be , as in the previous example, you would never be expected to write that number out completely. However, a problem with an answer of can be reduced to , or 600.  If , there are ways of permuting all elements of . We next consider the more general situation where we would like to permute elements out of a set of objects, where .  Choosing Club Officers A club of twenty-five members will hold an election for president, secretary, and treasurer in that order. Assume a person can hold only one position. How many ways are there of choosing these three officers? By the rule of products there are ways of making a selection.   Permutation Permutation An ordered arrangement of elements selected from a set of elements, , where no two elements of the arrangement are the same, is called a permutation of objects taken at a time. The total number of such permutations is denoted by .    Permutation Counting Formula Permutation Counting Formula The number of possible permutations of elements taken from a set of elements is .    Case I: If we have .  Case II: If ,then we have positions to fill using elements and   Position 1 can be filled by any one of elements  Position 2 can be filled by any one of elements     Position k can be filled by any one of elements   Hence, by the rule of products, .    It is important to note that the derivation of the permutation formula given above was done solely through the rule of products. This serves to reiterate our introductory remarks in this section that permutation problems are really rule-of-products problems. We close this section with several examples.  Another example of choosing officers  A club has eight members eligible to serve as president, vice-president, and treasurer. How many ways are there of choosing these officers?  Solution 1: Using the rule of products. There are eight possible choices for the presidency, seven for the vice-presidency, and six for the office of treasurer. By the rule of products there are ways of choosing these officers.  Solution 2: Using the permutation formula. We want the total number of permutations of eight objects taken three at a time:      Preference Voting, revisited  To count the number of ways to vote for five candidates in a preference system, we can use the permutation formula. We want the number of permutations of five candidates taken five at a time: .   Ordering of digits under different conditions  Consider only the digits 1, 2, 3, 4, and 5.   How many three-digit numbers can be formed if no repetition of digits can occur?  How many three-digit numbers can be formed if repetition of digits is allowed?  How many three-digit numbers can be formed if only non-consecutive repetition of digits are allowed?   Solutions to (a): Solution 1: Using the rule of products. We have any one of five choices for digit one, any one of four choices for digit two, and three choices for digit three. Hence, different three-digit numbers can be formed.  Solution 2; Using the permutation formula. We want the total number of permutations of five digits taken three at a time: .  Solution to (b): The definition of permutation indicates ...no two elements in each list are the same. Hence the permutation formula cannot be used. However, the rule of products still applies. We have any one of five choices for the first digit, five choices for the second, and five for the third. So there are possible different three-digit numbers if repetition is allowed.  Solution to (c): Again, the rule of products applies here. We have any one of five choices for the first digit, but then for the next two digits we have four choices since we are not allowed to repeat the previous digit So there are possible different three-digit numbers if only non-consecutive repetitions are allowed.      If a raffle has three different prizes and there are 1,000 raffle tickets sold, how many different ways can the prizes be distributed?     How many three-digit numbers can be formed from the digits 1, 2, 3 if no repetition of digits is allowed? List the three-digit numbers.  How many two-digit numbers can be formed if no repetition of digits is allowed? List them.  How many two-digit numbers can be obtained if repetition is allowed?     . 123, 213, 132, 312, 231, 321.  . 12, 21, 12, 31, 23, 32. Notice that this list is the previous one with the last digit removed from each number.     How many eight-letter words can be formed from the 26 letters in the alphabet? Even without concerning ourselves about whether the words make sense, there are two interpretations of this problem. Answer both.  With repetition:  Without repetition:    Let be a set with . Determine           The state finals of a high school track meet involves fifteen schools. How many ways can these schools be listed in the program?   Consider the three-digit numbers that can be formed from the digits 1, 2, 3, 4, and 5 with no repetition of digits allowed.   How many of these are even numbers?  How many are greater than 250?     To be even, the units digit would need to be either 2 or 4. We can select the units digit two ways. Then to select the two other digits, we put two of the remaining 4 digits in order, so there are ways to do that. Therefore, there are even numbers.  To get the correct count here we anticipate the addition law that we will see in the next section. For the number to be greater than 250, the hundredths digit can be 3, 4 or 5; or it could be where is 1, 3 or 4. There are numbers that start with 3, 4, or 5. To that we add the three other numbers that start with 25 to give us      All 15 players on the Tall U. basketball team are capable of playing any position.  How many ways can the coach at Tall U. fill the five starting positions in a game?  What is the answer if the center must be one of two players?       With circular arrangements like the ones described in this problem it's common to assume that two arrangements are considered that same if one can be rotated to equal the other. However, since you can't rotate planted shrubs so easily, you might want to also count the possibilities assuming there are five designated positions: front, side right, side left, back right, back left.  How many ways can a gardener plant five different species of shrubs in a circle?  What is the answer if two of the shrubs are the same?  What is the answer if all the shrubs are identical?     if rotations are the same, with no rotations.  If you place one of the three distinct shrubs on the circle, there are ways to place the other two of the distinct shrubs. That is the number of possibilities if we allow rotations since the other two identical shrubs will be filled into the remaining positions. If rotations are not allowed, we place the three distinct shrubs ways and the other two shrubs fill in the circle, so there are 60 possible arrangements.  There is only one arrangement with either set of rules.    The president of the Math and Computer Club would like to arrange a meeting with six attendees, the president included. There will be three computer science majors and three math majors at the meeting. How many ways can the six people be seated at a circular table if the president does not want people with the same majors to sit next to one other? If the president is sitting at 12 o'clock on the table, then the two members from her major need to sit at 4 and 8 o'clock. There are two ways to arrange them. The other majors sit at 2, 6, and 10 o'clock and can be placed ways, so the final answer is   Six people apply for three identical jobs and all are qualified for the positions. Two will work in New York and the other one will work in San Diego. How many ways can the positions be filled?  If we first fill the two positions in New York, we can select a first person and then a second person ways, but since the jobs are identical, each pair of persons wold be counted twice, so there are 15 ways to fill the NY positions. Then we can fill the SD position in one of 4 ways. Therefore there are ways to fill the jobs. Another way to get 60 is to think of filling three different positions, ways to do that. We then divide by 2 because the two NY positions are really identical.    Let . Determine the cardinality of    What is the answer to the previous part if  If , determine the number of -tuples in , , where each coordinate is different from the other coordinates.      Case 1: . Since the coordinates must be different, this case is impossible.  Case 2: .     "
},
{
  "id": "ordering_a_set",
  "level": "2",
  "url": "s-permutations.html#ordering_a_set",
  "type": "Example",
  "number": "2.2.1",
  "title": "Ordering the elements of a set.",
  "body": "Ordering the elements of a set How many different ways can we order the three different elements of the set ? Since we have three choices for position one, two choices for position two, and one choice for the third position, we have, by the rule of products, different ways of ordering the three letters. We illustrate through a tree diagram.   A tree to enumerate permutations of a three element set.   A tree to enumerate permutations of a three element set.    Each of the six orderings is called a permutation of the set .  "
},
{
  "id": "ordering_a_schedule",
  "level": "2",
  "url": "s-permutations.html#ordering_a_schedule",
  "type": "Example",
  "number": "2.2.3",
  "title": "Preference Voting.",
  "body": "Preference Voting In an election that uses preference voting, voters rank candidates etc. in order of their preference instead of just selecting their first preference. How many different ways can a voter cast a ballot with five candidates? Each way to vote is a permutation of the five candidates. There are different ways to vote.  "
},
{
  "id": "some_orderings_of_a_baseball_team",
  "level": "2",
  "url": "s-permutations.html#some_orderings_of_a_baseball_team",
  "type": "Example",
  "number": "2.2.4",
  "title": "Some orderings of a baseball team.",
  "body": "Some orderings of a baseball team The alphabetical ordering of the players of a baseball team is one permutation of the set of players. Other orderings of the players' names might be done by batting average, age, or height. The information that determines the ordering is called the key. We would expect that each key would give a different permutation of the names. If there are twenty-five players on the team, there are different permutations of the players.  This number of permutations is huge. In fact it is 15511210043330985984000000, but writing it like this isn't all that instructive, while leaving it as a product as we originally had makes it easier to see where the number comes from. We just need to find a more compact way of writing these products.  "
},
{
  "id": "Definition-Factorial",
  "level": "2",
  "url": "s-permutations.html#Definition-Factorial",
  "type": "Definition",
  "number": "2.2.5",
  "title": "Factorial.",
  "body": "Factorial  Factorial factorial, the product of the first positive integers  If is a positive integer then factorial is the product of the first positive integers and is denoted . Additionally, we define zero factorial, , to be 1.  "
},
{
  "id": "choosing-club-officers",
  "level": "2",
  "url": "s-permutations.html#choosing-club-officers",
  "type": "Example",
  "number": "2.2.6",
  "title": "Choosing Club Officers.",
  "body": "Choosing Club Officers A club of twenty-five members will hold an election for president, secretary, and treasurer in that order. Assume a person can hold only one position. How many ways are there of choosing these three officers? By the rule of products there are ways of making a selection.  "
},
{
  "id": "permutation",
  "level": "2",
  "url": "s-permutations.html#permutation",
  "type": "Definition",
  "number": "2.2.7",
  "title": "Permutation.",
  "body": "Permutation Permutation An ordered arrangement of elements selected from a set of elements, , where no two elements of the arrangement are the same, is called a permutation of objects taken at a time. The total number of such permutations is denoted by .  "
},
{
  "id": "permutations-counting-formula",
  "level": "2",
  "url": "s-permutations.html#permutations-counting-formula",
  "type": "Theorem",
  "number": "2.2.8",
  "title": "Permutation Counting Formula.",
  "body": " Permutation Counting Formula Permutation Counting Formula The number of possible permutations of elements taken from a set of elements is .    Case I: If we have .  Case II: If ,then we have positions to fill using elements and   Position 1 can be filled by any one of elements  Position 2 can be filled by any one of elements     Position k can be filled by any one of elements   Hence, by the rule of products, .   "
},
{
  "id": "more_club_officers",
  "level": "2",
  "url": "s-permutations.html#more_club_officers",
  "type": "Example",
  "number": "2.2.9",
  "title": "Another example of choosing officers.",
  "body": "Another example of choosing officers  A club has eight members eligible to serve as president, vice-president, and treasurer. How many ways are there of choosing these officers?  Solution 1: Using the rule of products. There are eight possible choices for the presidency, seven for the vice-presidency, and six for the office of treasurer. By the rule of products there are ways of choosing these officers.  Solution 2: Using the permutation formula. We want the total number of permutations of eight objects taken three at a time:    "
},
{
  "id": "course-ordering-revisited",
  "level": "2",
  "url": "s-permutations.html#course-ordering-revisited",
  "type": "Example",
  "number": "2.2.10",
  "title": "Preference Voting, revisited.",
  "body": " Preference Voting, revisited  To count the number of ways to vote for five candidates in a preference system, we can use the permutation formula. We want the number of permutations of five candidates taken five at a time: .  "
},
{
  "id": "ordering-digits",
  "level": "2",
  "url": "s-permutations.html#ordering-digits",
  "type": "Example",
  "number": "2.2.11",
  "title": "Ordering of digits under different conditions.",
  "body": "Ordering of digits under different conditions  Consider only the digits 1, 2, 3, 4, and 5.   How many three-digit numbers can be formed if no repetition of digits can occur?  How many three-digit numbers can be formed if repetition of digits is allowed?  How many three-digit numbers can be formed if only non-consecutive repetition of digits are allowed?   Solutions to (a): Solution 1: Using the rule of products. We have any one of five choices for digit one, any one of four choices for digit two, and three choices for digit three. Hence, different three-digit numbers can be formed.  Solution 2; Using the permutation formula. We want the total number of permutations of five digits taken three at a time: .  Solution to (b): The definition of permutation indicates ...no two elements in each list are the same. Hence the permutation formula cannot be used. However, the rule of products still applies. We have any one of five choices for the first digit, five choices for the second, and five for the third. So there are possible different three-digit numbers if repetition is allowed.  Solution to (c): Again, the rule of products applies here. We have any one of five choices for the first digit, but then for the next two digits we have four choices since we are not allowed to repeat the previous digit So there are possible different three-digit numbers if only non-consecutive repetitions are allowed.   "
},
{
  "id": "s-permutations-3-1",
  "level": "2",
  "url": "s-permutations.html#s-permutations-3-1",
  "type": "Exercise",
  "number": "2.2.2.1",
  "title": "",
  "body": "If a raffle has three different prizes and there are 1,000 raffle tickets sold, how many different ways can the prizes be distributed?  "
},
{
  "id": "s-permutations-3-2",
  "level": "2",
  "url": "s-permutations.html#s-permutations-3-2",
  "type": "Exercise",
  "number": "2.2.2.2",
  "title": "",
  "body": "  How many three-digit numbers can be formed from the digits 1, 2, 3 if no repetition of digits is allowed? List the three-digit numbers.  How many two-digit numbers can be formed if no repetition of digits is allowed? List them.  How many two-digit numbers can be obtained if repetition is allowed?     . 123, 213, 132, 312, 231, 321.  . 12, 21, 12, 31, 23, 32. Notice that this list is the previous one with the last digit removed from each number.    "
},
{
  "id": "s-permutations-3-3",
  "level": "2",
  "url": "s-permutations.html#s-permutations-3-3",
  "type": "Exercise",
  "number": "2.2.2.3",
  "title": "",
  "body": "How many eight-letter words can be formed from the 26 letters in the alphabet? Even without concerning ourselves about whether the words make sense, there are two interpretations of this problem. Answer both.  With repetition:  Without repetition:   "
},
{
  "id": "s-permutations-3-4",
  "level": "2",
  "url": "s-permutations.html#s-permutations-3-4",
  "type": "Exercise",
  "number": "2.2.2.4",
  "title": "",
  "body": "Let be a set with . Determine          "
},
{
  "id": "s-permutations-3-5",
  "level": "2",
  "url": "s-permutations.html#s-permutations-3-5",
  "type": "Exercise",
  "number": "2.2.2.5",
  "title": "",
  "body": "The state finals of a high school track meet involves fifteen schools. How many ways can these schools be listed in the program?  "
},
{
  "id": "s-permutations-3-6",
  "level": "2",
  "url": "s-permutations.html#s-permutations-3-6",
  "type": "Exercise",
  "number": "2.2.2.6",
  "title": "",
  "body": "Consider the three-digit numbers that can be formed from the digits 1, 2, 3, 4, and 5 with no repetition of digits allowed.   How many of these are even numbers?  How many are greater than 250?     To be even, the units digit would need to be either 2 or 4. We can select the units digit two ways. Then to select the two other digits, we put two of the remaining 4 digits in order, so there are ways to do that. Therefore, there are even numbers.  To get the correct count here we anticipate the addition law that we will see in the next section. For the number to be greater than 250, the hundredths digit can be 3, 4 or 5; or it could be where is 1, 3 or 4. There are numbers that start with 3, 4, or 5. To that we add the three other numbers that start with 25 to give us    "
},
{
  "id": "s-permutations-3-7",
  "level": "2",
  "url": "s-permutations.html#s-permutations-3-7",
  "type": "Exercise",
  "number": "2.2.2.7",
  "title": "",
  "body": " All 15 players on the Tall U. basketball team are capable of playing any position.  How many ways can the coach at Tall U. fill the five starting positions in a game?  What is the answer if the center must be one of two players?      "
},
{
  "id": "s-permutations-3-8",
  "level": "2",
  "url": "s-permutations.html#s-permutations-3-8",
  "type": "Exercise",
  "number": "2.2.2.8",
  "title": "",
  "body": "With circular arrangements like the ones described in this problem it's common to assume that two arrangements are considered that same if one can be rotated to equal the other. However, since you can't rotate planted shrubs so easily, you might want to also count the possibilities assuming there are five designated positions: front, side right, side left, back right, back left.  How many ways can a gardener plant five different species of shrubs in a circle?  What is the answer if two of the shrubs are the same?  What is the answer if all the shrubs are identical?     if rotations are the same, with no rotations.  If you place one of the three distinct shrubs on the circle, there are ways to place the other two of the distinct shrubs. That is the number of possibilities if we allow rotations since the other two identical shrubs will be filled into the remaining positions. If rotations are not allowed, we place the three distinct shrubs ways and the other two shrubs fill in the circle, so there are 60 possible arrangements.  There is only one arrangement with either set of rules.   "
},
{
  "id": "s-permutations-3-9",
  "level": "2",
  "url": "s-permutations.html#s-permutations-3-9",
  "type": "Exercise",
  "number": "2.2.2.9",
  "title": "",
  "body": "The president of the Math and Computer Club would like to arrange a meeting with six attendees, the president included. There will be three computer science majors and three math majors at the meeting. How many ways can the six people be seated at a circular table if the president does not want people with the same majors to sit next to one other? If the president is sitting at 12 o'clock on the table, then the two members from her major need to sit at 4 and 8 o'clock. There are two ways to arrange them. The other majors sit at 2, 6, and 10 o'clock and can be placed ways, so the final answer is  "
},
{
  "id": "s-permutations-3-10",
  "level": "2",
  "url": "s-permutations.html#s-permutations-3-10",
  "type": "Exercise",
  "number": "2.2.2.10",
  "title": "",
  "body": "Six people apply for three identical jobs and all are qualified for the positions. Two will work in New York and the other one will work in San Diego. How many ways can the positions be filled?  If we first fill the two positions in New York, we can select a first person and then a second person ways, but since the jobs are identical, each pair of persons wold be counted twice, so there are 15 ways to fill the NY positions. Then we can fill the SD position in one of 4 ways. Therefore there are ways to fill the jobs. Another way to get 60 is to think of filling three different positions, ways to do that. We then divide by 2 because the two NY positions are really identical.  "
},
{
  "id": "s-permutations-3-11",
  "level": "2",
  "url": "s-permutations.html#s-permutations-3-11",
  "type": "Exercise",
  "number": "2.2.2.11",
  "title": "",
  "body": " Let . Determine the cardinality of    What is the answer to the previous part if  If , determine the number of -tuples in , , where each coordinate is different from the other coordinates.      Case 1: . Since the coordinates must be different, this case is impossible.  Case 2: .   "
},
{
  "id": "s-partitions-and-law-of-addition",
  "level": "1",
  "url": "s-partitions-and-law-of-addition.html",
  "type": "Section",
  "number": "2.3",
  "title": "Partitions of Sets and the Law of Addition",
  "body": " Partitions of Sets and the Law of Addition  Partitions  One way of counting the number of students in your class would be to count the number in each row and to add these totals. Of course this problem is simple because there are no duplications, no person is sitting in two different rows. The basic counting technique that you used involves an extremely important first step, namely that of partitioning a set. The concept of a partition must be clearly understood before we proceed further.   Partition Partition A partition of set is a set of one or more nonempty subsets of : , such that every element of is in exactly one set. Symbolically,    If then    The subsets in a partition are often referred to as blocks. Note how our definition allows us to partition infinite sets, and to partition a set into an infinite number of subsets. Of course, if is finite the number of subsets can be no larger than .  Some partitions of a four element set  Let . Examples of partitions of are:          How many others are there, do you suppose?  There are 15 different partitions. The most efficient way to count them all is to classify them by the size of blocks. For example, the partition has block sizes 1, 1, and 2.    Some Integer Partitions Two examples of partitions of set of integers are   and   .   The set of subsets is not a partition because the two subsets have a nonempty intersection. A second example of a non-partition is because one of the blocks, when is empty.  One could also think of the concept of partitioning a set as a packaging problem. How can one package a carton of, say, twenty-four cans? We could use: four six-packs, three eight-packs, two twelve-packs, etc. In all cases: (a) the sum of all cans in all packs must be twenty-four, and (b) a can must be in one and only one pack.   Addition Laws  The Basic Law Of Addition: Basic Law Of Addition: If is a finite set, and if is a partition of , then    The basic law of addition can be rephrased as follows: If is a finite set where and where whenever , then   Counting All Students The number of students in a class could be determined by adding the numbers of students who are freshmen, sophomores, juniors, and seniors, and those who belong to none of these categories. However, you probably couldn't add the students by major, since some students may have double majors.  Counting Students in Disjoint Classes The sophomore computer science majors were told they must take one and only one of the following courses that are open only to them: Cryptography, Data Structures, or Javascript. The numbers in each course, respectively, for sophomore CS majors, were 75, 60, 55. How many sophomore CS majors are there? The Law of Addition applies here. There are exactly CS majors since the rosters of the three courses listed above would be a partition of the CS majors.   Counting Students in Non-disjoint Classes It was determined that all junior computer science majors take at least one of the following courses: Algorithms, Logic Design, and Compiler Construction. Assume the number in each course was 75, 60 and 55, respectively for the three courses listed. Further investigation indicated ten juniors took all three courses, twenty-five took Algorithms and Logic Design, twelve took Algorithms and Compiler Construction, and fifteen took Logic Design and Compiler Construction. How many junior C.S. majors are there?  was a simple application of the law of addition, however in this example some students are taking two or more courses, so a simple application of the law of addition would lead to double or triple counting. We rephrase information in the language of sets to describe the situation more explicitly.   = the set of all junior computer science majors   = the set of all junior computer science majors who took Algorithms   = the set of all junior computer science majors who took Logic Design   = the set of all junior computer science majors who took Compiler Construction  Since all junior CS majors must take at least one of the courses, the number we want is:   A Venn diagram is helpful to visualize the problem. In this case the universal set can stand for all students in the university.   Venn Diagram   A three set Venn Diagram to solve the Junior CS student example.     We see that the whole universal set is naturally partitioned into subsets that are labeled by the numbers 1 through 8, and the set is partitioned into subsets labeled 1 through 7. The region labeled 8 represents all students who are not junior CS majors. Note also that students in the subsets labeled 2, 3, and 4 are double counted, and those in the subset labeled 1 are triple counted. To adjust, we must subtract the numbers in regions 2, 3 and 4. This can be done by subtracting the numbers in the intersections of each pair of sets. However, the individuals in region 1 will have been removed three times, just as they had been originally added three times. Therefore, we must finally add their number back in.    The ideas used in this latest example gives rise to a basic counting technique:   Laws of Inclusion-Exclusion  Inclusion-Exclusion, Laws of   Given finite sets , then  The Two Set Inclusion-Exclusion Law:  The Three Set Inclusion-Exclusion Law:     The inclusion-exclusion laws extend to more than three sets, as will be explored in the exercises.  In this section we saw that being able to partition a set into disjoint subsets gives rise to a handy counting technique. Given a set, there are many ways to partition depending on what one would wish to accomplish. One natural partitioning of sets is apparent when one draws a Venn diagram. This particular partitioning of a set will be discussed further in Chapters 4 and 13.    Exercises  List all partitions of the set .     Which of the following collections of subsets of the plane, , are partitions?    The set of all circles in  The set of all circles in centered at the origin together with the set    A student, on an exam paper, defined the term partition the following way: Let be a set. A partition of is any set of nonempty subsets of such that each element of is in one of the subsets. Is this definition correct? Why?  No. By this definition it is possible that an element of might belong to two of the subsets.   Let and be subsets of a set . Draw a Venn diagram of this situation and shade in the subsets , , , and . Use the resulting diagram and the definition of partition to convince yourself that the subset of these four subsets that are nonempty form a partition of .   Show that is a partition of . Describe this partition using only words. The first subset is all the even integers and the second is all the odd integers. These two sets do not intersect and they cover the integers completely.    A group of 30 students were surveyed and it was found that 18 of them took Calculus and 12 took Physics. If all students took at least one course, how many took both Calculus and Physics? Illustrate using a Venn diagram.  What is the answer to the question in part (a) if five students did not take either of the two courses? Illustrate using a Venn diagram.      Let be the set of calculus students and the physics students. Since all 30 students took at least one of the courses,, and we can place a zero outside the circles in the Venn diagram to the left. which implies that is empty.  If five students did not take either of the two courses, then and which implies that .       Venn Diagram for part (a)      Venn Diagram for part (b)       A survey of 90 people, 47 of them played tennis and 42 of them swam. If 17 of them participated in both activities, how many of them participated in neither? Since 17 participated in both activities, 30 of the tennis players only played tennis and 25 of the swimmers only swam. Therefore, of those who were surveyed participated in an activity and so 18 did not.   A survey of 300 people found that 60 owned an iPhone, 75 owned a Blackberry, and 30 owned an Android phone. Furthermore, 40 owned both an iPhone and a Blackberry, 12 owned both an iPhone and an Android phone, and 8 owned a Blackberry and an Android phone. Finally, 3 owned all three phones.   How many people surveyed owned none of the three phones?  How many people owned a Blackberry but not an iPhone?  How many owned a Blackberry but not an Android?     192  35  67     Regarding ,  Use the two set inclusion-exclusion law to derive the three set inclusion-exclusion law. Note: A knowledge of basic set laws is needed for this exercise.  State and derive the inclusion-exclusion law for four sets.  We assume that .   The law for four sets is   Derivation:    To complete your spring schedule, you must add Calculus and Physics. At 9:30, there are three Calculus sections and two Physics sections; while at 11:30, there are two Calculus sections and three Physics sections. How many ways can you complete your schedule if your only open periods are 9:30 and 11:30?  We can partition the possible schedules into two sets, the ones for which you take Calculus at 9:30, and those for which you take Physics at 9:30. This is a true partition because of the conditions of the problem. If you take Calculus at 9:30 you can register three different ways and then choose one of the three 11:30 Physics sections. That's 9 possible choices in all. If you take Physics at 9:30, then there two choices and then two choices for Calculus at 11:30, so 4 choices in all. By the Law of Addition there are ways to register.   The definition of given in Chapter 1 is awkward. If we use the definition to list elements in , we will have duplications such as , and Try to write a more precise definition of the rational numbers so that there is no duplication of elements.  Partition the set of fractions into blocks, where each block contains fractions that are numerically equivalent. Describe how you would determine whether two fractions belong to the same block. Redefine the rational numbers to be this partition. Each rational number is a set of fractions.   "
},
{
  "id": "partition",
  "level": "2",
  "url": "s-partitions-and-law-of-addition.html#partition",
  "type": "Definition",
  "number": "2.3.1",
  "title": "Partition.",
  "body": " Partition Partition A partition of set is a set of one or more nonempty subsets of : , such that every element of is in exactly one set. Symbolically,    If then   "
},
{
  "id": "some-partitions-4",
  "level": "2",
  "url": "s-partitions-and-law-of-addition.html#some-partitions-4",
  "type": "Example",
  "number": "2.3.2",
  "title": "Some partitions of a four element set.",
  "body": "Some partitions of a four element set  Let . Examples of partitions of are:          How many others are there, do you suppose?  There are 15 different partitions. The most efficient way to count them all is to classify them by the size of blocks. For example, the partition has block sizes 1, 1, and 2.   "
},
{
  "id": "some-integer-partitions",
  "level": "2",
  "url": "s-partitions-and-law-of-addition.html#some-integer-partitions",
  "type": "Example",
  "number": "2.3.3",
  "title": "Some Integer Partitions.",
  "body": "Some Integer Partitions Two examples of partitions of set of integers are   and   .   The set of subsets is not a partition because the two subsets have a nonempty intersection. A second example of a non-partition is because one of the blocks, when is empty. "
},
{
  "id": "basic-law-addition",
  "level": "2",
  "url": "s-partitions-and-law-of-addition.html#basic-law-addition",
  "type": "Theorem",
  "number": "2.3.4",
  "title": "The Basic Law Of Addition:.",
  "body": "The Basic Law Of Addition: Basic Law Of Addition: If is a finite set, and if is a partition of , then   "
},
{
  "id": "counting-all-students",
  "level": "2",
  "url": "s-partitions-and-law-of-addition.html#counting-all-students",
  "type": "Example",
  "number": "2.3.5",
  "title": "Counting All Students.",
  "body": "Counting All Students The number of students in a class could be determined by adding the numbers of students who are freshmen, sophomores, juniors, and seniors, and those who belong to none of these categories. However, you probably couldn't add the students by major, since some students may have double majors. "
},
{
  "id": "student-counting-disjoint",
  "level": "2",
  "url": "s-partitions-and-law-of-addition.html#student-counting-disjoint",
  "type": "Example",
  "number": "2.3.6",
  "title": "Counting Students in Disjoint Classes.",
  "body": "Counting Students in Disjoint Classes The sophomore computer science majors were told they must take one and only one of the following courses that are open only to them: Cryptography, Data Structures, or Javascript. The numbers in each course, respectively, for sophomore CS majors, were 75, 60, 55. How many sophomore CS majors are there? The Law of Addition applies here. There are exactly CS majors since the rosters of the three courses listed above would be a partition of the CS majors.  "
},
{
  "id": "student-counting-nondisjoint",
  "level": "2",
  "url": "s-partitions-and-law-of-addition.html#student-counting-nondisjoint",
  "type": "Example",
  "number": "2.3.7",
  "title": "Counting Students in Non-disjoint Classes.",
  "body": "Counting Students in Non-disjoint Classes It was determined that all junior computer science majors take at least one of the following courses: Algorithms, Logic Design, and Compiler Construction. Assume the number in each course was 75, 60 and 55, respectively for the three courses listed. Further investigation indicated ten juniors took all three courses, twenty-five took Algorithms and Logic Design, twelve took Algorithms and Compiler Construction, and fifteen took Logic Design and Compiler Construction. How many junior C.S. majors are there?  was a simple application of the law of addition, however in this example some students are taking two or more courses, so a simple application of the law of addition would lead to double or triple counting. We rephrase information in the language of sets to describe the situation more explicitly.   = the set of all junior computer science majors   = the set of all junior computer science majors who took Algorithms   = the set of all junior computer science majors who took Logic Design   = the set of all junior computer science majors who took Compiler Construction  Since all junior CS majors must take at least one of the courses, the number we want is:   A Venn diagram is helpful to visualize the problem. In this case the universal set can stand for all students in the university.   Venn Diagram   A three set Venn Diagram to solve the Junior CS student example.     We see that the whole universal set is naturally partitioned into subsets that are labeled by the numbers 1 through 8, and the set is partitioned into subsets labeled 1 through 7. The region labeled 8 represents all students who are not junior CS majors. Note also that students in the subsets labeled 2, 3, and 4 are double counted, and those in the subset labeled 1 are triple counted. To adjust, we must subtract the numbers in regions 2, 3 and 4. This can be done by subtracting the numbers in the intersections of each pair of sets. However, the individuals in region 1 will have been removed three times, just as they had been originally added three times. Therefore, we must finally add their number back in.   "
},
{
  "id": "inclusion-exclusion",
  "level": "2",
  "url": "s-partitions-and-law-of-addition.html#inclusion-exclusion",
  "type": "Theorem",
  "number": "2.3.9",
  "title": "Laws of Inclusion-Exclusion.",
  "body": " Laws of Inclusion-Exclusion  Inclusion-Exclusion, Laws of   Given finite sets , then  The Two Set Inclusion-Exclusion Law:  The Three Set Inclusion-Exclusion Law:    "
},
{
  "id": "exercises-2-3-2",
  "level": "2",
  "url": "s-partitions-and-law-of-addition.html#exercises-2-3-2",
  "type": "Exercise",
  "number": "2.3.3.1",
  "title": "",
  "body": "List all partitions of the set .    "
},
{
  "id": "exercises-2-3-3",
  "level": "2",
  "url": "s-partitions-and-law-of-addition.html#exercises-2-3-3",
  "type": "Exercise",
  "number": "2.3.3.2",
  "title": "",
  "body": "Which of the following collections of subsets of the plane, , are partitions?    The set of all circles in  The set of all circles in centered at the origin together with the set   "
},
{
  "id": "exercises-2-3-4",
  "level": "2",
  "url": "s-partitions-and-law-of-addition.html#exercises-2-3-4",
  "type": "Exercise",
  "number": "2.3.3.3",
  "title": "",
  "body": "A student, on an exam paper, defined the term partition the following way: Let be a set. A partition of is any set of nonempty subsets of such that each element of is in one of the subsets. Is this definition correct? Why?  No. By this definition it is possible that an element of might belong to two of the subsets.  "
},
{
  "id": "exercises-2-3-5",
  "level": "2",
  "url": "s-partitions-and-law-of-addition.html#exercises-2-3-5",
  "type": "Exercise",
  "number": "2.3.3.4",
  "title": "",
  "body": "Let and be subsets of a set . Draw a Venn diagram of this situation and shade in the subsets , , , and . Use the resulting diagram and the definition of partition to convince yourself that the subset of these four subsets that are nonempty form a partition of .  "
},
{
  "id": "exercises-2-3-6",
  "level": "2",
  "url": "s-partitions-and-law-of-addition.html#exercises-2-3-6",
  "type": "Exercise",
  "number": "2.3.3.5",
  "title": "",
  "body": "Show that is a partition of . Describe this partition using only words. The first subset is all the even integers and the second is all the odd integers. These two sets do not intersect and they cover the integers completely.  "
},
{
  "id": "exercises-2-3-7",
  "level": "2",
  "url": "s-partitions-and-law-of-addition.html#exercises-2-3-7",
  "type": "Exercise",
  "number": "2.3.3.6",
  "title": "",
  "body": " A group of 30 students were surveyed and it was found that 18 of them took Calculus and 12 took Physics. If all students took at least one course, how many took both Calculus and Physics? Illustrate using a Venn diagram.  What is the answer to the question in part (a) if five students did not take either of the two courses? Illustrate using a Venn diagram.      Let be the set of calculus students and the physics students. Since all 30 students took at least one of the courses,, and we can place a zero outside the circles in the Venn diagram to the left. which implies that is empty.  If five students did not take either of the two courses, then and which implies that .       Venn Diagram for part (a)      Venn Diagram for part (b)     "
},
{
  "id": "exercises-2-3-8",
  "level": "2",
  "url": "s-partitions-and-law-of-addition.html#exercises-2-3-8",
  "type": "Exercise",
  "number": "2.3.3.7",
  "title": "",
  "body": " A survey of 90 people, 47 of them played tennis and 42 of them swam. If 17 of them participated in both activities, how many of them participated in neither? Since 17 participated in both activities, 30 of the tennis players only played tennis and 25 of the swimmers only swam. Therefore, of those who were surveyed participated in an activity and so 18 did not.  "
},
{
  "id": "exercises-2-3-9",
  "level": "2",
  "url": "s-partitions-and-law-of-addition.html#exercises-2-3-9",
  "type": "Exercise",
  "number": "2.3.3.8",
  "title": "",
  "body": "A survey of 300 people found that 60 owned an iPhone, 75 owned a Blackberry, and 30 owned an Android phone. Furthermore, 40 owned both an iPhone and a Blackberry, 12 owned both an iPhone and an Android phone, and 8 owned a Blackberry and an Android phone. Finally, 3 owned all three phones.   How many people surveyed owned none of the three phones?  How many people owned a Blackberry but not an iPhone?  How many owned a Blackberry but not an Android?     192  35  67   "
},
{
  "id": "exercises-2-3-10",
  "level": "2",
  "url": "s-partitions-and-law-of-addition.html#exercises-2-3-10",
  "type": "Exercise",
  "number": "2.3.3.9",
  "title": "",
  "body": " Regarding ,  Use the two set inclusion-exclusion law to derive the three set inclusion-exclusion law. Note: A knowledge of basic set laws is needed for this exercise.  State and derive the inclusion-exclusion law for four sets.  We assume that .   The law for four sets is   Derivation:   "
},
{
  "id": "exercises-2-3-11",
  "level": "2",
  "url": "s-partitions-and-law-of-addition.html#exercises-2-3-11",
  "type": "Exercise",
  "number": "2.3.3.10",
  "title": "",
  "body": "To complete your spring schedule, you must add Calculus and Physics. At 9:30, there are three Calculus sections and two Physics sections; while at 11:30, there are two Calculus sections and three Physics sections. How many ways can you complete your schedule if your only open periods are 9:30 and 11:30?  We can partition the possible schedules into two sets, the ones for which you take Calculus at 9:30, and those for which you take Physics at 9:30. This is a true partition because of the conditions of the problem. If you take Calculus at 9:30 you can register three different ways and then choose one of the three 11:30 Physics sections. That's 9 possible choices in all. If you take Physics at 9:30, then there two choices and then two choices for Calculus at 11:30, so 4 choices in all. By the Law of Addition there are ways to register.  "
},
{
  "id": "exercises-2-3-12",
  "level": "2",
  "url": "s-partitions-and-law-of-addition.html#exercises-2-3-12",
  "type": "Exercise",
  "number": "2.3.3.11",
  "title": "",
  "body": "The definition of given in Chapter 1 is awkward. If we use the definition to list elements in , we will have duplications such as , and Try to write a more precise definition of the rational numbers so that there is no duplication of elements.  Partition the set of fractions into blocks, where each block contains fractions that are numerically equivalent. Describe how you would determine whether two fractions belong to the same block. Redefine the rational numbers to be this partition. Each rational number is a set of fractions. "
},
{
  "id": "s-combinations-and-the-binomial-theorem",
  "level": "1",
  "url": "s-combinations-and-the-binomial-theorem.html",
  "type": "Section",
  "number": "2.4",
  "title": "Combinations and the Binomial Theorem",
  "body": " Combinations and the Binomial Theorem   Combinations Combinations  In Section 2.1 we investigated the most basic concept in combinatorics, namely, the rule of products. It is of paramount importance to keep this fundamental rule in mind. In Section 2.2 we saw a subclass of rule-of-products problems, permutations, and we derived a formula as a computational aid to assist us. In this section we will investigate another counting formula, one that is used to count combinations, which are subsets of a certain size.  In many rule-of-products applications the ordering is important, such as the batting order of a baseball team. In other cases it is not important, as in placing coins in a vending machine or in the listing of the elements of a set. Order is important in permutations. Order is not important in combinations.  Counting Permutations How many different ways are there to permute three letters from the set ? From the there are different orderings of three letters from   Counting with No Order  How many ways can we select a set of three letters from ? Note here that we are not concerned with the order of the three letters. By trial and error, abc, abd, acd, and bcd are the only listings possible. To repeat, we were looking for all three-element subsets of the set . Order is not important in sets. The notation for choosing 3 elements from 4 is most commonly or occasionally , either of which is read 4 choose 3 or the number of combinations for four objects taken three at a time.   Binomial Coefficient  Binomial Coefficient    choose , the number of element subsets of an element set.    Let and be nonnegative integers. The binomial coefficient represents the number of combinations of objects taken at a time, and is read choose .    We would now like to investigate the relationship between permutation and combination problems in order to derive a formula for  Let us reconsider the . There are different orderings for each of the three-element subsets. The table below lists each subset of and all permutations of each subset on the same line. .  Hence,  We generalize this result in the following theorem:  Binomial Coefficient Formula Binomial Coefficient Formula   If and are nonnegative integers with , then the number -element subsets of an element set is equal to .  Proof 1: There are ways of ordering the elements of any element set. Therefore, .  Proof 2: To construct a permutation of objects from a set of elements, we can first choose one of the subsets of objects and second, choose one of the permutations of those objects. By the rule of products, and solving for we get the desired formula.   Flipping Coins Assume an evenly balanced coin is tossed five times. In how many ways can three heads be obtained? This is a combination problem, because the order in which the heads appear does not matter. We can think of this as a situation involving sets by considering the set of flips of the coin, 1 through 5, in which heads comes up. The number of ways to get three heads is .  Counting five ordered flips two ways  We determine the total number of ordered ways a fair coin can land if tossed five consecutive times. The five tosses can produce any one of the following mutually exclusive, disjoint events: 5 heads, 4 heads, 3 heads, 2 heads, 1 head, or 0 heads. For example, by the previous example, there are sequences in which three heads appear. Counting the other possibilities in the same way, by the law of addition we have: ways to observe the five flips.  Of course, we could also have applied the extended rule of products, and since there are two possible outcomes for each of the five tosses, we have ways.  You might think that counting something two ways is a waste of time but solving a problem two different ways often is instructive and leads to valuable insights. In this case, it suggests a general formula for the sum . In the case of , we get so it is reasonable to expect that the general sum is , and it is. A logical argument to prove the general statment simply involves generalizing the previous example to coin flips.  A Committee of Five A committee usually starts as an unstructured set of people selected from a larger membership. Therefore, a committee can be thought of as a combination. If a club of 25 members has a five-member social committee, there are different possible social committees. If any structure or restriction is placed on the way the social committee is to be selected, the number of possible committees will probably change. For example, if the club has a rule that the treasurer must be on the social committee, then the number of possibilities is reduced to .  If we further require that a chairperson other than the treasurer be selected for the social committee, we have different possible social committees. The choice of the four non-treasurers accounts for the factor while the need to choose a chairperson accounts for the 4.  Binomial Coefficients - Extreme Cases By simply applying the definition of a as a number of subsets we see that there is way of choosing a combination of zero elements from a set of . In addition, we see that there is way of choosing a combination of elements from a set of .  We could compute these values using the formula we have developed, but no arithmetic is really needed here. Other properties of binomial coefficients that can be derived using the subset definition will be seen in the exercises     The Binomial Theorem  The binomial theorem gives us a formula for expanding , where is a nonnegative integer. The coefficients of this expansion are precisely the binomial coefficients that we have used to count combinations. Using high school algebra we can expand the expression for integers from 0 to 5:    In the expansion of we note that the coefficient of the third term is , and that of the sixth term is . We can rewrite the expansion as .  In summary, in the expansion of we note:   The first term is and the last term is .  With each successive term, exponents of decrease by 1 as those of increase by 1. For any term the sum of the exponents is .  The coefficient of is .  The triangular array of binomial coefficients is called Pascal's triangle after the seventeenth-century French mathematician Blaise Pascal. Note that each number in the triangle other than the 1's at the ends of each row is the sum of the two numbers to the right and left of it in the row above.  The Binomial Theorem Binomial Theorem, The  If , and and are numbers, then .   This theorem will be proven using a logical procedure called mathematical induction, which will be introduced in Chapter 3.   Identifying a term in an expansion Find the third term in the expansion of . The third term, when , is .   A Binomial Expansion Expand . If we replace and in the Binomial Theorem with and , respectively, we get .    SageMath Note SageMath Note bridge hands  A bridge hand is a 13 element subset of a standard 52 card deck. The order in which the cards come to the player doesn't matter. From the point of view of a single player, the number of possible bridge hands is , which can be easily computed with .   In bridge, the location of a hand in relation to the dealer has some bearing on the game. An even truer indication of the number of possible hands takes into account player's possible hand. It is customary to refer to bridge positions as West, North, East and South. We can apply the rule of product to get the total number of bridge hands with the following logic. West can get any of the hands identified above. Then North get 13 of the remaining 39 cards and so has possible hands. East then gets 13 of the 26 remaining cards, which has possibilities. South gets the remaining cards. Therefore the number of bridge hands is computed using the Product Rule.     The judiciary committee at a college is made up of three faculty members and four students. If ten faculty members and 25 students have been nominated for the committee, how many judiciary committees could be formed at this point?    Suppose that a single character is stored in a computer using eight bits.  a. How many bit patterns have exactly three 1's?  b. How many bit patterns have at least two 1's?  Think of the set of positions that contain a 1 to turn this is into a question about sets. (a) (b)  How many subsets of contain at least seven elements?     The congressional committees on mathematics and computer science are made up of five representatives each, and a congressional rule is that the two committees must be disjoint. If there are 385 members of congress, how many ways could the committees be selected?  The number of possible committees is . This number is approximately , so its exact value isn't very instructive.  Lattice Paths The image below shows a 6 by 6 grid and an example of a lattice path that could be taken from to , which is a path taken by traveling along grid lines going only to the right and up. How many different lattice paths are there of this type? Generalize to the case of lattice paths from to for any nonnegative integers and .   A lattice path   An example of a lattice path consisting of seven parallel horizontal line segments, equally spaced together with seven parallel vertical line segments, equally spaced. Together they form a square that is divided into 36 small squares. The bottom left point of intersection of lines is the origin and the top right is the point . A path is traced by traveling along the lines starting a the origin and ending at .    Think of each path as a sequence of instructions to go right (R) and up (U). Each path can be described as a sequence or R's and U's with exactly six of each. The six positions in which R's could be placed can be selected from the twelve positions in the sequence ways. We can generalize this logic and see that there are paths from to .    How many of the lattice paths from to pass through as the one in does?  How many the paths pass through but not necessarily ?  How many the paths pass through and avoid ?        In the answer to the last part, the factor is the number of ways to go from to ). Naturally, that is equal to 1, but we include it to recognize that we are subtracting the paths that pass through both of these points.   A poker game is played with 52 cards. At the start of a game, each player gets five of the cards. The order in which cards are dealt doesn't matter.  How many hands of five cards are possible?  If there are four people playing, how many initial five-card hands are possible, taking into account all players and their positions at the table? Position with respect to the dealer does matter.         A flush in a five-card poker hand is five cards of the same suit. The suits are spades, clubs, diamonds and hearts. How many spade flushes are possible in a 52-card deck? How many flushes are possible in any suit? There are different spade flushes. The number of flushes of any suit is four times this number.  How many five-card poker hands using 52 cards contain exactly two aces?   In poker, a full house is three-of-a-kind and a pair in one hand; for example, three fives and two queens. How many full houses are possible from a 52-card deck? You can use the sage cell in the to do this calculation, but also write your answer in terms of binomial coefficients. There are different spade flushes. The number of flushes of any suit is four times this number.  A class of twelve computer science students are to be divided into three groups of 3, 4, and 5 students to work on a project. How many ways can this be done if every student is to be in exactly one group?   Explain in words why the following equalities are true based on number of subsets, and then verify the equalities using the formula for binomial coefficients.     , .    There are one element subsets of an element set.  For each element subset of an element set there is a companion set, its complement, with elements.   There are ten points, on a plane, no three on the same line.   How many lines are determined by the points?  How many triangles are determined by the points?         How many ways can persons be grouped into pairs when is even? Assume the order of the pairs matters, but not the order within the pairs. For example, if , the six different groupings would be   Let . Then the number of ordered pairings is   Use the binomial theorem to prove that if is a finite set, then Assume . If we let in the Binomial Theorem, we obtain , with the right side of the equality counting all subsets of containing elements. Hence      A state's lottery involves choosing six different numbers out of a possible 36. How many ways can a person choose six numbers?  What is the probability of a person winning with one bet?    Since all of the combinations are equally likely,    Use the binomial theorem to calculate .    In the card game Blackjack, there are one or more players and a dealer. Initially, each player is dealt two cards and the dealer is dealt one card down and one facing up. As in bridge, the order of the hands, but not the order of the cards in the hands, matters. Starting with a single 52 card deck, and three players, how many ways can the first two cards be dealt out? You can use the sage cell in the to do this calculation. Since there is a difference between the up and down cards that the dealer gets the order does matter for the dealer. The number of possible hands for three players will be     "
},
{
  "id": "counting-permuations-multiple-ways",
  "level": "2",
  "url": "s-combinations-and-the-binomial-theorem.html#counting-permuations-multiple-ways",
  "type": "Example",
  "number": "2.4.1",
  "title": "Counting Permutations.",
  "body": "Counting Permutations How many different ways are there to permute three letters from the set ? From the there are different orderings of three letters from  "
},
{
  "id": "four-choose-three",
  "level": "2",
  "url": "s-combinations-and-the-binomial-theorem.html#four-choose-three",
  "type": "Example",
  "number": "2.4.2",
  "title": "Counting with No Order.",
  "body": "Counting with No Order  How many ways can we select a set of three letters from ? Note here that we are not concerned with the order of the three letters. By trial and error, abc, abd, acd, and bcd are the only listings possible. To repeat, we were looking for all three-element subsets of the set . Order is not important in sets. The notation for choosing 3 elements from 4 is most commonly or occasionally , either of which is read 4 choose 3 or the number of combinations for four objects taken three at a time.  "
},
{
  "id": "binomial-coefficient",
  "level": "2",
  "url": "s-combinations-and-the-binomial-theorem.html#binomial-coefficient",
  "type": "Definition",
  "number": "2.4.3",
  "title": "Binomial Coefficient.",
  "body": "Binomial Coefficient  Binomial Coefficient    choose , the number of element subsets of an element set.    Let and be nonnegative integers. The binomial coefficient represents the number of combinations of objects taken at a time, and is read choose .   "
},
{
  "id": "binomial-coefficient-formula",
  "level": "2",
  "url": "s-combinations-and-the-binomial-theorem.html#binomial-coefficient-formula",
  "type": "Theorem",
  "number": "2.4.4",
  "title": "Binomial Coefficient Formula.",
  "body": "Binomial Coefficient Formula Binomial Coefficient Formula   If and are nonnegative integers with , then the number -element subsets of an element set is equal to .  Proof 1: There are ways of ordering the elements of any element set. Therefore, .  Proof 2: To construct a permutation of objects from a set of elements, we can first choose one of the subsets of objects and second, choose one of the permutations of those objects. By the rule of products, and solving for we get the desired formula.  "
},
{
  "id": "flipping-coins",
  "level": "2",
  "url": "s-combinations-and-the-binomial-theorem.html#flipping-coins",
  "type": "Example",
  "number": "2.4.5",
  "title": "Flipping Coins.",
  "body": "Flipping Coins Assume an evenly balanced coin is tossed five times. In how many ways can three heads be obtained? This is a combination problem, because the order in which the heads appear does not matter. We can think of this as a situation involving sets by considering the set of flips of the coin, 1 through 5, in which heads comes up. The number of ways to get three heads is . "
},
{
  "id": "five-flips",
  "level": "2",
  "url": "s-combinations-and-the-binomial-theorem.html#five-flips",
  "type": "Example",
  "number": "2.4.6",
  "title": "Counting five ordered flips two ways.",
  "body": "Counting five ordered flips two ways  We determine the total number of ordered ways a fair coin can land if tossed five consecutive times. The five tosses can produce any one of the following mutually exclusive, disjoint events: 5 heads, 4 heads, 3 heads, 2 heads, 1 head, or 0 heads. For example, by the previous example, there are sequences in which three heads appear. Counting the other possibilities in the same way, by the law of addition we have: ways to observe the five flips.  Of course, we could also have applied the extended rule of products, and since there are two possible outcomes for each of the five tosses, we have ways. "
},
{
  "id": "committee-of-five",
  "level": "2",
  "url": "s-combinations-and-the-binomial-theorem.html#committee-of-five",
  "type": "Example",
  "number": "2.4.7",
  "title": "A Committee of Five.",
  "body": "A Committee of Five A committee usually starts as an unstructured set of people selected from a larger membership. Therefore, a committee can be thought of as a combination. If a club of 25 members has a five-member social committee, there are different possible social committees. If any structure or restriction is placed on the way the social committee is to be selected, the number of possible committees will probably change. For example, if the club has a rule that the treasurer must be on the social committee, then the number of possibilities is reduced to .  If we further require that a chairperson other than the treasurer be selected for the social committee, we have different possible social committees. The choice of the four non-treasurers accounts for the factor while the need to choose a chairperson accounts for the 4. "
},
{
  "id": "extreme-binomial-cases",
  "level": "2",
  "url": "s-combinations-and-the-binomial-theorem.html#extreme-binomial-cases",
  "type": "Example",
  "number": "2.4.8",
  "title": "Binomial Coefficients - Extreme Cases.",
  "body": "Binomial Coefficients - Extreme Cases By simply applying the definition of a as a number of subsets we see that there is way of choosing a combination of zero elements from a set of . In addition, we see that there is way of choosing a combination of elements from a set of .  We could compute these values using the formula we have developed, but no arithmetic is really needed here. Other properties of binomial coefficients that can be derived using the subset definition will be seen in the exercises  "
},
{
  "id": "binomial-theorem",
  "level": "2",
  "url": "s-combinations-and-the-binomial-theorem.html#binomial-theorem",
  "type": "Theorem",
  "number": "2.4.9",
  "title": "The Binomial Theorem.",
  "body": "The Binomial Theorem Binomial Theorem, The  If , and and are numbers, then .   This theorem will be proven using a logical procedure called mathematical induction, which will be introduced in Chapter 3.  "
},
{
  "id": "term-in-an-expansion",
  "level": "2",
  "url": "s-combinations-and-the-binomial-theorem.html#term-in-an-expansion",
  "type": "Example",
  "number": "2.4.10",
  "title": "Identifying a term in an expansion.",
  "body": "Identifying a term in an expansion Find the third term in the expansion of . The third term, when , is .  "
},
{
  "id": "a-full-expansion",
  "level": "2",
  "url": "s-combinations-and-the-binomial-theorem.html#a-full-expansion",
  "type": "Example",
  "number": "2.4.11",
  "title": "A Binomial Expansion.",
  "body": "A Binomial Expansion Expand . If we replace and in the Binomial Theorem with and , respectively, we get .  "
},
{
  "id": "exercises-2-4-1",
  "level": "2",
  "url": "s-combinations-and-the-binomial-theorem.html#exercises-2-4-1",
  "type": "Exercise",
  "number": "2.4.4.1",
  "title": "",
  "body": "The judiciary committee at a college is made up of three faculty members and four students. If ten faculty members and 25 students have been nominated for the committee, how many judiciary committees could be formed at this point?   "
},
{
  "id": "exercises-2-4-2",
  "level": "2",
  "url": "s-combinations-and-the-binomial-theorem.html#exercises-2-4-2",
  "type": "Exercise",
  "number": "2.4.4.2",
  "title": "",
  "body": "Suppose that a single character is stored in a computer using eight bits.  a. How many bit patterns have exactly three 1's?  b. How many bit patterns have at least two 1's?  Think of the set of positions that contain a 1 to turn this is into a question about sets. (a) (b) "
},
{
  "id": "exercises-2-4-3",
  "level": "2",
  "url": "s-combinations-and-the-binomial-theorem.html#exercises-2-4-3",
  "type": "Exercise",
  "number": "2.4.4.3",
  "title": "",
  "body": "How many subsets of contain at least seven elements?    "
},
{
  "id": "exercises-2-4-4",
  "level": "2",
  "url": "s-combinations-and-the-binomial-theorem.html#exercises-2-4-4",
  "type": "Exercise",
  "number": "2.4.4.4",
  "title": "",
  "body": "The congressional committees on mathematics and computer science are made up of five representatives each, and a congressional rule is that the two committees must be disjoint. If there are 385 members of congress, how many ways could the committees be selected?  The number of possible committees is . This number is approximately , so its exact value isn't very instructive. "
},
{
  "id": "exercises-2-4-5",
  "level": "2",
  "url": "s-combinations-and-the-binomial-theorem.html#exercises-2-4-5",
  "type": "Exercise",
  "number": "2.4.4.5",
  "title": "",
  "body": "Lattice Paths The image below shows a 6 by 6 grid and an example of a lattice path that could be taken from to , which is a path taken by traveling along grid lines going only to the right and up. How many different lattice paths are there of this type? Generalize to the case of lattice paths from to for any nonnegative integers and .   A lattice path   An example of a lattice path consisting of seven parallel horizontal line segments, equally spaced together with seven parallel vertical line segments, equally spaced. Together they form a square that is divided into 36 small squares. The bottom left point of intersection of lines is the origin and the top right is the point . A path is traced by traveling along the lines starting a the origin and ending at .    Think of each path as a sequence of instructions to go right (R) and up (U). Each path can be described as a sequence or R's and U's with exactly six of each. The six positions in which R's could be placed can be selected from the twelve positions in the sequence ways. We can generalize this logic and see that there are paths from to . "
},
{
  "id": "exercises-2-4-6",
  "level": "2",
  "url": "s-combinations-and-the-binomial-theorem.html#exercises-2-4-6",
  "type": "Exercise",
  "number": "2.4.4.6",
  "title": "",
  "body": "  How many of the lattice paths from to pass through as the one in does?  How many the paths pass through but not necessarily ?  How many the paths pass through and avoid ?        In the answer to the last part, the factor is the number of ways to go from to ). Naturally, that is equal to 1, but we include it to recognize that we are subtracting the paths that pass through both of these points.  "
},
{
  "id": "exercises-2-4-7",
  "level": "2",
  "url": "s-combinations-and-the-binomial-theorem.html#exercises-2-4-7",
  "type": "Exercise",
  "number": "2.4.4.7",
  "title": "",
  "body": "A poker game is played with 52 cards. At the start of a game, each player gets five of the cards. The order in which cards are dealt doesn't matter.  How many hands of five cards are possible?  If there are four people playing, how many initial five-card hands are possible, taking into account all players and their positions at the table? Position with respect to the dealer does matter.        "
},
{
  "id": "exercises-2-4-8",
  "level": "2",
  "url": "s-combinations-and-the-binomial-theorem.html#exercises-2-4-8",
  "type": "Exercise",
  "number": "2.4.4.8",
  "title": "",
  "body": "A flush in a five-card poker hand is five cards of the same suit. The suits are spades, clubs, diamonds and hearts. How many spade flushes are possible in a 52-card deck? How many flushes are possible in any suit? There are different spade flushes. The number of flushes of any suit is four times this number. "
},
{
  "id": "exercises-2-4-9",
  "level": "2",
  "url": "s-combinations-and-the-binomial-theorem.html#exercises-2-4-9",
  "type": "Exercise",
  "number": "2.4.4.9",
  "title": "",
  "body": "How many five-card poker hands using 52 cards contain exactly two aces?  "
},
{
  "id": "exercises-2-4-10",
  "level": "2",
  "url": "s-combinations-and-the-binomial-theorem.html#exercises-2-4-10",
  "type": "Exercise",
  "number": "2.4.4.10",
  "title": "",
  "body": "In poker, a full house is three-of-a-kind and a pair in one hand; for example, three fives and two queens. How many full houses are possible from a 52-card deck? You can use the sage cell in the to do this calculation, but also write your answer in terms of binomial coefficients. There are different spade flushes. The number of flushes of any suit is four times this number. "
},
{
  "id": "exercises-2-4-11",
  "level": "2",
  "url": "s-combinations-and-the-binomial-theorem.html#exercises-2-4-11",
  "type": "Exercise",
  "number": "2.4.4.11",
  "title": "",
  "body": "A class of twelve computer science students are to be divided into three groups of 3, 4, and 5 students to work on a project. How many ways can this be done if every student is to be in exactly one group?  "
},
{
  "id": "exercises-2-4-12",
  "level": "2",
  "url": "s-combinations-and-the-binomial-theorem.html#exercises-2-4-12",
  "type": "Exercise",
  "number": "2.4.4.12",
  "title": "",
  "body": "Explain in words why the following equalities are true based on number of subsets, and then verify the equalities using the formula for binomial coefficients.     , .    There are one element subsets of an element set.  For each element subset of an element set there is a companion set, its complement, with elements.  "
},
{
  "id": "exercises-2-4-13",
  "level": "2",
  "url": "s-combinations-and-the-binomial-theorem.html#exercises-2-4-13",
  "type": "Exercise",
  "number": "2.4.4.13",
  "title": "",
  "body": "There are ten points, on a plane, no three on the same line.   How many lines are determined by the points?  How many triangles are determined by the points?        "
},
{
  "id": "exercises-2-4-14",
  "level": "2",
  "url": "s-combinations-and-the-binomial-theorem.html#exercises-2-4-14",
  "type": "Exercise",
  "number": "2.4.4.14",
  "title": "",
  "body": "How many ways can persons be grouped into pairs when is even? Assume the order of the pairs matters, but not the order within the pairs. For example, if , the six different groupings would be   Let . Then the number of ordered pairings is  "
},
{
  "id": "exercises-2-4-15",
  "level": "2",
  "url": "s-combinations-and-the-binomial-theorem.html#exercises-2-4-15",
  "type": "Exercise",
  "number": "2.4.4.15",
  "title": "",
  "body": "Use the binomial theorem to prove that if is a finite set, then Assume . If we let in the Binomial Theorem, we obtain , with the right side of the equality counting all subsets of containing elements. Hence   "
},
{
  "id": "exercises-2-4-16",
  "level": "2",
  "url": "s-combinations-and-the-binomial-theorem.html#exercises-2-4-16",
  "type": "Exercise",
  "number": "2.4.4.16",
  "title": "",
  "body": "  A state's lottery involves choosing six different numbers out of a possible 36. How many ways can a person choose six numbers?  What is the probability of a person winning with one bet?    Since all of the combinations are equally likely,   "
},
{
  "id": "exercises-2-4-17",
  "level": "2",
  "url": "s-combinations-and-the-binomial-theorem.html#exercises-2-4-17",
  "type": "Exercise",
  "number": "2.4.4.17",
  "title": "",
  "body": "Use the binomial theorem to calculate .   "
},
{
  "id": "exercises-2-4-18",
  "level": "2",
  "url": "s-combinations-and-the-binomial-theorem.html#exercises-2-4-18",
  "type": "Exercise",
  "number": "2.4.4.18",
  "title": "",
  "body": "In the card game Blackjack, there are one or more players and a dealer. Initially, each player is dealt two cards and the dealer is dealt one card down and one facing up. As in bridge, the order of the hands, but not the order of the cards in the hands, matters. Starting with a single 52 card deck, and three players, how many ways can the first two cards be dealt out? You can use the sage cell in the to do this calculation. Since there is a difference between the up and down cards that the dealer gets the order does matter for the dealer. The number of possible hands for three players will be   "
},
{
  "id": "s-propositions-logic-operators",
  "level": "1",
  "url": "s-propositions-logic-operators.html",
  "type": "Section",
  "number": "3.1",
  "title": "Propositions and Logical Operators",
  "body": "Propositions and Logical Operators  Propositions  Proposition Proposition A proposition is a sentence to which one and only one of the terms true or false can be meaningfully applied.  Some Propositions  Four is even, , and are propositions.  In traditional logic, a declarative statement with a definite truth value is considered a proposition. Although our ultimate aim is to discuss mathematical logic, we won't separate ourselves completely from the traditional setting. This is natural because the basic assumptions, or postulates, of mathematical logic are modeled after the logic we use in everyday life. Since compound sentences are frequently used in everyday speech, we expect that logical propositions contain connectives like the word and. The statement Europa supports life or Mars supports life is a proposition and, hence, must have a definite truth value. Whatever that truth value is, it should be the same as the truth value of Mars supports life or Europa supports life.   Logical Operations  There are several ways in which we commonly combine simple statements into compound ones. The words\/phrases and , or , not , if ... then... , and ...if and only if ... can be added to one or more propositions to create a new proposition. To avoid any confusion, we will precisely define each one's meaning and introduce its standard symbol. With the exception of negation ( not ), all of the operations act on pairs of propositions. Since each proposition has two possible truth values, there are four ways that truth can be assigned to two propositions. In defining the effect that a logical operation has on two propositions, the result must be specified for all four cases. The most convenient way of doing this is with a truth table, which we will illustrate by defining the word and .  Logical Conjunction Conjunction, Logical   the conjunction,   If and are propositions, their conjunction, (denoted ), is defined by the truth table    Notes:   To read this truth table, you must realize that any one line represents a case: one possible set of values for and .  The numbers 0 and 1 are used to denote false and true, respectively. This is consistent with the way that many programming languages treat logical, or Boolean, variables since a single bit, 0 or 1, can represent a truth value.  For each case, the symbol under represents the truth value of . The same is true for . The symbol under represents its truth value for that case. For example, the second row of the truth table represents the case in which is false, is true, and the resulting truth value for is false. As in everyday speech, is true only when both propositions are true.  Just as the letters , and are frequently used in algebra to represent numeric variables, , and seem to be the most commonly used symbols for logical variables. When we say that is a logical variable, we mean that any proposition can take the place of .  One final comment: The order in which we list the cases in a truth table is standardized in this book. If the truth table involves two simple propositions, the numbers under the simple propositions can be interpreted as the two-digit binary integers in increasing order, 00, 01, 10, and 11, for 0, 1, 2, and 3, respectively.    Logical Disjunction Disjunction, Logical  the disjunction,   If and are propositions, their disjunction, (denoted ), is defined by the truth table    Logical Negation Negation, Logical  the negation of , not   If is a proposition, its negation, , denoted , and is defined by the truth table    Note: Negation is the only standard operator that acts on a single proposition; hence only two cases are needed.  Consider the following propositions from everyday speech:  I'm going to quit if I don't get a raise.  If I pass the final, then I'll graduate.  I'll be going to the movies provided that my car starts.    All three propositions are conditional, they can all be restated to fit into the form If Condition , then Conclusion . For example, the first statement can be rewritten as If I don't get a raise, then I'm going to quit.  A conditional statement is meant to be interpreted as a guarantee; if the condition is true, then the conclusion is expected to be true. It says no more and no less.  Conditional Statement Conditional Statement   The conditional proposition If then .   The conditional statement If then , denoted , is defined by the truth table   Truth Table for      0 0 1  0 1 1  1 0 0  1 1 1      Analysis of a Conditional Proposition Assume your instructor told you If you receive a grade of 95 or better in the final examination, then you will receive an A in this course. Your instructor has made a promise to you. If you fulfill his condition, you expect the conclusion (getting an A) to be forthcoming. Suppose your graded final has been returned to you. Has your instructor told the truth or is your instructor guilty of a falsehood?  Case I: Your final exam score was less than 95 (the condition is false) and you did not receive an A (the conclusion is false). The instructor told the truth.  Case II: Your final exam score was less than 95, yet you received an A for the course. The instructor told the truth. (Perhaps your overall course average was excellent.)  Case III: Your final exam score was greater than 95, but you did not receive an A. The instructor lied.  Case IV: Your final exam score was greater than 95, and you received an A. The instructor told the truth.  To sum up, the only case in which a conditional proposition is false is when the condition is true and the conclusion is false.   The order of the condition and conclusion in a conditional proposition is important. If the condition and conclusion are exchanged, a different proposition is produced.  Converse Converse The converse of the proposition is the proposition .  The converse of If you receive a grade of 95 or better in the final exam, then you will receive an A in this course, is If you receive an A in this course, then you received a grade of 95 or better in the final exam. It should be clear that these two statements say different things.  There is a proposition related to that does have the same logical meaning. This is the contrapositive.  Contrapositive Contrapositive  The contrapositive of the proposition is the proposition .  As we will see when we discuss logical proofs, we can prove a conditional proposition by proving its contrapositive, which may be somewhat easier.  Finally, there a third variation on the proposition , the inverse, which we will see that has the same logical meaning as the converse.  Logical Inverse  Inverse Logical  The inverse of the proposition is the proposition .  The inverse of If it snows today, we have a day off. would be If it doesn't snow today, we don't have a day off. Can you see that the original proposition and the inverse are saying different things?  Our final logical operator is a conjunction of two conditionals  Biconditional Proposition Biconditional Proposition   The biconditional proposition if and only if   If and are propositions, the biconditional statement if and only if , denoted , is defined by the truth table    Note that is true when and have the same truth values. It is common to abbreviate if and only if to iff.  Although if ... then... and ...if and only if ... are frequently used in everyday speech, there are several alternate forms that you should be aware of. They are summarized in the following lists.  All of the following are equivalent to If then :   implies .  follows from .  , only if .  , if .  is sufficient for .  is necessary for .    All of the following are equivalent to if and only if :    is necessary and sufficient for .  is equivalent to .  If , then , and if , then .  If , then and conversely.      Exercises  Let = I like discrete structures , = I will pass this course and = I will do my assignments. Express each of the following propositions in symbolic form:   I like discrete structures and I will pass this course.  I will do my assignments or I will not pass this course.  It is not true that I both like discrete structures, and will do my assignments.  I will not do my assignment and I will not pass this course.               For each of the following propositions, identify simple propositions, express the compound proposition in symbolic form, and determine whether it is true or false:   The world is flat or zero is an even integer.  If 432,802 is a multiple of 4, then 432,802 is even.  5 is a prime number and 6 is not divisible by 4.  and .  and .  The sum of two even integers is even and the sum of two odd integers is odd.     : The world is flat, : zero is an even integer. is true since is true.  : 432,802 is a multiple of 4, : 432,802 is a multiple of 4. is true because is false.  : 5 is a prime number, : 6 is not divisible by 4. is true since both and are true.  : , : . is true since both and are true.  : , : . is false since is false.  :The sum of two even integers is even, : The sum of two odd integers is odd. is false since is false.    Let , = 8 is an even integer, and = 11 is a prime number. Express the following as a statement in English and determine whether the statement is true or false:                   and 8 is an even integer. False.  If then 8 is an even integer. True.  If and 8 is an even integer then 11 is a prime number. True.  If then either 8 is an even integer or 11 is not a prime number. True.  If then either 8 is an odd integer or 11 is not a prime number. False.  If 8 is not an even integer then . True.    Rewrite each of the following statements using the other conditional forms:   It is sufficient for an integer to be even that it is a multiple of four.  The fact that a polygon is a square is a sufficient condition that it is a rectangle.  If , then .  If , then or .   is a necessary condition for .    Some possible answers.  If an in integer is a multiple of four, then it is even.  If a polygon is a square, then it is a rectangle.  It is necessary that for to be true.  It is sufficient for , that or . For this example, the two propositions are equivalent so it is true that , is necessary and sufficient for or , however the sample answer given here literally restates the given proposition.  If then .    Write the converse of the propositions in exercise 4. Compare the truth of each proposition and its converse. Only the converse of is true. The converse of (a) is It it necessary for an integer to be a even that it be a multiple of four. This is false because 6 is even and it isn't a multiple of four.   "
},
{
  "id": "def-proposition",
  "level": "2",
  "url": "s-propositions-logic-operators.html#def-proposition",
  "type": "Definition",
  "number": "3.1.1",
  "title": "Proposition.",
  "body": "Proposition Proposition A proposition is a sentence to which one and only one of the terms true or false can be meaningfully applied. "
},
{
  "id": "some-propositions",
  "level": "2",
  "url": "s-propositions-logic-operators.html#some-propositions",
  "type": "Example",
  "number": "3.1.2",
  "title": "Some Propositions.",
  "body": "Some Propositions  Four is even, , and are propositions. "
},
{
  "id": "def-conjunction",
  "level": "2",
  "url": "s-propositions-logic-operators.html#def-conjunction",
  "type": "Definition",
  "number": "3.1.3",
  "title": "Logical Conjunction.",
  "body": "Logical Conjunction Conjunction, Logical   the conjunction,   If and are propositions, their conjunction, (denoted ), is defined by the truth table   "
},
{
  "id": "def-disjunction",
  "level": "2",
  "url": "s-propositions-logic-operators.html#def-disjunction",
  "type": "Definition",
  "number": "3.1.4",
  "title": "Logical Disjunction.",
  "body": "Logical Disjunction Disjunction, Logical  the disjunction,   If and are propositions, their disjunction, (denoted ), is defined by the truth table   "
},
{
  "id": "def-negation",
  "level": "2",
  "url": "s-propositions-logic-operators.html#def-negation",
  "type": "Definition",
  "number": "3.1.5",
  "title": "Logical Negation.",
  "body": "Logical Negation Negation, Logical  the negation of , not   If is a proposition, its negation, , denoted , and is defined by the truth table   "
},
{
  "id": "def-conditional",
  "level": "2",
  "url": "s-propositions-logic-operators.html#def-conditional",
  "type": "Definition",
  "number": "3.1.6",
  "title": "Conditional Statement.",
  "body": "Conditional Statement Conditional Statement   The conditional proposition If then .   The conditional statement If then , denoted , is defined by the truth table   Truth Table for      0 0 1  0 1 1  1 0 0  1 1 1     "
},
{
  "id": "conditional-analysis",
  "level": "2",
  "url": "s-propositions-logic-operators.html#conditional-analysis",
  "type": "Example",
  "number": "3.1.8",
  "title": "Analysis of a Conditional Proposition.",
  "body": "Analysis of a Conditional Proposition Assume your instructor told you If you receive a grade of 95 or better in the final examination, then you will receive an A in this course. Your instructor has made a promise to you. If you fulfill his condition, you expect the conclusion (getting an A) to be forthcoming. Suppose your graded final has been returned to you. Has your instructor told the truth or is your instructor guilty of a falsehood?  Case I: Your final exam score was less than 95 (the condition is false) and you did not receive an A (the conclusion is false). The instructor told the truth.  Case II: Your final exam score was less than 95, yet you received an A for the course. The instructor told the truth. (Perhaps your overall course average was excellent.)  Case III: Your final exam score was greater than 95, but you did not receive an A. The instructor lied.  Case IV: Your final exam score was greater than 95, and you received an A. The instructor told the truth.  To sum up, the only case in which a conditional proposition is false is when the condition is true and the conclusion is false.  "
},
{
  "id": "def-converse",
  "level": "2",
  "url": "s-propositions-logic-operators.html#def-converse",
  "type": "Definition",
  "number": "3.1.9",
  "title": "Converse.",
  "body": "Converse Converse The converse of the proposition is the proposition . "
},
{
  "id": "def-contrapositive",
  "level": "2",
  "url": "s-propositions-logic-operators.html#def-contrapositive",
  "type": "Definition",
  "number": "3.1.10",
  "title": "Contrapositive.",
  "body": "Contrapositive Contrapositive  The contrapositive of the proposition is the proposition . "
},
{
  "id": "def-inverse-proposition",
  "level": "2",
  "url": "s-propositions-logic-operators.html#def-inverse-proposition",
  "type": "Definition",
  "number": "3.1.11",
  "title": "Logical Inverse.",
  "body": "Logical Inverse  Inverse Logical  The inverse of the proposition is the proposition . "
},
{
  "id": "def-biconditional",
  "level": "2",
  "url": "s-propositions-logic-operators.html#def-biconditional",
  "type": "Definition",
  "number": "3.1.12",
  "title": "Biconditional Proposition.",
  "body": "Biconditional Proposition Biconditional Proposition   The biconditional proposition if and only if   If and are propositions, the biconditional statement if and only if , denoted , is defined by the truth table   "
},
{
  "id": "exercises-3-1-2",
  "level": "2",
  "url": "s-propositions-logic-operators.html#exercises-3-1-2",
  "type": "Exercise",
  "number": "3.1.3.1",
  "title": "",
  "body": "Let = I like discrete structures , = I will pass this course and = I will do my assignments. Express each of the following propositions in symbolic form:   I like discrete structures and I will pass this course.  I will do my assignments or I will not pass this course.  It is not true that I both like discrete structures, and will do my assignments.  I will not do my assignment and I will not pass this course.              "
},
{
  "id": "exercises-3-1-3",
  "level": "2",
  "url": "s-propositions-logic-operators.html#exercises-3-1-3",
  "type": "Exercise",
  "number": "3.1.3.2",
  "title": "",
  "body": "For each of the following propositions, identify simple propositions, express the compound proposition in symbolic form, and determine whether it is true or false:   The world is flat or zero is an even integer.  If 432,802 is a multiple of 4, then 432,802 is even.  5 is a prime number and 6 is not divisible by 4.  and .  and .  The sum of two even integers is even and the sum of two odd integers is odd.     : The world is flat, : zero is an even integer. is true since is true.  : 432,802 is a multiple of 4, : 432,802 is a multiple of 4. is true because is false.  : 5 is a prime number, : 6 is not divisible by 4. is true since both and are true.  : , : . is true since both and are true.  : , : . is false since is false.  :The sum of two even integers is even, : The sum of two odd integers is odd. is false since is false.   "
},
{
  "id": "exercises-3-1-4",
  "level": "2",
  "url": "s-propositions-logic-operators.html#exercises-3-1-4",
  "type": "Exercise",
  "number": "3.1.3.3",
  "title": "",
  "body": "Let , = 8 is an even integer, and = 11 is a prime number. Express the following as a statement in English and determine whether the statement is true or false:                   and 8 is an even integer. False.  If then 8 is an even integer. True.  If and 8 is an even integer then 11 is a prime number. True.  If then either 8 is an even integer or 11 is not a prime number. True.  If then either 8 is an odd integer or 11 is not a prime number. False.  If 8 is not an even integer then . True.   "
},
{
  "id": "exercises-3-1-5",
  "level": "2",
  "url": "s-propositions-logic-operators.html#exercises-3-1-5",
  "type": "Exercise",
  "number": "3.1.3.4",
  "title": "",
  "body": "Rewrite each of the following statements using the other conditional forms:   It is sufficient for an integer to be even that it is a multiple of four.  The fact that a polygon is a square is a sufficient condition that it is a rectangle.  If , then .  If , then or .   is a necessary condition for .    Some possible answers.  If an in integer is a multiple of four, then it is even.  If a polygon is a square, then it is a rectangle.  It is necessary that for to be true.  It is sufficient for , that or . For this example, the two propositions are equivalent so it is true that , is necessary and sufficient for or , however the sample answer given here literally restates the given proposition.  If then .   "
},
{
  "id": "exercises-3-1-6",
  "level": "2",
  "url": "s-propositions-logic-operators.html#exercises-3-1-6",
  "type": "Exercise",
  "number": "3.1.3.5",
  "title": "",
  "body": "Write the converse of the propositions in exercise 4. Compare the truth of each proposition and its converse. Only the converse of is true. The converse of (a) is It it necessary for an integer to be a even that it be a multiple of four. This is false because 6 is even and it isn't a multiple of four. "
},
{
  "id": "s-truth-tables",
  "level": "1",
  "url": "s-truth-tables.html",
  "type": "Section",
  "number": "3.2",
  "title": "Truth Tables and Propositions Generated by a Set",
  "body": "Truth Tables and Propositions Generated by a Set  Truth Tables  Consider the compound proposition , where , , and are propositions. This is an example of a proposition generated by , , and . We will define this terminology later in the section. Since each of the three simple propositions has two possible truth values, it follows that there are eight different combinations of truth values that determine a value for . These values can be obtained from a truth table for . To construct the truth table, we build from , , and and from the logical operators. The result is the truth table below. Strictly speaking, the first three columns and the last column make up the truth table for . The other columns are work space needed to build up to .   Truth Table for    0 0 0 0 1 0 0  0 0 1 0 1 1 1  0 1 0 0 0 0 0  0 1 1 0 0 0 0  1 0 0 0 1 0 0  1 0 1 0 1 1 1  1 1 0 1 0 0 1  1 1 1 1 0 0 1    Note that the first three columns of the truth table are an enumeration of the eight three-digit binary integers. This standardizes the order in which the cases are listed. In general, if is generated by simple propositions, then the truth table for will have rows with the first columns being an enumeration of the digit binary integers. In our example, we can see at a glance that for exactly four of the eight cases, will be true. For example, if and are true and is false (the sixth case), then is true.  Let be any set of propositions. We will give two definitions of a proposition generated by S. The first is a bit imprecise, but should be clear. The second definition is called a recursive definition . If you find it confusing, use the first definition and return to the second later.   Propositions Generated by a Set  Proposition Generated by a Set  Let be any set of propositions. A proposition generated by is any valid combination of propositions in with conjunction, disjunction, and negation. Or, to be more precise,  If , then is a proposition generated by , and  If and are propositions generated by , then so are , , , and .     Note: We have not included the conditional and biconditional in the definition because they can both be generated from conjunction, disjunction, and negation, as we will see later.  If is a finite set, then we may use slightly different terminology. For example, if , we might say that a proposition is generated by , and instead of from .  It is customary to use the following hierarchy for interpreting propositions, with parentheses overriding this order:  First: Negation  Second: Conjunction  Third: Disjunction  Fourth: The conditional operation  Fifth: The biconditional operation   Within any level of the hierarchy, work from left to right. Using these rules, is taken to mean . These precedence rules are universal, and are exactly those used by computer languages to interpret logical expressions.  Examples of the Hierarchy of Logical Operations  A few shortened expressions and their fully parenthesized versions:    is .   is .   is .   is .     A proposition generated by a set need not include each element of in its expression. For example, is a proposition generated by , and .    Exercises  Construct the truth tables of:                              Construct the truth tables of:                      Truth Table for      0 0 1  0 1 1  1 0 1  1 1 0       Truth Table for      0 0 0  0 1 0  1 0 1  1 1 0       Truth Table for     0 0 0 0 0  0 0 1 0 0  0 1 0 0 0  0 1 1 0 0  1 0 0 0 0  1 0 1 0 0  1 1 0 1 0  1 1 1 1 1       Truth Table for     0 0 0 0  0 0 1 0  0 1 0 0  0 1 1 1  1 0 0 0  1 0 1 1  1 1 0 1  1 1 1 1    Compare this to part (a)   Truth Table for       0 0 1  0 1 1  1 0 1  1 1 0      Truth Table for     0 0 0 0 0  0 0 0 1 1  0 0 1 0 1  0 0 1 1 1  0 1 0 0 1  0 1 0 1 1  0 1 1 0 1  0 1 1 1 1  1 0 0 0 1  1 0 0 1 1  1 0 1 0 1  1 0 1 1 1  1 1 0 0 1  1 1 0 1 1  1 1 1 0 1  1 1 1 1 1      Rewrite the following with as few extraneous parentheses as possible:               In what order are the operations in the following propositions performed?               Determine the number of rows in the truth table of a proposition containing four variables .  rows.  If there are 45 lines on a sheet of paper, and you want to reserve one line for each line in a truth table, how large could be if you can write truth tables of propositions generated by on the sheet of paper?  can be as large as 5. Five propositions require rows. With six propositions, you would need lines, which is too much.    "
},
{
  "id": "tt32-1",
  "level": "2",
  "url": "s-truth-tables.html#tt32-1",
  "type": "Table",
  "number": "3.2.1",
  "title": "Truth Table for <span xmlns:pf=\"https:\/\/prefigure.org\" class=\"process-math\">\\(c= (p \\land  q)\\lor  (\\neg q \\land  r)\\)<\/span>",
  "body": " Truth Table for    0 0 0 0 1 0 0  0 0 1 0 1 1 1  0 1 0 0 0 0 0  0 1 1 0 0 0 0  1 0 0 0 1 0 0  1 0 1 0 1 1 1  1 1 0 1 0 0 1  1 1 1 1 0 0 1   "
},
{
  "id": "def-proposition-generated-by-set",
  "level": "2",
  "url": "s-truth-tables.html#def-proposition-generated-by-set",
  "type": "Definition",
  "number": "3.2.2",
  "title": "Proposition Generated by a Set.",
  "body": "Proposition Generated by a Set  Let be any set of propositions. A proposition generated by is any valid combination of propositions in with conjunction, disjunction, and negation. Or, to be more precise,  If , then is a proposition generated by , and  If and are propositions generated by , then so are , , , and .    "
},
{
  "id": "hierarchy-examples",
  "level": "2",
  "url": "s-truth-tables.html#hierarchy-examples",
  "type": "Example",
  "number": "3.2.3",
  "title": "Examples of the Hierarchy of Logical Operations.",
  "body": "Examples of the Hierarchy of Logical Operations  A few shortened expressions and their fully parenthesized versions:    is .   is .   is .   is .    "
},
{
  "id": "exercises-3-2-2",
  "level": "2",
  "url": "s-truth-tables.html#exercises-3-2-2",
  "type": "Exercise",
  "number": "3.2.3.1",
  "title": "",
  "body": "Construct the truth tables of:                             "
},
{
  "id": "exercises-3-2-3",
  "level": "2",
  "url": "s-truth-tables.html#exercises-3-2-3",
  "type": "Exercise",
  "number": "3.2.3.2",
  "title": "",
  "body": "Construct the truth tables of:                      Truth Table for      0 0 1  0 1 1  1 0 1  1 1 0       Truth Table for      0 0 0  0 1 0  1 0 1  1 1 0       Truth Table for     0 0 0 0 0  0 0 1 0 0  0 1 0 0 0  0 1 1 0 0  1 0 0 0 0  1 0 1 0 0  1 1 0 1 0  1 1 1 1 1       Truth Table for     0 0 0 0  0 0 1 0  0 1 0 0  0 1 1 1  1 0 0 0  1 0 1 1  1 1 0 1  1 1 1 1    Compare this to part (a)   Truth Table for       0 0 1  0 1 1  1 0 1  1 1 0      Truth Table for     0 0 0 0 0  0 0 0 1 1  0 0 1 0 1  0 0 1 1 1  0 1 0 0 1  0 1 0 1 1  0 1 1 0 1  0 1 1 1 1  1 0 0 0 1  1 0 0 1 1  1 0 1 0 1  1 0 1 1 1  1 1 0 0 1  1 1 0 1 1  1 1 1 0 1  1 1 1 1 1     "
},
{
  "id": "exercises-3-2-4",
  "level": "2",
  "url": "s-truth-tables.html#exercises-3-2-4",
  "type": "Exercise",
  "number": "3.2.3.3",
  "title": "",
  "body": "Rewrite the following with as few extraneous parentheses as possible:              "
},
{
  "id": "exercises-3-2-5",
  "level": "2",
  "url": "s-truth-tables.html#exercises-3-2-5",
  "type": "Exercise",
  "number": "3.2.3.4",
  "title": "",
  "body": "In what order are the operations in the following propositions performed?              "
},
{
  "id": "exercises-3-2-6",
  "level": "2",
  "url": "s-truth-tables.html#exercises-3-2-6",
  "type": "Exercise",
  "number": "3.2.3.5",
  "title": "",
  "body": "Determine the number of rows in the truth table of a proposition containing four variables .  rows. "
},
{
  "id": "exercises-3-2-7",
  "level": "2",
  "url": "s-truth-tables.html#exercises-3-2-7",
  "type": "Exercise",
  "number": "3.2.3.6",
  "title": "",
  "body": "If there are 45 lines on a sheet of paper, and you want to reserve one line for each line in a truth table, how large could be if you can write truth tables of propositions generated by on the sheet of paper?  can be as large as 5. Five propositions require rows. With six propositions, you would need lines, which is too much.  "
},
{
  "id": "s-equivalence-implication",
  "level": "1",
  "url": "s-equivalence-implication.html",
  "type": "Section",
  "number": "3.3",
  "title": "Equivalence and Implication",
  "body": "Equivalence and Implication  Consider two propositions generated by and : and . At first glance, they are different propositions. In form, they are different, but they have the same meaning. One way to see this is to substitute actual propositions for and ; such as : I've been to Toronto; and : I've been to Chicago.  Then translates to I haven't been to both Toronto and Chicago, while is I haven't been to Toronto or I haven't been to Chicago. Determine the truth values of these propositions. Naturally, they will be true for some people and false for others. What is important is that no matter what truth values they have, and will have the same truth value. The easiest way to see this is by examining the truth tables of these propositions.   Truth Tables for and    0 0 1 1  0 1 1 1  1 0 1 1  1 1 0 0    In all four cases, and have the same truth value. Furthermore, when the biconditional operator is applied to them, the result is a value of true in all cases. A proposition such as this is called a tautology.   Tautologies and Contradictions  Tautology Tautology symbol for a tautology  An expression involving logical variables that is true in all cases is a tautology. The number 1 is used to symbolize a tautology.   Some Tautologies All of the following are tautologies because their truth tables consist of a column of 1's.   .         Contradiction Contradiction symbol for a contradiction An expression involving logical variables that is false for all cases is called a contradiction. The number 0 is used to symbolize a contradiction.  Some Contradictions  and are contradictions.    Equivalence  Equivalence Equivalence is logically equivalent to Let be a set of propositions and let and be propositions generated by . and are equivalent if and only if is a tautology. The equivalence of and is denoted .   Equivalence is to logic as equality is to algebra. Just as there are many ways of writing an algebraic expression, the same logical meaning can be expressed in many different ways.  Some Equivalences The following are all equivalences:    .   .     All tautologies are equivalent to one another.  An equivalence to .  All contradictions are equivalent to one another.  An equivalence to .   Implication  Consider the two propositions:    : The money is behind Door A; and  : The money is behind Door A or Door B.    Imagine that you were told that there is a large sum of money behind one of two doors marked A and B, and that one of the two propositions and is true and the other is false. Which door would you choose? All that you need to realize is that if is true, then will also be true. Since we know that this can't be the case, must be the true proposition and the money is behind Door B.  This is an example of a situation in which the truth of one proposition leads to the truth of another. Certainly, can be true when is false; but can't be true when is false. In this case, we say that implies .  Consider the truth table of , . If implies , then the third case can be ruled out, since it is the case that makes a conditional proposition false.  Implication  Implication  implies  Let be a set of propositions and let and be propositions generated by . We say that implies if is a tautology. We write to indicate this implication.   Disjunctive Addition A commonly used implication called disjunctive addition is , which is verified by truth table .   Truth Table to verify that      0 0 0 1  0 1 1 1  1 0 1 1  1 1 1 1    If we let represent The money is behind Door A and represent The money is behind Door B,  is a formalized version of the reasoning used in . A common name for this implication is disjunctive addition. In the next section we will consider some of the most commonly used implications and equivalences.  When we defined what we mean by a , we didn't include the conditional and biconditional operators. This was because of the two equivalences and . Therefore, any proposition that includes the conditional or biconditional operators can be written in an equivalent way using only conjunction, disjunction, and negation. We could even dispense with disjunction since is equivalent to a proposition that uses only conjunction and negation.   A Universal Operation  We close this section with a final logical operation, the Sheffer Stroke, that has the interesting property that all other logical operations can be created from it. You can explore this operation in  The Sheffer Stroke  Sheffer Stroke  the Sheffer Stroke of and  The Sheffer Stroke is the logical operator defined by the following truth table:   Truth Table for the Sheffer Stroke         0 0 1  0 1 1  1 0 1  1 1 0       Exercises  Given the following propositions generated by , , and , which are equivalent to one another?                        Construct the truth table for .  Give an example other than itself of a proposition generated by , , and that is equivalent to .  Give an example of a proposition other than that implies .  Give an example of a proposition other than that is implied by .    There are many valid answers to all be the first part of this exercises. We provide two of each.   Truth Table for     0 0 0 0  0 0 1 0  0 1 0 0  0 1 1 0  1 0 0 1  1 0 1 1  1 1 0 0  1 1 1 1    and both are equivalent to .  and both imply . So does any contradiction.   and both are implied by . So does any tautology.    Is an implication equivalent to its converse? Verify your answer using a truth table. No. In symbolic form the question is: Is ?  This table indicates that an implication is not always equivalent to its converse.   Suppose that is a proposition generated by , , and that is equivalent to . Write out the truth table for .   Truth Table for     0 0 0 1  0 0 1 1  0 1 0 0  0 1 1 0  1 0 0 1  1 0 1 1  1 1 0 1  1 1 1 1     How large is the largest set of propositions generated by and with the property that no two elements are equivalent? Let be any proposition generated by and . The truth table for has 4 rows and there are 2 choices for a truth value for for each row, so there are possible propositions.   Find a proposition that is equivalent to and uses only conjunction and negation. The simplest correct answer is .   Explain why a contradiction implies any proposition and any proposition implies a tautology.  and are tautologies.   The significance of the Sheffer Stroke is that it is a universal operation in that all other logical operations can be built from it.  Prove that is equivalent to .  Prove that .  Build using only the Sheffer Stroke.  Build using only the Sheffer Stroke.    Since the truth tables for and are equal, they are equivalent.  Prove that .      The simplest correct answer is .   Are the converse and inverse of a conditional proposition equivalent? Verify your answer using a truth table. Yes. In symbolic form the question is whether, if we have a conditional proposition , is ?    This table indicates that an converse is always equivalent to the inverse.    "
},
{
  "id": "tt33-1",
  "level": "2",
  "url": "s-equivalence-implication.html#tt33-1",
  "type": "Table",
  "number": "3.3.1",
  "title": "Truth Tables for <span xmlns:pf=\"https:\/\/prefigure.org\" class=\"process-math\">\\(\\neg (p \\land  q)\\)<\/span> and <span class=\"process-math\">\\(\\neg p \\lor  \\neg q\\)<\/span>",
  "body": " Truth Tables for and    0 0 1 1  0 1 1 1  1 0 1 1  1 1 0 0   "
},
{
  "id": "def-tautology",
  "level": "2",
  "url": "s-equivalence-implication.html#def-tautology",
  "type": "Definition",
  "number": "3.3.2",
  "title": "Tautology.",
  "body": "Tautology Tautology symbol for a tautology  An expression involving logical variables that is true in all cases is a tautology. The number 1 is used to symbolize a tautology.  "
},
{
  "id": "some-tautologies",
  "level": "2",
  "url": "s-equivalence-implication.html#some-tautologies",
  "type": "Example",
  "number": "3.3.3",
  "title": "Some Tautologies.",
  "body": "Some Tautologies All of the following are tautologies because their truth tables consist of a column of 1's.   .        "
},
{
  "id": "def-contradiction",
  "level": "2",
  "url": "s-equivalence-implication.html#def-contradiction",
  "type": "Definition",
  "number": "3.3.4",
  "title": "Contradiction.",
  "body": "Contradiction Contradiction symbol for a contradiction An expression involving logical variables that is false for all cases is called a contradiction. The number 0 is used to symbolize a contradiction. "
},
{
  "id": "some-contradictions",
  "level": "2",
  "url": "s-equivalence-implication.html#some-contradictions",
  "type": "Example",
  "number": "3.3.5",
  "title": "Some Contradictions.",
  "body": "Some Contradictions  and are contradictions.  "
},
{
  "id": "def-equivalence",
  "level": "2",
  "url": "s-equivalence-implication.html#def-equivalence",
  "type": "Definition",
  "number": "3.3.6",
  "title": "Equivalence.",
  "body": "Equivalence Equivalence is logically equivalent to Let be a set of propositions and let and be propositions generated by . and are equivalent if and only if is a tautology. The equivalence of and is denoted .  "
},
{
  "id": "ex-some-equivalences",
  "level": "2",
  "url": "s-equivalence-implication.html#ex-some-equivalences",
  "type": "Example",
  "number": "3.3.7",
  "title": "Some Equivalences.",
  "body": "Some Equivalences The following are all equivalences:    .   .    "
},
{
  "id": "equivalence_to_1",
  "level": "2",
  "url": "s-equivalence-implication.html#equivalence_to_1",
  "type": "Example",
  "number": "3.3.8",
  "title": "An equivalence to <span xmlns:pf=\"https:\/\/prefigure.org\" class=\"process-math\">\\(1\\)<\/span>.",
  "body": "An equivalence to . "
},
{
  "id": "equivalence_to_0",
  "level": "2",
  "url": "s-equivalence-implication.html#equivalence_to_0",
  "type": "Example",
  "number": "3.3.9",
  "title": "An equivalence to <span xmlns:pf=\"https:\/\/prefigure.org\" class=\"process-math\">\\(0\\)<\/span>.",
  "body": "An equivalence to . "
},
{
  "id": "s-equivalence-implication-5-3",
  "level": "2",
  "url": "s-equivalence-implication.html#s-equivalence-implication-5-3",
  "type": "Table",
  "number": "3.3.10",
  "title": "",
  "body": "  : The money is behind Door A; and  : The money is behind Door A or Door B.   "
},
{
  "id": "def-implication",
  "level": "2",
  "url": "s-equivalence-implication.html#def-implication",
  "type": "Definition",
  "number": "3.3.11",
  "title": "Implication.",
  "body": "Implication  Implication  implies  Let be a set of propositions and let and be propositions generated by . We say that implies if is a tautology. We write to indicate this implication.  "
},
{
  "id": "ex-disjunctive-addition",
  "level": "2",
  "url": "s-equivalence-implication.html#ex-disjunctive-addition",
  "type": "Example",
  "number": "3.3.12",
  "title": "Disjunctive Addition.",
  "body": "Disjunctive Addition A commonly used implication called disjunctive addition is , which is verified by truth table . "
},
{
  "id": "tt-disjunctive-addition",
  "level": "2",
  "url": "s-equivalence-implication.html#tt-disjunctive-addition",
  "type": "Table",
  "number": "3.3.13",
  "title": "Truth Table to verify that <span xmlns:pf=\"https:\/\/prefigure.org\" class=\"process-math\">\\(p \\Rightarrow (p \\lor  q)\\)<\/span>",
  "body": " Truth Table to verify that      0 0 0 1  0 1 1 1  1 0 1 1  1 1 1 1   "
},
{
  "id": "def-scheffer",
  "level": "2",
  "url": "s-equivalence-implication.html#def-scheffer",
  "type": "Definition",
  "number": "3.3.14",
  "title": "The Sheffer Stroke.",
  "body": "The Sheffer Stroke  Sheffer Stroke  the Sheffer Stroke of and  The Sheffer Stroke is the logical operator defined by the following truth table:   Truth Table for the Sheffer Stroke         0 0 1  0 1 1  1 0 1  1 1 0    "
},
{
  "id": "exercises-3-3-2",
  "level": "2",
  "url": "s-equivalence-implication.html#exercises-3-3-2",
  "type": "Exercise",
  "number": "3.3.5.1",
  "title": "",
  "body": "Given the following propositions generated by , , and , which are equivalent to one another?                     "
},
{
  "id": "exercises-3-3-3",
  "level": "2",
  "url": "s-equivalence-implication.html#exercises-3-3-3",
  "type": "Exercise",
  "number": "3.3.5.2",
  "title": "",
  "body": "  Construct the truth table for .  Give an example other than itself of a proposition generated by , , and that is equivalent to .  Give an example of a proposition other than that implies .  Give an example of a proposition other than that is implied by .    There are many valid answers to all be the first part of this exercises. We provide two of each.   Truth Table for     0 0 0 0  0 0 1 0  0 1 0 0  0 1 1 0  1 0 0 1  1 0 1 1  1 1 0 0  1 1 1 1    and both are equivalent to .  and both imply . So does any contradiction.   and both are implied by . So does any tautology.   "
},
{
  "id": "exercises-3-3-4",
  "level": "2",
  "url": "s-equivalence-implication.html#exercises-3-3-4",
  "type": "Exercise",
  "number": "3.3.5.3",
  "title": "",
  "body": "Is an implication equivalent to its converse? Verify your answer using a truth table. No. In symbolic form the question is: Is ?  This table indicates that an implication is not always equivalent to its converse.  "
},
{
  "id": "exercises-3-3-5",
  "level": "2",
  "url": "s-equivalence-implication.html#exercises-3-3-5",
  "type": "Exercise",
  "number": "3.3.5.4",
  "title": "",
  "body": "Suppose that is a proposition generated by , , and that is equivalent to . Write out the truth table for .   Truth Table for     0 0 0 1  0 0 1 1  0 1 0 0  0 1 1 0  1 0 0 1  1 0 1 1  1 1 0 1  1 1 1 1    "
},
{
  "id": "exercises-3-3-6",
  "level": "2",
  "url": "s-equivalence-implication.html#exercises-3-3-6",
  "type": "Exercise",
  "number": "3.3.5.5",
  "title": "",
  "body": "How large is the largest set of propositions generated by and with the property that no two elements are equivalent? Let be any proposition generated by and . The truth table for has 4 rows and there are 2 choices for a truth value for for each row, so there are possible propositions.  "
},
{
  "id": "exercises-3-3-7",
  "level": "2",
  "url": "s-equivalence-implication.html#exercises-3-3-7",
  "type": "Exercise",
  "number": "3.3.5.6",
  "title": "",
  "body": "Find a proposition that is equivalent to and uses only conjunction and negation. The simplest correct answer is .  "
},
{
  "id": "exercises-3-3-8",
  "level": "2",
  "url": "s-equivalence-implication.html#exercises-3-3-8",
  "type": "Exercise",
  "number": "3.3.5.7",
  "title": "",
  "body": "Explain why a contradiction implies any proposition and any proposition implies a tautology.  and are tautologies. "
},
{
  "id": "ex-sheffer",
  "level": "2",
  "url": "s-equivalence-implication.html#ex-sheffer",
  "type": "Exercise",
  "number": "3.3.5.8",
  "title": "",
  "body": " The significance of the Sheffer Stroke is that it is a universal operation in that all other logical operations can be built from it.  Prove that is equivalent to .  Prove that .  Build using only the Sheffer Stroke.  Build using only the Sheffer Stroke.    Since the truth tables for and are equal, they are equivalent.  Prove that .      The simplest correct answer is .  "
},
{
  "id": "exercises-3-3-10",
  "level": "2",
  "url": "s-equivalence-implication.html#exercises-3-3-10",
  "type": "Exercise",
  "number": "3.3.5.9",
  "title": "",
  "body": "Are the converse and inverse of a conditional proposition equivalent? Verify your answer using a truth table. Yes. In symbolic form the question is whether, if we have a conditional proposition , is ?    This table indicates that an converse is always equivalent to the inverse.  "
},
{
  "id": "s-logic-laws",
  "level": "1",
  "url": "s-logic-laws.html",
  "type": "Section",
  "number": "3.4",
  "title": "The Laws of Logic",
  "body": "The Laws of Logic   In this section, we will list the most basic equivalences and implications of logic. Most of the equivalences listed in Table should be obvious to the reader. Remember, 0 stands for contradiction, 1 for tautology. Many logical laws are similar to algebraic laws. For example, there is a logical law corresponding to the associative law of addition, . In fact, associativity of both conjunction and disjunction are among the laws of logic. Notice that with one exception, the laws are paired in such a way that exchanging the symbols , , 1 and 0 for , , 0, and 1, respectively, in any law gives you a second law. For example, results in . This is called a duality principle . For now, think of it as a way of remembering two laws for the price of one. We will leave it to the reader to verify a few of these laws with truth tables. However, the reader should be careful in applying duality to the conditional operator and implication since the dual involves taking the converse. For example, the dual of is , which is usually written .  Verification of an Identity Law The Identity Law can be verified with this truth table. The fact that is a tautology serves as a valid proof.   Truth table to demonstrate the identity law for conjunction.         0 1 0 1  1 1 1 1     Some of the logical laws in Table might be less obvious to you. For any that you are not comfortable with, substitute actual propositions for the logical variables. For example, if is John owns a pet store and is John likes pets, the detachment law should make sense.   Basic Logical Laws - Equivalences     Commutative Laws              Associative Laws       (     Distributive Laws            Identity Laws            Negation Laws            Idempotent Laws             Null Laws             Absorption Laws           DeMorgan's Laws           Involution Law             Basic Logical Laws - Common Implications and Equivalences  Modus Ponens see Detachment  Modus Tollens see Indirect Reasoning  Detachment  Indirect Reasoning   Detachment (AKA Modus Ponens)   Indirect Reasoning (AKA Modus Tollens)   Disjunctive Addition   Conjunctive Simplification  and  Disjunctive Simplification  and  Chain Rule    Conditional Equivalence   Biconditional Equivalences   Contrapositive       Exercises  Write the following in symbolic notation and determine whether it is a tautology: If I study then I will learn. I will not learn. Therefore, I do not study.  Let , The argument is: call the argument . .  Since is a tautology, the argument is valid.   Show that the common fallacy is not a law of logic. The following truth table shows that is not a tautology, and so the implication is not true.    Describe, in general, how duality can be applied to implications if we introduce the relation , read is implied by. We define this relation by . In any true statement , replace; with , with , 0 with 1, 1 with 0, with , and with . Leave all other connectives unchanged.   Write the dual of the following statements:           "
},
{
  "id": "ex-identity-and",
  "level": "2",
  "url": "s-logic-laws.html#ex-identity-and",
  "type": "Example",
  "number": "3.4.1",
  "title": "Verification of an Identity Law.",
  "body": "Verification of an Identity Law The Identity Law can be verified with this truth table. The fact that is a tautology serves as a valid proof.   Truth table to demonstrate the identity law for conjunction.         0 1 0 1  1 1 1 1    "
},
{
  "id": "table-equivalences",
  "level": "2",
  "url": "s-logic-laws.html#table-equivalences",
  "type": "Table",
  "number": "3.4.3",
  "title": "Basic Logical Laws - Equivalences",
  "body": " Basic Logical Laws - Equivalences     Commutative Laws              Associative Laws       (     Distributive Laws            Identity Laws            Negation Laws            Idempotent Laws             Null Laws             Absorption Laws           DeMorgan's Laws           Involution Law           "
},
{
  "id": "table-implications",
  "level": "2",
  "url": "s-logic-laws.html#table-implications",
  "type": "Table",
  "number": "3.4.4",
  "title": "Basic Logical Laws - Common Implications and Equivalences",
  "body": " Basic Logical Laws - Common Implications and Equivalences  Modus Ponens see Detachment  Modus Tollens see Indirect Reasoning  Detachment  Indirect Reasoning   Detachment (AKA Modus Ponens)   Indirect Reasoning (AKA Modus Tollens)   Disjunctive Addition   Conjunctive Simplification  and  Disjunctive Simplification  and  Chain Rule    Conditional Equivalence   Biconditional Equivalences   Contrapositive    "
},
{
  "id": "exercises-3-4-2",
  "level": "2",
  "url": "s-logic-laws.html#exercises-3-4-2",
  "type": "Exercise",
  "number": "3.4.2.1",
  "title": "",
  "body": "Write the following in symbolic notation and determine whether it is a tautology: If I study then I will learn. I will not learn. Therefore, I do not study.  Let , The argument is: call the argument . .  Since is a tautology, the argument is valid.  "
},
{
  "id": "exercises-3-4-3",
  "level": "2",
  "url": "s-logic-laws.html#exercises-3-4-3",
  "type": "Exercise",
  "number": "3.4.2.2",
  "title": "",
  "body": "Show that the common fallacy is not a law of logic. The following truth table shows that is not a tautology, and so the implication is not true.   "
},
{
  "id": "exercises-3-4-4",
  "level": "2",
  "url": "s-logic-laws.html#exercises-3-4-4",
  "type": "Exercise",
  "number": "3.4.2.3",
  "title": "",
  "body": "Describe, in general, how duality can be applied to implications if we introduce the relation , read is implied by. We define this relation by . In any true statement , replace; with , with , 0 with 1, 1 with 0, with , and with . Leave all other connectives unchanged.  "
},
{
  "id": "exercises-3-4-5",
  "level": "2",
  "url": "s-logic-laws.html#exercises-3-4-5",
  "type": "Exercise",
  "number": "3.4.2.4",
  "title": "",
  "body": "Write the dual of the following statements:         "
},
{
  "id": "s-math-systems",
  "level": "1",
  "url": "s-math-systems.html",
  "type": "Section",
  "number": "3.5",
  "title": "Mathematical Systems and Proofs",
  "body": "Mathematical Systems and Proofs  Mathematical Systems  In this section, we present an overview of what a mathematical system is and how logic plays an important role in one. The axiomatic method that we will use here will not be duplicated with as much formality anywhere else in the book, but we hope an emphasis on how mathematical facts are developed and organized will help to unify the concepts we will present. The system of propositions and logical operators we have developed will serve as a model for our discussion. Roughly, a mathematical system can be defined as follows.  Mathematical System A mathematical system consists of:   A set or universe, .  Definitions: sentences that explain the meaning of concepts that relate to the universe. Any term used in describing the universe itself is said to be undefined. All definitions are given in terms of these undefined concepts of objects.  Axioms: assertions about the properties of the universe and rules for creating and justifying more assertions. These rules always include the system of logic that we have developed to this point.  Theorems: the additional assertions mentioned above.    Euclidean Geometry In Euclidean geometry the universe consists of points and lines (two undefined terms). Among the definitions is a definition of parallel lines and among the axioms is the axiom that two distinct parallel lines never meet.   Propositional Calculus Propositional calculus is a formal name for the logical system that we've been discussing. The universe consists of propositions. The axioms are the truth tables for the logical operators and the key definitions are those of equivalence and implication. We use propositions to describe any other mathematical system; therefore, this is the minimum amount of structure that a mathematical system can have.   Theorem  A true proposition derived from the axioms of a mathematical system is called a theorem.  Theorems are normally expressed in terms of a finite number of propositions, , called the premises , and a proposition, , called the conclusion . These theorems take the form or more informally, For a theorem of this type, we say that the premises imply the conclusion. When a theorem is stated, it is assumed that the axioms of the system are true. In addition, any previously proven theorem can be considered an extension of the axioms and can be used in demonstrating that the new theorem is true. When the proof is complete, the new theorem can be used to prove subsequent theorems. A mathematical system can be visualized as an inverted pyramid with the axioms at the base and the theorems expanding out in various directions.   The body of knowledge in a mathematical system   Inverted pyramid of knowledge with the axioms at the narrow bottom, then early theorems at the next level up, and then new theorems in a wider area. Research is above and outside the pyramid.     Proof A proof of a theorem is a finite sequence of logically valid steps that demonstrate that the premises of a theorem imply its conclusion.   Exactly what constitutes a proof is not always clear. For example, a research mathematician might require only a few steps to prove a theorem to a collegue, but might take an hour to give an effective proof to a class of students. Therefore, what constitutes a proof often depends on the audience. But the audience is not the only factor. One of the most famous theorems in graph theory, , was proven in 1976, after over a century of effort by many mathematicians. Part of the proof consisted of having a computer check many different graphs for a certain property. Without the aid of the computer, this checking would have taken years. In the eyes of some mathematicians, this proof was considered questionable. Shorter proofs have been developed since 1976 and there is no controversy associated with The Four Color Theorem at this time.   Direct Proof  Theoretically, you can prove anything in propositional calculus with truth tables. In fact, the laws of logic stated in Section 3.4 are all theorems. Propositional calculus is one of the few mathematical systems for which any valid sentence can be determined true or false by mechanical means. A program to write truth tables is not too difficult to write; however, what can be done theoretically is not always practical. For example, is a theorem in propositional calculus. However, suppose that you wrote such a program and you had it write the truth table for The truth table will have cases. At one million cases per second, it would take approximately one hour to verify the theorem. Now if you decided to check a similar theorem, you would really have time trouble. There would be cases to check in the truth table. At one million cases per second it would take approximately days to check all cases. For most of the remainder of this section, we will discuss an alternate method for proving theorems in propositional calculus. It is the same method that we will use in a less formal way for proofs in other systems. Formal axiomatic methods would be too unwieldy to actually use in later sections. However, none of the theorems in later chapters would be stated if they couldn't be proven by the axiomatic method.  We will introduce two types of proof here, direct and indirect.  A typical direct proof Direct proof This is a theorem: . A direct proof of this theorem is:   Direct proof of   Step Proposition Justification  1. Premise  2. (1), conditional rule  3. Premise  4. (2), (3), chain rule  5. (4), contrapositive  6. Premise  7. (5), (6), chain rule  8. (7), conditional rule     Symbol that denotes the end of a proof. Can be replaced with QED Note that marks the end of a proof.  illustrates the usual method of formal proof in a formal mathematical system. The rules governing these proofs are:   A proof must end in a finite number of steps.  Each step must be either a premise or a proposition that is implied from previous steps using any valid equivalence or implication.  For a direct proof, the last step must be the conclusion of the theorem. For an indirect proof (see below), the last step must be a contradiction.  Justification Column. The column labeled justification is analogous to the comments that appear in most good computer programs. They simply make the proof more readable.   Two proofs of the same theorem Here are two direct proofs of :   Direct proof of   1. Premise  2. Premise  3. Disjunctive simplification, (1), (2)  4. Premise  5. Disjunctive simplification, (3), (4).    You are invited to justify the steps in this second proof:   Alternate proof of   1.  2.  3.  4.  5.  6.  7.  8.      The conclusion of a theorem is often a conditional proposition. The condition of the conclusion can be included as a premise in the proof of the theorem. The object of the proof is then to prove the consequence of the conclusion. This rule is justified by the logical law   Example of a proof with a conditional conclusion  The following proof of includes as a fourth premise. Inference of truth of completes the proof.   Proof of a theorem with a conditional conclusion.   1. Premise  2. Added premise  3. (1), (2), disjunction simplification  4. Premise  5. (3), (4), detachment  6. Premise  7. (5), (6), detachment.       Indirect Proof  Consider a theorem , where represents , the premises. The method of indirect proof Indirect proof is based on the equivalence . In words, this logical law states that if , then is always false; that is, is a contradiction. This means that a valid method of proof is to negate the conclusion of a theorem and add this negation to the premises. If a contradiction can be implied from this set of propositions, the proof is complete. For the proofs in this section, a contradiction will often take the form .  For proofs involving numbers, a contradiction might be or . Indirect proofs involving sets might conclude with or ( and ). Indirect proofs are often more convenient than direct proofs in certain situations. Indirect proofs are often called proofs by contradiction .  An Indirect Proof  Here is an example of an indirect proof of the theorem in .   An Indirect proof of   1. Negated conclusion  2. DeMorgan's Law, (1)  3. Conjunctive simplification, (2)  4. Premise  5. Indirect reasoning, (3), (4)  6. Conjunctive simplification, (2)  7. Premise  8. Indirect reasoning, (6), (7)  9. Conjunctive, (5), (8)  10. DeMorgan's Law, (9)  11. Premise  12. (10), (11)     Proof Style  The rules allow you to list the premises of a theorem immediately; however, a proof is much easier to follow if the premises are only listed when they are needed.  Yet Another Indirect Proof  Here is an indirect proof of .   Indirect proof of   1. Negation of the conclusion  2. Premise  3. (1), (2), detachment  4. (3), disjunctive addition  5. Premise  6. (4), (5)     As we mentioned at the outset of this section, we are only presenting an overview of what a mathematical system is. For greater detail on axiomatic theories, see Stoll (1961). An excellent description of how propositional calculus plays a part in artificial intelligence is contained in Hofstadter (1980). If you enjoy the challenge of constructing proofs in propositional calculus, you should enjoy the game WFF'N PROOF (1962), by L.E. Allen.    Exercises  Prove with truth tables:                Prove with truth tables:         Since is a tautology, the implication is true.    Since is a tautology, the equivalence is true.    Give direct and indirect proofs of:   .   .  .   .       Direct proof:                          Indirect proof:    Negated conclusion   Premise   Indirect Reasoning (1), (2)   Premise   Indirect Reasoning (1), (4)   Conjunctive (3), (5)   DeMorgan's law (6)   Premise   Indirect Reasoning (7), (8)   Premise   (9), (10)    Direct proof:                      Use         Indirect proof:                                Direct proof:    Premise   Added premise (conditional conclusion)   Involution (2)   Disjunctive simplification (1), (3)   Premise   Detachment (4), (5)   Premise   Detachment (6), (7)    Indirect proof:    Negated conclusion   Conditional equivalence (1)   DeMorgan (2)   Conjunctive simplification (3)   Premise   Conditional equivalence (5)   Detachment (4), (6)   Premise   Detachment (7), (8)   Premise   Detachment (9), (10)   Conjunctive simplification (3)   Conjunction (11), (12)     Direct proof:                      Indirect proof:    Negated conclusion   Premise   (1), (2)   Premise   Detachment (3), (4)   Premise   Detachment (5), (6)  (1), (7)      Give direct and indirect proofs of:    .   .   .      Direct proof:   Premise   Premise   Detachment (1),(2)   Premise   Indirect Reasoning (4),(5)     Indirect proof:   Negated Conclusion   Premise   Detachment (1),(2)   Involution (3)   Premise   Indirect Reasoning (4),(5)   Premise   Contradiction     Direct proof:   Premise   Conjunctive Simplification (1)   Premise   Involution (2)   Indirect Reasoning (4),(5)   Premise   Disjunctive Simplification (5),(6)      Are the following arguments valid? If they are valid, construct formal proofs; if they aren't valid, explain why not.   If wages increase, then there will be inflation. The cost of living will not increase if there is no inflation. Wages will increase. Therefore, the cost of living will increase.  If the races are fixed or the casinos are crooked, then the tourist trade will decline. If the tourist trade decreases, then the police will be happy. The police force is never happy. Therefore, the races are not fixed.    Let stand for Wages will increase,  stand for there will be inflation, and stand for cost of living will increase. Therefore the argument is: . The argument is invalid. The easiest way to see this is through a truth table, which has one case, the seventh, that this false. Let be the conjunction of all premises.   Let stand for the races are fixed,  stand for casinos are crooked,  stand for the tourist trade will decline, and stand for the police will be happy. Therefore, the argument is: . The argument is valid. Proof:    Premise   Premise   Indirect Reasoning (1), (2)   Premise   Indirect Reasoning (3), (4)   DeMorgan (5)   Conjunction simplification     My salad contains beets and okra. If my salad contains cottage cheese then it doesn't contain beets. My salad contains cottage cheese or hummus. Therefore, my salad contains hummus. This logical argument is identical to exercise 4(c), where , , and are that my salad contains cottage cheese, hummus, beets and okra, respectively.   Describe how could be proved in 199 steps.  and implies . It takes two steps to get to from This means it takes steps to get to (subtract 1 because is stated as a premise). A final step is needed to apply detachment to imply   "
},
{
  "id": "def-mathematical-system",
  "level": "2",
  "url": "s-math-systems.html#def-mathematical-system",
  "type": "Definition",
  "number": "3.5.1",
  "title": "Mathematical System.",
  "body": "Mathematical System A mathematical system consists of:   A set or universe, .  Definitions: sentences that explain the meaning of concepts that relate to the universe. Any term used in describing the universe itself is said to be undefined. All definitions are given in terms of these undefined concepts of objects.  Axioms: assertions about the properties of the universe and rules for creating and justifying more assertions. These rules always include the system of logic that we have developed to this point.  Theorems: the additional assertions mentioned above.   "
},
{
  "id": "ex-euclidean-geometry",
  "level": "2",
  "url": "s-math-systems.html#ex-euclidean-geometry",
  "type": "Example",
  "number": "3.5.2",
  "title": "Euclidean Geometry.",
  "body": "Euclidean Geometry In Euclidean geometry the universe consists of points and lines (two undefined terms). Among the definitions is a definition of parallel lines and among the axioms is the axiom that two distinct parallel lines never meet.  "
},
{
  "id": "ex-propositional-calculus",
  "level": "2",
  "url": "s-math-systems.html#ex-propositional-calculus",
  "type": "Example",
  "number": "3.5.3",
  "title": "Propositional Calculus.",
  "body": "Propositional Calculus Propositional calculus is a formal name for the logical system that we've been discussing. The universe consists of propositions. The axioms are the truth tables for the logical operators and the key definitions are those of equivalence and implication. We use propositions to describe any other mathematical system; therefore, this is the minimum amount of structure that a mathematical system can have.  "
},
{
  "id": "def-theorem",
  "level": "2",
  "url": "s-math-systems.html#def-theorem",
  "type": "Definition",
  "number": "3.5.4",
  "title": "Theorem.",
  "body": "Theorem  A true proposition derived from the axioms of a mathematical system is called a theorem. "
},
{
  "id": "knowledge-pyramid",
  "level": "2",
  "url": "s-math-systems.html#knowledge-pyramid",
  "type": "Figure",
  "number": "3.5.5",
  "title": "",
  "body": " The body of knowledge in a mathematical system   Inverted pyramid of knowledge with the axioms at the narrow bottom, then early theorems at the next level up, and then new theorems in a wider area. Research is above and outside the pyramid.    "
},
{
  "id": "def-proof",
  "level": "2",
  "url": "s-math-systems.html#def-proof",
  "type": "Definition",
  "number": "3.5.6",
  "title": "Proof.",
  "body": "Proof A proof of a theorem is a finite sequence of logically valid steps that demonstrate that the premises of a theorem imply its conclusion.  "
},
{
  "id": "proof-3-5-1",
  "level": "2",
  "url": "s-math-systems.html#proof-3-5-1",
  "type": "Example",
  "number": "3.5.7",
  "title": "A typical direct proof.",
  "body": "A typical direct proof Direct proof This is a theorem: . A direct proof of this theorem is:   Direct proof of   Step Proposition Justification  1. Premise  2. (1), conditional rule  3. Premise  4. (2), (3), chain rule  5. (4), contrapositive  6. Premise  7. (5), (6), chain rule  8. (7), conditional rule    "
},
{
  "id": "proof-3-5-2",
  "level": "2",
  "url": "s-math-systems.html#proof-3-5-2",
  "type": "Example",
  "number": "3.5.9",
  "title": "Two proofs of the same theorem.",
  "body": "Two proofs of the same theorem Here are two direct proofs of :   Direct proof of   1. Premise  2. Premise  3. Disjunctive simplification, (1), (2)  4. Premise  5. Disjunctive simplification, (3), (4).    You are invited to justify the steps in this second proof:   Alternate proof of   1.  2.  3.  4.  5.  6.  7.  8.     "
},
{
  "id": "ex-conditinal-conclusion",
  "level": "2",
  "url": "s-math-systems.html#ex-conditinal-conclusion",
  "type": "Example",
  "number": "3.5.12",
  "title": "Example of a proof with a conditional conclusion.",
  "body": "Example of a proof with a conditional conclusion  The following proof of includes as a fourth premise. Inference of truth of completes the proof.   Proof of a theorem with a conditional conclusion.   1. Premise  2. Added premise  3. (1), (2), disjunction simplification  4. Premise  5. (3), (4), detachment  6. Premise  7. (5), (6), detachment.     "
},
{
  "id": "sub-indirect-proof-2",
  "level": "2",
  "url": "s-math-systems.html#sub-indirect-proof-2",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "indirect proof "
},
{
  "id": "ex-indirect_proof_1",
  "level": "2",
  "url": "s-math-systems.html#ex-indirect_proof_1",
  "type": "Example",
  "number": "3.5.14",
  "title": "An Indirect Proof.",
  "body": "An Indirect Proof  Here is an example of an indirect proof of the theorem in .   An Indirect proof of   1. Negated conclusion  2. DeMorgan's Law, (1)  3. Conjunctive simplification, (2)  4. Premise  5. Indirect reasoning, (3), (4)  6. Conjunctive simplification, (2)  7. Premise  8. Indirect reasoning, (6), (7)  9. Conjunctive, (5), (8)  10. DeMorgan's Law, (9)  11. Premise  12. (10), (11)    "
},
{
  "id": "sub-indirect-proof-5",
  "level": "2",
  "url": "s-math-systems.html#sub-indirect-proof-5",
  "type": "Note",
  "number": "3.5.16",
  "title": "Proof Style.",
  "body": "Proof Style  The rules allow you to list the premises of a theorem immediately; however, a proof is much easier to follow if the premises are only listed when they are needed. "
},
{
  "id": "proof-yet-another",
  "level": "2",
  "url": "s-math-systems.html#proof-yet-another",
  "type": "Example",
  "number": "3.5.17",
  "title": "Yet Another Indirect Proof.",
  "body": "Yet Another Indirect Proof  Here is an indirect proof of .   Indirect proof of   1. Negation of the conclusion  2. Premise  3. (1), (2), detachment  4. (3), disjunctive addition  5. Premise  6. (4), (5)    "
},
{
  "id": "exercises-3-5-2",
  "level": "2",
  "url": "s-math-systems.html#exercises-3-5-2",
  "type": "Exercise",
  "number": "3.5.4.1",
  "title": "",
  "body": "Prove with truth tables:               "
},
{
  "id": "exercises-3-5-3",
  "level": "2",
  "url": "s-math-systems.html#exercises-3-5-3",
  "type": "Exercise",
  "number": "3.5.4.2",
  "title": "",
  "body": "Prove with truth tables:         Since is a tautology, the implication is true.    Since is a tautology, the equivalence is true.   "
},
{
  "id": "exercises-3-5-4",
  "level": "2",
  "url": "s-math-systems.html#exercises-3-5-4",
  "type": "Exercise",
  "number": "3.5.4.3",
  "title": "",
  "body": "Give direct and indirect proofs of:   .   .  .   .       Direct proof:                          Indirect proof:    Negated conclusion   Premise   Indirect Reasoning (1), (2)   Premise   Indirect Reasoning (1), (4)   Conjunctive (3), (5)   DeMorgan's law (6)   Premise   Indirect Reasoning (7), (8)   Premise   (9), (10)    Direct proof:                      Use         Indirect proof:                                Direct proof:    Premise   Added premise (conditional conclusion)   Involution (2)   Disjunctive simplification (1), (3)   Premise   Detachment (4), (5)   Premise   Detachment (6), (7)    Indirect proof:    Negated conclusion   Conditional equivalence (1)   DeMorgan (2)   Conjunctive simplification (3)   Premise   Conditional equivalence (5)   Detachment (4), (6)   Premise   Detachment (7), (8)   Premise   Detachment (9), (10)   Conjunctive simplification (3)   Conjunction (11), (12)     Direct proof:                      Indirect proof:    Negated conclusion   Premise   (1), (2)   Premise   Detachment (3), (4)   Premise   Detachment (5), (6)  (1), (7)     "
},
{
  "id": "exercises-3-5-5",
  "level": "2",
  "url": "s-math-systems.html#exercises-3-5-5",
  "type": "Exercise",
  "number": "3.5.4.4",
  "title": "",
  "body": "Give direct and indirect proofs of:    .   .   .      Direct proof:   Premise   Premise   Detachment (1),(2)   Premise   Indirect Reasoning (4),(5)     Indirect proof:   Negated Conclusion   Premise   Detachment (1),(2)   Involution (3)   Premise   Indirect Reasoning (4),(5)   Premise   Contradiction     Direct proof:   Premise   Conjunctive Simplification (1)   Premise   Involution (2)   Indirect Reasoning (4),(5)   Premise   Disjunctive Simplification (5),(6)     "
},
{
  "id": "exercises-3-5-6",
  "level": "2",
  "url": "s-math-systems.html#exercises-3-5-6",
  "type": "Exercise",
  "number": "3.5.4.5",
  "title": "",
  "body": "Are the following arguments valid? If they are valid, construct formal proofs; if they aren't valid, explain why not.   If wages increase, then there will be inflation. The cost of living will not increase if there is no inflation. Wages will increase. Therefore, the cost of living will increase.  If the races are fixed or the casinos are crooked, then the tourist trade will decline. If the tourist trade decreases, then the police will be happy. The police force is never happy. Therefore, the races are not fixed.    Let stand for Wages will increase,  stand for there will be inflation, and stand for cost of living will increase. Therefore the argument is: . The argument is invalid. The easiest way to see this is through a truth table, which has one case, the seventh, that this false. Let be the conjunction of all premises.   Let stand for the races are fixed,  stand for casinos are crooked,  stand for the tourist trade will decline, and stand for the police will be happy. Therefore, the argument is: . The argument is valid. Proof:    Premise   Premise   Indirect Reasoning (1), (2)   Premise   Indirect Reasoning (3), (4)   DeMorgan (5)   Conjunction simplification    "
},
{
  "id": "exercises-3-5-7",
  "level": "2",
  "url": "s-math-systems.html#exercises-3-5-7",
  "type": "Exercise",
  "number": "3.5.4.6",
  "title": "",
  "body": "My salad contains beets and okra. If my salad contains cottage cheese then it doesn't contain beets. My salad contains cottage cheese or hummus. Therefore, my salad contains hummus. This logical argument is identical to exercise 4(c), where , , and are that my salad contains cottage cheese, hummus, beets and okra, respectively.  "
},
{
  "id": "exercises-3-5-8",
  "level": "2",
  "url": "s-math-systems.html#exercises-3-5-8",
  "type": "Exercise",
  "number": "3.5.4.7",
  "title": "",
  "body": "Describe how could be proved in 199 steps.  and implies . It takes two steps to get to from This means it takes steps to get to (subtract 1 because is stated as a premise). A final step is needed to apply detachment to imply "
},
{
  "id": "s-propositions-over-universe",
  "level": "1",
  "url": "s-propositions-over-universe.html",
  "type": "Section",
  "number": "3.6",
  "title": "Propositions over a Universe",
  "body": "Propositions over a Universe  Propositions over a Universe  Consider the sentence He was a member of the Boston Red Sox. There is no way that we can assign a truth value to this sentence unless he is specified. For that reason, we would not consider it a proposition. However, he can be considered a variable that holds a place for any name. We might want to restrict the value of he to all names in the major-league baseball record books. If that is the case, we say that the sentence is a proposition over the set of major-league baseball players, past and present.  Proposition over a Universe  Let be a nonempty set. A proposition over is a sentence that contains a variable that can take on any value in and that has a definite truth value as a result of any such substitution.   Predicate A proposition over a universe is also referred to as a predicate.  Some propositions over a variety of universes    A few propositions over the integers are , , and is a multiple of 3.  A few propositions over the rational numbers are , , and .  A few propositions over the subsets of are , , and .     All of the laws of logic that we listed in Section 3.4 are valid for propositions over a universe. For example, if and are propositions over the integers, we can be certain that , because is a tautology and is true no matter what values the variables in and are given. If we specify and to be and , we can also say that implies . This is not a usual implication, but for the propositions under discussion, it is true. One way of describing this situation in general is with truth sets.   Truth Sets  Truth Set Truth Set  the truth set of  If is a proposition over , the truth set of is .   Truth Set Example The truth set of the proposition , taken as a proposition over the power set of is .  Truth sets depend on the universe Over the universe (the integers), the truth set of is . If the universe is expanded to the rational numbers, the truth set becomes . The term solution set is often used for the truth set of an equation such as the one in this example.  Tautologies and Contradictions over a Universe A proposition over is a tautology if its truth set is . It is a contradiction if its truth set is empty.    Tautology, Contradiction over  is a tautology over the rational numbers. is a contradiction over the rationals.  The truth sets of compound propositions can be expressed in terms of the truth sets of simple propositions. For example, if if and only if makes true. This is true if and only if makes both and true, which, in turn, is true if and only if . This explains why the truth set of the conjunction of two propositions equals the intersection of the truth sets of the two propositions. The following list summarizes the connection between compound and simple truth sets   Truth Sets of Compound Statements          Equivalence of propositions over a universe Two propositions, and , are equivalent if is a tautology. In terms of truth sets, this means that and are equivalent if .  Some pairs of equivalent propositions    and are equivalent propositions over the integers.   and are equivalent propositions over the power set of the natural numbers.    Implication for propositions over a universe If and are propositions over , implies if is a tautology.  Since the truth set of is , the Venn diagram for in shows that when .   Venn Diagram for    A two set Venn Diagram for     Examples of Implications   Over the natural numbers: since  Over the power set of the integers: implies  Over the power set of the integers,      Exercises  If , what are the truth sets of the following propositions?    .   and .   .   is a proper subset of .   .                  Over the universe of positive integers, define    :  is prime and .  :  is a power of 3.  :  is a divisor of 27.     What are the truth sets of these propositions?  Which of the three propositions implies one of the others?                   If , how many propositions over could you list without listing two that are equivalent? There are subsets of , allowing for the possibility of nonequivalent propositions over .    Given the propositions over the natural numbers:    , , and    What are the truth sets of:                       Suppose that is a proposition over . If , give two examples of propositions that are equivalent to .  Two possible answers: is odd and    Determine the truth sets of the following propositions over the positive integers:  .   Determine for and above.    and   .   .    Let the universe be , the set of integers. Which of the following propositions are equivalent over ?    :   :   :     and   "
},
{
  "id": "def-proposition-over-U",
  "level": "2",
  "url": "s-propositions-over-universe.html#def-proposition-over-U",
  "type": "Definition",
  "number": "3.6.1",
  "title": "Proposition over a Universe.",
  "body": "Proposition over a Universe  Let be a nonempty set. A proposition over is a sentence that contains a variable that can take on any value in and that has a definite truth value as a result of any such substitution.  "
},
{
  "id": "ex-some-propositions-over-U",
  "level": "2",
  "url": "s-propositions-over-universe.html#ex-some-propositions-over-U",
  "type": "Example",
  "number": "3.6.2",
  "title": "Some propositions over a variety of universes.",
  "body": "Some propositions over a variety of universes    A few propositions over the integers are , , and is a multiple of 3.  A few propositions over the rational numbers are , , and .  A few propositions over the subsets of are , , and .    "
},
{
  "id": "def-truth-set",
  "level": "2",
  "url": "s-propositions-over-universe.html#def-truth-set",
  "type": "Definition",
  "number": "3.6.3",
  "title": "Truth Set.",
  "body": "Truth Set Truth Set  the truth set of  If is a proposition over , the truth set of is .  "
},
{
  "id": "ex-set-prop",
  "level": "2",
  "url": "s-propositions-over-universe.html#ex-set-prop",
  "type": "Example",
  "number": "3.6.4",
  "title": "Truth Set Example.",
  "body": "Truth Set Example The truth set of the proposition , taken as a proposition over the power set of is . "
},
{
  "id": "ex-vary-U",
  "level": "2",
  "url": "s-propositions-over-universe.html#ex-vary-U",
  "type": "Example",
  "number": "3.6.5",
  "title": "Truth sets depend on the universe.",
  "body": "Truth sets depend on the universe Over the universe (the integers), the truth set of is . If the universe is expanded to the rational numbers, the truth set becomes . The term solution set is often used for the truth set of an equation such as the one in this example. "
},
{
  "id": "def-tautology-contradiction-over-U",
  "level": "2",
  "url": "s-propositions-over-universe.html#def-tautology-contradiction-over-U",
  "type": "Definition",
  "number": "3.6.6",
  "title": "Tautologies and Contradictions over a Universe.",
  "body": "Tautologies and Contradictions over a Universe A proposition over is a tautology if its truth set is . It is a contradiction if its truth set is empty.   "
},
{
  "id": "ex-tautology-contradiction-over-U",
  "level": "2",
  "url": "s-propositions-over-universe.html#ex-tautology-contradiction-over-U",
  "type": "Example",
  "number": "3.6.7",
  "title": "Tautology, Contradiction over <span xmlns:pf=\"https:\/\/prefigure.org\" class=\"process-math\">\\(\\mathbb{Q}\\)<\/span>.",
  "body": "Tautology, Contradiction over  is a tautology over the rational numbers. is a contradiction over the rationals. "
},
{
  "id": "table-truth-sets-compound-statements",
  "level": "2",
  "url": "s-propositions-over-universe.html#table-truth-sets-compound-statements",
  "type": "Table",
  "number": "3.6.8",
  "title": "Truth Sets of Compound Statements",
  "body": " Truth Sets of Compound Statements         "
},
{
  "id": "def-equivanlence-over-U",
  "level": "2",
  "url": "s-propositions-over-universe.html#def-equivanlence-over-U",
  "type": "Definition",
  "number": "3.6.9",
  "title": "Equivalence of propositions over a universe.",
  "body": "Equivalence of propositions over a universe Two propositions, and , are equivalent if is a tautology. In terms of truth sets, this means that and are equivalent if . "
},
{
  "id": "ex-some-equivalent-pairs",
  "level": "2",
  "url": "s-propositions-over-universe.html#ex-some-equivalent-pairs",
  "type": "Example",
  "number": "3.6.10",
  "title": "Some pairs of equivalent propositions.",
  "body": "Some pairs of equivalent propositions    and are equivalent propositions over the integers.   and are equivalent propositions over the power set of the natural numbers.   "
},
{
  "id": "def-implication-over-U",
  "level": "2",
  "url": "s-propositions-over-universe.html#def-implication-over-U",
  "type": "Definition",
  "number": "3.6.11",
  "title": "Implication for propositions over a universe.",
  "body": "Implication for propositions over a universe If and are propositions over , implies if is a tautology. "
},
{
  "id": "venn_diagram_truth_set_conditional",
  "level": "2",
  "url": "s-propositions-over-universe.html#venn_diagram_truth_set_conditional",
  "type": "Figure",
  "number": "3.6.12",
  "title": "",
  "body": " Venn Diagram for    A two set Venn Diagram for    "
},
{
  "id": "ex-implications-over-U",
  "level": "2",
  "url": "s-propositions-over-universe.html#ex-implications-over-U",
  "type": "Example",
  "number": "3.6.13",
  "title": "Examples of Implications.",
  "body": "Examples of Implications   Over the natural numbers: since  Over the power set of the integers: implies  Over the power set of the integers,   "
},
{
  "id": "exercises-3-6-2",
  "level": "2",
  "url": "s-propositions-over-universe.html#exercises-3-6-2",
  "type": "Exercise",
  "number": "3.6.3.1",
  "title": "",
  "body": "If , what are the truth sets of the following propositions?    .   and .   .   is a proper subset of .   .                 "
},
{
  "id": "exercises-3-6-3",
  "level": "2",
  "url": "s-propositions-over-universe.html#exercises-3-6-3",
  "type": "Exercise",
  "number": "3.6.3.2",
  "title": "",
  "body": "Over the universe of positive integers, define    :  is prime and .  :  is a power of 3.  :  is a divisor of 27.     What are the truth sets of these propositions?  Which of the three propositions implies one of the others?                  "
},
{
  "id": "exercises-3-6-4",
  "level": "2",
  "url": "s-propositions-over-universe.html#exercises-3-6-4",
  "type": "Exercise",
  "number": "3.6.3.3",
  "title": "",
  "body": "If , how many propositions over could you list without listing two that are equivalent? There are subsets of , allowing for the possibility of nonequivalent propositions over .  "
},
{
  "id": "exercises-3-6-5",
  "level": "2",
  "url": "s-propositions-over-universe.html#exercises-3-6-5",
  "type": "Exercise",
  "number": "3.6.3.4",
  "title": "",
  "body": " Given the propositions over the natural numbers:    , , and    What are the truth sets of:                      "
},
{
  "id": "exercises-3-6-6",
  "level": "2",
  "url": "s-propositions-over-universe.html#exercises-3-6-6",
  "type": "Exercise",
  "number": "3.6.3.5",
  "title": "",
  "body": "Suppose that is a proposition over . If , give two examples of propositions that are equivalent to .  Two possible answers: is odd and "
},
{
  "id": "exercises-3-6-7",
  "level": "2",
  "url": "s-propositions-over-universe.html#exercises-3-6-7",
  "type": "Exercise",
  "number": "3.6.3.6",
  "title": "",
  "body": "  Determine the truth sets of the following propositions over the positive integers:  .   Determine for and above.    and   .   .   "
},
{
  "id": "exercises-3-6-8",
  "level": "2",
  "url": "s-propositions-over-universe.html#exercises-3-6-8",
  "type": "Exercise",
  "number": "3.6.3.7",
  "title": "",
  "body": "Let the universe be , the set of integers. Which of the following propositions are equivalent over ?    :   :   :     and "
},
{
  "id": "s-induction",
  "level": "1",
  "url": "s-induction.html",
  "type": "Section",
  "number": "3.7",
  "title": "Mathematical Induction",
  "body": "Mathematical Induction  Introduction, First Example  In this section, we will examine mathematical induction, a technique for proving propositions over the positive integers. Mathematical induction reduces the proof that all of the positive integers belong to a truth set to a finite number of steps.  Formula for Triangular Numbers Consider the following proposition over the positive integers, which we will label : The sum of the positive integers from 1 to is . This is a well-known formula that is quite simple to verify for a given value of . For example, is: The sum of the positive integers from 1 to 5 is . Indeed, . However, this doesn't serve as a proof that is a tautology. All that we've established is that is in the truth set of . Since the positive integers are infinite, we certainly can't use this approach to prove the formula.   An Analogy : A proof by mathematical induction is similar to knocking over a row of closely spaced dominos that are standing on end. To knock over the dominos in , all you need to do is push the first domino over. To be assured that they all will be knocked over, some work must be done ahead of time. The dominos must be positioned so that if any domino is pushed is knocked over, it will push the next domino in the line.   An analogy for Mathematical Induction, Creative Commons photo by Ranveig Thattai   A line of dominos in the process of being knocked over.    Returning to imagine the propositions to be an infinite line of dominos. Let's see if these propositions are in the same formation as the dominos were. First, we will focus on one specific point of the line: and . We are not going to prove that either of these propositions is true, just that the truth of implies the truth of . In terms of our analogy, if is knocked over, it will knock over .  In proving , we will use as our premise. We must prove: The sum of the positive integers from 1 to 100 is . We start by observing that the sum of the positive integers from 1 to 100 is . That is, the sum of the positive integers from 1 to 100 equals the sum of the first ninety-nine plus the final number, 100. We can now apply our premise, , to the sum . After rearranging our numbers, we obtain the desired expression for : .  What we've just done is analogous to checking two dominos in a line and finding that they are properly positioned. Since we are dealing with an infinite line, we must check all pairs at once. This is accomplished by proving that for all : .  They are all lined up! Now look at : The sum of the positive integers from 1 to 1 is . Clearly, is true. This sets off a chain reaction. Since , is true. Since , is true; and so on.  The Principle of Mathematical Induction  Let be a proposition over the positive integers. If   is true, and  for all , ,  then is a tautology.   Note: The truth of is called the basis for the induction proof. The premise that is true in the second part is called the induction hypothesis . The proof that implies is called the induction step of the proof. Despite our analogy, the basis is usually done first in an induction proof. However, order doesn't really matter.   More Examples  Generalized Detachment Consider the implication over the positive integers. A proof that is a tautology follows. Basis: is . This is the logical law of detachment which we know is true. If you haven't done so yet, write out the truth table of to verify this step.  Induction: Assume that is true for some . We want to prove that must be true. That is: Here is a direct proof of :    Step Proposition Justification     Premises      ,       Premise           An example from Number Theory  For all , is a multiple of 3. An inductive proof follows:  Basis: is a multiple of 3. The basis is almost always this easy!  Induction: Assume that and is a multiple of 3. Consider . Is it a multiple of 3?  .  Yes, is the sum of two multiples of 3; therefore, it is also a multiple of 3.    Now we will discuss some of the variations of the principle of mathematical induction. The first simply allows for universes that are similar to such as or .  Principle of Mathematical Induction (Generalized)  If is a proposition over , where is any integer, then is a tautology if    is true, and  for all , .      A proof of the permutations formula  In Chapter 2, we stated that the number of different permutations of elements taken from an element set, , can be computed with the formula . We can prove this statement by induction on . For , let be the proposition .  Basis: states that if is the number of ways that elements can be selected from the empty set and arranged in order, then . This is true. A general law in combinatorics is that there is exactly one way of doing nothing.  Induction: Assume that is true for some natural number . It is left for us to prove that this assumption implies that is true. Suppose that we have a set of cardinality and want to select and arrange of its elements. There are two cases to consider, the first of which is easy. If , then there is one way of selecting zero elements from the set; hence and the formula works in this case.  The more challenging case is to verify the formula when is positive and less than or equal to . Here we count the value of by counting the number of ways that the first element in the arrangement can be filled and then counting the number of ways that the remaining elements can be filled in using the induction hypothesis.  There are possible choices for the first element. Since that leaves elements to fill in the remaining positions, there are ways of completing the arrangement. By the rule of products,     Course of Values Induction  A second variation allows for the expansion of the induction hypothesis. The course-of-values principle includes the previous generalization. It is also sometimes called strong induction .  The Course-of-Values Principle of Mathematical Induction  If is a proposition over , where is any integer, then is a tautology if    is true, and  for all , .      Prime number A prime number is defined as a positive integer that has exactly two positive divisors, 1 and itself. There are an infinite number of primes. The list of primes starts with . The proposition over that we will prove here is : can be written as the product of one or more primes. In most texts, the assertion that is a tautology would appear as  Existence of Prime Factorizations  Every positive integer greater than or equal to 2 has a prime decomposition.   If you were to encounter this theorem outside the context of a discussion of mathematical induction, it might not be obvious that the proof can be done by induction. Recognizing when an induction proof is appropriate is mostly a matter of experience. Now on to the proof!  Basis: Since 2 is a prime, it is already decomposed into primes (one of them).  Induction: Suppose that for some all of the integers have a prime decomposition. Notice the course-of-value hypothesis. Consider . Either is prime or it isn't. If is prime, it is already decomposed into primes. If not, then has a divisor, , other than 1 and . Hence, where both and are between 2 and . By the induction hypothesis, and have prime decompositions, and , respectively. Therefore, has the prime decomposition .    Peano Postulates and Induction  Mathematical induction originated in the late nineteenth century. Two mathematicians who were prominent in its development were Richard Dedekind and Giuseppe Peano. Dedekind developed a set of axioms that describe the positive integers. Peano refined these axioms and gave a logical interpretation to them. The axioms are usually called the Peano Postulates.  Peano Postulates The system of positive integers consists of a nonempty set, ; a least element of , denoted 1; and a successor function, s, with the properties   If , then there is an element of called the successor of , denoted .  No two elements of have the same successor.  No element of has as its successor.  If , , and , then .      Notes:   You might recognize as simply being .  Axiom 4 is the one that makes mathematical induction possible. In an induction proof, we simply apply that axiom to the truth set of a proposition.     Exercises  Prove that the sum of the first odd integers equals . We wish to prove that is true for . Recall that the th odd positive integer is .  Basis: for , is , which is true  Induction: Assume that for some , is true. Then we infer that follows:    Prove that if , then . Basis: For we observe that which establishes the truth of the basis.  Induction: Assume that for some the formula is true. Then    Prove that for : .  Proof:   Basis: We note that the proposition is true when : .  Induction: Assume that the proposition is true for some . This is the induction hypothesis. Therefore, the truth of the proposition for implies the truth of the proposition for .     Prove that for : . Let be the proposition , . The basis of an induction proof that this proposition is a tautology is to observe that if we have , which is true.  Now the induction step of the proof calls for assuming that for some , is true (this is the induction hypothesis ). We then proof that follows from the induction hypothesis. and we are done! \\quad   Use mathematical induction to show that for , .  Basis: For , we observe that  Induction: Assume that for some , the formula is true.  Then:   Prove that if , the generalized DeMorgan's Law is true: .   Basis: ( ) Proven with a truth table already.  Induction: Assume the generalized DeMorgan's Law with propositions is true for some .   The number of strings of zeros and ones that contain an even number of ones is . Prove this fact by induction for . Let be the set of strings of zeros and ones of length (we assume that is known). Let be the set of the even strings, and the odd strings. The problem is to prove that for , . Clearly, , and, if for some , it follows that by the following reasoning.  We partition according to the first bit:  Since and are disjoint, we can apply the addition law. Therefore,    Let be is a multiple of 5. Prove that is a tautology over . states that is a multiple of 5, which is true. Now assume that for some , is a multiple of 5. Now consider . The first term is explicitly a multiple of 5 and the second is a multiple of 5 by the induction hypothesis; and the sum of two multiples of five is also a multiple of five.   Suppose that there are people in a room, , and that they all shake hands with one another. Prove that handshakes will have occurred. Assume that for persons handshakes take place. If one more person enters the room, he or she will shake hands with people, Also, for , there are no handshakes, which matches the conjectured formula:    Prove that it is possible to make up any postage of eight cents or more using only three- and five-cent stamps. We can make eight cents in postage with one three and one five. This is our basis. Now assume we can make some amount , where . We can make in at least one of two ways. We observe that there must be at least three 3's or one 5 that are used in making cents. If at least one 5 was used, we replace it with two 3's and we have cents. Otherwise, we have three 3's and they can be replaced with two 5's to increase the postage amount to .   Generalized associativity. It is well known that if , , and are numbers, then no matter what order the sums in the expression are taken, the result is always the same. Call this fact . Prove using course-of-values induction that if , , and are numbers, then no matter what order the sums in the expression are taken, the result is always the same. Let be has the same value no matter how it is evaluated.  Basis: may be evaluated only two ways. Since + is associative, . Hence, is true.  Induction: Assume that for some , are all true. Now consider the sum . Any of the additions in this expression can be applied last. If the th addition is applied last, we have . No matter how the expression to the left and right of the addition are evaluated, the result will always be the same by the induction hypothesis, specifically and . We now can prove that . If ,    Let be the set of all numbers that can be produced by applying any of the rules below in any order a finite number of times.  Rule 1:  Rule 2:  Rule 3: If and have been produced by the rules, then .  Rule 4: If and have been produced by the rules, then .  Prove that . The number of times the rules are applied should be the integer that you do the induction on.  Let be \"Any number. produced with rule applications is an element of the set .\"  Basis: Any number produced from a single rule application is either or . Therefore is true.  Induction: Assume that for some , is true for all ; that is, any number produced with or fewer rule applications of the rules will be in . Let be produced with rule application \\(n+1\\) . If the application is to use rules # 1 or #2, then clearly . If rule #3 was used, then when and were produced in previous steps, and so are elements of by the induction hypothesis. and implies that and so .   Proofs involving objects that are defined recursively are often inductive. A recursive definition is similar to an inductive proof. It consists of a basis, usually the simple part of the definition, and the recursion, which defines complex objects in terms of simpler ones. For example, if is a real number and is a positive integer, we can define as follows:  Basis: .  Recursion: if , .  For example, = .  Prove that if , . There is much more on recursion in Chapter 8. Let be the proposition that for all . For , let for all . The basis for this proof follows directly from the basis for the definition of exponentiation.  Induction: Assume that for some , is true. Then    "
},
{
  "id": "ex-triangular-numbers",
  "level": "2",
  "url": "s-induction.html#ex-triangular-numbers",
  "type": "Example",
  "number": "3.7.1",
  "title": "Formula for Triangular Numbers.",
  "body": "Formula for Triangular Numbers Consider the following proposition over the positive integers, which we will label : The sum of the positive integers from 1 to is . This is a well-known formula that is quite simple to verify for a given value of . For example, is: The sum of the positive integers from 1 to 5 is . Indeed, . However, this doesn't serve as a proof that is a tautology. All that we've established is that is in the truth set of . Since the positive integers are infinite, we certainly can't use this approach to prove the formula.  "
},
{
  "id": "dominos",
  "level": "2",
  "url": "s-induction.html#dominos",
  "type": "Figure",
  "number": "3.7.2",
  "title": "",
  "body": " An analogy for Mathematical Induction, Creative Commons photo by Ranveig Thattai   A line of dominos in the process of being knocked over.   "
},
{
  "id": "th-math-induction-basic",
  "level": "2",
  "url": "s-induction.html#th-math-induction-basic",
  "type": "Theorem",
  "number": "3.7.3",
  "title": "The Principle of Mathematical Induction.",
  "body": "The Principle of Mathematical Induction  Let be a proposition over the positive integers. If   is true, and  for all , ,  then is a tautology.  "
},
{
  "id": "ex-logic-detachment",
  "level": "2",
  "url": "s-induction.html#ex-logic-detachment",
  "type": "Example",
  "number": "3.7.4",
  "title": "Generalized Detachment.",
  "body": "Generalized Detachment Consider the implication over the positive integers. A proof that is a tautology follows. Basis: is . This is the logical law of detachment which we know is true. If you haven't done so yet, write out the truth table of to verify this step.  Induction: Assume that is true for some . We want to prove that must be true. That is: Here is a direct proof of :    Step Proposition Justification     Premises      ,       Premise          "
},
{
  "id": "ex-number-theory-3s",
  "level": "2",
  "url": "s-induction.html#ex-number-theory-3s",
  "type": "Example",
  "number": "3.7.6",
  "title": "An example from Number Theory.",
  "body": "An example from Number Theory  For all , is a multiple of 3. An inductive proof follows:  Basis: is a multiple of 3. The basis is almost always this easy!  Induction: Assume that and is a multiple of 3. Consider . Is it a multiple of 3?  .  Yes, is the sum of two multiples of 3; therefore, it is also a multiple of 3.   "
},
{
  "id": "th-math-induction-generalized",
  "level": "2",
  "url": "s-induction.html#th-math-induction-generalized",
  "type": "Theorem",
  "number": "3.7.7",
  "title": "Principle of Mathematical Induction (Generalized).",
  "body": "Principle of Mathematical Induction (Generalized)  If is a proposition over , where is any integer, then is a tautology if    is true, and  for all , .     "
},
{
  "id": "ex-permuations-formula-proof",
  "level": "2",
  "url": "s-induction.html#ex-permuations-formula-proof",
  "type": "Example",
  "number": "3.7.8",
  "title": "A proof of the permutations formula.",
  "body": "A proof of the permutations formula  In Chapter 2, we stated that the number of different permutations of elements taken from an element set, , can be computed with the formula . We can prove this statement by induction on . For , let be the proposition .  Basis: states that if is the number of ways that elements can be selected from the empty set and arranged in order, then . This is true. A general law in combinatorics is that there is exactly one way of doing nothing.  Induction: Assume that is true for some natural number . It is left for us to prove that this assumption implies that is true. Suppose that we have a set of cardinality and want to select and arrange of its elements. There are two cases to consider, the first of which is easy. If , then there is one way of selecting zero elements from the set; hence and the formula works in this case.  The more challenging case is to verify the formula when is positive and less than or equal to . Here we count the value of by counting the number of ways that the first element in the arrangement can be filled and then counting the number of ways that the remaining elements can be filled in using the induction hypothesis.  There are possible choices for the first element. Since that leaves elements to fill in the remaining positions, there are ways of completing the arrangement. By the rule of products,   "
},
{
  "id": "th-math-induction-course-of-values",
  "level": "2",
  "url": "s-induction.html#th-math-induction-course-of-values",
  "type": "Theorem",
  "number": "3.7.9",
  "title": "The Course-of-Values Principle of Mathematical Induction.",
  "body": "The Course-of-Values Principle of Mathematical Induction  If is a proposition over , where is any integer, then is a tautology if    is true, and  for all , .     "
},
{
  "id": "s-induction-4-4",
  "level": "2",
  "url": "s-induction.html#s-induction-4-4",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "prime number "
},
{
  "id": "th-prime-factorizations-exist",
  "level": "2",
  "url": "s-induction.html#th-prime-factorizations-exist",
  "type": "Theorem",
  "number": "3.7.10",
  "title": "Existence of Prime Factorizations.",
  "body": "Existence of Prime Factorizations  Every positive integer greater than or equal to 2 has a prime decomposition.   If you were to encounter this theorem outside the context of a discussion of mathematical induction, it might not be obvious that the proof can be done by induction. Recognizing when an induction proof is appropriate is mostly a matter of experience. Now on to the proof!  Basis: Since 2 is a prime, it is already decomposed into primes (one of them).  Induction: Suppose that for some all of the integers have a prime decomposition. Notice the course-of-value hypothesis. Consider . Either is prime or it isn't. If is prime, it is already decomposed into primes. If not, then has a divisor, , other than 1 and . Hence, where both and are between 2 and . By the induction hypothesis, and have prime decompositions, and , respectively. Therefore, has the prime decomposition .   "
},
{
  "id": "sss-peano-postulates",
  "level": "2",
  "url": "s-induction.html#sss-peano-postulates",
  "type": "Axiom",
  "number": "3.7.11",
  "title": "Peano Postulates.",
  "body": "Peano Postulates The system of positive integers consists of a nonempty set, ; a least element of , denoted 1; and a successor function, s, with the properties   If , then there is an element of called the successor of , denoted .  No two elements of have the same successor.  No element of has as its successor.  If , , and , then .     "
},
{
  "id": "exercises-3-7-2",
  "level": "2",
  "url": "s-induction.html#exercises-3-7-2",
  "type": "Exercise",
  "number": "3.7.4.1",
  "title": "",
  "body": "Prove that the sum of the first odd integers equals . We wish to prove that is true for . Recall that the th odd positive integer is .  Basis: for , is , which is true  Induction: Assume that for some , is true. Then we infer that follows:   "
},
{
  "id": "exercises-3-7-3",
  "level": "2",
  "url": "s-induction.html#exercises-3-7-3",
  "type": "Exercise",
  "number": "3.7.4.2",
  "title": "",
  "body": "Prove that if , then . Basis: For we observe that which establishes the truth of the basis.  Induction: Assume that for some the formula is true. Then   "
},
{
  "id": "exercises-3-7-4",
  "level": "2",
  "url": "s-induction.html#exercises-3-7-4",
  "type": "Exercise",
  "number": "3.7.4.3",
  "title": "",
  "body": "Prove that for : .  Proof:   Basis: We note that the proposition is true when : .  Induction: Assume that the proposition is true for some . This is the induction hypothesis. Therefore, the truth of the proposition for implies the truth of the proposition for .    "
},
{
  "id": "exercises-3-7-5",
  "level": "2",
  "url": "s-induction.html#exercises-3-7-5",
  "type": "Exercise",
  "number": "3.7.4.4",
  "title": "",
  "body": "Prove that for : . Let be the proposition , . The basis of an induction proof that this proposition is a tautology is to observe that if we have , which is true.  Now the induction step of the proof calls for assuming that for some , is true (this is the induction hypothesis ). We then proof that follows from the induction hypothesis. and we are done! \\quad  "
},
{
  "id": "exercises-3-7-6",
  "level": "2",
  "url": "s-induction.html#exercises-3-7-6",
  "type": "Exercise",
  "number": "3.7.4.5",
  "title": "",
  "body": "Use mathematical induction to show that for , .  Basis: For , we observe that  Induction: Assume that for some , the formula is true.  Then:  "
},
{
  "id": "exercises-3-7-7",
  "level": "2",
  "url": "s-induction.html#exercises-3-7-7",
  "type": "Exercise",
  "number": "3.7.4.6",
  "title": "",
  "body": "Prove that if , the generalized DeMorgan's Law is true: .   Basis: ( ) Proven with a truth table already.  Induction: Assume the generalized DeMorgan's Law with propositions is true for some .  "
},
{
  "id": "exercises-3-7-8",
  "level": "2",
  "url": "s-induction.html#exercises-3-7-8",
  "type": "Exercise",
  "number": "3.7.4.7",
  "title": "",
  "body": "The number of strings of zeros and ones that contain an even number of ones is . Prove this fact by induction for . Let be the set of strings of zeros and ones of length (we assume that is known). Let be the set of the even strings, and the odd strings. The problem is to prove that for , . Clearly, , and, if for some , it follows that by the following reasoning.  We partition according to the first bit:  Since and are disjoint, we can apply the addition law. Therefore,   "
},
{
  "id": "exercises-3-7-9",
  "level": "2",
  "url": "s-induction.html#exercises-3-7-9",
  "type": "Exercise",
  "number": "3.7.4.8",
  "title": "",
  "body": "Let be is a multiple of 5. Prove that is a tautology over . states that is a multiple of 5, which is true. Now assume that for some , is a multiple of 5. Now consider . The first term is explicitly a multiple of 5 and the second is a multiple of 5 by the induction hypothesis; and the sum of two multiples of five is also a multiple of five.  "
},
{
  "id": "exercises-3-7-10",
  "level": "2",
  "url": "s-induction.html#exercises-3-7-10",
  "type": "Exercise",
  "number": "3.7.4.9",
  "title": "",
  "body": "Suppose that there are people in a room, , and that they all shake hands with one another. Prove that handshakes will have occurred. Assume that for persons handshakes take place. If one more person enters the room, he or she will shake hands with people, Also, for , there are no handshakes, which matches the conjectured formula:   "
},
{
  "id": "exercises-3-7-11",
  "level": "2",
  "url": "s-induction.html#exercises-3-7-11",
  "type": "Exercise",
  "number": "3.7.4.10",
  "title": "",
  "body": "Prove that it is possible to make up any postage of eight cents or more using only three- and five-cent stamps. We can make eight cents in postage with one three and one five. This is our basis. Now assume we can make some amount , where . We can make in at least one of two ways. We observe that there must be at least three 3's or one 5 that are used in making cents. If at least one 5 was used, we replace it with two 3's and we have cents. Otherwise, we have three 3's and they can be replaced with two 5's to increase the postage amount to .  "
},
{
  "id": "exercises-3-7-12",
  "level": "2",
  "url": "s-induction.html#exercises-3-7-12",
  "type": "Exercise",
  "number": "3.7.4.11",
  "title": "",
  "body": "Generalized associativity. It is well known that if , , and are numbers, then no matter what order the sums in the expression are taken, the result is always the same. Call this fact . Prove using course-of-values induction that if , , and are numbers, then no matter what order the sums in the expression are taken, the result is always the same. Let be has the same value no matter how it is evaluated.  Basis: may be evaluated only two ways. Since + is associative, . Hence, is true.  Induction: Assume that for some , are all true. Now consider the sum . Any of the additions in this expression can be applied last. If the th addition is applied last, we have . No matter how the expression to the left and right of the addition are evaluated, the result will always be the same by the induction hypothesis, specifically and . We now can prove that . If ,   "
},
{
  "id": "exercises-3-7-13",
  "level": "2",
  "url": "s-induction.html#exercises-3-7-13",
  "type": "Exercise",
  "number": "3.7.4.12",
  "title": "",
  "body": "Let be the set of all numbers that can be produced by applying any of the rules below in any order a finite number of times.  Rule 1:  Rule 2:  Rule 3: If and have been produced by the rules, then .  Rule 4: If and have been produced by the rules, then .  Prove that . The number of times the rules are applied should be the integer that you do the induction on.  Let be \"Any number. produced with rule applications is an element of the set .\"  Basis: Any number produced from a single rule application is either or . Therefore is true.  Induction: Assume that for some , is true for all ; that is, any number produced with or fewer rule applications of the rules will be in . Let be produced with rule application \\(n+1\\) . If the application is to use rules # 1 or #2, then clearly . If rule #3 was used, then when and were produced in previous steps, and so are elements of by the induction hypothesis. and implies that and so .  "
},
{
  "id": "exercises-3-7-14",
  "level": "2",
  "url": "s-induction.html#exercises-3-7-14",
  "type": "Exercise",
  "number": "3.7.4.13",
  "title": "",
  "body": "Proofs involving objects that are defined recursively are often inductive. A recursive definition is similar to an inductive proof. It consists of a basis, usually the simple part of the definition, and the recursion, which defines complex objects in terms of simpler ones. For example, if is a real number and is a positive integer, we can define as follows:  Basis: .  Recursion: if , .  For example, = .  Prove that if , . There is much more on recursion in Chapter 8. Let be the proposition that for all . For , let for all . The basis for this proof follows directly from the basis for the definition of exponentiation.  Induction: Assume that for some , is true. Then  "
},
{
  "id": "s-quantifiers",
  "level": "1",
  "url": "s-quantifiers.html",
  "type": "Section",
  "number": "3.8",
  "title": "Quantifiers",
  "body": "Quantifiers Quantifiers  As we saw in Section 3.6, if is a proposition over a universe , its truth set is equal to a subset of . In many cases, such as when is an equation, we are most concerned with whether is empty or not. In other cases, we might be interested in whether ; that is, whether is a tautology. Since the conditions and are so often an issue, we have a special system of notation for them.  The Existential Quantifier  The Existential Quantifier Existential Quantifier  The statement that is true for at least one value of  If is a proposition over with , we commonly say There exists an in such that (is true). We abbreviate this with the symbols . The symbol is called the existential quantifier. If the context is clear, the mention of is dropped: .   Some examples of existential quantifiers   is another way of saying that there is an integer that solves the equation . The fact that two such integers exist doesn't affect the truth of this proposition in any way.  simply states that 102 is a multiple of 3, which is true. On the other hand, states that 100 is a multiple of 3, which is false.  is false since the solution set of the equation in the real numbers is empty. It is common to write in this case.    There are a wide variety of ways that you can write a proposition with an existential quantifier. contains a list of different variations that could be used for both the existential and universal quantifiers.   The Universal Quantifier  The Universal Quantifier Universal Quantifier  The statement that is always true.  If is a proposition over with , we commonly say For all in , (is true). We abbreviate this with the symbols . The symbol is called the universal quantifier. If the context is clear, the mention of is dropped: .   Some Universal Quantifiers   We can say that the square of every real number is non-negative symbolically with a universal quantifier: .   says that the sum of zero and any integer is . This fact is called the identity property of zero for addition.     Notational Variations with Quantified Expressions   Universal Quantifier Existential Quantifier             is true for some  is true for all        The Negation of Quantified Propositions Quantifiers Negation  When you negate a quantified proposition, the existential and universal quantifiers complement one another.  Negation of an Existential Quantifier Over the universe of animals, define : is a fish and : lives in the water. We know that the proposition is not always true. In other words, is false. Another way of stating this fact is that there exists an animal that lives in the water and is not a fish; that is, .   Note that the negation of a universally quantified proposition is an existentially quantified proposition. In addition, when you negate an existentially quantified proposition, you get a universally quantified proposition. Symbolically,   Negation of Quantified Expressions       More Negations of Quantified Expressions   The ancient Greeks first discovered that is an irrational number; that is, is not a rational number. and both state this fact symbolically.   is equivalent to . They are either both true or both false.     Multiple Quantifiers Quantifiers Multiple  If a proposition has more than one variable, then you can quantify it more than once. For example, is a tautology over the set of all pairs of real numbers because it is true for each pair in . Another way to look at this proposition is as a proposition with two variables. The assertion that is a tautology could be quantified as or  In general, multiple universal quantifiers can be arranged in any order without logically changing the meaning of the resulting proposition. The same is true for multiple existential quantifiers. For example, is a proposition over . and are equivalent. A proposition with multiple existential quantifiers such as this one says that there are simultaneous values for the quantified variables that make the proposition true. A similar example is , which is always false; and the following are all equivalent:    When existential and universal quantifiers are mixed, the order cannot be exchanged without possibly changing the meaning of the proposition. For example, let be the positive real numbers, and have different logical values; is true, while is false.  Tips on Reading Multiply-Quantified Propositions. It is understandable that you would find propositions such as difficult to read. The trick to deciphering these expressions is to peel one quantifier off the proposition just as you would peel off the layers of an onion (but quantifiers shouldn't make you cry!). Since the outermost quantifier in is universal, says that is true for each value that can take on. Now take the time to select a value for , like 6. For the value that we selected, we get , which is obviously true since has a solution in the positive real numbers. We will get that same truth value no matter which positive real number we choose for ; therefore, is a tautology over and we are justified in saying that is true. The key to understanding propositions like on your own is to experiment with actual values for the outermost variables as we did above.  Now consider . To see that is false, we peel off the outer quantifier. Since it is an existential quantifier, all that says is that some positive real number makes : true. Choose a few values of to see if you can find one that makes true. For example, if we pick , we get , which is false, since is almost always different from 1. You should be able to convince yourself that no value of will make true. Therefore, is false.  Another way of convincing yourself that is false is to convince yourself that is true: In words, for each value of , there is a value for that makes . One such value is . Therefore, is true.  One final example that serves as a preview to how quantifiers appear in calculus.  The Limit of a Sequence> What does it mean that 0.999… = 1? The ellipsis ( ) implies that there are an infinite number of 9’s on the left of the equals sign. Any way to try to justify this equality boils down to the idea of limits. After many years of struggling with what this means, mathematicians have come up with a universally accepted interpretation involving quantifiers. It is that In calculus, the symbol is usually reserved for small positive real numbers. Let’s pick a value for and peel the universal quantifier off the statement above. Let’s try equal to . In addition we note that . With our choice of we get or This last statement is true - one value of that would work is . You just have to convince yourself that any positive value of , no matter how small, will produce a true statement. If you see that, you’ve convinced yourself that !     Exercises  Let be is cold-blooded, let be is a fish, and let be lives in the sea.   Translate into a formula: Every fish is cold-blooded.  Translate into English: .  Translate into English: .       There are objects in the sea which are not fish.  Every fish lives in the sea.     Let be is a mammal, let be is an animal, and let be is warm-blooded.   Translate into a formula: Every mammal is warm-blooded.  Translate into English: .      Some animal is not a mammal.    Over the universe of books, define the propositions : has a blue cover, : is a mathematics book, : is published in the United States, and : The bibliography of includes .  Translate into words:    .  .   .  .  Express using quantifiers: Every book with a blue cover is a mathematics book.  Express using quantifiers: There are mathematics books that are published outside the United States.  Express using quantifiers: Not all books have bibliographies.    There is a book with a cover that is not blue.  Every mathematics book that is published in the United States has a blue cover.  There exists a mathematics book with a cover that is not blue.  There exists a book that appears in the bibliography of every mathematics book.          Let the universe of discourse, , be the set of all people, and let be is the mother of .  Which of the following is a true statement? Translate it into English.       Translate the following statement into logical notation using quantifiers and the proposition : Everyone has a maternal grandmother.    Someone is everyone's mother. Clearly false.  Everyone has a mother. True      Translate into your own words and indicate whether it is true or false that .  The equation has a solution in the integers. (False)   Use quantifiers to say that is an irrational number.  Two possible answers: or    What do the following propositions say, where is the power set of ? Which of these propositions are true?   .   .   .    Every subset of has a cardinality different from its complement. (True)  There is a pair of disjoint subsets of both having cardinality 5. (False)  is a tautology. (True)    Use quantifiers to state that for every positive integer, there is a larger positive integer.    Use quantifiers to state that the sum of any two rational numbers is rational.   ( is a rational number.)   Over the universe of real numbers, use quantifiers to say that the equation has a solution for all values of and .  You will need three quantifiers.    Let be a positive integer. Describe using quantifiers:        Let        Prove that , but that converse is not true.   If we assume that , let such that is true This means that if we select any , is true; so indeed is true. We need only cite one example where the converese is false. Using the universe of integers, is true. However, is false.    "
},
{
  "id": "def-exist-quantifier",
  "level": "2",
  "url": "s-quantifiers.html#def-exist-quantifier",
  "type": "Definition",
  "number": "3.8.1",
  "title": "The Existential Quantifier.",
  "body": "The Existential Quantifier Existential Quantifier  The statement that is true for at least one value of  If is a proposition over with , we commonly say There exists an in such that (is true). We abbreviate this with the symbols . The symbol is called the existential quantifier. If the context is clear, the mention of is dropped: .  "
},
{
  "id": "ex-existential-misc",
  "level": "2",
  "url": "s-quantifiers.html#ex-existential-misc",
  "type": "Example",
  "number": "3.8.2",
  "title": "Some examples of existential quantifiers.",
  "body": "Some examples of existential quantifiers   is another way of saying that there is an integer that solves the equation . The fact that two such integers exist doesn't affect the truth of this proposition in any way.  simply states that 102 is a multiple of 3, which is true. On the other hand, states that 100 is a multiple of 3, which is false.  is false since the solution set of the equation in the real numbers is empty. It is common to write in this case.   "
},
{
  "id": "def-universal-quantifier",
  "level": "2",
  "url": "s-quantifiers.html#def-universal-quantifier",
  "type": "Definition",
  "number": "3.8.3",
  "title": "The Universal Quantifier.",
  "body": "The Universal Quantifier Universal Quantifier  The statement that is always true.  If is a proposition over with , we commonly say For all in , (is true). We abbreviate this with the symbols . The symbol is called the universal quantifier. If the context is clear, the mention of is dropped: .  "
},
{
  "id": "ex-universal-misc",
  "level": "2",
  "url": "s-quantifiers.html#ex-universal-misc",
  "type": "Example",
  "number": "3.8.4",
  "title": "Some Universal Quantifiers.",
  "body": "Some Universal Quantifiers   We can say that the square of every real number is non-negative symbolically with a universal quantifier: .   says that the sum of zero and any integer is . This fact is called the identity property of zero for addition.   "
},
{
  "id": "table-quantifier-variations",
  "level": "2",
  "url": "s-quantifiers.html#table-quantifier-variations",
  "type": "Table",
  "number": "3.8.5",
  "title": "Notational Variations with Quantified Expressions",
  "body": " Notational Variations with Quantified Expressions   Universal Quantifier Existential Quantifier             is true for some  is true for all     "
},
{
  "id": "ex-negated-existential",
  "level": "2",
  "url": "s-quantifiers.html#ex-negated-existential",
  "type": "Example",
  "number": "3.8.6",
  "title": "Negation of an Existential Quantifier.",
  "body": "Negation of an Existential Quantifier Over the universe of animals, define : is a fish and : lives in the water. We know that the proposition is not always true. In other words, is false. Another way of stating this fact is that there exists an animal that lives in the water and is not a fish; that is, .  "
},
{
  "id": "table-quantifier-negation",
  "level": "2",
  "url": "s-quantifiers.html#table-quantifier-negation",
  "type": "Table",
  "number": "3.8.7",
  "title": "Negation of Quantified Expressions",
  "body": " Negation of Quantified Expressions      "
},
{
  "id": "ex-more-negated-quantifiers",
  "level": "2",
  "url": "s-quantifiers.html#ex-more-negated-quantifiers",
  "type": "Example",
  "number": "3.8.8",
  "title": "More Negations of Quantified Expressions.",
  "body": "More Negations of Quantified Expressions   The ancient Greeks first discovered that is an irrational number; that is, is not a rational number. and both state this fact symbolically.   is equivalent to . They are either both true or both false.   "
},
{
  "id": "ss-multiple-quantifiers-11",
  "level": "2",
  "url": "s-quantifiers.html#ss-multiple-quantifiers-11",
  "type": "Example",
  "number": "3.8.9",
  "title": "The Limit of a Sequence&gt;.",
  "body": "The Limit of a Sequence> What does it mean that 0.999… = 1? The ellipsis ( ) implies that there are an infinite number of 9’s on the left of the equals sign. Any way to try to justify this equality boils down to the idea of limits. After many years of struggling with what this means, mathematicians have come up with a universally accepted interpretation involving quantifiers. It is that In calculus, the symbol is usually reserved for small positive real numbers. Let’s pick a value for and peel the universal quantifier off the statement above. Let’s try equal to . In addition we note that . With our choice of we get or This last statement is true - one value of that would work is . You just have to convince yourself that any positive value of , no matter how small, will produce a true statement. If you see that, you’ve convinced yourself that !  "
},
{
  "id": "exercises-3-8-2",
  "level": "2",
  "url": "s-quantifiers.html#exercises-3-8-2",
  "type": "Exercise",
  "number": "3.8.5.1",
  "title": "",
  "body": "Let be is cold-blooded, let be is a fish, and let be lives in the sea.   Translate into a formula: Every fish is cold-blooded.  Translate into English: .  Translate into English: .       There are objects in the sea which are not fish.  Every fish lives in the sea.   "
},
{
  "id": "exercises-3-8-3",
  "level": "2",
  "url": "s-quantifiers.html#exercises-3-8-3",
  "type": "Exercise",
  "number": "3.8.5.2",
  "title": "",
  "body": " Let be is a mammal, let be is an animal, and let be is warm-blooded.   Translate into a formula: Every mammal is warm-blooded.  Translate into English: .      Some animal is not a mammal.   "
},
{
  "id": "exercises-3-8-4",
  "level": "2",
  "url": "s-quantifiers.html#exercises-3-8-4",
  "type": "Exercise",
  "number": "3.8.5.3",
  "title": "",
  "body": "Over the universe of books, define the propositions : has a blue cover, : is a mathematics book, : is published in the United States, and : The bibliography of includes .  Translate into words:    .  .   .  .  Express using quantifiers: Every book with a blue cover is a mathematics book.  Express using quantifiers: There are mathematics books that are published outside the United States.  Express using quantifiers: Not all books have bibliographies.    There is a book with a cover that is not blue.  Every mathematics book that is published in the United States has a blue cover.  There exists a mathematics book with a cover that is not blue.  There exists a book that appears in the bibliography of every mathematics book.         "
},
{
  "id": "exercises-3-8-5",
  "level": "2",
  "url": "s-quantifiers.html#exercises-3-8-5",
  "type": "Exercise",
  "number": "3.8.5.4",
  "title": "",
  "body": "Let the universe of discourse, , be the set of all people, and let be is the mother of .  Which of the following is a true statement? Translate it into English.       Translate the following statement into logical notation using quantifiers and the proposition : Everyone has a maternal grandmother.    Someone is everyone's mother. Clearly false.  Everyone has a mother. True     "
},
{
  "id": "exercises-3-8-6",
  "level": "2",
  "url": "s-quantifiers.html#exercises-3-8-6",
  "type": "Exercise",
  "number": "3.8.5.5",
  "title": "",
  "body": "Translate into your own words and indicate whether it is true or false that .  The equation has a solution in the integers. (False)  "
},
{
  "id": "exercises-3-8-7",
  "level": "2",
  "url": "s-quantifiers.html#exercises-3-8-7",
  "type": "Exercise",
  "number": "3.8.5.6",
  "title": "",
  "body": "Use quantifiers to say that is an irrational number.  Two possible answers: or   "
},
{
  "id": "exercises-3-8-8",
  "level": "2",
  "url": "s-quantifiers.html#exercises-3-8-8",
  "type": "Exercise",
  "number": "3.8.5.7",
  "title": "",
  "body": "What do the following propositions say, where is the power set of ? Which of these propositions are true?   .   .   .    Every subset of has a cardinality different from its complement. (True)  There is a pair of disjoint subsets of both having cardinality 5. (False)  is a tautology. (True)   "
},
{
  "id": "exercises-3-8-9",
  "level": "2",
  "url": "s-quantifiers.html#exercises-3-8-9",
  "type": "Exercise",
  "number": "3.8.5.8",
  "title": "",
  "body": "Use quantifiers to state that for every positive integer, there is a larger positive integer.   "
},
{
  "id": "exercises-3-8-10",
  "level": "2",
  "url": "s-quantifiers.html#exercises-3-8-10",
  "type": "Exercise",
  "number": "3.8.5.9",
  "title": "",
  "body": "Use quantifiers to state that the sum of any two rational numbers is rational.   ( is a rational number.)  "
},
{
  "id": "exercises-3-8-11",
  "level": "2",
  "url": "s-quantifiers.html#exercises-3-8-11",
  "type": "Exercise",
  "number": "3.8.5.10",
  "title": "",
  "body": "Over the universe of real numbers, use quantifiers to say that the equation has a solution for all values of and .  You will need three quantifiers.   "
},
{
  "id": "exercises-3-8-12",
  "level": "2",
  "url": "s-quantifiers.html#exercises-3-8-12",
  "type": "Exercise",
  "number": "3.8.5.11",
  "title": "",
  "body": "Let be a positive integer. Describe using quantifiers:        Let       "
},
{
  "id": "exercises-3-8-13",
  "level": "2",
  "url": "s-quantifiers.html#exercises-3-8-13",
  "type": "Exercise",
  "number": "3.8.5.12",
  "title": "",
  "body": "Prove that , but that converse is not true.   If we assume that , let such that is true This means that if we select any , is true; so indeed is true. We need only cite one example where the converese is false. Using the universe of integers, is true. However, is false.  "
},
{
  "id": "s-proof-review",
  "level": "1",
  "url": "s-proof-review.html",
  "type": "Section",
  "number": "3.9",
  "title": "A Review of Methods of Proof",
  "body": "A Review of Methods of Proof  One of the major goals of this chapter is to acquaint the reader with the key concepts in the nature of proof in logic, which of course carries over into all areas of mathematics and its applications. In this section we will stop, reflect, and smell the roses, so that these key ideas are not lost in the many concepts covered in logic. In Chapter 4 we will use set theory as a vehicle for further practice and insights into methods of proof.   Key Concepts in Proof  All theorems in mathematics can be expressed in form If then ( ), or in the form if and only if ( ). The latter is equivalent to If then , and If then .   In If then ,  is the premise (or hypothesis) and is the conclusion. It is important to realize that a theorem makes a statement that is dependent on the premise being true.  There are two basic methods for proving :   Directly: Assume is true and prove is true.   Indirectly (or by contradiction): Assume is true and is false and prove that this leads to a contradiction of some premise, theorem, or basic truth.   The method of proof for If and only if theorems is found in the law . Hence to prove an If and only if statement one must prove an if . . . then ... statement and its converse.  The initial response of most people when confronted with the task of being told they must be able to read and do proofs is often Why? or I can't do proofs. To answer the first question, doing proofs or problem solving, even on the most trivial level, involves being able to read statements. First we must understand the problem and know the hypothesis; second, we must realize when we are done and we must understand the conclusion. To apply theorems or algorithms we must be able to read theorems and their proofs intelligently.  To be able to do the actual proofs of theorems we are forced to learn:   the actual meaning of the theorems, and  the basic definitions and concepts of the topic discussed.   For example, when we discuss rational numbers and refer to a number as being rational, this means we can substitute a fraction in place of , with the understanding that and are integers and . Therefore, to prove a theorem about rational numbers it is absolutely necessary that you know what a rational number looks like.  It's easy to comment on the response, I cannot do proofs. Have you tried? As elementary school students we may have been in awe of anyone who could handle algebraic expressions, especially complicated ones. We learned by trying and applying ourselves. Maybe we cannot solve all problems in algebra or calculus, but we are comfortable enough with these subjects to know that we can solve many and can express ourselves intelligently in these areas. The same remarks hold true for proofs.   The Art of Proving  First one must completely realize what is given, the hypothesis. The importance of this is usually overlooked by beginners. It makes sense, whenever you begin any task, to spend considerable time thinking about the tools at your disposal. Write down the premise in precise language. Similarly, you have to know when the task is finished. Write down the conclusion in precise language. Then you usually start with and attempt to show that follows logically. How do you begin? Basically you attack the proof the same way you solve a complicated equation in elementary algebra. You may not know exactly what each and every step is but you must try something. If we are lucky, follows naturally; if it doesn't, try something else. Often what is helpful is to work backward from . Finally, we have all learned, possibly the hard way, that mathematics is a participating sport, not a spectator sport. One learns proofs by doing them, not by watching others do them. We give several illustrations of how to set up the proofs of several examples. Our aim here is not to prove the statements given, but to concentrate on the logical procedure.  The Sum of Odd Integers We will outline a proof that the sum of any two odd integers is even. Our first step will be to write the theorem in the familiar conditional form: If and are odd integers, then is even. The premise and conclusion of this theorem should be clear now. Notice that if and are not both odd, then the conclusion may or may not be true. Our only objective is to show that the truth of the premise forces the conclusion to be true. Therefore, we can express the integers and in the form that all odd integers take; that is: This observation allows us to examine the sum and to verify that it must be even.  One final important point: This example involves two odd integers that may or may not be equal. If we use the fact that is odd and infer that for some integer , we can do a similar thing with . However, in this context we cannot write since we have already linked to . We need to use a different variable, maybe or - any other symbol that is not already used in our discussion.   The Square of an Even Integer Let . We will outline a proof that is even if and only if is even.  Outline of a proof: Since this is an If and only if theorem we must prove two things:   ( ) If is even, then is even. To do this directly, assume that is even and prove that is even. To do this indirectly, assume is even and that is odd, and reach a contradiction. It turns out that the latter of the two approaches is easiest here.  ( ) If is even, then is even. To do this directly, assume that is even and prove that is even.   Now that we have broken the theorem down into two parts and know what to prove, we proceed to prove the two implications. The final ingredient that we need is a convenient way of describing even integers. When we refer to an integer (or , or ,. . . ) as even, we can always replace it with a product of the form , where is an integer (more precisely, . In other words, for an integer to be even it must have a factor of two in its prime decomposition.   is irrational Our final example will be an outline of the proof that the square root of 2 is irrational (not an element of ). This is an example of the theorem that does not appear to be in the standard form. One way to rephrase the theorem is: If is a rational number, then . A direct proof of this theorem would require that we verify that the square of every rational number is not equal to 2. There is no convenient way of doing this, so we must turn to the indirect method of proof. In such a proof, we assume that is a rational number and that . This will lead to a contradiction. In order to reach this contradiction, we need to use the following facts:   A rational number is a quotient of two integers.  Every fraction can be reduced to lowest terms, so that the numerator and denominator have no common factor greater than 1.  If is an integer, is even if and only if is even.      Exercises  Prove that the sum of two odd positive integers is an even positive integer. You might want to read before attempting this. The given statement can be written in if , then format as: If and are two odd positive integers, then is an even integer.  Proof: Assume and are two positive odd integers. It can be shown that .  odd and positive for some ,  odd and positive for some .  Then, Therefore, is an even positive integer.   Write out a complete proof that if is an integer, is even if and only if is even.  ( ) We prove that if is even then is even by contradiction. If isn't even, it must be odd and for some integer . This means that which means that is odd, a contradiction.  ( ) If is even, then for some integer , and so is even   Write out a complete proof that is irrational. Proof: (Indirect) Assume to the contrary, that is a rational number. Then there exists where and where is in lowest terms, that is, and have no common factor other than 1.   Hence both and have a common factor, namely 2, which is a contradiction.  Prove that the cube root of 2 is an irrational number.  This proof is similar to the proof that the square root of 2 is irrational.  Assume to the contrary, that is a rational number. Then there exists where and where is in lowest terms, that is, and have no common factor other than 1.   Hence both and have a common factor, namely 2, which is a contradiction.    Prove that if and are real numbers such that , then or . Proof: (Indirect) Assume and . Assume to the contrary that is false, which is equivalent to . Hence . This contradicts the assumption that .   Order selected statments from the following list to construct a proof proof that the sum of two odd integers and is an even integer         is even     "
},
{
  "id": "ex-sumsofodds",
  "level": "2",
  "url": "s-proof-review.html#ex-sumsofodds",
  "type": "Example",
  "number": "3.9.1",
  "title": "The Sum of Odd Integers.",
  "body": "The Sum of Odd Integers We will outline a proof that the sum of any two odd integers is even. Our first step will be to write the theorem in the familiar conditional form: If and are odd integers, then is even. The premise and conclusion of this theorem should be clear now. Notice that if and are not both odd, then the conclusion may or may not be true. Our only objective is to show that the truth of the premise forces the conclusion to be true. Therefore, we can express the integers and in the form that all odd integers take; that is: This observation allows us to examine the sum and to verify that it must be even.  One final important point: This example involves two odd integers that may or may not be equal. If we use the fact that is odd and infer that for some integer , we can do a similar thing with . However, in this context we cannot write since we have already linked to . We need to use a different variable, maybe or - any other symbol that is not already used in our discussion.  "
},
{
  "id": "ex-squares-of-evens",
  "level": "2",
  "url": "s-proof-review.html#ex-squares-of-evens",
  "type": "Example",
  "number": "3.9.2",
  "title": "The Square of an Even Integer.",
  "body": "The Square of an Even Integer Let . We will outline a proof that is even if and only if is even.  Outline of a proof: Since this is an If and only if theorem we must prove two things:   ( ) If is even, then is even. To do this directly, assume that is even and prove that is even. To do this indirectly, assume is even and that is odd, and reach a contradiction. It turns out that the latter of the two approaches is easiest here.  ( ) If is even, then is even. To do this directly, assume that is even and prove that is even.   Now that we have broken the theorem down into two parts and know what to prove, we proceed to prove the two implications. The final ingredient that we need is a convenient way of describing even integers. When we refer to an integer (or , or ,. . . ) as even, we can always replace it with a product of the form , where is an integer (more precisely, . In other words, for an integer to be even it must have a factor of two in its prime decomposition.  "
},
{
  "id": "ex-sqrt-2-irrational",
  "level": "2",
  "url": "s-proof-review.html#ex-sqrt-2-irrational",
  "type": "Example",
  "number": "3.9.3",
  "title": "<span xmlns:pf=\"https:\/\/prefigure.org\" class=\"process-math\">\\(\\sqrt{2}\\)<\/span> is irrational.",
  "body": "is irrational Our final example will be an outline of the proof that the square root of 2 is irrational (not an element of ). This is an example of the theorem that does not appear to be in the standard form. One way to rephrase the theorem is: If is a rational number, then . A direct proof of this theorem would require that we verify that the square of every rational number is not equal to 2. There is no convenient way of doing this, so we must turn to the indirect method of proof. In such a proof, we assume that is a rational number and that . This will lead to a contradiction. In order to reach this contradiction, we need to use the following facts:   A rational number is a quotient of two integers.  Every fraction can be reduced to lowest terms, so that the numerator and denominator have no common factor greater than 1.  If is an integer, is even if and only if is even.   "
},
{
  "id": "exercises-3-9-2",
  "level": "2",
  "url": "s-proof-review.html#exercises-3-9-2",
  "type": "Exercise",
  "number": "3.9.3.1",
  "title": "",
  "body": "Prove that the sum of two odd positive integers is an even positive integer. You might want to read before attempting this. The given statement can be written in if , then format as: If and are two odd positive integers, then is an even integer.  Proof: Assume and are two positive odd integers. It can be shown that .  odd and positive for some ,  odd and positive for some .  Then, Therefore, is an even positive integer.  "
},
{
  "id": "exercises-3-9-3",
  "level": "2",
  "url": "s-proof-review.html#exercises-3-9-3",
  "type": "Exercise",
  "number": "3.9.3.2",
  "title": "",
  "body": "Write out a complete proof that if is an integer, is even if and only if is even.  ( ) We prove that if is even then is even by contradiction. If isn't even, it must be odd and for some integer . This means that which means that is odd, a contradiction.  ( ) If is even, then for some integer , and so is even  "
},
{
  "id": "exercises-3-9-4",
  "level": "2",
  "url": "s-proof-review.html#exercises-3-9-4",
  "type": "Exercise",
  "number": "3.9.3.3",
  "title": "",
  "body": "Write out a complete proof that is irrational. Proof: (Indirect) Assume to the contrary, that is a rational number. Then there exists where and where is in lowest terms, that is, and have no common factor other than 1.   Hence both and have a common factor, namely 2, which is a contradiction. "
},
{
  "id": "exercises-3-9-5",
  "level": "2",
  "url": "s-proof-review.html#exercises-3-9-5",
  "type": "Exercise",
  "number": "3.9.3.4",
  "title": "",
  "body": "Prove that the cube root of 2 is an irrational number.  This proof is similar to the proof that the square root of 2 is irrational.  Assume to the contrary, that is a rational number. Then there exists where and where is in lowest terms, that is, and have no common factor other than 1.   Hence both and have a common factor, namely 2, which is a contradiction.   "
},
{
  "id": "exercises-3-9-6",
  "level": "2",
  "url": "s-proof-review.html#exercises-3-9-6",
  "type": "Exercise",
  "number": "3.9.3.5",
  "title": "",
  "body": "Prove that if and are real numbers such that , then or . Proof: (Indirect) Assume and . Assume to the contrary that is false, which is equivalent to . Hence . This contradicts the assumption that .  "
},
{
  "id": "exercises-3-9-7",
  "level": "2",
  "url": "s-proof-review.html#exercises-3-9-7",
  "type": "Exercise",
  "number": "3.9.3.6",
  "title": "",
  "body": "Order selected statments from the following list to construct a proof proof that the sum of two odd integers and is an even integer         is even  "
},
{
  "id": "s-proof-methods-sets",
  "level": "1",
  "url": "s-proof-methods-sets.html",
  "type": "Section",
  "number": "4.1",
  "title": "Methods of Proof for Sets",
  "body": "Methods of Proof for Sets  If , , and are arbitrary sets, is it always true that ? There are a variety of ways that we could attempt to prove that this distributive law for intersection over union is indeed true. We start with a common non-proof and then work toward more acceptable methods.  Examples and Counterexamples  We could, for example, let , , and , and determine whether the distributive law is true for these values of , , and . In doing this we will have only determined that the distributive law is true for this one example. It does not prove the distributive law for all possible sets , , and and hence is an invalid method of proof. However, trying a few examples has considerable merit insofar as it makes us more comfortable with the statement in question. Indeed, if the statement is not true for the example, we have disproved the statement.  Counterexample  An example that disproves a statement is called a counterexample.  Disproving distributivity of addition over multiplication From basic algebra we learned that multiplication is distributive over addition. Is addition distributive over multiplication? That is, is always true? If we choose the values , , and , we find that . Therefore, this set of values serves as a counterexample to a distributive law of addition over multiplication.    Proof Using Venn Diagrams  In this method, we illustrate both sides of the statement via a Venn diagram and determine whether both Venn diagrams give us the same picture, For example, the left side of the distributive law is developed in and the right side in . Note that the final results give you the same shaded area.  The advantage of this method is that it is relatively quick and mechanical. The disadvantage is that it is workable only if there are a small number of sets under consideration. In addition, it doesn't work very well in a static environment like a book or test paper. Venn diagrams tend to work well if you have a potentially dynamic environment like a blackboard or video.   Development of the left side of the distributive law for sets   Development of the left side of the distributive law for sets     Development of the right side of the distributive law for sets   Development of the right side of the distributive law for sets     Proof using Set-membership Tables  Let be a subset of a universal set and let . To use this method we note that exactly one of the following is true: or . Denote the situation where by 1 and that where by 0. Working with two sets, and , and if , there are four possible outcomes of where can be. What are they? The set-membership table for is:   Membership Table for        0 0 0  0 1 1  1 0 1  1 1 1    This table illustrates that if and only if or .  In order to prove the distributive law via a set-membership table, write out the table for each side of the set statement to be proved and note that if and are two columns in a table, then the set statement is equal to the set statement if and only if corresponding entries in each row are the same.  To prove , first note that the statement involves three sets, , , and , so there are possibilities for the membership of an element in the sets.   Membership table to prove the distributive law of intersection over union    0 0 0 0 0 0 0 0  0 0 1 1 0 0 0 0  0 1 0 1 0 0 0 0  0 1 1 1 0 0 0 0  1 0 0 0 0 0 0 0  1 0 1 1 0 1 1 1  1 1 0 1 1 0 1 1  1 1 1 1 1 1 1 1    Since each entry in Column 7 is the same as the corresponding entry in Column 8, we have shown that for any sets , , and . The main advantage of this method is that it is mechanical. The main disadvantage is that it is reasonable to use only for a relatively small number of sets. If we are trying to prove a statement involving five sets, there are rows, which would test anyone's patience doing the work by hand.   Proof Using Definitions  This method involves using definitions and basic concepts to prove the given statement. This procedure forces one to learn, relearn, and understand basic definitions and concepts. It helps individuals to focus their attention on the main ideas of each topic and therefore is the most useful method of proof. One does not learn a topic by memorizing or occasionally glancing at core topics, but by using them in a variety of contexts. The word proof panics most people; however, everyone can become comfortable with proofs. Do not expect to prove every statement immediately. In fact, it is not our purpose to prove every theorem or fact encountered, only those that illustrate methods and\/or basic concepts. Throughout the text we will focus in on main techniques of proofs. Let's illustrate by proving the distributive law.   Proof Technique 1. State or restate the theorem so you understand what is given (the hypothesis) and what you are trying to prove (the conclusion).  The Distributive Law of Intersection over Union If , , and are sets, then .   What we can assume: , , and are sets.  What we are to prove: .  Commentary: What types of objects am I working with: sets? real numbers? propositions? The answer is sets: sets of elements that can be anything you care to imagine. The universe from which we draw our elements plays no part in the proof of this theorem.  We need to show that the two sets are equal. Let's call them the left-hand set ) and the right-hand set ( ). To prove that , we must prove two things: (a) , and (b) .  To prove part a and, similarly, part b, we must show that each element of is an element of . Once we have diagnosed the problem we are ready to begin.  We must prove: (a) .  Let : We must also prove (b) .  .   Proof Technique 2   To prove that , we must show that if , then .  To prove that , we must show:    and   .     To further illustrate the Proof-by-Definition technique, let's prove the following theorem.  Another Proof using Definitions If , , and are any sets, then .   Commentary; We again ask ourselves: What are we trying to prove? What types of objects are we dealing with? We realize that we wish to prove two facts: (a) , and (b) .  To prove part (a), and similarly part (b), we'll begin the same way. Let to show . What should be? What does a typical object in the look like?  Now, on to the actual proof.  (a) .  Let .   (b) .  Let .      Exercises   Prove the following:  Let , , and be sets. If and , then .  Let and be sets. Then .  Let be sets. If ( and ) then .  Let be sets. if and only if .  Let be sets. If then .    Assume that (condition of the conditional conclusion ). Since , by the definition of . and implies that . Therefore, if , then .   (Proof that ) Let be in . Therefore, x is in , but it is not in B; that is, and .  Assume that and . Let . By the two premises, and . Therefore, by the definition of intersection, .   (Indirect) Assume that is not a subset of . Therefore, there exists that does not belong to . . Therefore, and , a contradiction to the assumption that .  There are two cases to consider. The first is when is empty. Then the conclusion follows since both Cartesian products are empty.  If isn’t empty, we have two subcases, if is empty, , which is a subset of every set. Finally, the interesting subcase is when is not empty. Now we pick any pair . This means that is in and is in . Since is a subset of , is in and so . Therefore .     For any integer , let , the multiples of .   Prove that .  Is it true that ? Explain your answer.     ( )This half of the proof is bit tricky. Let . Since , for some integer ; and since , for some integer    ( ) If , then for some integer .    ( )    Disprove the following, assuming are sets:  .  .  implies .       If and , , while .  If and , , but is not in .  Let , , and .  If , , and , then the left hand side of the identity is while the right hand side is the empty set. Another example is , , and    Let be sets. Write the following in if . . . then . . . language and prove:  is a sufficient condition for .  is a necessary condition for .  is a necessary and sufficient condition for .    Prove by induction that if , , , ... , are sets, , then .  Proof: Let be .  Basis: We must show that is true. This was done by several methods in section 4.1.  Induction: Assume for some that is true. Then   Let , and be sets. Prove or disprove: The statement is false. The sets , and provide a counterexample. Looking ahead to Chapter 6, we would say that the relation of being non-disjoint is not transitive   "
},
{
  "id": "def-counterexample",
  "level": "2",
  "url": "s-proof-methods-sets.html#def-counterexample",
  "type": "Definition",
  "number": "4.1.1",
  "title": "Counterexample.",
  "body": "Counterexample  An example that disproves a statement is called a counterexample. "
},
{
  "id": "ex-addition-over-mult",
  "level": "2",
  "url": "s-proof-methods-sets.html#ex-addition-over-mult",
  "type": "Example",
  "number": "4.1.2",
  "title": "Disproving distributivity of addition over multiplication.",
  "body": "Disproving distributivity of addition over multiplication From basic algebra we learned that multiplication is distributive over addition. Is addition distributive over multiplication? That is, is always true? If we choose the values , , and , we find that . Therefore, this set of values serves as a counterexample to a distributive law of addition over multiplication.  "
},
{
  "id": "distrib-venn-lhs",
  "level": "2",
  "url": "s-proof-methods-sets.html#distrib-venn-lhs",
  "type": "Figure",
  "number": "4.1.3",
  "title": "",
  "body": " Development of the left side of the distributive law for sets   Development of the left side of the distributive law for sets   "
},
{
  "id": "distrib-venn-rhs",
  "level": "2",
  "url": "s-proof-methods-sets.html#distrib-venn-rhs",
  "type": "Figure",
  "number": "4.1.4",
  "title": "",
  "body": " Development of the right side of the distributive law for sets   Development of the right side of the distributive law for sets   "
},
{
  "id": "mt-union",
  "level": "2",
  "url": "s-proof-methods-sets.html#mt-union",
  "type": "Table",
  "number": "4.1.5",
  "title": "Membership Table for <span xmlns:pf=\"https:\/\/prefigure.org\" class=\"process-math\">\\(A \\cup  B\\)<\/span>",
  "body": " Membership Table for        0 0 0  0 1 1  1 0 1  1 1 1   "
},
{
  "id": "tab-mt-distr",
  "level": "2",
  "url": "s-proof-methods-sets.html#tab-mt-distr",
  "type": "Table",
  "number": "4.1.6",
  "title": "Membership table to prove the distributive law of intersection over union",
  "body": " Membership table to prove the distributive law of intersection over union    0 0 0 0 0 0 0 0  0 0 1 1 0 0 0 0  0 1 0 1 0 0 0 0  0 1 1 1 0 0 0 0  1 0 0 0 0 0 0 0  1 0 1 1 0 1 1 1  1 1 0 1 1 0 1 1  1 1 1 1 1 1 1 1   "
},
{
  "id": "th-distr-law-i-over-u",
  "level": "2",
  "url": "s-proof-methods-sets.html#th-distr-law-i-over-u",
  "type": "Theorem",
  "number": "4.1.7",
  "title": "The Distributive Law of Intersection over Union.",
  "body": "The Distributive Law of Intersection over Union If , , and are sets, then .   What we can assume: , , and are sets.  What we are to prove: .  Commentary: What types of objects am I working with: sets? real numbers? propositions? The answer is sets: sets of elements that can be anything you care to imagine. The universe from which we draw our elements plays no part in the proof of this theorem.  We need to show that the two sets are equal. Let's call them the left-hand set ) and the right-hand set ( ). To prove that , we must prove two things: (a) , and (b) .  To prove part a and, similarly, part b, we must show that each element of is an element of . Once we have diagnosed the problem we are ready to begin.  We must prove: (a) .  Let : We must also prove (b) .  .  "
},
{
  "id": "th-set-proof-example2",
  "level": "2",
  "url": "s-proof-methods-sets.html#th-set-proof-example2",
  "type": "Theorem",
  "number": "4.1.8",
  "title": "Another Proof using Definitions.",
  "body": "Another Proof using Definitions If , , and are any sets, then .   Commentary; We again ask ourselves: What are we trying to prove? What types of objects are we dealing with? We realize that we wish to prove two facts: (a) , and (b) .  To prove part (a), and similarly part (b), we'll begin the same way. Let to show . What should be? What does a typical object in the look like?  Now, on to the actual proof.  (a) .  Let .   (b) .  Let .   "
},
{
  "id": "exercise-4-1-1",
  "level": "2",
  "url": "s-proof-methods-sets.html#exercise-4-1-1",
  "type": "Exercise",
  "number": "4.1.5.1",
  "title": "",
  "body": " Prove the following:  Let , , and be sets. If and , then .  Let and be sets. Then .  Let be sets. If ( and ) then .  Let be sets. if and only if .  Let be sets. If then .    Assume that (condition of the conditional conclusion ). Since , by the definition of . and implies that . Therefore, if , then .   (Proof that ) Let be in . Therefore, x is in , but it is not in B; that is, and .  Assume that and . Let . By the two premises, and . Therefore, by the definition of intersection, .   (Indirect) Assume that is not a subset of . Therefore, there exists that does not belong to . . Therefore, and , a contradiction to the assumption that .  There are two cases to consider. The first is when is empty. Then the conclusion follows since both Cartesian products are empty.  If isn’t empty, we have two subcases, if is empty, , which is a subset of every set. Finally, the interesting subcase is when is not empty. Now we pick any pair . This means that is in and is in . Since is a subset of , is in and so . Therefore .   "
},
{
  "id": "s-proof-methods-sets-7-3",
  "level": "2",
  "url": "s-proof-methods-sets.html#s-proof-methods-sets-7-3",
  "type": "Exercise",
  "number": "4.1.5.2",
  "title": "",
  "body": " For any integer , let , the multiples of .   Prove that .  Is it true that ? Explain your answer.     ( )This half of the proof is bit tricky. Let . Since , for some integer ; and since , for some integer    ( ) If , then for some integer .    ( )   "
},
{
  "id": "s-proof-methods-sets-7-4",
  "level": "2",
  "url": "s-proof-methods-sets.html#s-proof-methods-sets-7-4",
  "type": "Exercise",
  "number": "4.1.5.3",
  "title": "",
  "body": "Disprove the following, assuming are sets:  .  .  implies .       If and , , while .  If and , , but is not in .  Let , , and .  If , , and , then the left hand side of the identity is while the right hand side is the empty set. Another example is , , and   "
},
{
  "id": "s-proof-methods-sets-7-5",
  "level": "2",
  "url": "s-proof-methods-sets.html#s-proof-methods-sets-7-5",
  "type": "Exercise",
  "number": "4.1.5.4",
  "title": "",
  "body": "Let be sets. Write the following in if . . . then . . . language and prove:  is a sufficient condition for .  is a necessary condition for .  is a necessary and sufficient condition for .   "
},
{
  "id": "ex-generalized_distrib",
  "level": "2",
  "url": "s-proof-methods-sets.html#ex-generalized_distrib",
  "type": "Exercise",
  "number": "4.1.5.5",
  "title": "",
  "body": "Prove by induction that if , , , ... , are sets, , then .  Proof: Let be .  Basis: We must show that is true. This was done by several methods in section 4.1.  Induction: Assume for some that is true. Then  "
},
{
  "id": "exercise-disjoint-not-transitive",
  "level": "2",
  "url": "s-proof-methods-sets.html#exercise-disjoint-not-transitive",
  "type": "Exercise",
  "number": "4.1.5.6",
  "title": "",
  "body": "Let , and be sets. Prove or disprove: The statement is false. The sets , and provide a counterexample. Looking ahead to Chapter 6, we would say that the relation of being non-disjoint is not transitive "
},
{
  "id": "s-laws-of-set-theory",
  "level": "1",
  "url": "s-laws-of-set-theory.html",
  "type": "Section",
  "number": "4.2",
  "title": "Laws of Set Theory",
  "body": "Laws of Set Theory  Tables of Laws  The following basic set laws can be derived using either the Basic Definition or the Set-Membership approach and can be illustrated by Venn diagrams.   Basic Laws of Set Theory    Commutative Laws   (1)  ( )   Associative Laws  (2)  ( )  Distributive Laws  (3)  ( )  Identity Laws  (4) ( )   Complement Laws  (5) ( )  Idempotent Laws  (6) ( )  Null Laws  (7) ( )  Absorption Laws  (8) ( )  DeMorgan's Laws  (9) ( )  Involution Law  (10)    It is quite clear that most of these laws resemble or, in fact, are analogues of laws in basic algebra and the algebra of propositions.   Proof Using Previously Proven Theorems  Once a few basic laws or theorems have been established, we frequently use them to prove additional theorems. This method of proof is usually more efficient than that of proof by Definition. To illustrate, let us prove the following Corollary to the Distributive Law. The term \"corollary\" is used for theorems that can be proven with relative ease from previously proven theorems.  A Corollary to the Distributive Law of Sets  Let A and B be sets. Then .    .     Proof Using the Indirect Method\/Contradiction  The procedure one most frequently uses to prove a theorem in mathematics is the Direct Method, as illustrated in and . Occasionally there are situations where this method is not applicable. Consider the following:   An Indirect Proof in Set Theory  Let be sets. If and , then .   Commentary: The usual and first approach would be to assume and is true and to attempt to prove is true. To do this you would need to show that nothing is contained in the set . Think about how you would show that something doesn't exist. It is very difficult to do directly.  The Indirect Method is much easier: If we assume the conclusion is false and we obtain a contradiction --- then the theorem must be true. This approach is on sound logical footing since it is exactly the same method of indirect proof that we discussed in .  Assume and , and . To prove that this cannot occur, let .  .  But this contradicts the second premise. Hence, the theorem is proven.    Exercises  In the exercises that follow it is most important that you outline the logical procedures or methods you use.     Prove the associative law for intersection (Law ) with a Venn diagram.  Prove DeMorgan's Law (Law 9) with a membership table.  Prove the Idempotent Law (Law 6) using basic definitions.     The last two columns are the same so the two sets must be equal.   Therefore, .  Therefore, and so we have .       Prove the Absorption Law (Law ) with a membership table.  Prove the Involution Law (Law 10) using basic definitions.       The first and last columns are the same so the two sets must be equal.   Therefore, .     Prove the following using the set theory laws, as well as any other theorems proved so far.              For all parts of this exercise, a reason should be supplied for each step. We have supplied reasons only for part a and left them out of the other parts to give you further practice.   .  .  Select any element, . One such element exists since is not empty. . Therefore,  .  .    Use previously proven theorems to prove the following.          .             Let's call part (b) of the Difference Law.       Hierarchy of Set Operations The rules that determine the order of evaluation in a set expression that involves more than one operation are similar to the rules for logic. In the absence of parentheses, complementations are done first, intersections second, and unions third. Parentheses are used to override this order. If the same operation appears two or more consecutive times, evaluate from left to right. In what order are the following expressions performed?   .  .                 There are several ways that we can use to format the proofs in this chapter. One that should be familiar to you from Chapter 3 is illustrated with the following alternate proof of part (a) in :   An alternate format for the proof of   (1)  Premise  (2)  (1), definition of intersection  (3) (  (2), definition of union  (4)   (3), distribute over   (5)   (4), definition of intersection  (6)   (5), definition of union     Prove part (b) of and using this format.    Direct Proof of the half of   (1)  Premise  (2)  (1), definition of Cartesian Product  (3)   (1), definition of Cartesian Product  (4)   (3),definition of intersection  (5)   (2),(4), definition of Cartesian Product  (6)   (3),definition of intersection  (7)   (2),(6), definition of Cartesian Product  (8)    (5),(7), definition of Cartesian Product    Indirect Proof of   (1)  Negated Conclusion - non-empty  (2)  (1), definition of intersection  (3)   Premise  (4)   (2),(3),definition of subset  (5)   (1), definition of intersection  (6)   (4),(5), definition of intersection  (7) Premise  (7) Contradiction (6),(7)        "
},
{
  "id": "table-set-laws",
  "level": "2",
  "url": "s-laws-of-set-theory.html#table-set-laws",
  "type": "Table",
  "number": "4.2.1",
  "title": "Basic Laws of Set Theory",
  "body": " Basic Laws of Set Theory    Commutative Laws   (1)  ( )   Associative Laws  (2)  ( )  Distributive Laws  (3)  ( )  Identity Laws  (4) ( )   Complement Laws  (5) ( )  Idempotent Laws  (6) ( )  Null Laws  (7) ( )  Absorption Laws  (8) ( )  DeMorgan's Laws  (9) ( )  Involution Law  (10)   "
},
{
  "id": "th-corollary-to-distr",
  "level": "2",
  "url": "s-laws-of-set-theory.html#th-corollary-to-distr",
  "type": "Corollary",
  "number": "4.2.2",
  "title": "A Corollary to the Distributive Law of Sets.",
  "body": "A Corollary to the Distributive Law of Sets  Let A and B be sets. Then .    .   "
},
{
  "id": "theorem-example-sets-contradiction",
  "level": "2",
  "url": "s-laws-of-set-theory.html#theorem-example-sets-contradiction",
  "type": "Theorem",
  "number": "4.2.3",
  "title": "An Indirect Proof in Set Theory.",
  "body": " An Indirect Proof in Set Theory  Let be sets. If and , then .   Commentary: The usual and first approach would be to assume and is true and to attempt to prove is true. To do this you would need to show that nothing is contained in the set . Think about how you would show that something doesn't exist. It is very difficult to do directly.  The Indirect Method is much easier: If we assume the conclusion is false and we obtain a contradiction --- then the theorem must be true. This approach is on sound logical footing since it is exactly the same method of indirect proof that we discussed in .  Assume and , and . To prove that this cannot occur, let .  .  But this contradicts the second premise. Hence, the theorem is proven.  "
},
{
  "id": "exer-4-2-3",
  "level": "2",
  "url": "s-laws-of-set-theory.html#exer-4-2-3",
  "type": "Exercise",
  "number": "4.2.4.1",
  "title": "",
  "body": "  Prove the associative law for intersection (Law ) with a Venn diagram.  Prove DeMorgan's Law (Law 9) with a membership table.  Prove the Idempotent Law (Law 6) using basic definitions.     The last two columns are the same so the two sets must be equal.   Therefore, .  Therefore, and so we have .   "
},
{
  "id": "exer-4-2-4",
  "level": "2",
  "url": "s-laws-of-set-theory.html#exer-4-2-4",
  "type": "Exercise",
  "number": "4.2.4.2",
  "title": "",
  "body": "   Prove the Absorption Law (Law ) with a membership table.  Prove the Involution Law (Law 10) using basic definitions.       The first and last columns are the same so the two sets must be equal.   Therefore, .    "
},
{
  "id": "exer-4-2-5",
  "level": "2",
  "url": "s-laws-of-set-theory.html#exer-4-2-5",
  "type": "Exercise",
  "number": "4.2.4.3",
  "title": "",
  "body": "Prove the following using the set theory laws, as well as any other theorems proved so far.              For all parts of this exercise, a reason should be supplied for each step. We have supplied reasons only for part a and left them out of the other parts to give you further practice.   .  .  Select any element, . One such element exists since is not empty. . Therefore,  .  .   "
},
{
  "id": "exer-4-2-6",
  "level": "2",
  "url": "s-laws-of-set-theory.html#exer-4-2-6",
  "type": "Exercise",
  "number": "4.2.4.4",
  "title": "",
  "body": "Use previously proven theorems to prove the following.          .             Let's call part (b) of the Difference Law.      "
},
{
  "id": "exer-4-2-7",
  "level": "2",
  "url": "s-laws-of-set-theory.html#exer-4-2-7",
  "type": "Exercise",
  "number": "4.2.4.5",
  "title": "Hierarchy of Set Operations.",
  "body": "Hierarchy of Set Operations The rules that determine the order of evaluation in a set expression that involves more than one operation are similar to the rules for logic. In the absence of parentheses, complementations are done first, intersections second, and unions third. Parentheses are used to override this order. If the same operation appears two or more consecutive times, evaluate from left to right. In what order are the following expressions performed?   .  .               "
},
{
  "id": "exer-4-2-8",
  "level": "2",
  "url": "s-laws-of-set-theory.html#exer-4-2-8",
  "type": "Exercise",
  "number": "4.2.4.6",
  "title": "",
  "body": " There are several ways that we can use to format the proofs in this chapter. One that should be familiar to you from Chapter 3 is illustrated with the following alternate proof of part (a) in :   An alternate format for the proof of   (1)  Premise  (2)  (1), definition of intersection  (3) (  (2), definition of union  (4)   (3), distribute over   (5)   (4), definition of intersection  (6)   (5), definition of union     Prove part (b) of and using this format.    Direct Proof of the half of   (1)  Premise  (2)  (1), definition of Cartesian Product  (3)   (1), definition of Cartesian Product  (4)   (3),definition of intersection  (5)   (2),(4), definition of Cartesian Product  (6)   (3),definition of intersection  (7)   (2),(6), definition of Cartesian Product  (8)    (5),(7), definition of Cartesian Product    Indirect Proof of   (1)  Negated Conclusion - non-empty  (2)  (1), definition of intersection  (3)   Premise  (4)   (2),(3),definition of subset  (5)   (1), definition of intersection  (6)   (4),(5), definition of intersection  (7) Premise  (7) Contradiction (6),(7)      "
},
{
  "id": "s-minsets",
  "level": "1",
  "url": "s-minsets.html",
  "type": "Section",
  "number": "4.3",
  "title": "Minsets",
  "body": "Minsets  Definition of Minsets  Let and be subsets of a set . Notice that the Venn diagram of is naturally partitioned into the subsets , , , and . Further we observe that , , , and can be described in terms of and as follows:   Venn Diagram of Minsets   Minsets generated by and      Minsets generated by two sets         Each is called a minset generated by and . We note that each minset is formed by taking the intersection of two sets where each may be either or its complement, . Note also, given two sets, there are minsets.  Minsets are occasionally called minterms .  The reader should note that if we apply all possible combinations of the operations intersection, union, and complementation to the sets and of , the smallest sets generated will be exactly the minsets, the minimum sets. Hence the derivation of the term minset.  Next, consider the Venn diagram containing three sets, , , and . Draw it right now and count the regions! What are the minsets generated by , , and ? How many are there? Following the procedures outlined above, we note that the following are three of the minsets. What are the others?   Three of the minsets generated by , , and        Minset Minset  Let be a set of subsets of set . Sets of the form , where each may be either or , is called a minset generated by , ,... and .  A concrete example of some minsets Consider the following example. Let with subsets and . How can we use set operations to and produce a partition of ? As a first attempt, we might try these three sets:      .    We have produced all elements of but we have 4 and 6 repeated in two sets. In place of and , let's try and , respectively:    and  .    We have now produced the elements 1, 2, 3, and 5 using , and yet we have not listed the elements 4 and 6. Most ways that we could combine and such as or will produce duplications of listed elements and will not produce both 4 and 6. However we note that , exactly the elements we need.  After more experimenting, we might reach a conclusion that each element of appears exactly once in one of the four minsets , , and . Hence, we have a partition of . In fact this is the finest partition of in that all other partitions we could generate consist of selected unions of these minsets.  At this point, we might ask and be able to answer the question How many different subsets of our universe can we generate from and ? The answer is , which is in this case. Notice that in general, it would be impossible to find two sets from which we could generate all subsets of since there will never be more than four nonempty minsets. If we allowed ourselves three subsets and tried to generat all sets from them, then the number of minsets would be . With only six elements in , there could be six minsets, each containing a single element. In that case we could generate the whole power set of .    Properties of Minsets  Minset Partition Theorem Let be a set and let ,  , be subsets of . The set of nonempty minsets generated by ,  , is a partition of .  The proof of this theorem is left to the reader.  One of the most significant facts about minsets is that any subset of that can be obtained from ,  , , using the standard set operations can be obtained in a standard form by taking the union of selected minsets.  Minset Normal Form  Minset Normal Form  A set is said to be in minset normal form when it is expressed as the union of zero or more distinct nonempty minsets.   Notes:   The union of zero sets is the empty set, .  Minset normal form is also called canonical form .   Compact Minset Notation Compact Minset Notation  Let be a set of subsets of set . If is equal to 0 or 1 and is any set, then is defined to be if and if . Then we can denote a minset compactly as an expression where   Another Concrete Example of Minsets Let , , and . Then              In this case, there are only three nonempty minsets, producing the partition . An example of a set that could not be produced from just and is the set of even elements of , . This is because and cannot be separated. They are in the same minset and any union of minsets either includes or excludes them both. In general, there are different minset normal forms because there are three nonempty minsets. This means that only 8 of the subsets of could be generated from any two sets and .     Exercises  Consider the subsets , , and , where .   List the nonempty minsets generated by .  How many elements of the power set of can be generated by , , and ? Compare this number with . Give an example of one subset that cannot be generated by , , and .        , as compared with . is one of the 992 sets that can't be generated.      Partition into the minsets generated by , , and .  How many different subsets of can you create using , and with the standard set operations?  Do there exist subsets whose minsets will generate every subset of ?      There are seven non-empty minsets. Using the compact notation they are , , , , , and . is empty.  different subsets can be generated.  There does not exist a set of three subsets that generates all subsets. This is because the maximum number of subsets that can be generated is . Another way to look at it is that the minsets are a partition of into eight minsets and at least one of those minsets must contain two or more elements. Those elements can't be separated by any set operations. The logic behind this second way of looking at the problem is going to be formalized in with the Pigeonhole Principle.      Partition the set of strings of 0's and 1's of length two or less, using the minsets generated by , and .   and generate minsets , and . Note: is the null string, which has length zero.    Let , and be subsets of a universal set ,   Symbolically list all minsets generated by , and .  Illustrate with a Venn diagram all minsets obtained in part (a).  Express the following sets in minset normal form: , , .         Eight minsets in a three set Venn diagram.   .       Partition with the nonempty minsets generated by and .  How many different subsets of can you generate from ?      , , ,   , since there are 3 nonempty minsets.    If is a partition of , how many minsets are generated by ?   There are just nonempty minset, and they are the sets in the partition.   Prove Let . For each , , or , since by the complement law. Let if , and otherwise. Since is in each , it must be in the minset . Now consider two different minsets , and , where each and is either or . Since these minsets are not equal, , for some . Therefore, , since two of the sets in the intersection are disjoint. Since every element of is in a minset and the minsets are disjoint, the nonempty minsets must form a partition of .     "
},
{
  "id": "fig-minsets-2",
  "level": "2",
  "url": "s-minsets.html#fig-minsets-2",
  "type": "Figure",
  "number": "4.3.1",
  "title": "",
  "body": " Venn Diagram of Minsets   Minsets generated by and    "
},
{
  "id": "tab-minsets-2",
  "level": "2",
  "url": "s-minsets.html#tab-minsets-2",
  "type": "Table",
  "number": "4.3.2",
  "title": "Minsets generated by two sets",
  "body": " Minsets generated by two sets        "
},
{
  "id": "tab-some-minsets-3",
  "level": "2",
  "url": "s-minsets.html#tab-some-minsets-3",
  "type": "Table",
  "number": "4.3.3",
  "title": "Three of the minsets generated by <span xmlns:pf=\"https:\/\/prefigure.org\" class=\"process-math\">\\(B_1\\text{,}\\)<\/span> <span class=\"process-math\">\\(B_2\\text{,}\\)<\/span> and <span class=\"process-math\">\\(B_3\\)<\/span>",
  "body": " Three of the minsets generated by , , and       "
},
{
  "id": "def-Minset",
  "level": "2",
  "url": "s-minsets.html#def-Minset",
  "type": "Definition",
  "number": "4.3.4",
  "title": "Minset.",
  "body": "Minset Minset  Let be a set of subsets of set . Sets of the form , where each may be either or , is called a minset generated by , ,... and . "
},
{
  "id": "ex-minset-example",
  "level": "2",
  "url": "s-minsets.html#ex-minset-example",
  "type": "Example",
  "number": "4.3.5",
  "title": "A concrete example of some minsets.",
  "body": "A concrete example of some minsets Consider the following example. Let with subsets and . How can we use set operations to and produce a partition of ? As a first attempt, we might try these three sets:      .    We have produced all elements of but we have 4 and 6 repeated in two sets. In place of and , let's try and , respectively:    and  .    We have now produced the elements 1, 2, 3, and 5 using , and yet we have not listed the elements 4 and 6. Most ways that we could combine and such as or will produce duplications of listed elements and will not produce both 4 and 6. However we note that , exactly the elements we need.  After more experimenting, we might reach a conclusion that each element of appears exactly once in one of the four minsets , , and . Hence, we have a partition of . In fact this is the finest partition of in that all other partitions we could generate consist of selected unions of these minsets.  At this point, we might ask and be able to answer the question How many different subsets of our universe can we generate from and ? The answer is , which is in this case. Notice that in general, it would be impossible to find two sets from which we could generate all subsets of since there will never be more than four nonempty minsets. If we allowed ourselves three subsets and tried to generat all sets from them, then the number of minsets would be . With only six elements in , there could be six minsets, each containing a single element. In that case we could generate the whole power set of .  "
},
{
  "id": "th-minset-partition",
  "level": "2",
  "url": "s-minsets.html#th-minset-partition",
  "type": "Theorem",
  "number": "4.3.8",
  "title": "Minset Partition Theorem.",
  "body": "Minset Partition Theorem Let be a set and let ,  , be subsets of . The set of nonempty minsets generated by ,  , is a partition of .  The proof of this theorem is left to the reader. "
},
{
  "id": "def-minset-normal-form",
  "level": "2",
  "url": "s-minsets.html#def-minset-normal-form",
  "type": "Definition",
  "number": "4.3.9",
  "title": "Minset Normal Form.",
  "body": "Minset Normal Form  Minset Normal Form  A set is said to be in minset normal form when it is expressed as the union of zero or more distinct nonempty minsets.  "
},
{
  "id": "s-minsets-3-6",
  "level": "2",
  "url": "s-minsets.html#s-minsets-3-6",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "canonical form "
},
{
  "id": "def-compact-Minset",
  "level": "2",
  "url": "s-minsets.html#def-compact-Minset",
  "type": "Definition",
  "number": "4.3.10",
  "title": "Compact Minset Notation.",
  "body": "Compact Minset Notation Compact Minset Notation  Let be a set of subsets of set . If is equal to 0 or 1 and is any set, then is defined to be if and if . Then we can denote a minset compactly as an expression where  "
},
{
  "id": "ex-concrete-minsets-2",
  "level": "2",
  "url": "s-minsets.html#ex-concrete-minsets-2",
  "type": "Example",
  "number": "4.3.11",
  "title": "Another Concrete Example of Minsets.",
  "body": "Another Concrete Example of Minsets Let , , and . Then              In this case, there are only three nonempty minsets, producing the partition . An example of a set that could not be produced from just and is the set of even elements of , . This is because and cannot be separated. They are in the same minset and any union of minsets either includes or excludes them both. In general, there are different minset normal forms because there are three nonempty minsets. This means that only 8 of the subsets of could be generated from any two sets and .  "
},
{
  "id": "exercises-4-3-2",
  "level": "2",
  "url": "s-minsets.html#exercises-4-3-2",
  "type": "Exercise",
  "number": "4.3.3.1",
  "title": "",
  "body": "Consider the subsets , , and , where .   List the nonempty minsets generated by .  How many elements of the power set of can be generated by , , and ? Compare this number with . Give an example of one subset that cannot be generated by , , and .        , as compared with . is one of the 992 sets that can't be generated.   "
},
{
  "id": "exercises-4-3-3",
  "level": "2",
  "url": "s-minsets.html#exercises-4-3-3",
  "type": "Exercise",
  "number": "4.3.3.2",
  "title": "",
  "body": "  Partition into the minsets generated by , , and .  How many different subsets of can you create using , and with the standard set operations?  Do there exist subsets whose minsets will generate every subset of ?      There are seven non-empty minsets. Using the compact notation they are , , , , , and . is empty.  different subsets can be generated.  There does not exist a set of three subsets that generates all subsets. This is because the maximum number of subsets that can be generated is . Another way to look at it is that the minsets are a partition of into eight minsets and at least one of those minsets must contain two or more elements. Those elements can't be separated by any set operations. The logic behind this second way of looking at the problem is going to be formalized in with the Pigeonhole Principle.    "
},
{
  "id": "exercises-4-3-4",
  "level": "2",
  "url": "s-minsets.html#exercises-4-3-4",
  "type": "Exercise",
  "number": "4.3.3.3",
  "title": "",
  "body": " Partition the set of strings of 0's and 1's of length two or less, using the minsets generated by , and .   and generate minsets , and . Note: is the null string, which has length zero.  "
},
{
  "id": "exercise-minsets-3",
  "level": "2",
  "url": "s-minsets.html#exercise-minsets-3",
  "type": "Exercise",
  "number": "4.3.3.4",
  "title": "",
  "body": " Let , and be subsets of a universal set ,   Symbolically list all minsets generated by , and .  Illustrate with a Venn diagram all minsets obtained in part (a).  Express the following sets in minset normal form: , , .         Eight minsets in a three set Venn diagram.   .    "
},
{
  "id": "exercises-4-3-6",
  "level": "2",
  "url": "s-minsets.html#exercises-4-3-6",
  "type": "Exercise",
  "number": "4.3.3.5",
  "title": "",
  "body": "  Partition with the nonempty minsets generated by and .  How many different subsets of can you generate from ?      , , ,   , since there are 3 nonempty minsets.   "
},
{
  "id": "exercises-4-3-7",
  "level": "2",
  "url": "s-minsets.html#exercises-4-3-7",
  "type": "Exercise",
  "number": "4.3.3.6",
  "title": "",
  "body": "If is a partition of , how many minsets are generated by ?   There are just nonempty minset, and they are the sets in the partition.  "
},
{
  "id": "exercises-4-3-8",
  "level": "2",
  "url": "s-minsets.html#exercises-4-3-8",
  "type": "Exercise",
  "number": "4.3.3.7",
  "title": "",
  "body": "Prove Let . For each , , or , since by the complement law. Let if , and otherwise. Since is in each , it must be in the minset . Now consider two different minsets , and , where each and is either or . Since these minsets are not equal, , for some . Therefore, , since two of the sets in the intersection are disjoint. Since every element of is in a minset and the minsets are disjoint, the nonempty minsets must form a partition of .  "
},
{
  "id": "s-duality-principle",
  "level": "1",
  "url": "s-duality-principle.html",
  "type": "Section",
  "number": "4.4",
  "title": "The Duality Principle",
  "body": "The Duality Principle   In Section 4.2, we observed that each of the labeled 1 through 9 had an analogue through . We notice that each of the laws in one column can be obtained from the corresponding law in the other column by replacing by , by , by , by , and leaving the complement unchanged.  Duality Principle for Sets Let be any identity involving sets and the operations complement, intersection and union. If is obtained from by making the substitutions , , , and , then the statement is also true and it is called the dual of the statement .  Example of a dual The dual of is .  One should not underestimate the importance of this concept. It gives us a whole second set of identities, theorems, and concepts. For example, we can consider the dual of minsets and minset normal form to obtain what is called maxsets and maxset normal form .   Exercises   State the dual of each of the following:  .              Examine and then write a description of the principle of duality for logic.   Let be a true logical equivalence involving conjuction, disjunction, negation, contradiction and tautology ( , resp.)). Then the logical equivalence obtained by exchanging conjuction and disjuction ( ), and exchanging contradiction and tautology ( ) is also true. Note that there is no simple duel for the conditional operator, but one can use the fact that is equivalent to , which would have dual .   Write the dual of each of the following:   .          Use the principle of duality and the definition of minset to write the definition of maxset.  Maxset Maxset  Let be a set of subsets of set . Sets of the form , where each may be either or , is called a maxset generated by , ,... and .  Note that the set of maxsets is not a partition of , however one can prove that every set generated by the 's is the intersection of maxsets.    Let and let and .   Find the maxsets generated by and . Note the set of maxsets does not constitute a partition of . Can you explain why?  Write out the definition of maxset normal form.  Repeat for maxsets.    The maxsets are:           They do not form a partition of since it is not true that the intersection of any two of them is empty. A set is said to be in maxset normal form when it is expressed as the intersection of distinct nonempty maxsets or it is the universal set .   What is the dual of the expression in ?  The dual is .    "
},
{
  "id": "def-duality-sets",
  "level": "2",
  "url": "s-duality-principle.html#def-duality-sets",
  "type": "Definition",
  "number": "4.4.1",
  "title": "Duality Principle for Sets.",
  "body": "Duality Principle for Sets Let be any identity involving sets and the operations complement, intersection and union. If is obtained from by making the substitutions , , , and , then the statement is also true and it is called the dual of the statement . "
},
{
  "id": "ex-dual-example",
  "level": "2",
  "url": "s-duality-principle.html#ex-dual-example",
  "type": "Example",
  "number": "4.4.2",
  "title": "Example of a dual.",
  "body": "Example of a dual The dual of is . "
},
{
  "id": "exer-4-4-2",
  "level": "2",
  "url": "s-duality-principle.html#exer-4-4-2",
  "type": "Exercise",
  "number": "4.4.2.1",
  "title": "",
  "body": " State the dual of each of the following:  .            "
},
{
  "id": "exer-4-4-3",
  "level": "2",
  "url": "s-duality-principle.html#exer-4-4-3",
  "type": "Exercise",
  "number": "4.4.2.2",
  "title": "",
  "body": " Examine and then write a description of the principle of duality for logic.   Let be a true logical equivalence involving conjuction, disjunction, negation, contradiction and tautology ( , resp.)). Then the logical equivalence obtained by exchanging conjuction and disjuction ( ), and exchanging contradiction and tautology ( ) is also true. Note that there is no simple duel for the conditional operator, but one can use the fact that is equivalent to , which would have dual .  "
},
{
  "id": "exer-4-4-4",
  "level": "2",
  "url": "s-duality-principle.html#exer-4-4-4",
  "type": "Exercise",
  "number": "4.4.2.3",
  "title": "",
  "body": "Write the dual of each of the following:   .         "
},
{
  "id": "exer-4-4-5",
  "level": "2",
  "url": "s-duality-principle.html#exer-4-4-5",
  "type": "Exercise",
  "number": "4.4.2.4",
  "title": "",
  "body": "Use the principle of duality and the definition of minset to write the definition of maxset.  Maxset Maxset  Let be a set of subsets of set . Sets of the form , where each may be either or , is called a maxset generated by , ,... and .  Note that the set of maxsets is not a partition of , however one can prove that every set generated by the 's is the intersection of maxsets.  "
},
{
  "id": "exer-4-4-6",
  "level": "2",
  "url": "s-duality-principle.html#exer-4-4-6",
  "type": "Exercise",
  "number": "4.4.2.5",
  "title": "",
  "body": " Let and let and .   Find the maxsets generated by and . Note the set of maxsets does not constitute a partition of . Can you explain why?  Write out the definition of maxset normal form.  Repeat for maxsets.    The maxsets are:           They do not form a partition of since it is not true that the intersection of any two of them is empty. A set is said to be in maxset normal form when it is expressed as the intersection of distinct nonempty maxsets or it is the universal set .  "
},
{
  "id": "exer-4-4-7",
  "level": "2",
  "url": "s-duality-principle.html#exer-4-4-7",
  "type": "Exercise",
  "number": "4.4.2.6",
  "title": "",
  "body": "What is the dual of the expression in ?  The dual is .  "
},
{
  "id": "s-basic-matrix-definitions",
  "level": "1",
  "url": "s-basic-matrix-definitions.html",
  "type": "Section",
  "number": "5.1",
  "title": "Basic Definitions and Operations",
  "body": "Basic Definitions and Operations  Matrix Order and Equality  matrix  A matrix is a rectangular array of elements of the form    A convenient way of describing a matrix in general is to designate each entry via its position in the array. That is, the entry is the entry in the third row and fourth column of the matrix . Depending on the situation, we will decide in advance to which set the entries in a matrix will belong. For example, we might assume that each entry ( , ) is a real number. In that case we would use to stand for the set of all by matrices whose entries are real numbers. If we decide that the entries in a matrix must come from a set , we use to denote all such matrices.  The Order of a Matrix A matrix that has rows and columns is called an (read by ) matrix, and is said to have order .  Since it is rather cumbersome to write out the large rectangular array above each time we wish to discuss the generalized form of a matrix, it is common practice to replace the above by . In general, matrices are often given names that are capital letters and the corresponding lower case letter is used for individual entries. For example the entry in the third row, second column of a matrix called would be .  Orders of Some Matrices , , and are , , and matrices, respectively.   Since we now understand what a matrix looks like, we are in a position to investigate the operations of matrix algebra for which users have found the most applications.  First we ask ourselves: Is the matrix equal to the matrix ? No, they are not because the corresponding entries in the second row, second column of the two matrices are not equal.  Next, is equal to ? No, although the corresponding entries in the first two columns are identical, doesn't have a third column to compare to that of . We formalize these observations in the following definition.  Equality of Matrices  A matrix is said to be equal to matrix (written ) if and only if:    and have the same order, and  all corresponding entries are equal: that is, = for all appropriate and .      Matrix Addition and Scalar Multiplication  The first two operations we introduce are very natural and are not likely cause much confusion. The first is matrix addition. It seems natural that if and , then   However, if and , is there a natural way to add them to give us ? No, the orders of the two matrices must be identical.  Matrix Addition Matrix Addition  Let and be matrices. Then is an matrix where (read The th th entry of the matrix is obtained by adding the th th entry of to the th th entry of ). If the orders of and are not identical, is not defined.  In short, is defined if and only if and are of the same order.  Another frequently used operation is that of multiplying a matrix by a number, commonly called a scalar in this context. Scalars normally come from the same set as the entries in a matrix. For example, if , a scalar can be any real number.  A Scalar Product If and if and we wish to find , it seems natural to multiply each entry of by 3 so that , and this is precisely the way scalar multiplication is defined.  Scalar Multiplication Scalar Multiplication Let be an matrix and a scalar. Then is the matrix obtained by multiplying times each entry of ; that is .    Matrix Multiplication  A definition that is more awkward to motivate is the product of two matrices. See for an attempt to do so. In time, the reader will see that the following definition of the product of matrices will be very useful, and will provide an algebraic system that is quite similar to elementary algebra.   Matrix Multiplication Matrix Multiplication Let be an matrix and let be an matrix. The product of and , denoted by , is an matrix whose th row th column entry is for and .   The mechanics of computing one entry in the product of two matrices is illustrated in .   Computation of one entry in the product of two 3 by 3 matrices   Computation of one entry in the product of two 3 by 3 matrices. Two three by three matrices are shown with row 1 of the first matrix and column 2 of the second highlighted. Row 1 of the first matrix has entries 1, -1 and 0. Column 2 of the second matrix has 2, 3 and 4. The corresponding entries are multiplied and the products are added to produce the entry in row 1, column 2 of the product matrix. That number is 1 time 2 plus -1 times 3 plus 0 times 4, which is -1.    The computation of a product can take a considerable amount of time in comparison to the time required to add two matrices. Suppose that and are matrices; then is determined performing multiplications and additions. The full product takes multiplications and additions. This compares with additions for the sum of two matrices. The product of two 10 by 10 matrices will require 1,000 multiplications and 900 additions, clearly a job that you would assign to a computer. The sum of two matrices requires a more modest 100 additions. This analysis is based on the assumption that matrix multiplication will be done using the formula that is given in the definition. There are more advanced methods that, in theory, reduce operation counts. For example, Strassen's algorithm ( ) computes the product of two by matrices in operations. There are practical issues involved in actually using the algorithm in many situations. For example, round-off error can be more of a problem than with the standard formula.  A Matrix Product Let , a matrix, and let , a matrix. Then is a matrix:   Remarks:   The product is defined only if is an matrix and is an matrix; that is, the two inner numbers must be equal. Furthermore, the order of the product matrix is the outer numbers, in this case .   It is wise to first determine the order of a product matrix. For example, if is a matrix and is a matrix, then is a matrix of the form Then to obtain, for example, , we multiply corresponding entries in the third row of times the first column of and add the results.    Multiplication with a diagonal matrix Let and . Then   The net effect is to multiply the first row of by and the second row of by 3.  Note: . The columns of are multiplied by and 3 when the order is switched.  Remarks:   An matrix is called a square matrix .  If is a square matrix, is defined and is denoted by , and , etc.  The matrices whose entries are all 0 are denoted by , or simply , when no confusion arises regarding the order. the by zero matrix    Exercises   Let , , and    Compute and .  Compute and .  If , show that .  Show that .  Compute .  Compute .  Compute and , where is the zero matrix.  Compute , where 0 is the real number (scalar) zero.  Let and . Show that .   For parts c, d and i of this exercise, only a verification is needed. Here, we supply the result that will appear on both sides of the equality.                          Let , , and Compute, if possible;                               can not be computed - incompatible orders.      Let . Find a matrix such that and , where .     Find and where is as in Exercise 3, where and . What do you notice?  and  Find if . What is equal to?       Determine and  .  What is equal to for any ?  Prove your answer to part (b) by induction.     and .   for for all ?  The basis follows by the definition that . If, for any , , then      If show that is a way of expressing the system using matrices.  Express the following systems of equations using matrices:              equals if and only if both of the equalities are true.  (i)                  In this exercise, we propose to show how matrix multiplication is a natural operation. Suppose a bakery produces bread, cakes and pies every weekday, Monday through Friday. Based on past sales history, the bakery produces various numbers of each product each day, summarized in the matrix . It should be noted that the order could be described as number of days by number of products. For example, on Wednesday (the third day) the number of cakes (second product in our list) that are produced is .  The main ingredients of these products are flour, sugar and eggs. We assume that other ingredients are always in ample supply, but we need to be sure to have the three main ones available. For each of the three products, The amount of each ingredient that is needed is summarized in the , or number of products by number of ingredients matrix . For example, to bake a cake (second product) we need cups of flour (first ingredient). Regarding units: flour and sugar are given in cups per unit of each product, while eggs are given in individual eggs per unit of each product. These amounts are made up , so don't used them to do your own baking!    How many cups of flour will the bakery need every Monday? Pay close attention to how you compute your answer and the units of each number.  How many eggs will the bakery need every Wednesday?  Compute the matrix product . What do you notice?  Suppose the costs of ingredients are for a cup of flour, for a cup of sugar and for one egg. How can this information be put into a matrix that can meaningfully be multiplied by one of the other matrices in this problem?      The number of cups of flour needed every Monday is Notice that is the inner product of row 1 of with column 1 of .  Compute the inner product of row 3 of with column 3 of to get 23 eggs.   There are two possibilities. Let . If we compute we get the cost of ingredients for each of the individual products. If we compute we get the total cost of ingredients for each day.     "
},
{
  "id": "def-matrix",
  "level": "2",
  "url": "s-basic-matrix-definitions.html#def-matrix",
  "type": "Definition",
  "number": "5.1.1",
  "title": "matrix.",
  "body": "matrix  A matrix is a rectangular array of elements of the form   "
},
{
  "id": "def-matrix-order",
  "level": "2",
  "url": "s-basic-matrix-definitions.html#def-matrix-order",
  "type": "Definition",
  "number": "5.1.2",
  "title": "The Order of a Matrix.",
  "body": "The Order of a Matrix A matrix that has rows and columns is called an (read by ) matrix, and is said to have order . "
},
{
  "id": "example-orders-of-matrices",
  "level": "2",
  "url": "s-basic-matrix-definitions.html#example-orders-of-matrices",
  "type": "Example",
  "number": "5.1.3",
  "title": "Orders of Some Matrices.",
  "body": "Orders of Some Matrices , , and are , , and matrices, respectively.  "
},
{
  "id": "def-matrix-equality",
  "level": "2",
  "url": "s-basic-matrix-definitions.html#def-matrix-equality",
  "type": "Definition",
  "number": "5.1.4",
  "title": "Equality of Matrices.",
  "body": "Equality of Matrices  A matrix is said to be equal to matrix (written ) if and only if:    and have the same order, and  all corresponding entries are equal: that is, = for all appropriate and .    "
},
{
  "id": "def-matrix-addition",
  "level": "2",
  "url": "s-basic-matrix-definitions.html#def-matrix-addition",
  "type": "Definition",
  "number": "5.1.5",
  "title": "Matrix Addition.",
  "body": "Matrix Addition Matrix Addition  Let and be matrices. Then is an matrix where (read The th th entry of the matrix is obtained by adding the th th entry of to the th th entry of ). If the orders of and are not identical, is not defined. "
},
{
  "id": "ex-scalar-mult",
  "level": "2",
  "url": "s-basic-matrix-definitions.html#ex-scalar-mult",
  "type": "Example",
  "number": "5.1.6",
  "title": "A Scalar Product.",
  "body": "A Scalar Product If and if and we wish to find , it seems natural to multiply each entry of by 3 so that , and this is precisely the way scalar multiplication is defined. "
},
{
  "id": "def-scalar-multiplication",
  "level": "2",
  "url": "s-basic-matrix-definitions.html#def-scalar-multiplication",
  "type": "Definition",
  "number": "5.1.7",
  "title": "Scalar Multiplication.",
  "body": "Scalar Multiplication Scalar Multiplication Let be an matrix and a scalar. Then is the matrix obtained by multiplying times each entry of ; that is .  "
},
{
  "id": "def-matrix-multiplication",
  "level": "2",
  "url": "s-basic-matrix-definitions.html#def-matrix-multiplication",
  "type": "Definition",
  "number": "5.1.8",
  "title": "Matrix Multiplication.",
  "body": "Matrix Multiplication Matrix Multiplication Let be an matrix and let be an matrix. The product of and , denoted by , is an matrix whose th row th column entry is for and .  "
},
{
  "id": "fig-one-matrix-product-entry",
  "level": "2",
  "url": "s-basic-matrix-definitions.html#fig-one-matrix-product-entry",
  "type": "Figure",
  "number": "5.1.9",
  "title": "",
  "body": " Computation of one entry in the product of two 3 by 3 matrices   Computation of one entry in the product of two 3 by 3 matrices. Two three by three matrices are shown with row 1 of the first matrix and column 2 of the second highlighted. Row 1 of the first matrix has entries 1, -1 and 0. Column 2 of the second matrix has 2, 3 and 4. The corresponding entries are multiplied and the products are added to produce the entry in row 1, column 2 of the product matrix. That number is 1 time 2 plus -1 times 3 plus 0 times 4, which is -1.   "
},
{
  "id": "ex-matrix-product",
  "level": "2",
  "url": "s-basic-matrix-definitions.html#ex-matrix-product",
  "type": "Example",
  "number": "5.1.10",
  "title": "A Matrix Product.",
  "body": "A Matrix Product Let , a matrix, and let , a matrix. Then is a matrix:  "
},
{
  "id": "ex-diagonal-product",
  "level": "2",
  "url": "s-basic-matrix-definitions.html#ex-diagonal-product",
  "type": "Example",
  "number": "5.1.11",
  "title": "Multiplication with a diagonal matrix.",
  "body": "Multiplication with a diagonal matrix Let and . Then   The net effect is to multiply the first row of by and the second row of by 3.  Note: . The columns of are multiplied by and 3 when the order is switched. "
},
{
  "id": "exercises-5-1-2",
  "level": "2",
  "url": "s-basic-matrix-definitions.html#exercises-5-1-2",
  "type": "Exercise",
  "number": "5.1.4.1",
  "title": "",
  "body": " Let , , and    Compute and .  Compute and .  If , show that .  Show that .  Compute .  Compute .  Compute and , where is the zero matrix.  Compute , where 0 is the real number (scalar) zero.  Let and . Show that .   For parts c, d and i of this exercise, only a verification is needed. Here, we supply the result that will appear on both sides of the equality.                         "
},
{
  "id": "exercises-5-1-3",
  "level": "2",
  "url": "s-basic-matrix-definitions.html#exercises-5-1-3",
  "type": "Exercise",
  "number": "5.1.4.2",
  "title": "",
  "body": "Let , , and Compute, if possible;                               can not be computed - incompatible orders.     "
},
{
  "id": "exercises-5-1-4",
  "level": "2",
  "url": "s-basic-matrix-definitions.html#exercises-5-1-4",
  "type": "Exercise",
  "number": "5.1.4.3",
  "title": "",
  "body": "Let . Find a matrix such that and , where .    "
},
{
  "id": "exercises-5-1-5",
  "level": "2",
  "url": "s-basic-matrix-definitions.html#exercises-5-1-5",
  "type": "Exercise",
  "number": "5.1.4.4",
  "title": "",
  "body": "Find and where is as in Exercise 3, where and . What do you notice?  and "
},
{
  "id": "exercises-5-1-6",
  "level": "2",
  "url": "s-basic-matrix-definitions.html#exercises-5-1-6",
  "type": "Exercise",
  "number": "5.1.4.5",
  "title": "",
  "body": "Find if . What is equal to?    "
},
{
  "id": "exercises-5-1-7",
  "level": "2",
  "url": "s-basic-matrix-definitions.html#exercises-5-1-7",
  "type": "Exercise",
  "number": "5.1.4.6",
  "title": "",
  "body": "  Determine and  .  What is equal to for any ?  Prove your answer to part (b) by induction.     and .   for for all ?  The basis follows by the definition that . If, for any , , then   "
},
{
  "id": "exercises-5-1-8",
  "level": "2",
  "url": "s-basic-matrix-definitions.html#exercises-5-1-8",
  "type": "Exercise",
  "number": "5.1.4.7",
  "title": "",
  "body": "  If show that is a way of expressing the system using matrices.  Express the following systems of equations using matrices:              equals if and only if both of the equalities are true.  (i)                "
},
{
  "id": "exercise-bakery",
  "level": "2",
  "url": "s-basic-matrix-definitions.html#exercise-bakery",
  "type": "Exercise",
  "number": "5.1.4.8",
  "title": "",
  "body": " In this exercise, we propose to show how matrix multiplication is a natural operation. Suppose a bakery produces bread, cakes and pies every weekday, Monday through Friday. Based on past sales history, the bakery produces various numbers of each product each day, summarized in the matrix . It should be noted that the order could be described as number of days by number of products. For example, on Wednesday (the third day) the number of cakes (second product in our list) that are produced is .  The main ingredients of these products are flour, sugar and eggs. We assume that other ingredients are always in ample supply, but we need to be sure to have the three main ones available. For each of the three products, The amount of each ingredient that is needed is summarized in the , or number of products by number of ingredients matrix . For example, to bake a cake (second product) we need cups of flour (first ingredient). Regarding units: flour and sugar are given in cups per unit of each product, while eggs are given in individual eggs per unit of each product. These amounts are made up , so don't used them to do your own baking!    How many cups of flour will the bakery need every Monday? Pay close attention to how you compute your answer and the units of each number.  How many eggs will the bakery need every Wednesday?  Compute the matrix product . What do you notice?  Suppose the costs of ingredients are for a cup of flour, for a cup of sugar and for one egg. How can this information be put into a matrix that can meaningfully be multiplied by one of the other matrices in this problem?      The number of cups of flour needed every Monday is Notice that is the inner product of row 1 of with column 1 of .  Compute the inner product of row 3 of with column 3 of to get 23 eggs.   There are two possibilities. Let . If we compute we get the cost of ingredients for each of the individual products. If we compute we get the total cost of ingredients for each day.   "
},
{
  "id": "s-special-matrices",
  "level": "1",
  "url": "s-special-matrices.html",
  "type": "Section",
  "number": "5.2",
  "title": "Special Types of Matrices",
  "body": " Special Types of Matrices  Diagonal Matrices  We have already investigated, in exercises in the previous section, one special type of matrix. That was the zero matrix, and found that it behaves in matrix algebra in an analogous fashion to the real number 0; that is, as the additive identity. We will now investigate the properties of a few other special matrices.  Diagonal Matrix A square matrix D is called a diagonal matrix if = 0 whenever .  Some diagonal matrices  , , and are all diagonal matrices.   The Identity Matrix and Matrix Inverses  In the example above, the diagonal matrix whose diagonal entries are all 1's has the distinctive property that for any other matrix we have . For example:  Multiplying by the Identity Matrix If , then and .  In other words, the matrix behaves in matrix algebra like the real number 1; that is, as a multiplicative identity. In matrix algebra, the matrix is called simply the identity matrix. Convince yourself that if is any matrix .  Identity Matrix  Identity Matrix  The identity matrix  The diagonal matrix whose diagonal components are all 1's is called the identity matrix. If the context is clear, we simply use .  In the set of real numbers we recall that, given a nonzero real number , there exists a real number such that . We know that real numbers commute under multiplication so that the two equations can be summarized as . Further we know that . Do we have an analogous situation in ? Can we define the multiplicative inverse of an matrix ? It seems natural to imitate the definition of multiplicative inverse in the real numbers.  Matrix Inverse Inverse Matrix inverse, the multiplicative inverse of Let be an matrix. If there exists an matrix such that , then is a multiplicative inverse of (called simply an inverse of ) and is denoted by  When we are doing computations involving matrices, it would be helpful to know that when we find , the answer we obtain is the only inverse of the given matrix. This would let us refer to the inverse of a matrix. We refrained from saying that in the definition, but the theorem below justifies it.  Remark: Those unfamiliar with the laws of matrix algebra should return to the following proof after they have familiarized themselves with the Laws of Matrix Algebra in Section 5.5.  Inverses are unique The inverse of an matrix A, when it exists, is unique. Let be an matrix. Assume to the contrary, that has two (different) inverses, say and . Then   Let . What is ? Without too much difficulty, by trial and error, we determine that . This might lead us to guess that the inverse is found by taking the reciprocal of all nonzero entries of a matrix. Alas, it isn't that easy!  If , the reciprocal rule would tell us that the inverse of is . Try computing and you will see that you don't get the identity matrix. So, what is  ? In order to understand more completely the notion of the inverse of a matrix, it would be beneficial to have a formula that would enable us to compute the inverse of at least a matrix. To do this, we introduce the definition of the determinant of a matrix.  Determinant of a 2 by 2 matrix  The determinant of , 2 by 2 case  Let . The determinant of is the number .   In addition to , common notation for the determinant of matrix is . This is particularly common when writing out the whole matrix, which case we would write for the determinant of the general matrix.  Some determinants of two by two matrices If then . If then   Inverse of 2 by 2 matrix Let . If , then . See Exercise 4 at the end of this section.  Finding Inverses Can we find the inverses of the matrices in ? If then The reader should verify that .  The second matrix, , has a determinant equal to zero. If we tried to apply the formula in , we would be dividing by zero. For this reason, the formula can't be applied and in fact does not exist.  Remarks:  In general, if is a matrix and if , then does not exist.  A formula for the inverse of matrices can be derived that also involves . Hence, in general, if the determinant of a matrix is zero, the matrix does not have an inverse. However the formula for even a matrix is very long and is not the most efficient way to compute the inverse of a matrix.  In Chapter 12 we will develop a technique to compute the inverse of a higher-order matrix, if it exists.  Matrix inversion comes first in the hierarchy of matrix operations; therefore, is .    Exercises  For the given matrices find if it exists and verify that . If does not exist explain why.               Use the definition of the inverse of a matrix to find :      No inverse exists.             For the given matrices find if it exists and verify that . If does not exist explain why.             , where .        had no inverse. Its determinant is 0.            Let and . Verify that .  Let and be invertible matrices. Prove that . Why is the right side of the above statement written backwards ? Is this necessary? Hint: Use    Let A and B be by invertible matrices.     Similarly, .  By , is the only inverse of . If we tried to invert with , we would be unsuccessful since we cannot rearrange the order of the matrices.   Let . Derive the formula for .   If we multiply by an two by two matrix of unknowns, , and equate that product with the identity matrix, we get  If we equate these two matrices, there are two sets of similar pairs of equations that need to be solved. If we focus only on the first columns, the equations are   If we assume that both and are both nozero and we multiply the two equation on the left by and , respectively we get We can then subtract the second equation from the first, eliminating to get We can then substitute this value of into to get . Notice that the algebra makes sense as long as . Also and can't both be zero. If either is zero we can derive the same final result. The other set of equations for the second columns are solved in a similar way.   Linearity of Determinants   Let and be 2-by-2 matrices. Show that .  It can be shown that the statement in part (a) is true for all matrices. Let be any invertible matrix. Prove that . Note: The determinant of the identity matrix is 1 for all .  Verify that the equation in part (b) is true for the matrix in exercise 1(a) of this section.     Let and . .    . Now solve for .    while .    Prove by induction that for , .  The basis follows by the definition of the first power of a matrix. If, for any , the formula is true, then      Use the assumptions in to prove by induction that if , .   Basis:  Induction: Assume for some .      Prove: If the determinant of a matrix is zero, then does not have an inverse. Hint: Use the indirect method of proof and exercise 5.   (Indirect Proof) Assume that but exits.       Let be matrices. Assume that is invertible. If , prove by induction that is true for .  Given that where what is ?  Assume  Basis: ): is given.  Induction: Assume that for some positive integer ,         "
},
{
  "id": "def-diagonal-matrix",
  "level": "2",
  "url": "s-special-matrices.html#def-diagonal-matrix",
  "type": "Definition",
  "number": "5.2.1",
  "title": "Diagonal Matrix.",
  "body": "Diagonal Matrix A square matrix D is called a diagonal matrix if = 0 whenever . "
},
{
  "id": "example-diagonal-matrices",
  "level": "2",
  "url": "s-special-matrices.html#example-diagonal-matrices",
  "type": "Example",
  "number": "5.2.2",
  "title": "Some diagonal matrices.",
  "body": "Some diagonal matrices  , , and are all diagonal matrices. "
},
{
  "id": "ex-matrix-identity-product",
  "level": "2",
  "url": "s-special-matrices.html#ex-matrix-identity-product",
  "type": "Example",
  "number": "5.2.3",
  "title": "Multiplying by the Identity Matrix.",
  "body": "Multiplying by the Identity Matrix If , then and . "
},
{
  "id": "def-identity-matrix",
  "level": "2",
  "url": "s-special-matrices.html#def-identity-matrix",
  "type": "Definition",
  "number": "5.2.4",
  "title": "Identity Matrix.",
  "body": "Identity Matrix  Identity Matrix  The identity matrix  The diagonal matrix whose diagonal components are all 1's is called the identity matrix. If the context is clear, we simply use . "
},
{
  "id": "def-matrix-inverse",
  "level": "2",
  "url": "s-special-matrices.html#def-matrix-inverse",
  "type": "Definition",
  "number": "5.2.5",
  "title": "Matrix Inverse.",
  "body": "Matrix Inverse Inverse Matrix inverse, the multiplicative inverse of Let be an matrix. If there exists an matrix such that , then is a multiplicative inverse of (called simply an inverse of ) and is denoted by "
},
{
  "id": "theorem-unique-inverse",
  "level": "2",
  "url": "s-special-matrices.html#theorem-unique-inverse",
  "type": "Theorem",
  "number": "5.2.6",
  "title": "Inverses are unique.",
  "body": "Inverses are unique The inverse of an matrix A, when it exists, is unique. Let be an matrix. Assume to the contrary, that has two (different) inverses, say and . Then  "
},
{
  "id": "determinant-2by2",
  "level": "2",
  "url": "s-special-matrices.html#determinant-2by2",
  "type": "Definition",
  "number": "5.2.7",
  "title": "Determinant of a 2 by 2 matrix.",
  "body": "Determinant of a 2 by 2 matrix  The determinant of , 2 by 2 case  Let . The determinant of is the number .  "
},
{
  "id": "ex-some-determinants",
  "level": "2",
  "url": "s-special-matrices.html#ex-some-determinants",
  "type": "Example",
  "number": "5.2.8",
  "title": "Some determinants of two by two matrices.",
  "body": "Some determinants of two by two matrices If then . If then  "
},
{
  "id": "theorem-inverse-two-by-two",
  "level": "2",
  "url": "s-special-matrices.html#theorem-inverse-two-by-two",
  "type": "Theorem",
  "number": "5.2.9",
  "title": "Inverse of 2 by 2 matrix.",
  "body": "Inverse of 2 by 2 matrix Let . If , then . See Exercise 4 at the end of this section. "
},
{
  "id": "ex-finding-inverses",
  "level": "2",
  "url": "s-special-matrices.html#ex-finding-inverses",
  "type": "Example",
  "number": "5.2.10",
  "title": "Finding Inverses.",
  "body": "Finding Inverses Can we find the inverses of the matrices in ? If then The reader should verify that .  The second matrix, , has a determinant equal to zero. If we tried to apply the formula in , we would be dividing by zero. For this reason, the formula can't be applied and in fact does not exist. "
},
{
  "id": "exercises-5-4-2",
  "level": "2",
  "url": "s-special-matrices.html#exercises-5-4-2",
  "type": "Exercise",
  "number": "5.2.3.1",
  "title": "",
  "body": "For the given matrices find if it exists and verify that . If does not exist explain why.               Use the definition of the inverse of a matrix to find :      No inverse exists.            "
},
{
  "id": "exercises-5-4-3",
  "level": "2",
  "url": "s-special-matrices.html#exercises-5-4-3",
  "type": "Exercise",
  "number": "5.2.3.2",
  "title": "",
  "body": "For the given matrices find if it exists and verify that . If does not exist explain why.             , where .        had no inverse. Its determinant is 0.         "
},
{
  "id": "prob-5-2-3",
  "level": "2",
  "url": "s-special-matrices.html#prob-5-2-3",
  "type": "Exercise",
  "number": "5.2.3.3",
  "title": "",
  "body": "  Let and . Verify that .  Let and be invertible matrices. Prove that . Why is the right side of the above statement written backwards ? Is this necessary? Hint: Use    Let A and B be by invertible matrices.     Similarly, .  By , is the only inverse of . If we tried to invert with , we would be unsuccessful since we cannot rearrange the order of the matrices.  "
},
{
  "id": "exercises-5-4-5",
  "level": "2",
  "url": "s-special-matrices.html#exercises-5-4-5",
  "type": "Exercise",
  "number": "5.2.3.4",
  "title": "",
  "body": "Let . Derive the formula for .   If we multiply by an two by two matrix of unknowns, , and equate that product with the identity matrix, we get  If we equate these two matrices, there are two sets of similar pairs of equations that need to be solved. If we focus only on the first columns, the equations are   If we assume that both and are both nozero and we multiply the two equation on the left by and , respectively we get We can then subtract the second equation from the first, eliminating to get We can then substitute this value of into to get . Notice that the algebra makes sense as long as . Also and can't both be zero. If either is zero we can derive the same final result. The other set of equations for the second columns are solved in a similar way.  "
},
{
  "id": "prob-5-2-5",
  "level": "2",
  "url": "s-special-matrices.html#prob-5-2-5",
  "type": "Exercise",
  "number": "5.2.3.5",
  "title": "Linearity of Determinants.",
  "body": "Linearity of Determinants   Let and be 2-by-2 matrices. Show that .  It can be shown that the statement in part (a) is true for all matrices. Let be any invertible matrix. Prove that . Note: The determinant of the identity matrix is 1 for all .  Verify that the equation in part (b) is true for the matrix in exercise 1(a) of this section.     Let and . .    . Now solve for .    while .   "
},
{
  "id": "exercises-5-4-7",
  "level": "2",
  "url": "s-special-matrices.html#exercises-5-4-7",
  "type": "Exercise",
  "number": "5.2.3.6",
  "title": "",
  "body": "Prove by induction that for , .  The basis follows by the definition of the first power of a matrix. If, for any , the formula is true, then     "
},
{
  "id": "exercises-5-4-8",
  "level": "2",
  "url": "s-special-matrices.html#exercises-5-4-8",
  "type": "Exercise",
  "number": "5.2.3.7",
  "title": "",
  "body": "Use the assumptions in to prove by induction that if , .   Basis:  Induction: Assume for some .     "
},
{
  "id": "exercises-5-4-9",
  "level": "2",
  "url": "s-special-matrices.html#exercises-5-4-9",
  "type": "Exercise",
  "number": "5.2.3.8",
  "title": "",
  "body": "Prove: If the determinant of a matrix is zero, then does not have an inverse. Hint: Use the indirect method of proof and exercise 5.   (Indirect Proof) Assume that but exits.    "
},
{
  "id": "exercises-5-4-10",
  "level": "2",
  "url": "s-special-matrices.html#exercises-5-4-10",
  "type": "Exercise",
  "number": "5.2.3.9",
  "title": "",
  "body": "  Let be matrices. Assume that is invertible. If , prove by induction that is true for .  Given that where what is ?  Assume  Basis: ): is given.  Induction: Assume that for some positive integer ,       "
},
{
  "id": "s-laws-of-matrix-algebra",
  "level": "1",
  "url": "s-laws-of-matrix-algebra.html",
  "type": "Section",
  "number": "5.3",
  "title": "Laws of Matrix Algebra",
  "body": "Laws of Matrix Algebra Laws of Matrix Algebra  The Laws  The following is a summary of the basic laws of matrix operations. Assume that the indicated operations are defined; that is, that the orders of the matrices , and are such that the operations make sense.   Laws of Matrix Algebra   (1) Commutative Law of Addition  (2) Associative Law of Addition  (3) Distributive Law of a Scalar over Matrices , where .  (4) Distributive Law of Scalars over a Matrix , where .  (5) Associative Law of Scalar Multiplication , where .  (6) Zero Matrix Annihilates all Products , where is the zero matrix.  (7) Zero Scalar Annihilates all Products , where 0 on the left is the scalar zero.  (8) Zero Matrix is an identity for Addition .  (9) Negation produces additive inverses .  (10) Right Distributive Law of Matrix Multiplication .  (11) Left Distributive Law of Matrix Multiplication .  (12) Associative Law of Multiplication .  (13) Identity Matrix is a Multiplicative Identity and .  (14) Involution Property of Inverses If exists, .  (15) Inverse of Product Rule If and exist,      Commentary  More Precise Statement of one Law  If we wished to write out each of the above laws more completely, we would specify the orders of the matrices. For example, Law 10 should read:  Let , , and be , , and matrices, respectively, then    Remarks:   Notice the absence of the law  . Why?  Is it really necessary to have both a right (No. 11) and a left (No. 10) distributive law? Why?     Exercises  Rewrite the above laws specifying as in the orders of the matrices.   Let and be by matrices. Then ,  Let , , and be by matrices. Then .  Let and be by matrices, and let . Then ,  Let be an by matrix, and let . Then .  Let be an by matrix, and let . Then  Let be the zero matrix, of size , and let be a matrix of size . Then .  Let be an matrix, and . Then .  Let be an matrix, and let be the zero matrix. Then .  Let be an matrix. Then , where is the zero matrix.  Let , , and be , , and matrices respectively. Then .  Let , , and be , , and matrices respectively. Then .  Let , , and be , , and matrices respectively. Then .  Let be an matrix, the identity matrix, and the identity matrix. Then  Let be an matrix. Then if exists, .  Let and be matrices. Then if and exist, .    Verify each of the Laws of Matrix Algebra using examples.   Let , , and . Compute the following as efficiently as possible by using any of the Laws of Matrix Algebra:                   , which is given in part (a).   by part c    Let and . Compute the following as efficiently as possible by using any of the Laws of Matrix Algebra:                        Square the answer to (b) to get  Invert the answer to (a) to get  Multiply the answer to (b) times to get    Let and be matrices of real numbers. Is ? Explain.    "
},
{
  "id": "table-matrix-laws",
  "level": "2",
  "url": "s-laws-of-matrix-algebra.html#table-matrix-laws",
  "type": "Table",
  "number": "5.3.1",
  "title": "Laws of Matrix Algebra",
  "body": " Laws of Matrix Algebra   (1) Commutative Law of Addition  (2) Associative Law of Addition  (3) Distributive Law of a Scalar over Matrices , where .  (4) Distributive Law of Scalars over a Matrix , where .  (5) Associative Law of Scalar Multiplication , where .  (6) Zero Matrix Annihilates all Products , where is the zero matrix.  (7) Zero Scalar Annihilates all Products , where 0 on the left is the scalar zero.  (8) Zero Matrix is an identity for Addition .  (9) Negation produces additive inverses .  (10) Right Distributive Law of Matrix Multiplication .  (11) Left Distributive Law of Matrix Multiplication .  (12) Associative Law of Multiplication .  (13) Identity Matrix is a Multiplicative Identity and .  (14) Involution Property of Inverses If exists, .  (15) Inverse of Product Rule If and exist,    "
},
{
  "id": "ex-statement-precise",
  "level": "2",
  "url": "s-laws-of-matrix-algebra.html#ex-statement-precise",
  "type": "Example",
  "number": "5.3.2",
  "title": "More Precise Statement of one Law.",
  "body": "More Precise Statement of one Law  If we wished to write out each of the above laws more completely, we would specify the orders of the matrices. For example, Law 10 should read:  Let , , and be , , and matrices, respectively, then   "
},
{
  "id": "exercises-5-5-2",
  "level": "2",
  "url": "s-laws-of-matrix-algebra.html#exercises-5-5-2",
  "type": "Exercise",
  "number": "5.3.3.1",
  "title": "",
  "body": "Rewrite the above laws specifying as in the orders of the matrices.   Let and be by matrices. Then ,  Let , , and be by matrices. Then .  Let and be by matrices, and let . Then ,  Let be an by matrix, and let . Then .  Let be an by matrix, and let . Then  Let be the zero matrix, of size , and let be a matrix of size . Then .  Let be an matrix, and . Then .  Let be an matrix, and let be the zero matrix. Then .  Let be an matrix. Then , where is the zero matrix.  Let , , and be , , and matrices respectively. Then .  Let , , and be , , and matrices respectively. Then .  Let , , and be , , and matrices respectively. Then .  Let be an matrix, the identity matrix, and the identity matrix. Then  Let be an matrix. Then if exists, .  Let and be matrices. Then if and exist, .   "
},
{
  "id": "exercises-5-5-3",
  "level": "2",
  "url": "s-laws-of-matrix-algebra.html#exercises-5-5-3",
  "type": "Exercise",
  "number": "5.3.3.2",
  "title": "",
  "body": "Verify each of the Laws of Matrix Algebra using examples.  "
},
{
  "id": "exercises-5-5-4",
  "level": "2",
  "url": "s-laws-of-matrix-algebra.html#exercises-5-5-4",
  "type": "Exercise",
  "number": "5.3.3.3",
  "title": "",
  "body": "Let , , and . Compute the following as efficiently as possible by using any of the Laws of Matrix Algebra:                   , which is given in part (a).   by part c   "
},
{
  "id": "exercises-5-5-5",
  "level": "2",
  "url": "s-laws-of-matrix-algebra.html#exercises-5-5-5",
  "type": "Exercise",
  "number": "5.3.3.4",
  "title": "",
  "body": "Let and . Compute the following as efficiently as possible by using any of the Laws of Matrix Algebra:                        Square the answer to (b) to get  Invert the answer to (a) to get  Multiply the answer to (b) times to get   "
},
{
  "id": "exercises-5-5-6",
  "level": "2",
  "url": "s-laws-of-matrix-algebra.html#exercises-5-5-6",
  "type": "Exercise",
  "number": "5.3.3.5",
  "title": "",
  "body": "Let and be matrices of real numbers. Is ? Explain.  "
},
{
  "id": "s-matrix-oddities",
  "level": "1",
  "url": "s-matrix-oddities.html",
  "type": "Section",
  "number": "5.4",
  "title": "Matrix Oddities",
  "body": "Matrix Oddities Matrix Oddities  Dissimilarities with elementary algebra  We have seen that matrix algebra is similar in many ways to elementary algebra. Indeed, if we want to solve the matrix equation for the unknown , we imitate the procedure used in elementary algebra for solving the equation . One assumption we need is that is a square matrix that has an inverse. Notice how exactly the same properties are used in the following detailed solutions of both equations.    Equation in the algebra of real numbers  Equation in matrix algebra   if if exists  Associative Property  Inverse Property  Identity Property     Certainly the solution process for solving is the same as that of solving .  The solution of is . In fact, we usually write the solution of both equations as . In matrix algebra, the solution of is , which is not necessarily equal to . So in matrix algebra, since the commutative law (under multiplication) is not true, we have to be more careful in the methods we use to solve equations.  It is clear from the above that if we wrote the solution of as , we would not know how to interpret . Does it mean or ? Because of this, is never written as .  Matrix Oddities  Some of the main dissimilarities between matrix algebra and elementary algebra are that in matrix algebra:    may be different from .  There exist matrices and such that , and yet and .  There exist matrices where , and yet .  There exist matrices where with and  There exist matrices where , where and     Exercises  Discuss each of the Matrix Oddities with respect to elementary algebra.  In elementary algebra (the algebra of real numbers), each of the given oddities does not exist.    may be different from . Not so in elementary algebra, since by the commutative law of multiplication.  There exist matrices and such that , yet and . In elementary algebra, the only way is if either or is zero. There are no exceptions.  There exist matrices , , yet . In elementary algebra, .  There exist matrices . where and . In elementary algebra, .  There exist matrices where but and . In elementary algebra, .    Determine matrices which show that each of the Matrix Oddities are true.    Almost any two random matrices will not commute. Several examples have appeared in exercises already.  If and , . Another pair that works is and .  Two possible 's are and . There are many more.  Two possible 's are and .  Two examples are and .    Prove or disprove the following implications.    and   .    exists, and if you multiply the equation on both sides by , you obtain .  Counterexample:     Let be the set of real matrices. Let be the subset of matrices defined by if and only if . Let be defined by if and only if .   Determine the cardinality of .  Consider the special case and prove that a sufficient condition for is that has a zero determinant (i.e., is singular) and where is the sum of the main diagonal elements of .  Is the condition of part b a necessary condition?     The cardinality of is one, by part (a) of exercise 3 above.    Similarly,   The condition is not necessary since the trace of the two by two identity matrix is 2.    Write each of the following systems in the form , and then solve the systems using matrices.                 , and    , and    , and    , and  The matrix of coefficients for this system has a zero determinant; therefore, it has no inverse. The system cannot be solved by this method. In fact, the system has no solution.     For those who know calculus:   Write the series expansion for centered around .  Use the idea of exercise 6 to write what would be a plausible definition of where is an matrix.  If and , use the series in part (b) to show that and .  Show that .   Show that .  Is ?    The power series expansion of is . Therefore, it is reasonable to define the matrix exponential to be , assuming this sum converges.  If , then we observe that for all positive . Therefore which agrees with the stated value in the problem. The value of is even easier to derive since is the zero matrix for . Thus, , which equals the matrix that is given in the problem.  We observe that . They disagree in the first row, second column.    "
},
{
  "id": "s-matrix-oddities-3-3",
  "level": "2",
  "url": "s-matrix-oddities.html#s-matrix-oddities-3-3",
  "type": "Table",
  "number": "5.4.1",
  "title": "",
  "body": "  Equation in the algebra of real numbers  Equation in matrix algebra   if if exists  Associative Property  Inverse Property  Identity Property    "
},
{
  "id": "ss-matrix-oddities",
  "level": "2",
  "url": "s-matrix-oddities.html#ss-matrix-oddities",
  "type": "Observation",
  "number": "5.4.2",
  "title": "Matrix Oddities.",
  "body": "Matrix Oddities  Some of the main dissimilarities between matrix algebra and elementary algebra are that in matrix algebra:    may be different from .  There exist matrices and such that , and yet and .  There exist matrices where , and yet .  There exist matrices where with and  There exist matrices where , where and   "
},
{
  "id": "exercises-5-6-2",
  "level": "2",
  "url": "s-matrix-oddities.html#exercises-5-6-2",
  "type": "Exercise",
  "number": "5.4.2.1",
  "title": "",
  "body": "Discuss each of the Matrix Oddities with respect to elementary algebra.  In elementary algebra (the algebra of real numbers), each of the given oddities does not exist.    may be different from . Not so in elementary algebra, since by the commutative law of multiplication.  There exist matrices and such that , yet and . In elementary algebra, the only way is if either or is zero. There are no exceptions.  There exist matrices , , yet . In elementary algebra, .  There exist matrices . where and . In elementary algebra, .  There exist matrices where but and . In elementary algebra, .   "
},
{
  "id": "exercises-5-6-3",
  "level": "2",
  "url": "s-matrix-oddities.html#exercises-5-6-3",
  "type": "Exercise",
  "number": "5.4.2.2",
  "title": "",
  "body": "Determine matrices which show that each of the Matrix Oddities are true.    Almost any two random matrices will not commute. Several examples have appeared in exercises already.  If and , . Another pair that works is and .  Two possible 's are and . There are many more.  Two possible 's are and .  Two examples are and .   "
},
{
  "id": "exercises-5-6-4",
  "level": "2",
  "url": "s-matrix-oddities.html#exercises-5-6-4",
  "type": "Exercise",
  "number": "5.4.2.3",
  "title": "",
  "body": "Prove or disprove the following implications.    and   .    exists, and if you multiply the equation on both sides by , you obtain .  Counterexample:    "
},
{
  "id": "exercises-5-6-5",
  "level": "2",
  "url": "s-matrix-oddities.html#exercises-5-6-5",
  "type": "Exercise",
  "number": "5.4.2.4",
  "title": "",
  "body": "Let be the set of real matrices. Let be the subset of matrices defined by if and only if . Let be defined by if and only if .   Determine the cardinality of .  Consider the special case and prove that a sufficient condition for is that has a zero determinant (i.e., is singular) and where is the sum of the main diagonal elements of .  Is the condition of part b a necessary condition?     The cardinality of is one, by part (a) of exercise 3 above.    Similarly,   The condition is not necessary since the trace of the two by two identity matrix is 2.   "
},
{
  "id": "exercises-5-6-6",
  "level": "2",
  "url": "s-matrix-oddities.html#exercises-5-6-6",
  "type": "Exercise",
  "number": "5.4.2.5",
  "title": "",
  "body": "Write each of the following systems in the form , and then solve the systems using matrices.                 , and    , and    , and    , and  The matrix of coefficients for this system has a zero determinant; therefore, it has no inverse. The system cannot be solved by this method. In fact, the system has no solution.   "
},
{
  "id": "exercises-5-6-7",
  "level": "2",
  "url": "s-matrix-oddities.html#exercises-5-6-7",
  "type": "Exercise",
  "number": "5.4.2.6",
  "title": "",
  "body": "For those who know calculus:   Write the series expansion for centered around .  Use the idea of exercise 6 to write what would be a plausible definition of where is an matrix.  If and , use the series in part (b) to show that and .  Show that .   Show that .  Is ?    The power series expansion of is . Therefore, it is reasonable to define the matrix exponential to be , assuming this sum converges.  If , then we observe that for all positive . Therefore which agrees with the stated value in the problem. The value of is even easier to derive since is the zero matrix for . Thus, , which equals the matrix that is given in the problem.  We observe that . They disagree in the first row, second column.  "
},
{
  "id": "s-basic-definitions",
  "level": "1",
  "url": "s-basic-definitions.html",
  "type": "Section",
  "number": "6.1",
  "title": "Basic Definitions",
  "body": "Basic Definitions   In Chapter 1 we introduced the concept of the Cartesian product of sets. Let's assume that a person owns three shirts and two pairs of slacks. More precisely, let and . Then is the set of all six possible combinations of shirts and slacks that the individual could wear. However, an individual may wish to restrict himself or herself to combinations which are color coordinated, or related. This may not be all possible pairs in but will certainly be a subset of . For example, one such subset may be   Relations between two sets  Relation Relation Let and be sets. A relation from into is any subset of .  A simple example Let and . Then is a relation from into . Of course, there are many others we could describe; 64, to be exact.  Divisibility Example Let and define a relation from into by if and only if divides evenly into . The set of pairs that qualify for membership is .   Relations on a Set  Relation on a Set Relation on a Set A relation from a set into itself is called a relation on .  The relation divides in will appear throughout the book. Here is a general definition on the whole set of integers.  Divides  Divides  divides , or divides evenly into  Let , . We say that divides , denoted , if and only if there exists an integer such that .   Be very careful in writing about the relation divides. The vertical line symbol use for this relation, if written carelessly, can look like division. While is either true or false, is a number.  Based on the equation , we can say that is equivalent to , or divides evenly into . In fact the divides is short for divides evenly into. You might find the equation initially easier to understand, but in the long run we will find the equation more convenient.  Sometimes it is helpful to illustrate a relation with a graph. Consider . A graph of can be drawn as in . The arrows indicate that 1 is related to 4 under . Also, 2 is related to 4 under , and 3 is related to 5, while the upper arrow denotes that is a relation from the whole set into the set .   The graph of a relation   The graph of a relation. On the left, there is a set consisting of three points, 1, 2, and 3. On the right a set consisting of two points 4 and 5. Arrows corresponding to the ordered pairs (1,4), (2,4), and (3,5) connect the points in the two sets.    A typical element in a relation is an ordered pair . In some cases, can be described by actually listing the pairs which are in , as in the previous examples. This may not be convenient if is relatively large. Other notations are used with certain well-known relations. Consider the less than or equal relation on the real numbers. We could define it as a set of ordered pairs this way: However, the notation is clear and self-explanatory; it is a more natural, and hence preferred, notation to use than .  Many of the relations we will work with resemble the relation , so is a common way to express the fact that is related to through the relation .  Relation Notation Relation Notation Let be a relation from a set into a set . Then the fact that is frequently written . is related to through the relation   Composition of Relations  With , , and , let be the relation divides, from into , and let be the relation from into . So and .  Notice that in that we can, for certain elements of , go through elements in to results in . That is:            Relation Composition - a graphical view   The graphs of two relations being composed.    Based on this observation, we can define a new relation, call it , from into . In order for to be in , it must be possible to travel along a path in from to . In other words, if and only if . The name was chosen because it reminds us that this new relation was formed by the two previous relations and . The complete listing of all elements in is . We summarize in a definition.  Composition of Relations Composition of Relations the composition of relation with relation Let be a relation from a set into a set , and let be a relation from into a set . The composition of with , written , is the set of pairs of the form , where if and only if there exists such that and .   Remark: A word of warning to those readers familiar with composition of functions. (For those who are not, disregard this remark. It will be repeated at an appropriate place in the next chapter.) As indicated above, the traditional way of describing a composition of two relations is where is the first relation and the second. However, function composition is traditionally expressed in the opposite order: , where is the first function and is the second.   Exercises  For each of the following relations defined on , determine which of the given ordered pairs belong to    iff ; (2, 3), (2, 4), (2, 8), (2, 17)   iff ; (2, 3), (3, 2), (2, 4), (5, 8)   iff ; (1,1), (2, 3), (2, 4), (2, 6)             Nim The following relations are on . Let and    List all elements in .  List all elements in .  Illustrate and via a diagraph.  In one version of the game of nim players A and B take turns removing one or two stones from a pile. The player who manages to remove the last stone wins. Explain how these two relations describe the winning moves for B if A plays first with nine stones in the pile at the start of the game.     .   .     Three columns, each with the numbers 0 through 9 lined up in order. The left column and middle column are connected with directed edges corresponding with the elements of while the middle and right columns correspond with pairs in .      Three columns, each with the numbers 0 through 9 lined up in order. The left column and middle column are connected with directed edges corresponding with the elements of while the middle and right columns correspond with pairs in .     In this version of the game, the player who can leave their opponent with a multiple of 3 stones for their next move will always win. If A starts with 9 stones, whatever the first move is, B can counter by leaving A with 6 stones for their next turn. In A's next move there will be 3 stones and then 0 stones, which means B loses and A wins.    Let and define on by iff . We define and . Find:                  Given and , relations on , and , what are and ? Hint: Even when a relation involves infinite sets, you can often get insights into them by drawing partial graphs.   and .   Let be the relation on the power set, , of a finite set of cardinality defined by iff .   Consider the specific case , and determine the cardinality of the set .  What is the cardinality of for an arbitrary ? Express your answer in terms of . (Hint: There are three places that each element of S can go in building an element of .)   When , there are 27 pairs in the relation.  Imagine building a pair of disjoint subsets of . For each element of there are three places that it can go: into the first set of the ordered pair, into the second set, or into neither set. Therefore the number of pairs in the relation is , by the product rule.    Consider the two relations on people: , where if 's mother is ; and , where if and are siblings. Describe, in words, the two relations and in simple English terms.  if and only if is 's aunt or uncle. if and only if is 's mother and has sibling with the same mother.   Let , , and be relations on any set . Prove that if then .  Assume . This implies that there exist such that and . We are given that , which implies that . Combining this with implies that , which proves that .   "
},
{
  "id": "Relation",
  "level": "2",
  "url": "s-basic-definitions.html#Relation",
  "type": "Definition",
  "number": "6.1.1",
  "title": "Relation.",
  "body": "Relation Relation Let and be sets. A relation from into is any subset of . "
},
{
  "id": "ex-simple-relation",
  "level": "2",
  "url": "s-basic-definitions.html#ex-simple-relation",
  "type": "Example",
  "number": "6.1.2",
  "title": "A simple example.",
  "body": "A simple example Let and . Then is a relation from into . Of course, there are many others we could describe; 64, to be exact. "
},
{
  "id": "ex-divides-example",
  "level": "2",
  "url": "s-basic-definitions.html#ex-divides-example",
  "type": "Example",
  "number": "6.1.3",
  "title": "Divisibility Example.",
  "body": "Divisibility Example Let and define a relation from into by if and only if divides evenly into . The set of pairs that qualify for membership is . "
},
{
  "id": "relation-on-a-set",
  "level": "2",
  "url": "s-basic-definitions.html#relation-on-a-set",
  "type": "Definition",
  "number": "6.1.4",
  "title": "Relation on a Set.",
  "body": "Relation on a Set Relation on a Set A relation from a set into itself is called a relation on . "
},
{
  "id": "Divides",
  "level": "2",
  "url": "s-basic-definitions.html#Divides",
  "type": "Definition",
  "number": "6.1.5",
  "title": "Divides.",
  "body": "Divides  Divides  divides , or divides evenly into  Let , . We say that divides , denoted , if and only if there exists an integer such that .  "
},
{
  "id": "graph-6-1-1-relation",
  "level": "2",
  "url": "s-basic-definitions.html#graph-6-1-1-relation",
  "type": "Figure",
  "number": "6.1.6",
  "title": "",
  "body": " The graph of a relation   The graph of a relation. On the left, there is a set consisting of three points, 1, 2, and 3. On the right a set consisting of two points 4 and 5. Arrows corresponding to the ordered pairs (1,4), (2,4), and (3,5) connect the points in the two sets.   "
},
{
  "id": "table-relation-composition-example",
  "level": "2",
  "url": "s-basic-definitions.html#table-relation-composition-example",
  "type": "Table",
  "number": "6.1.7",
  "title": "",
  "body": "        "
},
{
  "id": "graph-6-1-relation-composition",
  "level": "2",
  "url": "s-basic-definitions.html#graph-6-1-relation-composition",
  "type": "Figure",
  "number": "6.1.8",
  "title": "",
  "body": " Relation Composition - a graphical view   The graphs of two relations being composed.   "
},
{
  "id": "def-composition-of-relations",
  "level": "2",
  "url": "s-basic-definitions.html#def-composition-of-relations",
  "type": "Definition",
  "number": "6.1.9",
  "title": "Composition of Relations.",
  "body": "Composition of Relations Composition of Relations the composition of relation with relation Let be a relation from a set into a set , and let be a relation from into a set . The composition of with , written , is the set of pairs of the form , where if and only if there exists such that and .  "
},
{
  "id": "exercises-6-1-2",
  "level": "2",
  "url": "s-basic-definitions.html#exercises-6-1-2",
  "type": "Exercise",
  "number": "6.1.4.1",
  "title": "",
  "body": "For each of the following relations defined on , determine which of the given ordered pairs belong to    iff ; (2, 3), (2, 4), (2, 8), (2, 17)   iff ; (2, 3), (3, 2), (2, 4), (5, 8)   iff ; (1,1), (2, 3), (2, 4), (2, 6)            "
},
{
  "id": "exercises-6-1-3",
  "level": "2",
  "url": "s-basic-definitions.html#exercises-6-1-3",
  "type": "Exercise",
  "number": "6.1.4.2",
  "title": "",
  "body": "Nim The following relations are on . Let and    List all elements in .  List all elements in .  Illustrate and via a diagraph.  In one version of the game of nim players A and B take turns removing one or two stones from a pile. The player who manages to remove the last stone wins. Explain how these two relations describe the winning moves for B if A plays first with nine stones in the pile at the start of the game.     .   .     Three columns, each with the numbers 0 through 9 lined up in order. The left column and middle column are connected with directed edges corresponding with the elements of while the middle and right columns correspond with pairs in .      Three columns, each with the numbers 0 through 9 lined up in order. The left column and middle column are connected with directed edges corresponding with the elements of while the middle and right columns correspond with pairs in .     In this version of the game, the player who can leave their opponent with a multiple of 3 stones for their next move will always win. If A starts with 9 stones, whatever the first move is, B can counter by leaving A with 6 stones for their next turn. In A's next move there will be 3 stones and then 0 stones, which means B loses and A wins.   "
},
{
  "id": "exercises-6-1-4",
  "level": "2",
  "url": "s-basic-definitions.html#exercises-6-1-4",
  "type": "Exercise",
  "number": "6.1.4.3",
  "title": "",
  "body": "Let and define on by iff . We define and . Find:                 "
},
{
  "id": "exercises-6-1-5",
  "level": "2",
  "url": "s-basic-definitions.html#exercises-6-1-5",
  "type": "Exercise",
  "number": "6.1.4.4",
  "title": "",
  "body": "Given and , relations on , and , what are and ? Hint: Even when a relation involves infinite sets, you can often get insights into them by drawing partial graphs.   and .  "
},
{
  "id": "exercises-6-1-6",
  "level": "2",
  "url": "s-basic-definitions.html#exercises-6-1-6",
  "type": "Exercise",
  "number": "6.1.4.5",
  "title": "",
  "body": "Let be the relation on the power set, , of a finite set of cardinality defined by iff .   Consider the specific case , and determine the cardinality of the set .  What is the cardinality of for an arbitrary ? Express your answer in terms of . (Hint: There are three places that each element of S can go in building an element of .)   When , there are 27 pairs in the relation.  Imagine building a pair of disjoint subsets of . For each element of there are three places that it can go: into the first set of the ordered pair, into the second set, or into neither set. Therefore the number of pairs in the relation is , by the product rule.   "
},
{
  "id": "exercises-6-1-7",
  "level": "2",
  "url": "s-basic-definitions.html#exercises-6-1-7",
  "type": "Exercise",
  "number": "6.1.4.6",
  "title": "",
  "body": "Consider the two relations on people: , where if 's mother is ; and , where if and are siblings. Describe, in words, the two relations and in simple English terms.  if and only if is 's aunt or uncle. if and only if is 's mother and has sibling with the same mother.  "
},
{
  "id": "exercises-6-1-8",
  "level": "2",
  "url": "s-basic-definitions.html#exercises-6-1-8",
  "type": "Exercise",
  "number": "6.1.4.7",
  "title": "",
  "body": "Let , , and be relations on any set . Prove that if then .  Assume . This implies that there exist such that and . We are given that , which implies that . Combining this with implies that , which proves that . "
},
{
  "id": "s-graphs-of-relations-on-a-set",
  "level": "1",
  "url": "s-graphs-of-relations-on-a-set.html",
  "type": "Section",
  "number": "6.2",
  "title": "Graphs of Relations on a Set",
  "body": "Graphs of Relations on a Set   In this section we introduce directed graphs as a way to visualize relations on a set.   Digraphs  Let , and let In representing this relation as a graph, elements of are called the vertices of the graph. They are typically represented by labeled points or small circles. We connect vertex to vertex with an arrow, called an edge, going from vertex to vertex if and only if . This type of graph of a relation is called a directed graph or digraph . is a digraph for . Notice that since 0 is related to itself, we draw a self-loop at 0. Directed graph Digraph   Digraph of a relation   Digraph of the relation with four nodes, 0, 1, 2, and 3; and edges between them corresponding to the pairs in the relation. One pair, is displayed as a loop starting at ending at node 0.     The actual location of the vertices in a digraph is immaterial. The actual location of vertices we choose is called an embedding of a graph Embedding of a graph . The main idea is to place the vertices in such a way that the graph is easy to read. After drawing a rough-draft graph of a relation, we may decide to relocate the vertices so that the final result will be neater. could also be presented as in .   Alternate embedding of the previous directed graph   Digraph of the relation r, alternate embedding     A vertex of a graph is also called a node, point, or a junction. An edge of a graph is also referred to as an arc, a line, or a branch. Do not be concerned if two graphs of a given relation look different as long as the connections between vertices are the same in the two graphs.  Another directed graph Consider the relation whose digraph is . What information does this give us? The graph tells us that is a relation on and that .   Digraph of the relation   Digraph of the relation      We will be building on the next example in the following section.  Ordering subsets of a two element universe Let , and let . Then is a relation on whose digraph is .   Graph for set containment on subsets of    Graph for set containment on subsets of    We will see in the next section that since has certain structural properties that describe partial orderings. We will be able to draw a much simpler type graph than this one, but for now the graph above serves our purposes.    Exercises  Let , and let be the relation on . Draw a digraph for .   Digraph for exercise 1   Digraph for     Let , and let be the relation divides on . Draw a digraph for .    Digraph for divisibility on the divisors of 24    Let . Define on by if and only if is even. Draw a digraph for .  See   Digraph of the relation     Let be the set of strings of 0's and 1's of length 2 or less. This includes the empty string, , which is the only string of length zero.  Define the relation of on by if is contained within . For example, . Draw a digraph for this relation.  Do the same for the relation defined by if is a prefix of . For example, , but is false.     (a)   Digraph for string containment for bit strings of length 2 or less    (b)   Digraph for the string prefix relation on strings of length 2 or less      Recall the relation in Exercise 5 of Section 6.1, defined on the power set, , of a set . The definition was iff . Draw the digraph for where .   Let , the divisors of 12, and define on by if and only if and share a common divisor greater than 1. Draw a digraph for .    Digraph for the relation of sharing a common factor greater than one on the divisors of 12     "
},
{
  "id": "s-graphs-of-relations-on-a-set-3-2",
  "level": "2",
  "url": "s-graphs-of-relations-on-a-set.html#s-graphs-of-relations-on-a-set-3-2",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "directed graph digraph "
},
{
  "id": "fig-graph-6-2-1",
  "level": "2",
  "url": "s-graphs-of-relations-on-a-set.html#fig-graph-6-2-1",
  "type": "Figure",
  "number": "6.2.1",
  "title": "",
  "body": " Digraph of a relation   Digraph of the relation with four nodes, 0, 1, 2, and 3; and edges between them corresponding to the pairs in the relation. One pair, is displayed as a loop starting at ending at node 0.    "
},
{
  "id": "s-graphs-of-relations-on-a-set-3-4",
  "level": "2",
  "url": "s-graphs-of-relations-on-a-set.html#s-graphs-of-relations-on-a-set-3-4",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "embedding of a graph "
},
{
  "id": "fig-graph-6-2-2",
  "level": "2",
  "url": "s-graphs-of-relations-on-a-set.html#fig-graph-6-2-2",
  "type": "Figure",
  "number": "6.2.2",
  "title": "",
  "body": " Alternate embedding of the previous directed graph   Digraph of the relation r, alternate embedding    "
},
{
  "id": "ex-another-simple-graph",
  "level": "2",
  "url": "s-graphs-of-relations-on-a-set.html#ex-another-simple-graph",
  "type": "Example",
  "number": "6.2.3",
  "title": "Another directed graph.",
  "body": "Another directed graph Consider the relation whose digraph is . What information does this give us? The graph tells us that is a relation on and that .   Digraph of the relation   Digraph of the relation     "
},
{
  "id": "ex-subsets-2-ordering",
  "level": "2",
  "url": "s-graphs-of-relations-on-a-set.html#ex-subsets-2-ordering",
  "type": "Example",
  "number": "6.2.5",
  "title": "Ordering subsets of a two element universe.",
  "body": "Ordering subsets of a two element universe Let , and let . Then is a relation on whose digraph is .   Graph for set containment on subsets of    Graph for set containment on subsets of    We will see in the next section that since has certain structural properties that describe partial orderings. We will be able to draw a much simpler type graph than this one, but for now the graph above serves our purposes.  "
},
{
  "id": "exercises-6-2-2",
  "level": "2",
  "url": "s-graphs-of-relations-on-a-set.html#exercises-6-2-2",
  "type": "Exercise",
  "number": "6.2.2.1",
  "title": "",
  "body": "Let , and let be the relation on . Draw a digraph for .   Digraph for exercise 1   Digraph for    "
},
{
  "id": "exercises-6-2-3",
  "level": "2",
  "url": "s-graphs-of-relations-on-a-set.html#exercises-6-2-3",
  "type": "Exercise",
  "number": "6.2.2.2",
  "title": "",
  "body": "Let , and let be the relation divides on . Draw a digraph for .    Digraph for divisibility on the divisors of 24   "
},
{
  "id": "exercises-6-2-4",
  "level": "2",
  "url": "s-graphs-of-relations-on-a-set.html#exercises-6-2-4",
  "type": "Exercise",
  "number": "6.2.2.3",
  "title": "",
  "body": "Let . Define on by if and only if is even. Draw a digraph for .  See   Digraph of the relation    "
},
{
  "id": "ex-string-relations",
  "level": "2",
  "url": "s-graphs-of-relations-on-a-set.html#ex-string-relations",
  "type": "Exercise",
  "number": "6.2.2.4",
  "title": "",
  "body": "Let be the set of strings of 0's and 1's of length 2 or less. This includes the empty string, , which is the only string of length zero.  Define the relation of on by if is contained within . For example, . Draw a digraph for this relation.  Do the same for the relation defined by if is a prefix of . For example, , but is false.     (a)   Digraph for string containment for bit strings of length 2 or less    (b)   Digraph for the string prefix relation on strings of length 2 or less     "
},
{
  "id": "exercises-6-2-6",
  "level": "2",
  "url": "s-graphs-of-relations-on-a-set.html#exercises-6-2-6",
  "type": "Exercise",
  "number": "6.2.2.5",
  "title": "",
  "body": "Recall the relation in Exercise 5 of Section 6.1, defined on the power set, , of a set . The definition was iff . Draw the digraph for where .  "
},
{
  "id": "exercises-6-2-7",
  "level": "2",
  "url": "s-graphs-of-relations-on-a-set.html#exercises-6-2-7",
  "type": "Exercise",
  "number": "6.2.2.6",
  "title": "",
  "body": "Let , the divisors of 12, and define on by if and only if and share a common divisor greater than 1. Draw a digraph for .    Digraph for the relation of sharing a common factor greater than one on the divisors of 12   "
},
{
  "id": "s-properties-of-relations",
  "level": "1",
  "url": "s-properties-of-relations.html",
  "type": "Section",
  "number": "6.3",
  "title": "Properties of Relations",
  "body": "Properties of Relations  Individual Properties  Consider the set and the relations divides and on . We notice that these two relations on have three properties in common:   Every element in divides itself and is less than or equal to itself. This is called the reflexive property.  If we search for two elements from where the first divides the second and the second divides the first, then we are forced to choose the two numbers to be the same. In other words, no two different numbers are related in both directions. The reader can verify that a similar fact is true for the relation on . This is called the antisymmetric property.  Next if we choose three values (not necessarily distinct) from such that the first divides the second and the second divides the third, then we always find that the first number divides the third. Again, the same is true if we replace divides with is less than or equal to. This is called the transitive property.   Relations that satisfy these properties are of special interest to us. Formal definitions of the properties follow.  Reflexive Relation Reflexive Relation Let be a set and let be a relation on . Then is reflexive if and only if for all .  Antisymmetric Relation Antisymmetric Relation Let be a set and let be a relation on . Then is antisymmetric if and only if whenever and then is false.   An equivalent condition for antisymmetry is that if and then . You are encouraged to convince yourself that this is true. This condition is often more convenient to prove than the definition, even though the definition is probably easier to understand.  A word of warning about antisymmetry: Students frequently find it difficult to understand this definition. Keep in mind that this term is defined through an If...then... statement. The question that you must ask is: Is it true that whenever there are elements and from where and , it follows that is not related to ? If so, then the relation is antisymmetric.  Another way to determine whether a relation is antisymmetric is to examine (or imagine) its digraph. The relation is not antisymmetric if there exists a pair of vertices that are connected by edges in both directions.  Transitive Relation Transitive Relation Let be a set and let be a relation on . is transitive if and only if whenever and then .   Partial Orderings  Not all relations have all three of the properties discussed above, but those that do are a special type of relation.  Partial Ordering Partial Ordering Poset A relation on a set that is reflexive, antisymmetric, and transitive is called a partial ordering on . A set on which there is a partial ordering relation defined is called a partially ordered set Partially ordered set or poset .  Set Containment as a Partial Ordering Let be a set. Then together with the relation (set containment) is a poset. To prove this we observe that the three properties hold, as discussed in Chapter 4.   Let . The fact that follows from the definition of subset. Hence, set containment is reflexive.  Let and assume that and . Could it be that ? No. There must be some element such that , but . This is exactly what we need to conclude that is not contained in . Hence, set containment is antisymmetric.  Let and assume that and . Does it follow that ? Yes, if , then because . Now that we have and we have assumed , we conclude that . Therefore, and so set containment is transitive.  is the graph for the set containment relation on the power set of .   Hasse Diagram Ordering Diagram is helpful insofar as it reminds us that each set is a subset of itself and shows us at a glance the relationship between the various subsets in . However, when a relation is a partial ordering, we can streamline a graph like this one. The streamlined form of a graph is called a Hasse diagram or ordering diagram . A Hasse diagram takes into account the following facts.   By the reflexive property, each vertex must be related to itself, so the arrows from a vertex to itself (called self-loops ) are not drawn in a Hasse diagram. They are simply assumed.  By the antisymmetry property, connections between two distinct elements in a directed graph can only go one way, if at all. When there is a connection, we agree to always place the second element above the first (as we do above with the connection from to ). For this reason, we can just draw a connection without an arrow, just a line.  By the transitive property, if there are edges connecting one element up to a second element and the second element up to a third element, then there will be a direct connection from the first to the third. We see this in with connected to and then connected to . Notice the edge connecting to . Whenever we identify this situation, remove the connection from the first to the third in a Hasse diagram and simply observe that an upward path of any length implies that the lower element is related to the upper one.   Using these observations as a guide, we can draw a Hasse diagram for on as in .   Hasse diagram for set containment on subsets of    Hasse diagram for set containment on subsets of    Definition of a relation using a Hasse diagram Consider the partial ordering relation whose Hasse diagram is .   Hasse diagram for the pentagonal poset   Hasse diagram for the pentagonal poset     How do we read this diagram? What is ? What is ? What does the digraph of look like? Certainly and , , , , etc., Notice that is implied by the fact that there is a path of length three upward from 1 to 5. This follows from the edges that are shown and the transitive property that is presumed in a poset. Since and , we know that . We then combine with to infer . Without going into details why, here is a complete list of pairs defined by .  A digraph for is . It is certainly more complicated to read and difficult to draw than the Hasse diagram.   Digraph for the pentagonal poset   Digraph for the pentagonal poset      A classic example of a partial ordering relation is on the real numbers, . Indeed, when graphing partial ordering relations, it is natural to plot the elements from the given poset starting with the least element to the greatest and to use terms like least,  greatest, etc. Because of this the reader should be forewarned that some texts use the symbol for arbitrary partial orderings. This can be quite confusing for the novice, so we continue to use generic letters , , etc.   Equivalence Relations Equivalence Relations  Another common property of relations is symmetry.  Symmetric Relation  Symmetric Relation Let be a relation on a set . is symmetric if and only if whenever , it follows that .  Consider the relation of equality defined on any set . Certainly implies that so equality is a symmetric relation on .  Surprisingly, equality is also an antisymmetric relation on . This is due to the fact that the condition that defines the antisymmetry property, and , is a contradiction. Remember, a conditional proposition is always true when the condition is false. So a relation can be both symmetric and antisymmetric on a set! Again recall that these terms are not negatives of one other. That said, there are very few important relations other than equality that are both symmetric and antisymmetric.  Equivalence Relation Equivalence Relation A relation on a set is called an equivalence relation if and only if it is reflexive, symmetric, and transitive.  The classic example of an equivalence relation is equality on a set . In fact, the term equivalence relation is used because those relations which satisfy the definition behave quite like the equality relation. Here is another important equivalence relation.  Equivalent Fractions Let be the set of nonzero integers. One of the most basic equivalence relations in mathematics is the relation on defined by if and only if . We will leave it to the reader to, verify that is indeed an equivalence relation. Be aware that since the elements of are ordered pairs, proving symmetry involves four numbers and transitivity involves six numbers. Two ordered pairs, and , are related if the fractions and are numerically equal.  Reflecting on these comments on fractions, we see that any fraction is a member of a set of equivalent fractions that can be exchanged for one another when doing arithmetic. This is an instance of an important property of all equivalence relations that motivates the following definition.   Equivalence Classes  Equivalence Classes  The equivalence class of a   Partition of with respect to an equivalence relation  Let be an equivalence relation on , and . The equivalence class of is the set, , of all elements to which is related. The set of all equivalence classes with respect to is denoted , read mod .    When we want to make it clear that an equivalence class defined by an element is based on a specific equivalence relation we would refer to it as the equivalence class of under . Whenever we encounter an equivalence relation on a set, we should immediately think about how the set is partitioned because of the following theorem.   Let be an equivalence relation on . Then the set of all distinct equivalence classes determined by form a partition of denoted and read mod .  We leave it to the reader to prove this theorem. All three properties of an equivalence relation play a role in the proof.   Our next example involves the following fundamental relations on the set of integers.  Congruence Modulo  Congruence Modulo  is congruent to modulo  is congruent to modulo  Let be a positive integer, . We define congruence modulo n to be the relation defined on the integers by    We observe the following about congruence modulo :   This relation is reflexive, for if , .  This relation is symmetric. We can prove this through the following chain of implications. .   Finally, this relation is transitive. We leave it to the reader to prove that if and , then .   Frequently, you will see the equivalent notation for congruence modulo .  Random Relations usually have no properties Consider the relation s described by the digraph in . This was created by randomly selecting whether or not two elements from were related or not. Convince yourself that the following are true:   This relation is not reflexive.  It is not antisymmetric.  Also, it is not symmetric.  It is not transitive.  Is an equivalence relation or a partial ordering?    Digraph of a random relation   A random relation     Not every random choice of a relation will be so totally negative, but as the underlying set increases, the likelihood any of the properties are true begins to vanish.   Exercises   Prove that on the set of positive integers is a partial ordering. Note that this will imply that the relation is a partial ordering on any subset of the positive integers as well.    Divides is reflexive because, if is any positive integer, and so  Divides is antisymmetric. Suppose and are two distinct positive integers. One of them has to be less than the other, so we will assume . If , then for some positive integer , where we have . But this means that and since is not a positive integer, .  Divides is transitive. If , and are positive integers such that and , there must be two positive integers and such that and . Combining these equalities we get and so .      Let and . Draw a Hasse diagram for on .  Let . Draw a Hasse diagram for divides on .  Compare the graphs of parts a and b. What can you observe?  Repeat the previous steps with and .       Figure for the first part    See exercise 1 for the answers to parts (a) and (b); and observe that the ordering diagrams are the same if we disregard the names of the vertices.   Here is a Hasse diagram for the subsets of .    Hasse Diagram for a set containment for subsets of a three element set      Here is a Hasse diagram for divides on .    Hasse Diagram for a the relation divides on the set of divisors of 30.     As with the outcome of the first three parts of this exercise, the two Hasse diagram are identical if we ignore the labels.    Consider the relations defined by the digraphs in .  Determine whether the given relations are reflexive, symmetric, antisymmetric, or transitive. Try to develop procedures for determining the validity of these properties from the graphs,  Which of the graphs are of equivalence relations or of partial orderings?    Some digraphs of relations   Some digraphs of relations for exercise 3 of section 6.3     Properties of relations defined by digraphs   Part reflexive? symmetric? antisymmetric? transitive?  i yes no no yes  ii yes no yes yes  iii no no no no  iv no yes yes yes  v yes yes no yes  vi yes no yes yes  vii no no no no     See  Graphs ii and vi show partial ordering relations. Graph v is of an equivalence relation.    Determine which of the following are equivalence relations and\/or partial ordering relations for the given sets:    , and defined by if and only if is parallel to . Assume every line is parallel to itself.   and defined by if and only if .     This is an equivalence relation. For each possible slope of a line, all lines with that slope make up one equivalence class. There is one additional equivalence class made up of all vertical lines.  This is not an equivalence relation, nor is it a partial ordering. The reason is that it is not transitive. and yet is not related to ..    Consider the relation on defined by .   Is reflexive?  Is symmetric?  Is transitive?  Draw a graph of .   No, since , for example  Yes, because .  No, since and , but , for example.  See      Solution to number 5c of section 6.3     Let and let   Verify that is an equivalence relation on .  Find for each element , and observe that forms a partition of .    and    Let be an equivalence relation on an arbitrary nonempty set . Prove that the set of all equivalence classes under constitutes a partition of .  Let be any element of . since is reflexive, so each element of is in some equivalence class. Therefore, the union of all equivalence classes equals . Next we show that any two equivalence classes are either identical or disjoint and we are done. Let and be two equivalence classes, and assume that . We want to show that . To show that , let . . Also, there exists an element, , of that is in the intersection of and by our assumption. Therefore,  Next,   Similarly, .    Define on the power set of some set by . Prove that is an equivalence relation. What are the equivalence classes under if ?   Assume , and are any subsets of .  Reflexive:  Symmetric:  Transitive:  If , the set of equivalence classes is the partition   Consider the following relations on . Which are equivalence relations? For the equivalence relations, list the equivalence classes.    iff the English spellings of and begin with the same letter.   iff is a positive integer.   iff is an even integer.   Equivalence Relation, , and  Not an Equivalence Relation.  Equivalence Relation, and     Let be a positive integer greater than or equal to two.   Prove that congruence modulo is transitive.  What are the equivalence classes under congruence modulo 2? How many distinct equivalence classes are there?  What are the equivalence classes under congruence modulo 10? How many distinct equivalence classes are there?     Let .  The sets of odd and even integers are the equivalence classes.  There are ten equivalence classes. Each of the integers through are in separate equivalence classes. One example is the equivalence class of 3: Notice that the positive elements all end in 3 but the negative ones end in 7.    In this exercise, we prove that implication is a partial ordering. Let be any set of propositions, no two of which is equivalent to one another.   Verify that is a tautology, thereby showing that is a reflexive relation on .  Prove that is antisymmetric on . Note: we do not use when speaking of propositions, but rather equivalence, .  Prove that is transitive on .  Given that is the proposition on , draw the Hasse diagram for the relation on .     The proof follows from the biconditional equivalence in .  Apply the chain rule.   See .      Solution to number 11 of Section 6.3       "
},
{
  "id": "def-reflexive-relation",
  "level": "2",
  "url": "s-properties-of-relations.html#def-reflexive-relation",
  "type": "Definition",
  "number": "6.3.1",
  "title": "Reflexive Relation.",
  "body": "Reflexive Relation Reflexive Relation Let be a set and let be a relation on . Then is reflexive if and only if for all . "
},
{
  "id": "def-antisymmetric-relation",
  "level": "2",
  "url": "s-properties-of-relations.html#def-antisymmetric-relation",
  "type": "Definition",
  "number": "6.3.2",
  "title": "Antisymmetric Relation.",
  "body": "Antisymmetric Relation Antisymmetric Relation Let be a set and let be a relation on . Then is antisymmetric if and only if whenever and then is false.  "
},
{
  "id": "def-transitive-relation",
  "level": "2",
  "url": "s-properties-of-relations.html#def-transitive-relation",
  "type": "Definition",
  "number": "6.3.3",
  "title": "Transitive Relation.",
  "body": "Transitive Relation Transitive Relation Let be a set and let be a relation on . is transitive if and only if whenever and then . "
},
{
  "id": "partial-ordering",
  "level": "2",
  "url": "s-properties-of-relations.html#partial-ordering",
  "type": "Definition",
  "number": "6.3.4",
  "title": "Partial Ordering.",
  "body": "Partial Ordering Partial Ordering Poset A relation on a set that is reflexive, antisymmetric, and transitive is called a partial ordering on . A set on which there is a partial ordering relation defined is called a partially ordered set Partially ordered set or poset . "
},
{
  "id": "ex-subset-partial-ordering",
  "level": "2",
  "url": "s-properties-of-relations.html#ex-subset-partial-ordering",
  "type": "Example",
  "number": "6.3.5",
  "title": "Set Containment as a Partial Ordering.",
  "body": "Set Containment as a Partial Ordering Let be a set. Then together with the relation (set containment) is a poset. To prove this we observe that the three properties hold, as discussed in Chapter 4.   Let . The fact that follows from the definition of subset. Hence, set containment is reflexive.  Let and assume that and . Could it be that ? No. There must be some element such that , but . This is exactly what we need to conclude that is not contained in . Hence, set containment is antisymmetric.  Let and assume that and . Does it follow that ? Yes, if , then because . Now that we have and we have assumed , we conclude that . Therefore, and so set containment is transitive.  is the graph for the set containment relation on the power set of .  "
},
{
  "id": "ss-partial-ordering-5",
  "level": "2",
  "url": "s-properties-of-relations.html#ss-partial-ordering-5",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Hasse diagram ordering diagram "
},
{
  "id": "subsets_2_hasse",
  "level": "2",
  "url": "s-properties-of-relations.html#subsets_2_hasse",
  "type": "Figure",
  "number": "6.3.6",
  "title": "",
  "body": " Hasse diagram for set containment on subsets of    Hasse diagram for set containment on subsets of   "
},
{
  "id": "ex-def-by-hasse",
  "level": "2",
  "url": "s-properties-of-relations.html#ex-def-by-hasse",
  "type": "Example",
  "number": "6.3.7",
  "title": "Definition of a relation using a Hasse diagram.",
  "body": "Definition of a relation using a Hasse diagram Consider the partial ordering relation whose Hasse diagram is .   Hasse diagram for the pentagonal poset   Hasse diagram for the pentagonal poset     How do we read this diagram? What is ? What is ? What does the digraph of look like? Certainly and , , , , etc., Notice that is implied by the fact that there is a path of length three upward from 1 to 5. This follows from the edges that are shown and the transitive property that is presumed in a poset. Since and , we know that . We then combine with to infer . Without going into details why, here is a complete list of pairs defined by .  A digraph for is . It is certainly more complicated to read and difficult to draw than the Hasse diagram.   Digraph for the pentagonal poset   Digraph for the pentagonal poset    "
},
{
  "id": "def-symmetric-relation",
  "level": "2",
  "url": "s-properties-of-relations.html#def-symmetric-relation",
  "type": "Definition",
  "number": "6.3.10",
  "title": "Symmetric Relation.",
  "body": "Symmetric Relation  Symmetric Relation Let be a relation on a set . is symmetric if and only if whenever , it follows that . "
},
{
  "id": "def-equivalence-relation",
  "level": "2",
  "url": "s-properties-of-relations.html#def-equivalence-relation",
  "type": "Definition",
  "number": "6.3.11",
  "title": "Equivalence Relation.",
  "body": "Equivalence Relation Equivalence Relation A relation on a set is called an equivalence relation if and only if it is reflexive, symmetric, and transitive. "
},
{
  "id": "ex-fraction-equivalence",
  "level": "2",
  "url": "s-properties-of-relations.html#ex-fraction-equivalence",
  "type": "Example",
  "number": "6.3.12",
  "title": "Equivalent Fractions.",
  "body": "Equivalent Fractions Let be the set of nonzero integers. One of the most basic equivalence relations in mathematics is the relation on defined by if and only if . We will leave it to the reader to, verify that is indeed an equivalence relation. Be aware that since the elements of are ordered pairs, proving symmetry involves four numbers and transitivity involves six numbers. Two ordered pairs, and , are related if the fractions and are numerically equal.  Reflecting on these comments on fractions, we see that any fraction is a member of a set of equivalent fractions that can be exchanged for one another when doing arithmetic. This is an instance of an important property of all equivalence relations that motivates the following definition. "
},
{
  "id": "def-equivalence-classes",
  "level": "2",
  "url": "s-properties-of-relations.html#def-equivalence-classes",
  "type": "Definition",
  "number": "6.3.13",
  "title": "Equivalence Classes.",
  "body": " Equivalence Classes  Equivalence Classes  The equivalence class of a   Partition of with respect to an equivalence relation  Let be an equivalence relation on , and . The equivalence class of is the set, , of all elements to which is related. The set of all equivalence classes with respect to is denoted , read mod .   "
},
{
  "id": "equivalence-partition",
  "level": "2",
  "url": "s-properties-of-relations.html#equivalence-partition",
  "type": "Theorem",
  "number": "6.3.14",
  "title": "",
  "body": " Let be an equivalence relation on . Then the set of all distinct equivalence classes determined by form a partition of denoted and read mod .  We leave it to the reader to prove this theorem. All three properties of an equivalence relation play a role in the proof.  "
},
{
  "id": "def-congruence-mod-n",
  "level": "2",
  "url": "s-properties-of-relations.html#def-congruence-mod-n",
  "type": "Definition",
  "number": "6.3.15",
  "title": "Congruence Modulo <span xmlns:pf=\"https:\/\/prefigure.org\" class=\"process-math\">\\(n\\)<\/span>.",
  "body": "Congruence Modulo  Congruence Modulo  is congruent to modulo  is congruent to modulo  Let be a positive integer, . We define congruence modulo n to be the relation defined on the integers by   "
},
{
  "id": "ex-no-propery-relation",
  "level": "2",
  "url": "s-properties-of-relations.html#ex-no-propery-relation",
  "type": "Example",
  "number": "6.3.16",
  "title": "Random Relations usually have no properties.",
  "body": "Random Relations usually have no properties Consider the relation s described by the digraph in . This was created by randomly selecting whether or not two elements from were related or not. Convince yourself that the following are true:   This relation is not reflexive.  It is not antisymmetric.  Also, it is not symmetric.  It is not transitive.  Is an equivalence relation or a partial ordering?    Digraph of a random relation   A random relation     Not every random choice of a relation will be so totally negative, but as the underlying set increases, the likelihood any of the properties are true begins to vanish. "
},
{
  "id": "exercises-6-3-2",
  "level": "2",
  "url": "s-properties-of-relations.html#exercises-6-3-2",
  "type": "Exercise",
  "number": "6.3.4.1",
  "title": "",
  "body": " Prove that on the set of positive integers is a partial ordering. Note that this will imply that the relation is a partial ordering on any subset of the positive integers as well.    Divides is reflexive because, if is any positive integer, and so  Divides is antisymmetric. Suppose and are two distinct positive integers. One of them has to be less than the other, so we will assume . If , then for some positive integer , where we have . But this means that and since is not a positive integer, .  Divides is transitive. If , and are positive integers such that and , there must be two positive integers and such that and . Combining these equalities we get and so .   "
},
{
  "id": "exercises-6-3-3",
  "level": "2",
  "url": "s-properties-of-relations.html#exercises-6-3-3",
  "type": "Exercise",
  "number": "6.3.4.2",
  "title": "",
  "body": "  Let and . Draw a Hasse diagram for on .  Let . Draw a Hasse diagram for divides on .  Compare the graphs of parts a and b. What can you observe?  Repeat the previous steps with and .       Figure for the first part    See exercise 1 for the answers to parts (a) and (b); and observe that the ordering diagrams are the same if we disregard the names of the vertices.   Here is a Hasse diagram for the subsets of .    Hasse Diagram for a set containment for subsets of a three element set      Here is a Hasse diagram for divides on .    Hasse Diagram for a the relation divides on the set of divisors of 30.     As with the outcome of the first three parts of this exercise, the two Hasse diagram are identical if we ignore the labels.  "
},
{
  "id": "exercises-6-3-4",
  "level": "2",
  "url": "s-properties-of-relations.html#exercises-6-3-4",
  "type": "Exercise",
  "number": "6.3.4.3",
  "title": "",
  "body": " Consider the relations defined by the digraphs in .  Determine whether the given relations are reflexive, symmetric, antisymmetric, or transitive. Try to develop procedures for determining the validity of these properties from the graphs,  Which of the graphs are of equivalence relations or of partial orderings?    Some digraphs of relations   Some digraphs of relations for exercise 3 of section 6.3     Properties of relations defined by digraphs   Part reflexive? symmetric? antisymmetric? transitive?  i yes no no yes  ii yes no yes yes  iii no no no no  iv no yes yes yes  v yes yes no yes  vi yes no yes yes  vii no no no no     See  Graphs ii and vi show partial ordering relations. Graph v is of an equivalence relation.   "
},
{
  "id": "exercises-6-3-5",
  "level": "2",
  "url": "s-properties-of-relations.html#exercises-6-3-5",
  "type": "Exercise",
  "number": "6.3.4.4",
  "title": "",
  "body": "Determine which of the following are equivalence relations and\/or partial ordering relations for the given sets:    , and defined by if and only if is parallel to . Assume every line is parallel to itself.   and defined by if and only if .     This is an equivalence relation. For each possible slope of a line, all lines with that slope make up one equivalence class. There is one additional equivalence class made up of all vertical lines.  This is not an equivalence relation, nor is it a partial ordering. The reason is that it is not transitive. and yet is not related to ..   "
},
{
  "id": "exercises-6-3-6",
  "level": "2",
  "url": "s-properties-of-relations.html#exercises-6-3-6",
  "type": "Exercise",
  "number": "6.3.4.5",
  "title": "",
  "body": "Consider the relation on defined by .   Is reflexive?  Is symmetric?  Is transitive?  Draw a graph of .   No, since , for example  Yes, because .  No, since and , but , for example.  See      Solution to number 5c of section 6.3    "
},
{
  "id": "exercises-6-3-7",
  "level": "2",
  "url": "s-properties-of-relations.html#exercises-6-3-7",
  "type": "Exercise",
  "number": "6.3.4.6",
  "title": "",
  "body": "Let and let   Verify that is an equivalence relation on .  Find for each element , and observe that forms a partition of .    and  "
},
{
  "id": "exercises-6-3-8",
  "level": "2",
  "url": "s-properties-of-relations.html#exercises-6-3-8",
  "type": "Exercise",
  "number": "6.3.4.7",
  "title": "",
  "body": " Let be an equivalence relation on an arbitrary nonempty set . Prove that the set of all equivalence classes under constitutes a partition of .  Let be any element of . since is reflexive, so each element of is in some equivalence class. Therefore, the union of all equivalence classes equals . Next we show that any two equivalence classes are either identical or disjoint and we are done. Let and be two equivalence classes, and assume that . We want to show that . To show that , let . . Also, there exists an element, , of that is in the intersection of and by our assumption. Therefore,  Next,   Similarly, .   "
},
{
  "id": "exercises-6-3-9",
  "level": "2",
  "url": "s-properties-of-relations.html#exercises-6-3-9",
  "type": "Exercise",
  "number": "6.3.4.8",
  "title": "",
  "body": "Define on the power set of some set by . Prove that is an equivalence relation. What are the equivalence classes under if ?   Assume , and are any subsets of .  Reflexive:  Symmetric:  Transitive:  If , the set of equivalence classes is the partition  "
},
{
  "id": "exercises-6-3-10",
  "level": "2",
  "url": "s-properties-of-relations.html#exercises-6-3-10",
  "type": "Exercise",
  "number": "6.3.4.9",
  "title": "",
  "body": "Consider the following relations on . Which are equivalence relations? For the equivalence relations, list the equivalence classes.    iff the English spellings of and begin with the same letter.   iff is a positive integer.   iff is an even integer.   Equivalence Relation, , and  Not an Equivalence Relation.  Equivalence Relation, and    "
},
{
  "id": "exercises-6-3-11",
  "level": "2",
  "url": "s-properties-of-relations.html#exercises-6-3-11",
  "type": "Exercise",
  "number": "6.3.4.10",
  "title": "",
  "body": "Let be a positive integer greater than or equal to two.   Prove that congruence modulo is transitive.  What are the equivalence classes under congruence modulo 2? How many distinct equivalence classes are there?  What are the equivalence classes under congruence modulo 10? How many distinct equivalence classes are there?     Let .  The sets of odd and even integers are the equivalence classes.  There are ten equivalence classes. Each of the integers through are in separate equivalence classes. One example is the equivalence class of 3: Notice that the positive elements all end in 3 but the negative ones end in 7.   "
},
{
  "id": "exercises-6-3-12",
  "level": "2",
  "url": "s-properties-of-relations.html#exercises-6-3-12",
  "type": "Exercise",
  "number": "6.3.4.11",
  "title": "",
  "body": "In this exercise, we prove that implication is a partial ordering. Let be any set of propositions, no two of which is equivalent to one another.   Verify that is a tautology, thereby showing that is a reflexive relation on .  Prove that is antisymmetric on . Note: we do not use when speaking of propositions, but rather equivalence, .  Prove that is transitive on .  Given that is the proposition on , draw the Hasse diagram for the relation on .     The proof follows from the biconditional equivalence in .  Apply the chain rule.   See .      Solution to number 11 of Section 6.3    "
},
{
  "id": "s-matrices-of-relations",
  "level": "1",
  "url": "s-matrices-of-relations.html",
  "type": "Section",
  "number": "6.4",
  "title": "Matrices of Relations",
  "body": "Matrices of Relations  We have discussed two of the many possible ways of representing a relation, namely as a digraph or as a set of ordered pairs. In this section we will discuss the representation of relations by matrices.  Representing a Relation with a Matrix  Adjacency Matrix Adjacency Matrix Let and be finite sets of cardinality and , respectively. Let be a relation from into . Then can be represented by the matrix defined by  is called the adjacency matrix (or the relation matrix) of .   A simple example  Let and let be the relation on . Since is a relation from into the same set (the of the definition), we have , , and , while , , and . Next, since   , we have  , we have  , we have   , we have   All other entries of are zero, so     Composition as Matrix Multiplication  From the definition of and of composition, we note that The adjacency matrix of is .  We do not write only for notational purposes. In fact, can be obtained from the matrix product ; however, we must use a slightly different form of arithmetic.  Boolean Arithmetic  Boolean Arithmetic Boolean arithmetic is the arithmetic defined on using Boolean addition and Boolean multiplication, defined by            Notice that from Chapter 3, this is the arithmetic of logic, where replaces or and replaces and.  Composition by Multiplication Suppose that and . Then using Boolean arithmetic, and .  Composition is Matrix Multiplication Let , , and be finite sets where is a relation from into and is a relation from into . If and are the adjacency matrices of and , respectively, then the product using Boolean arithmetic is the adjacency matrix of the composition .  Remark: A convenient help in constructing the adjacency matrix of a relation from a set into a set is to write the elements from in a column preceding the first column of the adjacency matrix, and the elements of in a row above the first row. Initially, in would be To fill in the matrix, is 1 if and only if . So that, since the pair , the entry of corresponding to the row labeled 2 and the column labeled 5 in the matrix is a 1.  Relations and Information This final example gives an insight into how relational data base programs can systematically answer questions pertaining to large masses of information. Matrices (on the left) and (on the right) define the relations and where if software can be run with operating system , and if operating system can run on computer . Although the relation between the software and computers is not implicit from the data given, we can easily compute this information. The matrix of is , which is This matrix tells us at a glance which software will run on the computers listed. In this case, all software will run on all computers with the exception of program P2, which will not run on the computer C3, and programs P3 and P4, which will not run on the computer C1.   Exercises  Let , , and . Let be the relation from into defined by , and let be the relation from into defined by .   Determine the adjacency matrices of and .  Use the definition of composition to find .  Verify the result in part b by finding the product of the adjacency matrices of and .       and           Determine the adjacency matrix of each relation given by the following digraphs.    Directed graph of the relation on with edges .   Directed graph of the relation on with edges .    Using the matrices found in part (a), determine the matrices of and .  Draw the digraphs of and directly from the definition of the square of relation and compare your results with those of part (b).       >    >    Directed graph of the relation on with edges .   Directed graph of the relation on with edges .      Suppose that the matrices in are relations on . What relations do and describe?   R : if and only if  S : if and only if is less than .     Let be the set of weekdays, Monday through Friday, let be a set of employees of a tutoring center, and let be a set of computer languages for which tutoring is offered, . We define (schedule) from into by if is scheduled to work on day . We also define from into by if can tutor students in language . If and are defined by matrices      Compute using Boolean arithmetic and give an interpretation of the relation it defines, and  Compute using regular arithmetic and give an interpretation of what the result describes.     Using Boolean arithmetic, . The entry corresponding to a day's row and a language's column, a indicates whether a tutor is available on that day for that language. A zero indicates that no tutor is available on that day for that language.  Using regular arithmetic, . The entries indicate the number of tutors that are available on any day for any language. For example, the in row 1, column 2 indicates that there are two tutors available on Mondays in Basic.    How many different reflexive, symmetric relations are there on a set with three elements?  Consider the possible matrices. The graph of a relation on three elements has nine entries. The three entries in the diagonal must be 1 in order for the relation to be reflexive. In addition, to make the relation symmetric, the off-diaginal entries can be paired up so that they are equal. For example if the entry in row 1 column 2 is equal to 1, the entry in row 2 column 1 must also be 1. This means that three entries, the ones above the diagonal determine the whole matrix, so there are different reflexive, symmetric relations on a three element set.  Let . Let be the relation on with adjacency matrix   Explain why is a partial ordering on .  Draw its Hasse diagram.     The relation is reflexive because for all . It is antisymmetric because there are no pairs with and . This is clear because there are no 1's above the diagonal of the matrix. Finally, if one squares using Boolean arithmetic, you don't get a 1 appearing in a new position. This means that if whenever then is a partial ordering on . See exercise 8 below for a proof of this claim.   Hasse diagram with c below a and b, and d below b.     Define relations and on by and .   Represent and as both graphs and matrices.  Determine , , and ; and represent them clearly in any way.    and        Let be a relation on a set .   Prove that if is a transitive if and only if .  Find an example of a transitive relation for which .     ( ) Assume is transitive and let .   ( ) Assume    If be the transitive relation less than on the integers. Notice that , but and so .    We define on the set of all relation matrices by the rule that if and are any two relation matrices, if and only if for all .   Prove that is a partial ordering on all relation matrices.  Prove that , but the converse is not true.  If and are matrices of equivalence relations and , how are the equivalence classes defined by related to the equivalence classes defined by ?    Reflexive: for all , therefore  Antisymmetric: Assume and for all . Therefore, for all and so  Transitive: Assume and are matrices where and , for all . Then for all , and so .  .  To verify that the converse is not true we need only one example. For , let and all other entries equal , and let be the zero matrix. Since and are both the zero matrix, , but since is false.  The matrices are defined on the same set . Let be the equivalence classes defined by and let be those defined by . Claim: .     "
},
{
  "id": "def-adjacency-matrix",
  "level": "2",
  "url": "s-matrices-of-relations.html#def-adjacency-matrix",
  "type": "Definition",
  "number": "6.4.1",
  "title": "Adjacency Matrix.",
  "body": "Adjacency Matrix Adjacency Matrix Let and be finite sets of cardinality and , respectively. Let be a relation from into . Then can be represented by the matrix defined by  is called the adjacency matrix (or the relation matrix) of .  "
},
{
  "id": "ex-first-6-4",
  "level": "2",
  "url": "s-matrices-of-relations.html#ex-first-6-4",
  "type": "Example",
  "number": "6.4.2",
  "title": "A simple example.",
  "body": "A simple example  Let and let be the relation on . Since is a relation from into the same set (the of the definition), we have , , and , while , , and . Next, since   , we have  , we have  , we have   , we have   All other entries of are zero, so   "
},
{
  "id": "def-boolean-arithmetic",
  "level": "2",
  "url": "s-matrices-of-relations.html#def-boolean-arithmetic",
  "type": "Definition",
  "number": "6.4.3",
  "title": "Boolean Arithmetic.",
  "body": "Boolean Arithmetic  Boolean Arithmetic Boolean arithmetic is the arithmetic defined on using Boolean addition and Boolean multiplication, defined by           "
},
{
  "id": "ex-composition-matrices",
  "level": "2",
  "url": "s-matrices-of-relations.html#ex-composition-matrices",
  "type": "Example",
  "number": "6.4.5",
  "title": "Composition by Multiplication.",
  "body": "Composition by Multiplication Suppose that and . Then using Boolean arithmetic, and . "
},
{
  "id": "theorem-composition-is-multiplication",
  "level": "2",
  "url": "s-matrices-of-relations.html#theorem-composition-is-multiplication",
  "type": "Theorem",
  "number": "6.4.6",
  "title": "Composition is Matrix Multiplication.",
  "body": "Composition is Matrix Multiplication Let , , and be finite sets where is a relation from into and is a relation from into . If and are the adjacency matrices of and , respectively, then the product using Boolean arithmetic is the adjacency matrix of the composition . "
},
{
  "id": "ex-relations-information",
  "level": "2",
  "url": "s-matrices-of-relations.html#ex-relations-information",
  "type": "Example",
  "number": "6.4.7",
  "title": "Relations and Information.",
  "body": "Relations and Information This final example gives an insight into how relational data base programs can systematically answer questions pertaining to large masses of information. Matrices (on the left) and (on the right) define the relations and where if software can be run with operating system , and if operating system can run on computer . Although the relation between the software and computers is not implicit from the data given, we can easily compute this information. The matrix of is , which is This matrix tells us at a glance which software will run on the computers listed. In this case, all software will run on all computers with the exception of program P2, which will not run on the computer C3, and programs P3 and P4, which will not run on the computer C1. "
},
{
  "id": "exercises-6-4-2",
  "level": "2",
  "url": "s-matrices-of-relations.html#exercises-6-4-2",
  "type": "Exercise",
  "number": "6.4.3.1",
  "title": "",
  "body": "Let , , and . Let be the relation from into defined by , and let be the relation from into defined by .   Determine the adjacency matrices of and .  Use the definition of composition to find .  Verify the result in part b by finding the product of the adjacency matrices of and .       and        "
},
{
  "id": "exercises-6-4-3",
  "level": "2",
  "url": "s-matrices-of-relations.html#exercises-6-4-3",
  "type": "Exercise",
  "number": "6.4.3.2",
  "title": "",
  "body": "  Determine the adjacency matrix of each relation given by the following digraphs.    Directed graph of the relation on with edges .   Directed graph of the relation on with edges .    Using the matrices found in part (a), determine the matrices of and .  Draw the digraphs of and directly from the definition of the square of relation and compare your results with those of part (b).       >    >    Directed graph of the relation on with edges .   Directed graph of the relation on with edges .     "
},
{
  "id": "exercises-6-4-4",
  "level": "2",
  "url": "s-matrices-of-relations.html#exercises-6-4-4",
  "type": "Exercise",
  "number": "6.4.3.3",
  "title": "",
  "body": "Suppose that the matrices in are relations on . What relations do and describe?   R : if and only if  S : if and only if is less than .    "
},
{
  "id": "exercises-6-4-5",
  "level": "2",
  "url": "s-matrices-of-relations.html#exercises-6-4-5",
  "type": "Exercise",
  "number": "6.4.3.4",
  "title": "",
  "body": "Let be the set of weekdays, Monday through Friday, let be a set of employees of a tutoring center, and let be a set of computer languages for which tutoring is offered, . We define (schedule) from into by if is scheduled to work on day . We also define from into by if can tutor students in language . If and are defined by matrices      Compute using Boolean arithmetic and give an interpretation of the relation it defines, and  Compute using regular arithmetic and give an interpretation of what the result describes.     Using Boolean arithmetic, . The entry corresponding to a day's row and a language's column, a indicates whether a tutor is available on that day for that language. A zero indicates that no tutor is available on that day for that language.  Using regular arithmetic, . The entries indicate the number of tutors that are available on any day for any language. For example, the in row 1, column 2 indicates that there are two tutors available on Mondays in Basic.   "
},
{
  "id": "exercises-6-4-6",
  "level": "2",
  "url": "s-matrices-of-relations.html#exercises-6-4-6",
  "type": "Exercise",
  "number": "6.4.3.5",
  "title": "",
  "body": "How many different reflexive, symmetric relations are there on a set with three elements?  Consider the possible matrices. The graph of a relation on three elements has nine entries. The three entries in the diagonal must be 1 in order for the relation to be reflexive. In addition, to make the relation symmetric, the off-diaginal entries can be paired up so that they are equal. For example if the entry in row 1 column 2 is equal to 1, the entry in row 2 column 1 must also be 1. This means that three entries, the ones above the diagonal determine the whole matrix, so there are different reflexive, symmetric relations on a three element set. "
},
{
  "id": "exercises-6-4-7",
  "level": "2",
  "url": "s-matrices-of-relations.html#exercises-6-4-7",
  "type": "Exercise",
  "number": "6.4.3.6",
  "title": "",
  "body": "Let . Let be the relation on with adjacency matrix   Explain why is a partial ordering on .  Draw its Hasse diagram.     The relation is reflexive because for all . It is antisymmetric because there are no pairs with and . This is clear because there are no 1's above the diagonal of the matrix. Finally, if one squares using Boolean arithmetic, you don't get a 1 appearing in a new position. This means that if whenever then is a partial ordering on . See exercise 8 below for a proof of this claim.   Hasse diagram with c below a and b, and d below b.    "
},
{
  "id": "exercises-6-4-8",
  "level": "2",
  "url": "s-matrices-of-relations.html#exercises-6-4-8",
  "type": "Exercise",
  "number": "6.4.3.7",
  "title": "",
  "body": "Define relations and on by and .   Represent and as both graphs and matrices.  Determine , , and ; and represent them clearly in any way.    and       "
},
{
  "id": "exercises-6-4-9",
  "level": "2",
  "url": "s-matrices-of-relations.html#exercises-6-4-9",
  "type": "Exercise",
  "number": "6.4.3.8",
  "title": "",
  "body": "Let be a relation on a set .   Prove that if is a transitive if and only if .  Find an example of a transitive relation for which .     ( ) Assume is transitive and let .   ( ) Assume    If be the transitive relation less than on the integers. Notice that , but and so .   "
},
{
  "id": "exercises-6-4-10",
  "level": "2",
  "url": "s-matrices-of-relations.html#exercises-6-4-10",
  "type": "Exercise",
  "number": "6.4.3.9",
  "title": "",
  "body": "We define on the set of all relation matrices by the rule that if and are any two relation matrices, if and only if for all .   Prove that is a partial ordering on all relation matrices.  Prove that , but the converse is not true.  If and are matrices of equivalence relations and , how are the equivalence classes defined by related to the equivalence classes defined by ?    Reflexive: for all , therefore  Antisymmetric: Assume and for all . Therefore, for all and so  Transitive: Assume and are matrices where and , for all . Then for all , and so .  .  To verify that the converse is not true we need only one example. For , let and all other entries equal , and let be the zero matrix. Since and are both the zero matrix, , but since is false.  The matrices are defined on the same set . Let be the equivalence classes defined by and let be those defined by . Claim: .   "
},
{
  "id": "s-closure-operations-on-relations",
  "level": "1",
  "url": "s-closure-operations-on-relations.html",
  "type": "Section",
  "number": "6.5",
  "title": "Closure Operations on Relations",
  "body": "Closure Operations on Relations   In Section 6.1, we studied relations and one important operation on relations, namely composition. This operation enables us to generate new relations from previously known relations. In Section 6.3, we discussed some key properties of relations. We now wish to consider the situation of constructing a new relation from an existing relation where, first, contains and, second, satisfies the transitive property.   Transitive Closure  Consider a telephone network in which the main office is connected to, and can communicate to, individuals and . Both and can communicate to another person, ; however, the main office cannot communicate with . Assume communication is only one way, as indicated. This situation can be described by the relation . We would like to change the system so that the main office can communicate with person and still maintain the previous system. We, of course, want the most economical system.  This can be rephrased as follows; Find the smallest relation which contains as a subset and which is transitive; .  Transitive Closure Transitive Closure The transitive closure of Let be a set and be a relation on . The transitive closure of , denoted by , is the smallest transitive relation that contains as a subset.  Let , and let be a relation on . This relation is called the successor relation on since each element is related to its successor. How do we compute ? By inspection we note that must be in . Let's analyze why. This is so because and , and the transitive property forces to be in .  In general, it follows that if and then . This condition is exactly the membership requirement for the pair to be in the composition . So every element in must be an element in . So we now know that, contains at least . In particular, for this example, since and , we have   Is the relation transitive? Again, by inspection, is not an element of , but and . Therefore, the composition produces , and it must be an element of since and are required to be in . This shows that . This process must be continued until the resulting relation is transitive. If is finite, as is true in this example, the transitive closure will be obtained in a finite number of steps. For this example,   Transitive Closure on a Finite Set If is a relation on a set and , then the transitive closure of is the union of the first powers of r. That is,  Let's now consider the matrix analogue of the transitive closure.  Consider the relation on the set . The matrix of is  Recall that can be determined through computing the matrix powers . For our example,              How do we relate to the powers of ?  Matrix of a Transitive Closure Let be a relation on a finite set and its matrix. Let be the matrix of , the transitive closure of . Then , using Boolean arithmetic.  Using this theorem, we find is the matrix consisting of all , thus, is all of .   Algorithms for computing transitive closure  Let be a relation on the set with relation matrix . The matrix of the transitive closure , can be computed by the equation . By using ordinary polynomial evaluation methods, you can compute with matrix multiplications:  For example, if , .  We can make use of the fact that if is a relation matrix, due to the fact that in Boolean arithmetic. Let . Then  .  Similarly, and by induction we can prove   Notice how each matrix multiplication doubles the number of terms that have been added to the sum that you currently have computed. In algorithmic form, we can compute as follows.  Transitive Closure Algorithm  Let be a relation matrix and let be its transitive closure matrix, which is to be computed as matrix    1.0. S = R 2.0 T= S*(I+S) 3.0 While T != S 3.1 S = T 3.2 T= S*(I+S) \/\/ using Boolean arithmetic 4.0 Return T        Often the higher-powered terms in do not contribute anything to . When the condition becomes true in Step 3, this is an indication that no higher-powered terms are needed.  To compute using this algorithm, you need to perform no more than matrix multiplications, where is the least integer that is greater than or equal to . For example, if is a relation on 25 elements, no more than matrix multiplications are needed.    A second algorithm, Warshall's Algorithm, reduces computation time to the time that it takes to multiply two square matrices with the same order as the relation matrix in question.  Warshall's Algorithm  Let be an relation matrix and let be its transitive closure matrix, which is to be computed as matrix using Boolean arithmetic    1.0 T = R 2.0 for k = 1 to n: for i = 1 to n: for j = 1 to n: T[i,j]= T[i,j] + T[i,k] * T[k,j] 3.0 Return T       Exercises  Let and . Compute using the matrix representation of . Verify your results by checking against the result obtained directly from the definition of transitive closure.   Let and . Determine by any means. Represent your answer as a matrix.       Draw digraphs of the relations , , , and where is defined in the first exercise above.  Verify that in terms of the graph of , if and only if is reachable from along a path of any finite nonzero length.    See graphs below.  For example, and using one can go from 0 to 4 using a path of length 3.     Digraph of   digraph of S     Digraph of       Digraph of     Digraph of      Let be the relation represented by the following digraph.   Determine based on paths in the give graph.  Verify your result in part (a) by using the .    Digraph of in exercise 4.   Graph for exercise 4 of Section 6.5           Step  ?        1,2  -      3a  No      3b  No      3c  Yes          Define reflexive closure and symmetric closure by imitating the definition of transitive closure.  Use your definitions to compute the reflexive and symmetric closures of examples in the text.  What are the transitive reflexive closures of these examples?  Convince yourself that the reflexive closure of the relation on the set of positive integers is .   Definition: Reflexive Closure. Let be a relation on . The reflexive closure of is the smallest reflexive relation that contains .  Theorem: The reflexive closure of is the union of with    What common relations on are the transitive closures of the following relations?    if and only if .   if and only if .     is the less than relation on the integers.   is congruence mod 2, .       Let be any set and a relation on , prove that .  Is the transitive closure of a symmetric relation always both symmetric and reflexive? Explain.  By the definition of transitive closure, is the smallest relation which contains ; therefore, it is transitive. The transitive closure of , , is the smallest transitive relation that contains . Since is transitive, .  The transitive closure of a symmetric relation is symmetric, but it may not be reflexive. If one element is not related to any elements, then the transitive closure will not relate that element to others.     The definition of the of refers to the smallest transitive relation that contains as a subset. Show that the intersection of all transitive relations on containing is a transitive relation containing and is precisely .  Let be the set of all transitive relations that contain ; and let . We claim that is transitive. If this is verified, it must be since it would be the smallest transitive relation that contains .  Assume . For any , by the definition of and since is transitive, . Therefore    "
},
{
  "id": "def-transitive-closure",
  "level": "2",
  "url": "s-closure-operations-on-relations.html#def-transitive-closure",
  "type": "Definition",
  "number": "6.5.1",
  "title": "Transitive Closure.",
  "body": "Transitive Closure Transitive Closure The transitive closure of Let be a set and be a relation on . The transitive closure of , denoted by , is the smallest transitive relation that contains as a subset. "
},
{
  "id": "theorem-transitive-closure-formula",
  "level": "2",
  "url": "s-closure-operations-on-relations.html#theorem-transitive-closure-formula",
  "type": "Theorem",
  "number": "6.5.2",
  "title": "Transitive Closure on a Finite Set.",
  "body": "Transitive Closure on a Finite Set If is a relation on a set and , then the transitive closure of is the union of the first powers of r. That is, "
},
{
  "id": "s-closure-operations-on-relations-3-12",
  "level": "2",
  "url": "s-closure-operations-on-relations.html#s-closure-operations-on-relations-3-12",
  "type": "Table",
  "number": "6.5.3",
  "title": "",
  "body": "           "
},
{
  "id": "theorem-matrix-transitive-closure",
  "level": "2",
  "url": "s-closure-operations-on-relations.html#theorem-matrix-transitive-closure",
  "type": "Theorem",
  "number": "6.5.4",
  "title": "Matrix of a Transitive Closure.",
  "body": "Matrix of a Transitive Closure Let be a relation on a finite set and its matrix. Let be the matrix of , the transitive closure of . Then , using Boolean arithmetic. "
},
{
  "id": "alg-trans-closure",
  "level": "2",
  "url": "s-closure-operations-on-relations.html#alg-trans-closure",
  "type": "Algorithm",
  "number": "6.5.5",
  "title": "Transitive Closure Algorithm.",
  "body": "Transitive Closure Algorithm  Let be a relation matrix and let be its transitive closure matrix, which is to be computed as matrix    1.0. S = R 2.0 T= S*(I+S) 3.0 While T != S 3.1 S = T 3.2 T= S*(I+S) \/\/ using Boolean arithmetic 4.0 Return T     "
},
{
  "id": "s-closure-operations-on-relations-4-9",
  "level": "2",
  "url": "s-closure-operations-on-relations.html#s-closure-operations-on-relations-4-9",
  "type": "Note",
  "number": "6.5.7",
  "title": "",
  "body": "  Often the higher-powered terms in do not contribute anything to . When the condition becomes true in Step 3, this is an indication that no higher-powered terms are needed.  To compute using this algorithm, you need to perform no more than matrix multiplications, where is the least integer that is greater than or equal to . For example, if is a relation on 25 elements, no more than matrix multiplications are needed.   "
},
{
  "id": "alg-warshall",
  "level": "2",
  "url": "s-closure-operations-on-relations.html#alg-warshall",
  "type": "Algorithm",
  "number": "6.5.8",
  "title": "Warshall’s Algorithm.",
  "body": "Warshall's Algorithm  Let be an relation matrix and let be its transitive closure matrix, which is to be computed as matrix using Boolean arithmetic    1.0 T = R 2.0 for k = 1 to n: for i = 1 to n: for j = 1 to n: T[i,j]= T[i,j] + T[i,k] * T[k,j] 3.0 Return T     "
},
{
  "id": "exercises-6-5-2",
  "level": "2",
  "url": "s-closure-operations-on-relations.html#exercises-6-5-2",
  "type": "Exercise",
  "number": "6.5.3.1",
  "title": "",
  "body": "Let and . Compute using the matrix representation of . Verify your results by checking against the result obtained directly from the definition of transitive closure.  "
},
{
  "id": "exercises-6-5-3",
  "level": "2",
  "url": "s-closure-operations-on-relations.html#exercises-6-5-3",
  "type": "Exercise",
  "number": "6.5.3.2",
  "title": "",
  "body": "Let and . Determine by any means. Represent your answer as a matrix.    "
},
{
  "id": "exercises-6-5-4",
  "level": "2",
  "url": "s-closure-operations-on-relations.html#exercises-6-5-4",
  "type": "Exercise",
  "number": "6.5.3.3",
  "title": "",
  "body": "  Draw digraphs of the relations , , , and where is defined in the first exercise above.  Verify that in terms of the graph of , if and only if is reachable from along a path of any finite nonzero length.    See graphs below.  For example, and using one can go from 0 to 4 using a path of length 3.     Digraph of   digraph of S     Digraph of       Digraph of     Digraph of     "
},
{
  "id": "exercises-6-5-5",
  "level": "2",
  "url": "s-closure-operations-on-relations.html#exercises-6-5-5",
  "type": "Exercise",
  "number": "6.5.3.4",
  "title": "",
  "body": "Let be the relation represented by the following digraph.   Determine based on paths in the give graph.  Verify your result in part (a) by using the .    Digraph of in exercise 4.   Graph for exercise 4 of Section 6.5           Step  ?        1,2  -      3a  No      3b  No      3c  Yes       "
},
{
  "id": "exercises-6-5-6",
  "level": "2",
  "url": "s-closure-operations-on-relations.html#exercises-6-5-6",
  "type": "Exercise",
  "number": "6.5.3.5",
  "title": "",
  "body": "  Define reflexive closure and symmetric closure by imitating the definition of transitive closure.  Use your definitions to compute the reflexive and symmetric closures of examples in the text.  What are the transitive reflexive closures of these examples?  Convince yourself that the reflexive closure of the relation on the set of positive integers is .   Definition: Reflexive Closure. Let be a relation on . The reflexive closure of is the smallest reflexive relation that contains .  Theorem: The reflexive closure of is the union of with   "
},
{
  "id": "exercises-6-5-7",
  "level": "2",
  "url": "s-closure-operations-on-relations.html#exercises-6-5-7",
  "type": "Exercise",
  "number": "6.5.3.6",
  "title": "",
  "body": "What common relations on are the transitive closures of the following relations?    if and only if .   if and only if .     is the less than relation on the integers.   is congruence mod 2, .    "
},
{
  "id": "exercises-6-5-8",
  "level": "2",
  "url": "s-closure-operations-on-relations.html#exercises-6-5-8",
  "type": "Exercise",
  "number": "6.5.3.7",
  "title": "",
  "body": "  Let be any set and a relation on , prove that .  Is the transitive closure of a symmetric relation always both symmetric and reflexive? Explain.  By the definition of transitive closure, is the smallest relation which contains ; therefore, it is transitive. The transitive closure of , , is the smallest transitive relation that contains . Since is transitive, .  The transitive closure of a symmetric relation is symmetric, but it may not be reflexive. If one element is not related to any elements, then the transitive closure will not relate that element to others.   "
},
{
  "id": "exercises-6-5-9",
  "level": "2",
  "url": "s-closure-operations-on-relations.html#exercises-6-5-9",
  "type": "Exercise",
  "number": "6.5.3.8",
  "title": "",
  "body": " The definition of the of refers to the smallest transitive relation that contains as a subset. Show that the intersection of all transitive relations on containing is a transitive relation containing and is precisely .  Let be the set of all transitive relations that contain ; and let . We claim that is transitive. If this is verified, it must be since it would be the smallest transitive relation that contains .  Assume . For any , by the definition of and since is transitive, . Therefore  "
},
{
  "id": "s-function-def-notation",
  "level": "1",
  "url": "s-function-def-notation.html",
  "type": "Section",
  "number": "7.1",
  "title": "Definition and Notation",
  "body": "Definition and Notation  Fundamentals  Function Function A function, , from into A function from a set into a set is a relation from into such that each element of is related to exactly one element of the set . The set is called the domain of the function and the set is called the codomain .   The reader should note that a function is a relation from into with two important restrictions:   Each element in the set , the domain of , must be related to some element of , the codomain.  The phrase is related to exactly one element of the set means that if and , then .   A function as a list of ordered pairs  Let and , and if , then is a function from into .  Array Representation of Functions  For relatively small domains, a convenient way to define a function if there is no simple formula that will do it, is to use a two line array with the first line listing all of the domain elements. For each domain element, it's image is placed below it in the second line. If is is a function with domain then we could write   For example, the function in could be defined as   A function as a set of ordered pairs in set-builder notation Let be the real numbers. Then is a function from into , or, more simply, is a function on .  It is customary to use a different system of notation for functions than the one we used for relations. If is a function from the set into the set , we will write .  The reader is probably more familiar with the notation for describing functions that is used in basic algebra or calculus courses. For example, or both define the function . Here the domain was assumed to be those elements of whose substitutions for make sense, the nonzero real numbers, and the codomain was assumed to be . In most cases, we will make a point of listing the domain and codomain in addition to describing what the function does in order to define a function.  The terms mapping , map , and transformation are also used for functions.  The Set of Functions Between Two Sets  Set of Functions Between Two Sets Functions Between Two Sets Set of  The set of all functions from into Given two sets, and , the set of all functions from into is denoted .  The notation used for sets of functions makes sense in light of .  One way to imagine a function and what it does is to think of it as a machine. The machine could be mechanical, electronic, hydraulic, or abstract. Imagine that the machine only accepts certain objects as raw materials or input. The possible raw materials make up the domain. Given some input, the machine produces a finished product that depends on the input. The possible finished products that we imagine could come out of this process make up the codomain.  A definition based on images We can define a function based on specifying the codomain element to which each domain element is related. For example, defined by is an alternate description of .  Image of an element under a function Image of an Element. The image of under Let , read Let be a function from the set into the set . If , then is used to denote that element of to which is related. is called the image of , or, more precisely, the image of under . We write to indicate that the image of is .  In , the image of 2 under is 4; that is, . In , the image of under is 1; that is, .  Range of a Function Range of a Function. Range of function The range of a function is the set of images of its domain. If , then the range of is denoted , and .   Note that the range of a function is a subset of its codomain. is also read as the image of the set under the function or simply the image of .  In , . Notice that 2 and 3 are not images of any element of . In addition, note that both 1 and 4 are related to more than one element of the domain: and . This does not violate the definition of a function. Go back and read the definition if this isn't clear to you.  In , the range of is equal to its codomain, . If is any real number, we can demonstrate that it belongs to by finding a real number for which . By the definition of , , which leads us to the equation . This equation always has a solution, ; thus .  The formula that we used to describe the image of a real number under , , is preferred over the set notation for due to its brevity. Any time a function can be described with a rule or formula, we will use this form of description. In , the image of each element of is its square. To describe that fact, we write ( ), or defined by .  There are many ways that a function can be described. Many factors, such as the complexity of the function, dictate its representation.  Data as a function Suppose a survey of 1,000 persons is done asking how many hours of television each watches per day. Consider the function defined by This function will probably have no formula such as the ones for and above.  Conditional definition of a function Consider the function defined by the set No simple single formula could describe , but if we assume that the pattern given continues, we can write    Functions of Two Variables Functions Of two Variables  If the domain of a function is the Cartesian product of two sets, then our notation and terminology changes slightly. For example, consider the function defined by . For this function, we would drop one set of parentheses and write , not . We call a function of two variables. From one point of view, this function is no different from any others that we have seen. The elements of the domain happen to be slightly more complicated. On the other hand, we can look at the individual components of the ordered pairs as being separate. If we interpret as giving us the cost of producing quantities of two products, we can imagine varying while is fixed, or vice versa.  The same observations can be made for function of three or more variables.   SageMath Note SageMath Note Functions  There are several ways to define a function in Sage. The simplest way to implement is as follows.    Sage is built upon the programming language Python, which is a strongly typed language and so you can't evaluate expressions such as f('Hello') . However a function such as , as defined above, will accept any type of number, so a bit more work is needed to restrict the inputs of to the integers.  A second way to define a function in Sage is based on Python syntax.    Non-Functions  We close this section with two examples of relations that are not functions.  A non-function Let and let . Here is not a function from into because \\(3\\) is not related to anything in the codomain. In other words, does not act on, or use, all elements of .  Another non-function Let and let . We note that acts on all of . However, is still not a function since and and the condition on each domain element being related to exactly one element of the codomain is violated.    Exercises  Let and . Determine which of the following are functions. Explain.    , where .   , where .   , where .   , where .   , where .     Yes  Yes  No  No  Yes    Find the ranges of the following functions on :    .   .   .     Range of .  Range of .  Range of .    Find the ranges of each of the relations that are functions in Exercise 1.    Range of  Range of  is not a function.  is not a function.  Range of    Characteristic function of the set Let be a set and let be any subset of . Let be defined by   The function is called the characteristic function Characteristic function of .   If and , list the elements of .  If and , list the elements of .  If , what are and ?      .   .   and ?     If and are finite sets, how many different functions are there from into ?  For each of the elements of , there are possible images, so there are functions from into .   Let be a set with subsets and .   Show that defined by is the characteristic function of .  What characteristic function is defined by ?  How are the characteristic functions of and related?      and Therefore,      .    kernel of a function Let be a function with domain and codomain . Consider the relation defined on the domain of by if and only if . The relation is called the kernel of .   Prove that is an equivalence relation.  For the specific case of , where is the set of integers, let be defined by . Describe the equivalence classes of the kernel for this specific function.    Let , where is the largest power of two that evenly divides ; for example, . Describe the equivalence classes of the kernel of .  There is an equivalence class for nonnegative integer . The class for is the set of odd multiples of ,    "
},
{
  "id": "def-function",
  "level": "2",
  "url": "s-function-def-notation.html#def-function",
  "type": "Definition",
  "number": "7.1.1",
  "title": "Function.",
  "body": "Function Function A function, , from into A function from a set into a set is a relation from into such that each element of is related to exactly one element of the set . The set is called the domain of the function and the set is called the codomain .  "
},
{
  "id": "ex-function-1",
  "level": "2",
  "url": "s-function-def-notation.html#ex-function-1",
  "type": "Example",
  "number": "7.1.2",
  "title": "A function as a list of ordered pairs.",
  "body": "A function as a list of ordered pairs  Let and , and if , then is a function from into . "
},
{
  "id": "ss-fundamentals-functions-6",
  "level": "2",
  "url": "s-function-def-notation.html#ss-fundamentals-functions-6",
  "type": "Note",
  "number": "7.1.3",
  "title": "Array Representation of Functions.",
  "body": "Array Representation of Functions  For relatively small domains, a convenient way to define a function if there is no simple formula that will do it, is to use a two line array with the first line listing all of the domain elements. For each domain element, it's image is placed below it in the second line. If is is a function with domain then we could write   For example, the function in could be defined as  "
},
{
  "id": "ex-function-2",
  "level": "2",
  "url": "s-function-def-notation.html#ex-function-2",
  "type": "Example",
  "number": "7.1.4",
  "title": "A function as a set of ordered pairs in set-builder notation.",
  "body": "A function as a set of ordered pairs in set-builder notation Let be the real numbers. Then is a function from into , or, more simply, is a function on . "
},
{
  "id": "ss-fundamentals-functions-10",
  "level": "2",
  "url": "s-function-def-notation.html#ss-fundamentals-functions-10",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "mapping map transformation "
},
{
  "id": "def-set-of-functions",
  "level": "2",
  "url": "s-function-def-notation.html#def-set-of-functions",
  "type": "Definition",
  "number": "7.1.5",
  "title": "The Set of Functions Between Two Sets.",
  "body": "The Set of Functions Between Two Sets  Set of Functions Between Two Sets Functions Between Two Sets Set of  The set of all functions from into Given two sets, and , the set of all functions from into is denoted . "
},
{
  "id": "ex-non-pair-description",
  "level": "2",
  "url": "s-function-def-notation.html#ex-non-pair-description",
  "type": "Example",
  "number": "7.1.6",
  "title": "A definition based on images.",
  "body": "A definition based on images We can define a function based on specifying the codomain element to which each domain element is related. For example, defined by is an alternate description of . "
},
{
  "id": "def-image-of-an-element",
  "level": "2",
  "url": "s-function-def-notation.html#def-image-of-an-element",
  "type": "Definition",
  "number": "7.1.7",
  "title": "Image of an element under a function.",
  "body": "Image of an element under a function Image of an Element. The image of under Let , read Let be a function from the set into the set . If , then is used to denote that element of to which is related. is called the image of , or, more precisely, the image of under . We write to indicate that the image of is . "
},
{
  "id": "def-range-of-function",
  "level": "2",
  "url": "s-function-def-notation.html#def-range-of-function",
  "type": "Definition",
  "number": "7.1.8",
  "title": "Range of a Function.",
  "body": "Range of a Function Range of a Function. Range of function The range of a function is the set of images of its domain. If , then the range of is denoted , and .  "
},
{
  "id": "ex-data-function",
  "level": "2",
  "url": "s-function-def-notation.html#ex-data-function",
  "type": "Example",
  "number": "7.1.9",
  "title": "Data as a function.",
  "body": "Data as a function Suppose a survey of 1,000 persons is done asking how many hours of television each watches per day. Consider the function defined by This function will probably have no formula such as the ones for and above. "
},
{
  "id": "ex-conditional-function",
  "level": "2",
  "url": "s-function-def-notation.html#ex-conditional-function",
  "type": "Example",
  "number": "7.1.10",
  "title": "Conditional definition of a function.",
  "body": "Conditional definition of a function Consider the function defined by the set No simple single formula could describe , but if we assume that the pattern given continues, we can write  "
},
{
  "id": "ex-nonfunction1",
  "level": "2",
  "url": "s-function-def-notation.html#ex-nonfunction1",
  "type": "Example",
  "number": "7.1.11",
  "title": "A non-function.",
  "body": "A non-function Let and let . Here is not a function from into because \\(3\\) is not related to anything in the codomain. In other words, does not act on, or use, all elements of . "
},
{
  "id": "ex-nonfunction2",
  "level": "2",
  "url": "s-function-def-notation.html#ex-nonfunction2",
  "type": "Example",
  "number": "7.1.12",
  "title": "Another non-function.",
  "body": "Another non-function Let and let . We note that acts on all of . However, is still not a function since and and the condition on each domain element being related to exactly one element of the codomain is violated. "
},
{
  "id": "exercise-7-1-1",
  "level": "2",
  "url": "s-function-def-notation.html#exercise-7-1-1",
  "type": "Exercise",
  "number": "7.1.5.1",
  "title": "",
  "body": "Let and . Determine which of the following are functions. Explain.    , where .   , where .   , where .   , where .   , where .     Yes  Yes  No  No  Yes   "
},
{
  "id": "exercises-7-1-3",
  "level": "2",
  "url": "s-function-def-notation.html#exercises-7-1-3",
  "type": "Exercise",
  "number": "7.1.5.2",
  "title": "",
  "body": "Find the ranges of the following functions on :    .   .   .     Range of .  Range of .  Range of .   "
},
{
  "id": "exercises-7-1-4",
  "level": "2",
  "url": "s-function-def-notation.html#exercises-7-1-4",
  "type": "Exercise",
  "number": "7.1.5.3",
  "title": "",
  "body": "Find the ranges of each of the relations that are functions in Exercise 1.    Range of  Range of  is not a function.  is not a function.  Range of   "
},
{
  "id": "exercise-characteristic-function",
  "level": "2",
  "url": "s-function-def-notation.html#exercise-characteristic-function",
  "type": "Exercise",
  "number": "7.1.5.4",
  "title": "",
  "body": "Characteristic function of the set Let be a set and let be any subset of . Let be defined by   The function is called the characteristic function Characteristic function of .   If and , list the elements of .  If and , list the elements of .  If , what are and ?      .   .   and ?   "
},
{
  "id": "exercise-counting-functions",
  "level": "2",
  "url": "s-function-def-notation.html#exercise-counting-functions",
  "type": "Exercise",
  "number": "7.1.5.5",
  "title": "",
  "body": " If and are finite sets, how many different functions are there from into ?  For each of the elements of , there are possible images, so there are functions from into .  "
},
{
  "id": "exercises-7-1-7",
  "level": "2",
  "url": "s-function-def-notation.html#exercises-7-1-7",
  "type": "Exercise",
  "number": "7.1.5.6",
  "title": "",
  "body": "Let be a set with subsets and .   Show that defined by is the characteristic function of .  What characteristic function is defined by ?  How are the characteristic functions of and related?      and Therefore,      .   "
},
{
  "id": "exercises-7-1-8",
  "level": "2",
  "url": "s-function-def-notation.html#exercises-7-1-8",
  "type": "Exercise",
  "number": "7.1.5.7",
  "title": "",
  "body": "kernel of a function Let be a function with domain and codomain . Consider the relation defined on the domain of by if and only if . The relation is called the kernel of .   Prove that is an equivalence relation.  For the specific case of , where is the set of integers, let be defined by . Describe the equivalence classes of the kernel for this specific function.   "
},
{
  "id": "exercises-7-1-9",
  "level": "2",
  "url": "s-function-def-notation.html#exercises-7-1-9",
  "type": "Exercise",
  "number": "7.1.5.8",
  "title": "",
  "body": "Let , where is the largest power of two that evenly divides ; for example, . Describe the equivalence classes of the kernel of .  There is an equivalence class for nonnegative integer . The class for is the set of odd multiples of ,  "
},
{
  "id": "s-properties-of-functions",
  "level": "1",
  "url": "s-properties-of-functions.html",
  "type": "Section",
  "number": "7.2",
  "title": "Properties of Functions",
  "body": "Properties of Functions Properties of Functions  Properties  Consider the following functions:  Let and , and define by   Let and , and define by   The first function, , gives us more information about the set than the second function, . Since clearly has four elements, tells us that contains at least four elements since each element of is mapped onto a different element of . The properties that has, and does not have, are the most basic properties that we look for in a function. The following definitions summarize the basic vocabulary for function properties.  Injective Function, Injection Function Injective Function One-to-one Injection  A function is injective if An injective function is called an injection, or a one-to-one function.  Notice that the condition for an injective function is logically equivalent to . for all . This is often a more convenient condition to prove than what is given in the definition.  Surjective Function, Surjection Function Surjective Function Onto Surjection  A function is surjective if its range, , is equal to its codomain, . A surjective function is called a surjection, or an onto function.  Notice that the condition for a surjective function is equivalent to .   Bijective Function, Bijection  Function Bijective  Bijection  A function is bijective if it is both injective and surjective. Bijective functions are also called one-to-one, onto functions.  The function that we opened this section with is bijective. The function is neither injective nor surjective.  Injective but not surjective function Let and , and define by , , and . Then is injective but not surjective.  Characteristic Functions The characteristic function, , in is surjective if is a proper subset of , but never injective if .   Counting  Seating Students Let be the set of students who are sitting in a classroom, let be the set of seats in the classroom, and let be the function which maps each student into the chair he or she is sitting in. When is one to one? When is it onto? Under normal circumstances, would always be injective since no two different students would be in the same seat. In order for to be surjective, we need all seats to be used, so is a surjection if the classroom is filled to capacity.   Functions can also be used for counting the elements in large finite sets or in infinite sets. Let's say we wished to count the occupants in an auditorium containing 1,500 seats. If each seat is occupied, the answer is obvious, 1,500 people. What we have done is to set up a one-to-one correspondence, or bijection, from seats to people. We formalize in a definition.  Cardinality Cardinality. has cardinality Two sets are said to have the same cardinality if there exists a bijection between them. If a set has the same cardinality as the set , then we say its cardinality is .  The function that opened this section serves to show that the two sets and have the same cardinality. Notice in applying the definition of cardinality, we don't actually appear to count either set, we just match up the elements. However, matching the letters in with the numbers 1, 2, 3, and 4 is precisely how we count the letters.  Countable Set Countable Set If a set is finite or has the same cardinality as the set of positive integers, it is called a countable set.  Counting the Alphabet The alphabet has cardinality 26 through the following bijection into the set . .  As many evens as all positive integers Recall that . Paradoxically, has the same cardinality as the set of positive integers. To prove this, we must find a bijection from to . Such a function isn't unique, but this one is the simplest: where . Two statements must be proven to justify our claim that is a bijection:    is one-to-one.  Proof: Let and assume that . We must prove that .   is onto.  Proof: Let . We want to show that there exists an element such that . If , for some by the definition of . So we have . Hence, each element of 2 is the image of some element of .    Another way to look at any function with as its domain is creating a list of the form . In the previous example, the list is . This infinite list clearly has no duplicate entries and every even positive integer appears in the list eventually.  A function is a bijection if the infinite list contains no duplicates, and every element of appears on in the list. In this case, we say the is countably infinite , or simply countable .  A First Paradox of Infinity When studying infinity, paradoxes abound. One of the first instances of this is when we observe that the set of even positive integers, in spite of the fact that they make up only half of the positive integers, has the same cardinality as the whole set of positive integers. This follows from our definition of cardinality with the function , which is a bijection from the positive integers to the even positive integers. We can make a similar observation that the seemingly smaller set of powers of , , also has the same cardinality as the positive integer. Here, the function serves as our justification.  Going in the opposite direction, there are seemingly larger sets than the positive integer that are countably infinite. One such example is the Cartesian product of the positive integers with itself, . A function that justifies this claim doesn't have such a neat formula, but it would start like this:               See the pattern? If it continues, every positive integer will map to a different pair and every pair of positive integer will be in the range of .   Readers who have studied real analysis should recall that the set of rational numbers is a countable set, while the set of real numbers is not a countable set. See the exercises at the end of this section for an another example of such a set.  We close this section with a theorem called the Pigeonhole Principle, which has numerous applications even though it is an obvious, common-sense statement. Never underestimate the importance of simple ideas. The Pigeonhole Principle states that if there are more pigeons than pigeonholes, then two or more pigeons must share the same pigeonhole. A more rigorous mathematical statement of the principle follows.  The Pigeonhole Principle Pigeonhole Principle Let be a function from a finite set into a finite set . If and , then there exists an element of that is the image under of at least elements of X.  Assume no such element exists. For each , let . Then it must be that . Furthermore, the set of nonempty form a partition of . Therefore, which is a contradiction.   A duplicate name is assured Assume that a room contains four students with the first names John, James, and Mary. Prove that two students have the same first name. We can visualize a mapping from the set of students to the set of first names; each student has a first name. The pigeonhole principle applies with , and we can conclude that at least two of the students have the same first name.     Exercises  Determine which of the functions in of Section 7.1 are one- to-one and which are onto.  The only one-to-one function and the only onto function is .     Determine all bijections from into .  Determine all bijections from into .                 There are no bijections since the range of any function on a domain having three elements cannot have more than three elements, so it can't be onto.    Which of the following are one-to-one, onto, or both?    defined by .   defined by .   defined by .   defined by , where is the ceiling of , the smallest integer greater than or equal to .   defined by .  defined by .    is onto but not one-to-one: .  is one-to-one and onto.  is one-to-one but not onto.  is onto but not one-to-one.  is one-to-one but not onto.  is one-to-one but not onto.    Which of the following are injections, surjections, or bijections on , the set of real numbers?    .   .               is a bijection.   is neither an injection nor a surjection.    is a bijection.   is an injection, but not a surjection.   is a bijection.   is a surjection but not and injection.    Suppose that pairs of socks are mixed up in your sock drawer. Use the Pigeonhole Principle to explain why, if you pick socks at random, at least two will make up a matching pair.  Let and and define where the pair of socks that belongs to . By the Pigeonhole principle, there exist two socks that were selected from the same pair.   Given five points on the unit square, , prove that there are two of the points a distance of no more than from one another.  Create four pigeonholes by drawing a vertical line and a horizontal line, both through the center of the square. You can assign the boundaries between these small squares any way to care to. Now just observe that among the five points, there must be at least one square containing two points. Their maximium distance is    Let . Find functions, if they exist that have the properties specified below.   A function that is one-to-one and onto.  A function that is neither one-to-one nor onto.  A function that is one-to-one but not onto.  A function that is onto but not one-to-one.    , for example   , for example  None exist.  None exist.     Define functions, if they exist, on the positive integers, , with the same properties as in Exercise 7 (if possible).  Let and be finite sets where . Is it possible to define a function that is one-to-one but not onto? Is it possible to find a function that is onto but not one-to-one?     There are an infinite number of correct answers here, but we will list some of the simplest ones.  , defined by is one to one and onto.  , defined by is one to one but not onto.  , defined by is onto but not one to one.  , defined by is neither one to one nor onto.   It is not possible to define a function that is one-to-one but not onto; nor is it possible to find a function that is onto but not one-to-one. The two properties imply one another on finite sets of the same cardinality.     Prove that the set of natural numbers is countable.  Prove that the set of integers is countable.  Prove that the set of rational numbers is countable.   Use defined by .  Use the function defined by if is even and if is odd.  The proof is due to Georg Cantor (1845-1918), and involves listing the rationals through a definite procedure so that none are omitted and duplications are avoided. In the first row list all nonnegative rationals with denominator 1, in the second all nonnegative rationals with denominator 2, etc. In this listing, of course, there are duplications, for example, , , , etc. To obtain a list without duplications follow the arrows in , listing only the circled numbers.  We obtain: Each nonnegative rational appears in this list exactly once. We now must insert in this list the negative rationals, and follow the same scheme to obtain: which can be paired off with the elements of .     Enumeration of the rational numbers.       Prove that the set of finite strings of 0's and 1's is countable.  Prove that the set of odd integers is countable.  Prove that the set is countable.  Prove that the set is countable.    For each part, we provide a bijection with domain that proves that that each set is countable.  If , map it to the string Note that will map to the empty string, .  Map each odd positive integer to itself and any even positive integer, to  Similar to the function that proves that the rationals are countable, we follow paths diagonally from bottom to left side of an array of ordered pairs, as indicated in . A formula for this function isn't so simple, but clearly, it exists and every ordered pair is is that image of some positive integer.   If is the function in the previous part of this exercise that maps the positive integers to , we list the images under of each positive integer on the axis of the array in . Each point in the array can be thought of as a triple with first coordinate the value and the second and third coordinates the coordinates of the value. We follow the same kind of paths in succession as the previous part the is exercise to list all triples.      Enumeration of the set    Visual describing why is countably infinite     Enumeration of the set    Visual describing why is countably infinite.      Use the Pigeonhole Principle to prove that an injection cannot exist between a finite set and a finite set if the cardinality of is greater than the cardinality of .  Let be any function from into . By the Pigeonhole principle with , there exists an element of that is the image of at least two elements of . Therefore, is not an injection.  The important properties of relations are not generally of interest for functions. Most functions are not reflexive, symmetric, antisymmetric, or transitive. Can you give examples of functions that do have these properties?   Here are a few examples. Given any domain , the only function that is reflexive is the function , defined by for all . This is the , which is formally introduced in the next section. This function has the other three properties, and is the equality relation on . There are other functions have some of the individual properties. For example, on the integer, the function defined by is symmetric. The function defined by is antisymmetric. A constant function such as the function that maps each integer to zero is transitive, but not very interesting as far as transitivity is concerned..   Prove that the set of all infinite sequences of 0's and 1's is not a countable set.  The proof is indirect and follows a technique called the Cantor diagonal process. Assume to the contrary that the set is countable, then the elements can be listed: where each is an infinite sequence of 0s and 1s. Consider the array:   We assume that this array contains all infinite sequences of 0s and 1s. Consider the sequence defined by   Notice that differs from each in the th position and so cannot be in the list. This is a contradiction, which completes our proof.   Prove that the set of all functions on the integers is an uncountable set.   We prove that the set is uncountable by contradiction. Assume there is a bijection from the positive integers into the set of all functions on the integers. We can define a function that is not in the range of , contradicting our assumption that is a bijection. Our definition is . Notice that because they map to different values.    "
},
{
  "id": "def-injective-function",
  "level": "2",
  "url": "s-properties-of-functions.html#def-injective-function",
  "type": "Definition",
  "number": "7.2.1",
  "title": "Injective Function, Injection.",
  "body": "Injective Function, Injection Function Injective Function One-to-one Injection  A function is injective if An injective function is called an injection, or a one-to-one function. "
},
{
  "id": "def-surjective-function",
  "level": "2",
  "url": "s-properties-of-functions.html#def-surjective-function",
  "type": "Definition",
  "number": "7.2.2",
  "title": "Surjective Function, Surjection.",
  "body": "Surjective Function, Surjection Function Surjective Function Onto Surjection  A function is surjective if its range, , is equal to its codomain, . A surjective function is called a surjection, or an onto function. "
},
{
  "id": "def-bijective-function",
  "level": "2",
  "url": "s-properties-of-functions.html#def-bijective-function",
  "type": "Definition",
  "number": "7.2.3",
  "title": "Bijective Function, Bijection.",
  "body": " Bijective Function, Bijection  Function Bijective  Bijection  A function is bijective if it is both injective and surjective. Bijective functions are also called one-to-one, onto functions. "
},
{
  "id": "ex-injective-notsurjective",
  "level": "2",
  "url": "s-properties-of-functions.html#ex-injective-notsurjective",
  "type": "Example",
  "number": "7.2.4",
  "title": "Injective but not surjective function.",
  "body": "Injective but not surjective function Let and , and define by , , and . Then is injective but not surjective. "
},
{
  "id": "ex-characteristic-properties",
  "level": "2",
  "url": "s-properties-of-functions.html#ex-characteristic-properties",
  "type": "Example",
  "number": "7.2.5",
  "title": "Characteristic Functions.",
  "body": "Characteristic Functions The characteristic function, , in is surjective if is a proper subset of , but never injective if . "
},
{
  "id": "ex-classroom",
  "level": "2",
  "url": "s-properties-of-functions.html#ex-classroom",
  "type": "Example",
  "number": "7.2.6",
  "title": "Seating Students.",
  "body": "Seating Students Let be the set of students who are sitting in a classroom, let be the set of seats in the classroom, and let be the function which maps each student into the chair he or she is sitting in. When is one to one? When is it onto? Under normal circumstances, would always be injective since no two different students would be in the same seat. In order for to be surjective, we need all seats to be used, so is a surjection if the classroom is filled to capacity.  "
},
{
  "id": "def-cardinality",
  "level": "2",
  "url": "s-properties-of-functions.html#def-cardinality",
  "type": "Definition",
  "number": "7.2.7",
  "title": "Cardinality.",
  "body": "Cardinality Cardinality. has cardinality Two sets are said to have the same cardinality if there exists a bijection between them. If a set has the same cardinality as the set , then we say its cardinality is . "
},
{
  "id": "def-countable-set",
  "level": "2",
  "url": "s-properties-of-functions.html#def-countable-set",
  "type": "Definition",
  "number": "7.2.8",
  "title": "Countable Set.",
  "body": "Countable Set Countable Set If a set is finite or has the same cardinality as the set of positive integers, it is called a countable set. "
},
{
  "id": "ex-the-alphabet",
  "level": "2",
  "url": "s-properties-of-functions.html#ex-the-alphabet",
  "type": "Example",
  "number": "7.2.9",
  "title": "Counting the Alphabet.",
  "body": "Counting the Alphabet The alphabet has cardinality 26 through the following bijection into the set . . "
},
{
  "id": "ex-evens-eq-all",
  "level": "2",
  "url": "s-properties-of-functions.html#ex-evens-eq-all",
  "type": "Example",
  "number": "7.2.10",
  "title": "As many evens as all positive integers.",
  "body": "As many evens as all positive integers Recall that . Paradoxically, has the same cardinality as the set of positive integers. To prove this, we must find a bijection from to . Such a function isn't unique, but this one is the simplest: where . Two statements must be proven to justify our claim that is a bijection:    is one-to-one.  Proof: Let and assume that . We must prove that .   is onto.  Proof: Let . We want to show that there exists an element such that . If , for some by the definition of . So we have . Hence, each element of 2 is the image of some element of .   "
},
{
  "id": "s-properties-of-functions-4-10",
  "level": "2",
  "url": "s-properties-of-functions.html#s-properties-of-functions-4-10",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "countably infinite countable "
},
{
  "id": "ex-infinity-paradox-1",
  "level": "2",
  "url": "s-properties-of-functions.html#ex-infinity-paradox-1",
  "type": "Example",
  "number": "7.2.11",
  "title": "A First Paradox of Infinity.",
  "body": "A First Paradox of Infinity When studying infinity, paradoxes abound. One of the first instances of this is when we observe that the set of even positive integers, in spite of the fact that they make up only half of the positive integers, has the same cardinality as the whole set of positive integers. This follows from our definition of cardinality with the function , which is a bijection from the positive integers to the even positive integers. We can make a similar observation that the seemingly smaller set of powers of , , also has the same cardinality as the positive integer. Here, the function serves as our justification.  Going in the opposite direction, there are seemingly larger sets than the positive integer that are countably infinite. One such example is the Cartesian product of the positive integers with itself, . A function that justifies this claim doesn't have such a neat formula, but it would start like this:               See the pattern? If it continues, every positive integer will map to a different pair and every pair of positive integer will be in the range of .  "
},
{
  "id": "th-pigeonhole-principle",
  "level": "2",
  "url": "s-properties-of-functions.html#th-pigeonhole-principle",
  "type": "Theorem",
  "number": "7.2.13",
  "title": "The Pigeonhole Principle.",
  "body": "The Pigeonhole Principle Pigeonhole Principle Let be a function from a finite set into a finite set . If and , then there exists an element of that is the image under of at least elements of X.  Assume no such element exists. For each , let . Then it must be that . Furthermore, the set of nonempty form a partition of . Therefore, which is a contradiction.  "
},
{
  "id": "ex-names",
  "level": "2",
  "url": "s-properties-of-functions.html#ex-names",
  "type": "Example",
  "number": "7.2.14",
  "title": "A duplicate name is assured.",
  "body": "A duplicate name is assured Assume that a room contains four students with the first names John, James, and Mary. Prove that two students have the same first name. We can visualize a mapping from the set of students to the set of first names; each student has a first name. The pigeonhole principle applies with , and we can conclude that at least two of the students have the same first name.  "
},
{
  "id": "exercises-7-2-2",
  "level": "2",
  "url": "s-properties-of-functions.html#exercises-7-2-2",
  "type": "Exercise",
  "number": "7.2.3.1",
  "title": "",
  "body": "Determine which of the functions in of Section 7.1 are one- to-one and which are onto.  The only one-to-one function and the only onto function is .  "
},
{
  "id": "exercises-7-2-3",
  "level": "2",
  "url": "s-properties-of-functions.html#exercises-7-2-3",
  "type": "Exercise",
  "number": "7.2.3.2",
  "title": "",
  "body": "  Determine all bijections from into .  Determine all bijections from into .                 There are no bijections since the range of any function on a domain having three elements cannot have more than three elements, so it can't be onto.   "
},
{
  "id": "exercises-7-2-4",
  "level": "2",
  "url": "s-properties-of-functions.html#exercises-7-2-4",
  "type": "Exercise",
  "number": "7.2.3.3",
  "title": "",
  "body": "Which of the following are one-to-one, onto, or both?    defined by .   defined by .   defined by .   defined by , where is the ceiling of , the smallest integer greater than or equal to .   defined by .  defined by .    is onto but not one-to-one: .  is one-to-one and onto.  is one-to-one but not onto.  is onto but not one-to-one.  is one-to-one but not onto.  is one-to-one but not onto.   "
},
{
  "id": "exercises-7-2-5",
  "level": "2",
  "url": "s-properties-of-functions.html#exercises-7-2-5",
  "type": "Exercise",
  "number": "7.2.3.4",
  "title": "",
  "body": "Which of the following are injections, surjections, or bijections on , the set of real numbers?    .   .               is a bijection.   is neither an injection nor a surjection.    is a bijection.   is an injection, but not a surjection.   is a bijection.   is a surjection but not and injection.   "
},
{
  "id": "exercises-7-2-6",
  "level": "2",
  "url": "s-properties-of-functions.html#exercises-7-2-6",
  "type": "Exercise",
  "number": "7.2.3.5",
  "title": "",
  "body": "Suppose that pairs of socks are mixed up in your sock drawer. Use the Pigeonhole Principle to explain why, if you pick socks at random, at least two will make up a matching pair.  Let and and define where the pair of socks that belongs to . By the Pigeonhole principle, there exist two socks that were selected from the same pair.  "
},
{
  "id": "exercises-7-2-7",
  "level": "2",
  "url": "s-properties-of-functions.html#exercises-7-2-7",
  "type": "Exercise",
  "number": "7.2.3.6",
  "title": "",
  "body": "Given five points on the unit square, , prove that there are two of the points a distance of no more than from one another.  Create four pigeonholes by drawing a vertical line and a horizontal line, both through the center of the square. You can assign the boundaries between these small squares any way to care to. Now just observe that among the five points, there must be at least one square containing two points. Their maximium distance is   "
},
{
  "id": "exercises-7-2-8",
  "level": "2",
  "url": "s-properties-of-functions.html#exercises-7-2-8",
  "type": "Exercise",
  "number": "7.2.3.7",
  "title": "",
  "body": "Let . Find functions, if they exist that have the properties specified below.   A function that is one-to-one and onto.  A function that is neither one-to-one nor onto.  A function that is one-to-one but not onto.  A function that is onto but not one-to-one.    , for example   , for example  None exist.  None exist.   "
},
{
  "id": "exercises-7-2-9",
  "level": "2",
  "url": "s-properties-of-functions.html#exercises-7-2-9",
  "type": "Exercise",
  "number": "7.2.3.8",
  "title": "",
  "body": " Define functions, if they exist, on the positive integers, , with the same properties as in Exercise 7 (if possible).  Let and be finite sets where . Is it possible to define a function that is one-to-one but not onto? Is it possible to find a function that is onto but not one-to-one?     There are an infinite number of correct answers here, but we will list some of the simplest ones.  , defined by is one to one and onto.  , defined by is one to one but not onto.  , defined by is onto but not one to one.  , defined by is neither one to one nor onto.   It is not possible to define a function that is one-to-one but not onto; nor is it possible to find a function that is onto but not one-to-one. The two properties imply one another on finite sets of the same cardinality.   "
},
{
  "id": "exercises-7-2-10",
  "level": "2",
  "url": "s-properties-of-functions.html#exercises-7-2-10",
  "type": "Exercise",
  "number": "7.2.3.9",
  "title": "",
  "body": " Prove that the set of natural numbers is countable.  Prove that the set of integers is countable.  Prove that the set of rational numbers is countable.   Use defined by .  Use the function defined by if is even and if is odd.  The proof is due to Georg Cantor (1845-1918), and involves listing the rationals through a definite procedure so that none are omitted and duplications are avoided. In the first row list all nonnegative rationals with denominator 1, in the second all nonnegative rationals with denominator 2, etc. In this listing, of course, there are duplications, for example, , , , etc. To obtain a list without duplications follow the arrows in , listing only the circled numbers.  We obtain: Each nonnegative rational appears in this list exactly once. We now must insert in this list the negative rationals, and follow the same scheme to obtain: which can be paired off with the elements of .     Enumeration of the rational numbers.     "
},
{
  "id": "exercises-7-2-11",
  "level": "2",
  "url": "s-properties-of-functions.html#exercises-7-2-11",
  "type": "Exercise",
  "number": "7.2.3.10",
  "title": "",
  "body": " Prove that the set of finite strings of 0's and 1's is countable.  Prove that the set of odd integers is countable.  Prove that the set is countable.  Prove that the set is countable.    For each part, we provide a bijection with domain that proves that that each set is countable.  If , map it to the string Note that will map to the empty string, .  Map each odd positive integer to itself and any even positive integer, to  Similar to the function that proves that the rationals are countable, we follow paths diagonally from bottom to left side of an array of ordered pairs, as indicated in . A formula for this function isn't so simple, but clearly, it exists and every ordered pair is is that image of some positive integer.   If is the function in the previous part of this exercise that maps the positive integers to , we list the images under of each positive integer on the axis of the array in . Each point in the array can be thought of as a triple with first coordinate the value and the second and third coordinates the coordinates of the value. We follow the same kind of paths in succession as the previous part the is exercise to list all triples.      Enumeration of the set    Visual describing why is countably infinite     Enumeration of the set    Visual describing why is countably infinite.     "
},
{
  "id": "exercises-7-2-12",
  "level": "2",
  "url": "s-properties-of-functions.html#exercises-7-2-12",
  "type": "Exercise",
  "number": "7.2.3.11",
  "title": "",
  "body": "Use the Pigeonhole Principle to prove that an injection cannot exist between a finite set and a finite set if the cardinality of is greater than the cardinality of .  Let be any function from into . By the Pigeonhole principle with , there exists an element of that is the image of at least two elements of . Therefore, is not an injection. "
},
{
  "id": "exercises-7-2-13",
  "level": "2",
  "url": "s-properties-of-functions.html#exercises-7-2-13",
  "type": "Exercise",
  "number": "7.2.3.12",
  "title": "",
  "body": "The important properties of relations are not generally of interest for functions. Most functions are not reflexive, symmetric, antisymmetric, or transitive. Can you give examples of functions that do have these properties?   Here are a few examples. Given any domain , the only function that is reflexive is the function , defined by for all . This is the , which is formally introduced in the next section. This function has the other three properties, and is the equality relation on . There are other functions have some of the individual properties. For example, on the integer, the function defined by is symmetric. The function defined by is antisymmetric. A constant function such as the function that maps each integer to zero is transitive, but not very interesting as far as transitivity is concerned..  "
},
{
  "id": "exercises-7-2-14",
  "level": "2",
  "url": "s-properties-of-functions.html#exercises-7-2-14",
  "type": "Exercise",
  "number": "7.2.3.13",
  "title": "",
  "body": "Prove that the set of all infinite sequences of 0's and 1's is not a countable set.  The proof is indirect and follows a technique called the Cantor diagonal process. Assume to the contrary that the set is countable, then the elements can be listed: where each is an infinite sequence of 0s and 1s. Consider the array:   We assume that this array contains all infinite sequences of 0s and 1s. Consider the sequence defined by   Notice that differs from each in the th position and so cannot be in the list. This is a contradiction, which completes our proof.  "
},
{
  "id": "exercises-7-2-15",
  "level": "2",
  "url": "s-properties-of-functions.html#exercises-7-2-15",
  "type": "Exercise",
  "number": "7.2.3.14",
  "title": "",
  "body": "Prove that the set of all functions on the integers is an uncountable set.   We prove that the set is uncountable by contradiction. Assume there is a bijection from the positive integers into the set of all functions on the integers. We can define a function that is not in the range of , contradicting our assumption that is a bijection. Our definition is . Notice that because they map to different values.  "
},
{
  "id": "s-function-composition",
  "level": "1",
  "url": "s-function-composition.html",
  "type": "Section",
  "number": "7.3",
  "title": "Function Composition",
  "body": "Function Composition   Now that we have a good understanding of what a function is, our next step is to consider an important operation on functions. Our purpose is not to develop the algebra of functions as completely as we did for the algebras of logic, matrices, and sets, but the reader should be aware of the similarities between the algebra of functions and that of matrices. We first define equality of functions.   Function Equality  Equality of Functions Function Equality  Let ; that is, let and both be functions from into . Then is equal to (denoted ) if and only if for all .  Two functions that have different domains cannot be equal. For example, defined by and defined by are not equal even though the formula that defines them is the same.  On the other hand, it is not uncommon for two functions to be equal even though they are defined differently. For example consider the functions and , where is defined by and is defined by appear to be very different functions. However, they are equal because for .   Function Composition  One of the most important operations on functions is that of composition.  Composition of Functions  Composition of Functions  Function Composition  The composition of with  Let and . Then the composition of followed by , written , is a function from into defined by , which is read of of .   The reader should note that it is traditional to write the composition of functions from right to left. Thus, in the above definition, the first function performed in computing is . On the other hand, for relations, the composition is read from left to right, so that the first relation is .  A basic example Let be defined by , , and . Let be defined by and . Then is defined by , and . For example, . Note that is not defined. Why?  Let be defined by and let be defined by . Then, since we have is defined by . Here is also defined and is defined by . Moreover, since for at least one real number, . Therefore, the commutative law is not true for functions under the operation of composition. However, the associative law is true for functions under the operation of composition.   Function composition is associative  If , , and , then .  Note: In order to prove that two functions are equal, we must use the definition of equality of functions. Assuming that the functions have the same domain, they are equal if, for each domain element, the images of that element under the two functions are equal.  We wish to prove that for all , which is the domain of both functions. Similarly, .  Notice that no matter how the functions in the expression is grouped, the final image of any element of is and so .  If is a function on a set , then the compositions , are valid, and we denote them as , . Repeated compositions of with itself can be defined recursively. We will discuss this form of definition in detail in .  Powers of Functions  Powers of Functions  the square of a function.  Let .    ; that is, , for .   For , ; that is, for .    Two useful theorems concerning composition are given below. The proofs are left for the exercises.  The composition of injections is an injection If and are injections, then is an injection.  The composition of surjections is a surjection If and are surjections, then is a surjection.   We would now like to define the concepts of identity and inverse for functions under composition. The motivation and descriptions of the definitions of these terms come from the definitions of the terms in the set of real numbers and for matrices. For real numbers, the numbers 0 and 1 play the unique role that and for any real number . 0 and 1 are the identity elements for the reals under the operations of addition and multiplication, respectively. Similarly, the zero matrix and the identity matrix are such that for any matrix , and . Hence, an elegant way of defining the identity function under the operation of composition would be to imitate the above well-known facts.  Identity Function  Identity Function  The identity function (on a set )  For any set , the identity function on is a function from onto , denoted by (or, more specifically, ) such that for all .   Based on the definition of , we can show that for all functions , .  The identity function on  If , then the identity function is defined by , and .  The identity function on The identity function on is defined by .   Inverse Functions  We will introduce the inverse of a function with a special case: the inverse of a function on a set. After you've taken the time to understand this concept, you can read about the inverse of a function from one set into another. The reader is encouraged to reread the definition of the inverse of a matrix in Section 5.2 ( ) to see that the following definition of the inverse function is a direct analogue of that definition.  Inverse of a Function on a Set  Inverse Function of a function on a set  The inverse of function read inverse  Let . If there exists a function such that , then is called the inverse of and is denoted by , read inverse.   Notice that in the definition we refer to the inverse as opposed to an inverse. It can be proven that a function can never have more than one inverse (see exercises).  An alternate description of the inverse of a function, which can be proven from the definition, is as follows: Let be such that . Then when it exists, is a function from to such that . Note that  undoes what does.  The inverse of a function on Let and let be the function defined on such that , , and . Then is defined by , , and .  Inverse of a real function If is defined by , then is the function that undoes what does. Since cubes real numbers, must be the reverse process, namely, takes cube roots. Therefore, is defined by . We should show that and . We will do the first, and the reader is encouraged to do the second. Therefore, . Why?  The definition of the inverse of a function alludes to the fact that not all functions have inverses. How do we determine when the inverse of a function exists?  Bijections have inverses Let . exists if and only if f is a bijection; i. e. f is one-to-one and onto. ( ) In this half of the proof, assume that exists and we must prove that is one-to-one and onto. To do so, it is convenient for us to use the relation notation, where is equivalent to . To prove that is one-to-one, assume that . Alternatively, that means and are elements of . We must show that . Since , and are in . By the fact that is a function and cannot have two images, and must be equal, so is one-to-one.  Next, to prove that is onto, observe that for to be a function, it must use all of its domain, namely A. Let be any element of . Then has an image under , . Another way of writing this is , By the definition of the inverse, this is equivalent to . Hence, is in the range of . Since was chosen arbitrarily, this shows that the range of must be all of .  ( ) Assume is one-to-one and onto and we are to prove exists. We leave this half of the proof to the reader.  Permutation Permutation A bijection of a set into itself is called a permutation of .   Next, we will consider the functions for which the domain and codomain are not necessarily equal. How do we define the inverse in this case?  Inverse of a Function (General Case) Let , If there exists a function such that and , then is called the inverse of and is denoted by , read inverse.   Note the slightly more complicated condition for the inverse in this case because the domains of and are different if and are different. The proof of the following theorem isn't really very different from the special case where .  When does a function have an inverse? Let . exists if and only if f is a bijection.  Another inverse Let and . Define by , , and . Then defined by , , and is the inverse of .    Exercises   Let , , and . Define by equal to the letter in the alphabet, and define by if is a vowel and if is a consonant.   Find .  Does it make sense to discuss ? If not, why not?  Does exist? Why?  Does exist? Why?    is defined by  No, since the domain of is not equal to the codomain of .  No, since is not surjective.  No, since is not injective.    Let . Define by , , and . Find , , and .        Let .   List all permutations of .  Find the inverse and square of each of the permutations of part a, where the square of a permutation, , is the composition .  Show that the composition of any two permutations of is a permutation of .  Prove that if is any set where , then the number of permutations of is .     The permutations of are and , defined in .     If and are permutations of , then they are both injections and their composition, , is a injection, by . By , is also a surjection; therefore, is a bijection on , a permutation.  Proof by induction: Basis: . The number of permutations of is one, the identity function, and 1! .  Induction: Assume that the number of permutations on a set with elements, , is !. Furthermore, assume that and that contains an element called . Let . We can reduce the definition of a permutation, , on to two steps. First, we select any one of the ! permutations on . (Note the use of the induction hypothesis.) Call it . This permutation almost completely defines a permutation on that we will call . For all in , we start by defining to be . We may be making some adjustments, but define it that way for now. Next, we select the image of , which can be done different ways, allowing for any value in . To keep our function bijective, we must adjust as follows: If we select , then we must find the element, , of such that , and redefine the image of to . If we had selected , then there is no adjustment needed. By the rule of products, the number of ways that we can define is      Define , , and , all functions on the integers, by , , and . Determine:                    Consider the following functions on the set of bit strings of length 4. In these definitions, addition is done modulo 2, so that . Which of these functions has an inverse? For those that have an inverse, what is it? For those that don't explain why.             has an inverse. .  has an inverse. .   does not have an inverse. One way to verify this is to note that is not one-to-one because .   has an inverse.     Inverse images. If is any function from into , we can describe the inverse image as a function from into , which is also commonly denoted . If , . If does have an inverse, the inverse image of is .   Let be defined by . What are , and ?  If , where , what is ?      , and      Let  , and all be functions from into defined by , and . Define:                      Define the following functions on the integers by , , and    Which of these functions are one-to-one?  Which of these functions are onto?  Express in simplest terms the compositions , , , , and .      and are one-to-one?   and are onto?  , , , , and .    Let be a nonempty set. Prove that if is a bijection on and , then is the identity function,   You have seen a similar proof in matrix algebra.  For the real matrix , .  Recall that a bijection from a set to itself is also referred to as a permutation of the set. Let be a permutation of such that becomes , becomes , etc.  Let . How many permutations of leave the determinant of invariant, that is, ?  Clearly, the identity function leaves the determinant invarient. There are three other functions that do the same. They are       All of these functions exchange entries that are in the same diagonal.   State and prove a theorem on inverse functions analogous to the one that says that if a matrix has an inverse, that inverse is unique.   If and has an inverse, then that inverse is unique.  Proof: Suppose that and are both inverses of , both having domain and codomain .    Let and be functions whose inverses exist. Prove that .  See Exercise 3 of Section 5.4.  We've proven that inverses are unique and so all we need to do is to demonstrate that inverts to be able to equate it with . We can verify that is also equal to with similar logic.   Prove and .   Let be elements of such that ; that is, . Since is injective, and since is injective, .  Let be an element of . We must show that there exists an element of whose image under is . Since is surjective, there exists an element of , , such that . Also, since is a surjection, there exists an element of , , such that , .   Prove the second half of .   Assume is a bijection. We need to show that exists. We do this by defining a function and then showing that it inverts , which justifies a claim that . So what is the function? Let . The fact that is a bijection implies that it is onto, which means there exists such that . That is unique is implied by the fact that is one to one. We define . Now we compose and :    Prove by induction that if and , are invertible functions on some nonempty set , then . The basis has been taken care of in .   Basis: : by .  Induction: Assume and and consider .    Our definition of cardinality states that two sets, and , have the same cardinality if there exists a bijection between the two sets. Why does it not matter whether the bijection is from into or into ?  Prove that has the same cardinality as is an equivalence relation on sets.    Every set has the same cardinality as itself because the identity function on that set is a bijection.  If has the same cardinality as , then there exists a bijection from into . But since is a bijection, it has an inverse from into that also is a bijection since is its inverse. Therefore, has the same cardinality as . Hence, having the same cardinality is a symmetric relation.  Finally, if has the same cardinality as , and has the same cardinality as , we know that there are two bijection: and . Since the composition of two bijections is a bijection, is a bijection that implies that has the same cardinality as . This establishes the transitive property and we have an equivalence relation.   Construct a table listing as many Laws of Function Composition as you can identify. Use previous lists of laws as a guide.   Based on the definition of the identity function, show that for all functions , .   If , Similarly .     "
},
{
  "id": "def-equality-of-functions",
  "level": "2",
  "url": "s-function-composition.html#def-equality-of-functions",
  "type": "Definition",
  "number": "7.3.1",
  "title": "Equality of Functions.",
  "body": "Equality of Functions Function Equality  Let ; that is, let and both be functions from into . Then is equal to (denoted ) if and only if for all . "
},
{
  "id": "def-composition-of-functions",
  "level": "2",
  "url": "s-function-composition.html#def-composition-of-functions",
  "type": "Definition",
  "number": "7.3.2",
  "title": "Composition of Functions.",
  "body": "Composition of Functions  Composition of Functions  Function Composition  The composition of with  Let and . Then the composition of followed by , written , is a function from into defined by , which is read of of .  "
},
{
  "id": "ex-simple-composition",
  "level": "2",
  "url": "s-function-composition.html#ex-simple-composition",
  "type": "Example",
  "number": "7.3.3",
  "title": "A basic example.",
  "body": "A basic example Let be defined by , , and . Let be defined by and . Then is defined by , and . For example, . Note that is not defined. Why?  Let be defined by and let be defined by . Then, since we have is defined by . Here is also defined and is defined by . Moreover, since for at least one real number, . Therefore, the commutative law is not true for functions under the operation of composition. However, the associative law is true for functions under the operation of composition. "
},
{
  "id": "function-composition-associative",
  "level": "2",
  "url": "s-function-composition.html#function-composition-associative",
  "type": "Theorem",
  "number": "7.3.4",
  "title": "Function composition is associative.",
  "body": " Function composition is associative  If , , and , then .  Note: In order to prove that two functions are equal, we must use the definition of equality of functions. Assuming that the functions have the same domain, they are equal if, for each domain element, the images of that element under the two functions are equal.  We wish to prove that for all , which is the domain of both functions. Similarly, .  Notice that no matter how the functions in the expression is grouped, the final image of any element of is and so . "
},
{
  "id": "def-powers-of-functions",
  "level": "2",
  "url": "s-function-composition.html#def-powers-of-functions",
  "type": "Definition",
  "number": "7.3.5",
  "title": "Powers of Functions.",
  "body": "Powers of Functions  Powers of Functions  the square of a function.  Let .    ; that is, , for .   For , ; that is, for .   "
},
{
  "id": "theorem-composition-of-injections",
  "level": "2",
  "url": "s-function-composition.html#theorem-composition-of-injections",
  "type": "Theorem",
  "number": "7.3.6",
  "title": "The composition of injections is an injection.",
  "body": "The composition of injections is an injection If and are injections, then is an injection. "
},
{
  "id": "theorem-composition-of-surjections",
  "level": "2",
  "url": "s-function-composition.html#theorem-composition-of-surjections",
  "type": "Theorem",
  "number": "7.3.7",
  "title": "The composition of surjections is a surjection.",
  "body": "The composition of surjections is a surjection If and are surjections, then is a surjection.  "
},
{
  "id": "def-identity-function",
  "level": "2",
  "url": "s-function-composition.html#def-identity-function",
  "type": "Definition",
  "number": "7.3.8",
  "title": "Identity Function.",
  "body": "Identity Function  Identity Function  The identity function (on a set )  For any set , the identity function on is a function from onto , denoted by (or, more specifically, ) such that for all .  "
},
{
  "id": "ex-an-identity-function",
  "level": "2",
  "url": "s-function-composition.html#ex-an-identity-function",
  "type": "Example",
  "number": "7.3.9",
  "title": "The identity function on <span xmlns:pf=\"https:\/\/prefigure.org\" class=\"process-math\">\\(\\{1,2,3\\}\\)<\/span>.",
  "body": "The identity function on  If , then the identity function is defined by , and . "
},
{
  "id": "ex-identity-on-reals",
  "level": "2",
  "url": "s-function-composition.html#ex-identity-on-reals",
  "type": "Example",
  "number": "7.3.10",
  "title": "The identity function on <span xmlns:pf=\"https:\/\/prefigure.org\" class=\"process-math\">\\(\\mathbb{R}\\)<\/span>.",
  "body": "The identity function on The identity function on is defined by . "
},
{
  "id": "def-inverse-function",
  "level": "2",
  "url": "s-function-composition.html#def-inverse-function",
  "type": "Definition",
  "number": "7.3.11",
  "title": "Inverse of a Function on a Set.",
  "body": "Inverse of a Function on a Set  Inverse Function of a function on a set  The inverse of function read inverse  Let . If there exists a function such that , then is called the inverse of and is denoted by , read inverse.  "
},
{
  "id": "ex-simple-inverse",
  "level": "2",
  "url": "s-function-composition.html#ex-simple-inverse",
  "type": "Example",
  "number": "7.3.12",
  "title": "The inverse of a function on <span xmlns:pf=\"https:\/\/prefigure.org\" class=\"process-math\">\\(\\{1,2,3\\}\\)<\/span>.",
  "body": "The inverse of a function on Let and let be the function defined on such that , , and . Then is defined by , , and . "
},
{
  "id": "ex-inverse-of-a-real-function",
  "level": "2",
  "url": "s-function-composition.html#ex-inverse-of-a-real-function",
  "type": "Example",
  "number": "7.3.13",
  "title": "Inverse of a real function.",
  "body": "Inverse of a real function If is defined by , then is the function that undoes what does. Since cubes real numbers, must be the reverse process, namely, takes cube roots. Therefore, is defined by . We should show that and . We will do the first, and the reader is encouraged to do the second. Therefore, . Why? "
},
{
  "id": "theorem-bijections-have-inverses",
  "level": "2",
  "url": "s-function-composition.html#theorem-bijections-have-inverses",
  "type": "Theorem",
  "number": "7.3.14",
  "title": "Bijections have inverses.",
  "body": "Bijections have inverses Let . exists if and only if f is a bijection; i. e. f is one-to-one and onto. ( ) In this half of the proof, assume that exists and we must prove that is one-to-one and onto. To do so, it is convenient for us to use the relation notation, where is equivalent to . To prove that is one-to-one, assume that . Alternatively, that means and are elements of . We must show that . Since , and are in . By the fact that is a function and cannot have two images, and must be equal, so is one-to-one.  Next, to prove that is onto, observe that for to be a function, it must use all of its domain, namely A. Let be any element of . Then has an image under , . Another way of writing this is , By the definition of the inverse, this is equivalent to . Hence, is in the range of . Since was chosen arbitrarily, this shows that the range of must be all of .  ( ) Assume is one-to-one and onto and we are to prove exists. We leave this half of the proof to the reader. "
},
{
  "id": "def-Permutation",
  "level": "2",
  "url": "s-function-composition.html#def-Permutation",
  "type": "Definition",
  "number": "7.3.15",
  "title": "Permutation.",
  "body": "Permutation Permutation A bijection of a set into itself is called a permutation of .  "
},
{
  "id": "def-general-inverse-function",
  "level": "2",
  "url": "s-function-composition.html#def-general-inverse-function",
  "type": "Definition",
  "number": "7.3.16",
  "title": "Inverse of a Function (General Case).",
  "body": "Inverse of a Function (General Case) Let , If there exists a function such that and , then is called the inverse of and is denoted by , read inverse.  "
},
{
  "id": "theorem-inverse-function-condition",
  "level": "2",
  "url": "s-function-composition.html#theorem-inverse-function-condition",
  "type": "Theorem",
  "number": "7.3.17",
  "title": "When does a function have an inverse?",
  "body": "When does a function have an inverse? Let . exists if and only if f is a bijection. "
},
{
  "id": "example-inverse-another",
  "level": "2",
  "url": "s-function-composition.html#example-inverse-another",
  "type": "Example",
  "number": "7.3.18",
  "title": "Another inverse.",
  "body": "Another inverse Let and . Define by , , and . Then defined by , , and is the inverse of . "
},
{
  "id": "exercises-7-3-2",
  "level": "2",
  "url": "s-function-composition.html#exercises-7-3-2",
  "type": "Exercise",
  "number": "7.3.4.1",
  "title": "",
  "body": " Let , , and . Define by equal to the letter in the alphabet, and define by if is a vowel and if is a consonant.   Find .  Does it make sense to discuss ? If not, why not?  Does exist? Why?  Does exist? Why?    is defined by  No, since the domain of is not equal to the codomain of .  No, since is not surjective.  No, since is not injective.   "
},
{
  "id": "exercises-7-3-3",
  "level": "2",
  "url": "s-function-composition.html#exercises-7-3-3",
  "type": "Exercise",
  "number": "7.3.4.2",
  "title": "",
  "body": "Let . Define by , , and . Find , , and .      "
},
{
  "id": "exercises-7-3-4",
  "level": "2",
  "url": "s-function-composition.html#exercises-7-3-4",
  "type": "Exercise",
  "number": "7.3.4.3",
  "title": "",
  "body": " Let .   List all permutations of .  Find the inverse and square of each of the permutations of part a, where the square of a permutation, , is the composition .  Show that the composition of any two permutations of is a permutation of .  Prove that if is any set where , then the number of permutations of is .     The permutations of are and , defined in .     If and are permutations of , then they are both injections and their composition, , is a injection, by . By , is also a surjection; therefore, is a bijection on , a permutation.  Proof by induction: Basis: . The number of permutations of is one, the identity function, and 1! .  Induction: Assume that the number of permutations on a set with elements, , is !. Furthermore, assume that and that contains an element called . Let . We can reduce the definition of a permutation, , on to two steps. First, we select any one of the ! permutations on . (Note the use of the induction hypothesis.) Call it . This permutation almost completely defines a permutation on that we will call . For all in , we start by defining to be . We may be making some adjustments, but define it that way for now. Next, we select the image of , which can be done different ways, allowing for any value in . To keep our function bijective, we must adjust as follows: If we select , then we must find the element, , of such that , and redefine the image of to . If we had selected , then there is no adjustment needed. By the rule of products, the number of ways that we can define is     "
},
{
  "id": "exercises-7-3-5",
  "level": "2",
  "url": "s-function-composition.html#exercises-7-3-5",
  "type": "Exercise",
  "number": "7.3.4.4",
  "title": "",
  "body": "Define , , and , all functions on the integers, by , , and . Determine:                   "
},
{
  "id": "exercises-7-3-6",
  "level": "2",
  "url": "s-function-composition.html#exercises-7-3-6",
  "type": "Exercise",
  "number": "7.3.4.5",
  "title": "",
  "body": "Consider the following functions on the set of bit strings of length 4. In these definitions, addition is done modulo 2, so that . Which of these functions has an inverse? For those that have an inverse, what is it? For those that don't explain why.             has an inverse. .  has an inverse. .   does not have an inverse. One way to verify this is to note that is not one-to-one because .   has an inverse.    "
},
{
  "id": "exercises-7-3-7",
  "level": "2",
  "url": "s-function-composition.html#exercises-7-3-7",
  "type": "Exercise",
  "number": "7.3.4.6",
  "title": "",
  "body": "Inverse images. If is any function from into , we can describe the inverse image as a function from into , which is also commonly denoted . If , . If does have an inverse, the inverse image of is .   Let be defined by . What are , and ?  If , where , what is ?      , and     "
},
{
  "id": "exercises-7-3-8",
  "level": "2",
  "url": "s-function-composition.html#exercises-7-3-8",
  "type": "Exercise",
  "number": "7.3.4.7",
  "title": "",
  "body": "Let  , and all be functions from into defined by , and . Define:                    "
},
{
  "id": "exercises-7-3-9",
  "level": "2",
  "url": "s-function-composition.html#exercises-7-3-9",
  "type": "Exercise",
  "number": "7.3.4.8",
  "title": "",
  "body": " Define the following functions on the integers by , , and    Which of these functions are one-to-one?  Which of these functions are onto?  Express in simplest terms the compositions , , , , and .      and are one-to-one?   and are onto?  , , , , and .   "
},
{
  "id": "exercises-7-3-10",
  "level": "2",
  "url": "s-function-composition.html#exercises-7-3-10",
  "type": "Exercise",
  "number": "7.3.4.9",
  "title": "",
  "body": "Let be a nonempty set. Prove that if is a bijection on and , then is the identity function,   You have seen a similar proof in matrix algebra. "
},
{
  "id": "exercises-7-3-11",
  "level": "2",
  "url": "s-function-composition.html#exercises-7-3-11",
  "type": "Exercise",
  "number": "7.3.4.10",
  "title": "",
  "body": "For the real matrix , .  Recall that a bijection from a set to itself is also referred to as a permutation of the set. Let be a permutation of such that becomes , becomes , etc.  Let . How many permutations of leave the determinant of invariant, that is, ?  Clearly, the identity function leaves the determinant invarient. There are three other functions that do the same. They are       All of these functions exchange entries that are in the same diagonal.  "
},
{
  "id": "exercises-7-3-12",
  "level": "2",
  "url": "s-function-composition.html#exercises-7-3-12",
  "type": "Exercise",
  "number": "7.3.4.11",
  "title": "",
  "body": "State and prove a theorem on inverse functions analogous to the one that says that if a matrix has an inverse, that inverse is unique.   If and has an inverse, then that inverse is unique.  Proof: Suppose that and are both inverses of , both having domain and codomain .   "
},
{
  "id": "exercise-composition-inverse-basis",
  "level": "2",
  "url": "s-function-composition.html#exercise-composition-inverse-basis",
  "type": "Exercise",
  "number": "7.3.4.12",
  "title": "",
  "body": "Let and be functions whose inverses exist. Prove that .  See Exercise 3 of Section 5.4.  We've proven that inverses are unique and so all we need to do is to demonstrate that inverts to be able to equate it with . We can verify that is also equal to with similar logic.  "
},
{
  "id": "exercises-7-3-14",
  "level": "2",
  "url": "s-function-composition.html#exercises-7-3-14",
  "type": "Exercise",
  "number": "7.3.4.13",
  "title": "",
  "body": "Prove and .   Let be elements of such that ; that is, . Since is injective, and since is injective, .  Let be an element of . We must show that there exists an element of whose image under is . Since is surjective, there exists an element of , , such that . Also, since is a surjection, there exists an element of , , such that , .  "
},
{
  "id": "exercises-7-3-15",
  "level": "2",
  "url": "s-function-composition.html#exercises-7-3-15",
  "type": "Exercise",
  "number": "7.3.4.14",
  "title": "",
  "body": "Prove the second half of .   Assume is a bijection. We need to show that exists. We do this by defining a function and then showing that it inverts , which justifies a claim that . So what is the function? Let . The fact that is a bijection implies that it is onto, which means there exists such that . That is unique is implied by the fact that is one to one. We define . Now we compose and :   "
},
{
  "id": "exercises-7-3-16",
  "level": "2",
  "url": "s-function-composition.html#exercises-7-3-16",
  "type": "Exercise",
  "number": "7.3.4.15",
  "title": "",
  "body": "Prove by induction that if and , are invertible functions on some nonempty set , then . The basis has been taken care of in .   Basis: : by .  Induction: Assume and and consider .   "
},
{
  "id": "exercises-7-3-17",
  "level": "2",
  "url": "s-function-composition.html#exercises-7-3-17",
  "type": "Exercise",
  "number": "7.3.4.16",
  "title": "",
  "body": "Our definition of cardinality states that two sets, and , have the same cardinality if there exists a bijection between the two sets. Why does it not matter whether the bijection is from into or into ?  Prove that has the same cardinality as is an equivalence relation on sets.    Every set has the same cardinality as itself because the identity function on that set is a bijection.  If has the same cardinality as , then there exists a bijection from into . But since is a bijection, it has an inverse from into that also is a bijection since is its inverse. Therefore, has the same cardinality as . Hence, having the same cardinality is a symmetric relation.  Finally, if has the same cardinality as , and has the same cardinality as , we know that there are two bijection: and . Since the composition of two bijections is a bijection, is a bijection that implies that has the same cardinality as . This establishes the transitive property and we have an equivalence relation.  "
},
{
  "id": "exercises-7-3-18",
  "level": "2",
  "url": "s-function-composition.html#exercises-7-3-18",
  "type": "Exercise",
  "number": "7.3.4.17",
  "title": "",
  "body": "Construct a table listing as many Laws of Function Composition as you can identify. Use previous lists of laws as a guide.  "
},
{
  "id": "exercises-7-3-19",
  "level": "2",
  "url": "s-function-composition.html#exercises-7-3-19",
  "type": "Exercise",
  "number": "7.3.4.18",
  "title": "",
  "body": "Based on the definition of the identity function, show that for all functions , .   If , Similarly .  "
},
{
  "id": "s-faces-of-recursion",
  "level": "1",
  "url": "s-faces-of-recursion.html",
  "type": "Section",
  "number": "8.1",
  "title": "The Many Faces of Recursion",
  "body": " The Many Faces of Recursion  Many Faces of Recursion, The  Consider the following definitions, all of which should be somewhat familiar to you. When reading them, concentrate on how they are similar.  Binomial Coefficients Polynomials and their evaluation  Here is a recursive definition of binomial coefficients, which we introduced in Chapter 2.  Binomial Coefficient - Recursion Definition Binomial Coefficient Recursive Definition Assume and . We define by    and  if    A word about definitions: Strictly speaking, when mathematical objects such as binomial coefficients are defined, they should be defined just once. Since we defined binomial coefficients earlier, in , other statements describing them should be theorems. The theorem, in this case, would be that the definition above is consistent with the original definition. Our point in this chapter in discussing recursion is to observe alternative definitions that have a recursive nature. In the exercises, you will have the opportunity to prove that the two definitions are indeed equivalent.  Here is how we can apply the recursive definition to compute .    Polynomials and Their Evaluation Polynomials  Polynomial Expression in over (Non-Recursive) Polynomial Expression Non-recursive). Let be an integer, . An degree polynomial in is an expression of the form , where are elements of some designated set of numbers, , called the set of coefficients and .  We refer to as a variable here, although the more precise term for is an indeterminate . There is a distinction between the terms indeterminate and variable, but that distinction will not come into play in our discussions.  Zeroth degree polynomials are called constant polynomials and are simply elements of the set of coefficients.  This definition is often introduced in algebra courses to describe expressions such as , a third-degree, or cubic, polynomial in . This definition has a drawback when the variable is given a value and the expression must be evaluated. For example, suppose that . Your first impulse is likely to do this:   A count of the number of operations performed shows that five multiplications and three additions\/subtractions were performed. The first two multiplications compute and , and the last three multiply the powers of 7 times the coefficients. This gives you the four terms; and adding\/subtracting a list of numbers requires addition\/subtractions. The following definition of a polynomial expression suggests another more efficient method of evaluation.  Polynomial Expression in over (Recursive) Polynomial Expression Recursive definition  Let be a set of coefficients and a variable.   A zeroth degree polynomial expression in over is a nonzero element of .  For , an degree polynomial expression in over is an expression of the form where is an degree polynomial expression in and .      We can easily verify that is a third-degree polynomial expression in over based on this definition: Notice that 4 is a zeroth degree polynomial since it is an integer. Therefore is a first-degree polynomial; therefore, is a second-degree polynomial in over ; therefore, is a third-degree polynomial in over . The final expression for is called its telescoping form . If we use it to calculate , we need only three multiplications and three additions\/subtractions. This is called Horner's method for evaluating a polynomial expression.  More Telescoping Polynomials   The telescoping form of is . Using Horner's method, computing the value of requires four multiplications and four additions\/subtractions for any real number .  has the telescoping form .    Many computer languages represent polynomials as lists of coefficients, usually starting with the constant term. For example, would be represented with the list . In both Mathematica and Sage, polynomial expressions can be entered and manipulated, so the list representation is only internal. Some programming languages require users to program polynomial operations with lists. We will leave these programming issues to another source.   Recursive Searching - The Binary Search Recursive Searching Binary Search  Next, we consider a recursive algorithm for a binary search within a sorted list of items. Suppose represent a list of items sorted by a numeric key in descending order. The item is denoted and its key value by . For example, each item might contain data on the buildings in a city and the key value might be the height of the building. Then would be the item for the tallest building and would be its height. The algorithm can be applied to search for an item in with key value . This would be accomplished by the execution of . When the algorithm is completed, the variable will have a value of if an item with the desired key value was found, and the value of will be the index of an item whose key is . If keeps the value , no such item exists in the list. The general idea behind the algorithm is illustrated in   General Scheme for a Binary Search   General Scheme for a Binary Search.    In the following implementation of the Binary Search in SageMath, we search within a sorted list of integers. Therefore, the items themselves are the keys.    Recursively Defined Sequences Sequences Recursively Defined  For the next two examples, consider a sequence of numbers to be a list of numbers consisting of a zeroth number, first number, second number, ... . If a sequence is given the name , the number of is usually written or .  Geometric Growth Sequence  Define the sequence of numbers by  .  These rules stipulate that each number in the list is 1.08 times the previous number, with the starting number equal to 100. For example    The Fibonacci Sequence Fibonacci Sequence The Fibonacci sequence is the sequence defined by     Recursion  All of the previous examples were presented recursively. That is, every object is described in one of two forms. One form is by a simple definition, which is usually called the basis for the recursion. The second form is by a recursive description in which objects are described in terms of themselves, with the following qualification. What is essential for a proper use of recursion is that the objects can be expressed in terms of simpler objects, where simpler means closer to the basis of the recursion. To avoid what might be considered a circular definition, the basis must be reached after a finite number of applications of the recursion.  To determine, for example, the fourth item in the Fibonacci sequence we repeatedly apply the recursive rule for until we are left with an expression involving and :    Iteration  On the other hand, we could compute a term in the Fibonacci sequence such as by starting with the basis terms and working forward as follows:          This is called an iterative computation of the Fibonacci sequence. Here we start with the basis and work our way forward to a less simple number, such as 5. Try to compute using the recursive definition for as we did for . It will take much more time than it would have taken to do the computations above. Iterative computations usually tend to be faster than computations that apply recursion. Therefore, one useful skill is being able to convert a recursive formula into a nonrecursive formula, such as one that requires only iteration or a faster method, if possible.  An iterative formula for is also much more efficient than an application of the recursive definition. The recursive definition is not without its merits, however. First, the recursive equation is often useful in manipulating algebraic expressions involving binomial coefficients. Second, it gives us an insight into the combinatoric interpretation of . In choosing elements from , there are ways of choosing all from , and there are ways of choosing the elements if is to be selected and the remaining elements come from . Note how we used the Law of Addition from Chapter 2 in our reasoning.  BinarySearch Revisited. In the binary search algorithm, the place where recursion is used is easy to pick out. When an item is examined and the key is not the one you want, the search is cut down to a sublist of no more than half the number of items that you were searching in before. Obviously, this is a simpler search. The basis is hidden in the algorithm. The two cases that complete the search can be thought of as the basis. Either you find an item that you want, or the sublist that you have been left to search in is empty, when .  BinarySearch can be translated without much difficulty into any language that allows recursive calls to its subprograms. The advantage to such a program is that its coding would be much shorter than a nonrecursive program that does a binary search. However, in most cases the recursive version will be slower and require more memory at execution time.   Induction and Recursion Induction and Recursion  The definition of the positive integers in terms of Peano's Postulates is a recursive definition. The basis element is the number 1 and the recursion is that if is a positive integer, then so is its successor. In this case, is the simple object and the recursion is of a forward type. Of course, the validity of an induction proof is based on our acceptance of this definition. Therefore, the appearance of induction proofs when recursion is used is no coincidence.  Proof of a formula for A formula for the sequence in is for . A proof by induction follow.  If , then , as defined. Now assume that for some , the formula for is true. hence the formula is true for  The formula that we have just proven for is called a closed form expression. It involves no recursion or summation signs.   Closed Form Expression Closed Form Expression. Let be an algebraic expression involving variables which are allowed to take on values from some predetermined set. is a closed form expression if there exists a number such that the evaluation of with any allowed values of the variables will take no more than operations (alternatively, time units).  Reducing a summation to closed form The sum is not a closed form expression because the number of additions needed to evaluate grows indefinitely with . A closed form expression that computes the value of is , which only requires operations.   Exercises   By the recursive definition of binomial coefficients, . Continue expanding to express it in terms of quantities defined by the basis. Check your result by applying the factorial definition of .     Define the sequence by and for , . Determine and prove by induction that .   Basis: Using the given formula, , which agrees with the basis of the recursive definition.  Induction: Assume that for some the formula is correct.    Let .   Write in telescoping form.  Use a calculator to compute using the original form of .  Use a calculator to compute using the telescoping form of .  Compare your speed in parts b and c.    in telescoping form:      Suppose that a list of nine items, , is sorted by key in decending order so that and . List the executions of the BinarySearch algorithms that would be needed to complete BinarySearch(1,9) when:  The search key is C = 12  The search key is C = 11   Assume that distinct items have distinct keys.    BinarySearch(1,9) probes and finds it to be less than 12 and then calls BinarySearch(1,4). BinarySearch(1,4) probes and finds it to be greater than 12 and then calls BinarySearch(3,4). BinarySearch(3,4) probes and finds 12.  BinarySearch(1,9) probes and finds it to be less than 11 and then calls BinarySearch(1,4). BinarySearch(1,4) probes and finds it to be greater than 11 and then calls BinarySearch(3,4). BinarySearch(3,4) probes and finds 12, which is greater than 11 and calls BinarySearch(4,4). BinarySearch(4,4) examines and since it is not equal to 11, the search ends unsuccessfully.    What is wrong with the following definition of ? and if .  The basis is not reached in a finite number of steps if you try to compute for a nonzero value of .  Prove the two definitions of binomials coefficients, and , are equivalent.  Basis: The basis for the definition of binomial coefficients states that , which is the number of zero element subsets of any set; i. e., the empty set. Also is consistent with the fact that the only element subset of an element set is the whole set itself.  Induction: Now assume . We will partition the element subsets of an element set according to whether some arbitrary last element is in a subset or not. If that last element isn't included in a subset, there are as many subsets as the the element subsets of an element set, . If the last element is included, then any subset will be made up with that element and an element subset from the other elements, so there are such subsets. The sum of the two alternative counts matches the recursive definition.   Prove by induction that if ,   "
},
{
  "id": "def-binomial-coefficient-recursive",
  "level": "2",
  "url": "s-faces-of-recursion.html#def-binomial-coefficient-recursive",
  "type": "Definition",
  "number": "8.1.1",
  "title": "Binomial Coefficient - Recursion Definition.",
  "body": "Binomial Coefficient - Recursion Definition Binomial Coefficient Recursive Definition Assume and . We define by    and  if   "
},
{
  "id": "ss-binomial-coefficients-5",
  "level": "2",
  "url": "s-faces-of-recursion.html#ss-binomial-coefficients-5",
  "type": "Observation",
  "number": "8.1.2",
  "title": "",
  "body": "A word about definitions: Strictly speaking, when mathematical objects such as binomial coefficients are defined, they should be defined just once. Since we defined binomial coefficients earlier, in , other statements describing them should be theorems. The theorem, in this case, would be that the definition above is consistent with the original definition. Our point in this chapter in discussing recursion is to observe alternative definitions that have a recursive nature. In the exercises, you will have the opportunity to prove that the two definitions are indeed equivalent. "
},
{
  "id": "def-polynomial-expression-nonrecursive",
  "level": "2",
  "url": "s-faces-of-recursion.html#def-polynomial-expression-nonrecursive",
  "type": "Definition",
  "number": "8.1.3",
  "title": "Polynomial Expression in <span xmlns:pf=\"https:\/\/prefigure.org\" class=\"process-math\">\\(x\\)<\/span> over <span class=\"process-math\">\\(S\\)<\/span> (Non-Recursive).",
  "body": "Polynomial Expression in over (Non-Recursive) Polynomial Expression Non-recursive). Let be an integer, . An degree polynomial in is an expression of the form , where are elements of some designated set of numbers, , called the set of coefficients and . "
},
{
  "id": "def-polynomial-expression-recursive",
  "level": "2",
  "url": "s-faces-of-recursion.html#def-polynomial-expression-recursive",
  "type": "Definition",
  "number": "8.1.4",
  "title": "Polynomial Expression in <span xmlns:pf=\"https:\/\/prefigure.org\" class=\"process-math\">\\(x\\)<\/span> over <span class=\"process-math\">\\(S\\)<\/span> (Recursive).",
  "body": "Polynomial Expression in over (Recursive) Polynomial Expression Recursive definition  Let be a set of coefficients and a variable.   A zeroth degree polynomial expression in over is a nonzero element of .  For , an degree polynomial expression in over is an expression of the form where is an degree polynomial expression in and .     "
},
{
  "id": "ss-polynomials-and-their-evaluation-9",
  "level": "2",
  "url": "s-faces-of-recursion.html#ss-polynomials-and-their-evaluation-9",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "telescoping form Horner's method "
},
{
  "id": "ex-more-telescoping_",
  "level": "2",
  "url": "s-faces-of-recursion.html#ex-more-telescoping_",
  "type": "Example",
  "number": "8.1.5",
  "title": "More Telescoping Polynomials.",
  "body": "More Telescoping Polynomials   The telescoping form of is . Using Horner's method, computing the value of requires four multiplications and four additions\/subtractions for any real number .  has the telescoping form .   "
},
{
  "id": "fig-binsearch",
  "level": "2",
  "url": "s-faces-of-recursion.html#fig-binsearch",
  "type": "Figure",
  "number": "8.1.6",
  "title": "",
  "body": " General Scheme for a Binary Search   General Scheme for a Binary Search.   "
},
{
  "id": "ex-geometric-growth",
  "level": "2",
  "url": "s-faces-of-recursion.html#ex-geometric-growth",
  "type": "Example",
  "number": "8.1.7",
  "title": "Geometric Growth Sequence.",
  "body": "Geometric Growth Sequence  Define the sequence of numbers by  .  These rules stipulate that each number in the list is 1.08 times the previous number, with the starting number equal to 100. For example   "
},
{
  "id": "ex-fibonacci-sequence",
  "level": "2",
  "url": "s-faces-of-recursion.html#ex-fibonacci-sequence",
  "type": "Example",
  "number": "8.1.8",
  "title": "The Fibonacci Sequence.",
  "body": "The Fibonacci Sequence Fibonacci Sequence The Fibonacci sequence is the sequence defined by   "
},
{
  "id": "ss-iteration-3",
  "level": "2",
  "url": "s-faces-of-recursion.html#ss-iteration-3",
  "type": "Table",
  "number": "8.1.9",
  "title": "",
  "body": "       "
},
{
  "id": "ex-geometric-squence-proof",
  "level": "2",
  "url": "s-faces-of-recursion.html#ex-geometric-squence-proof",
  "type": "Example",
  "number": "8.1.10",
  "title": "Proof of a formula for <span xmlns:pf=\"https:\/\/prefigure.org\" class=\"process-math\">\\(B\\)<\/span>.",
  "body": "Proof of a formula for A formula for the sequence in is for . A proof by induction follow.  If , then , as defined. Now assume that for some , the formula for is true. hence the formula is true for  The formula that we have just proven for is called a closed form expression. It involves no recursion or summation signs.  "
},
{
  "id": "def-closed-form-expression",
  "level": "2",
  "url": "s-faces-of-recursion.html#def-closed-form-expression",
  "type": "Definition",
  "number": "8.1.11",
  "title": "Closed Form Expression.",
  "body": "Closed Form Expression Closed Form Expression. Let be an algebraic expression involving variables which are allowed to take on values from some predetermined set. is a closed form expression if there exists a number such that the evaluation of with any allowed values of the variables will take no more than operations (alternatively, time units). "
},
{
  "id": "ex-summation-simplifed",
  "level": "2",
  "url": "s-faces-of-recursion.html#ex-summation-simplifed",
  "type": "Example",
  "number": "8.1.12",
  "title": "Reducing a summation to closed form.",
  "body": "Reducing a summation to closed form The sum is not a closed form expression because the number of additions needed to evaluate grows indefinitely with . A closed form expression that computes the value of is , which only requires operations. "
},
{
  "id": "s-faces-of-recursion-11-2",
  "level": "2",
  "url": "s-faces-of-recursion.html#s-faces-of-recursion-11-2",
  "type": "Exercise",
  "number": "8.1.8.1",
  "title": "",
  "body": " By the recursive definition of binomial coefficients, . Continue expanding to express it in terms of quantities defined by the basis. Check your result by applying the factorial definition of .    "
},
{
  "id": "s-faces-of-recursion-11-3",
  "level": "2",
  "url": "s-faces-of-recursion.html#s-faces-of-recursion-11-3",
  "type": "Exercise",
  "number": "8.1.8.2",
  "title": "",
  "body": "Define the sequence by and for , . Determine and prove by induction that .   Basis: Using the given formula, , which agrees with the basis of the recursive definition.  Induction: Assume that for some the formula is correct.   "
},
{
  "id": "s-faces-of-recursion-11-4",
  "level": "2",
  "url": "s-faces-of-recursion.html#s-faces-of-recursion-11-4",
  "type": "Exercise",
  "number": "8.1.8.3",
  "title": "",
  "body": "Let .   Write in telescoping form.  Use a calculator to compute using the original form of .  Use a calculator to compute using the telescoping form of .  Compare your speed in parts b and c.    in telescoping form:     "
},
{
  "id": "s-faces-of-recursion-11-5",
  "level": "2",
  "url": "s-faces-of-recursion.html#s-faces-of-recursion-11-5",
  "type": "Exercise",
  "number": "8.1.8.4",
  "title": "",
  "body": "Suppose that a list of nine items, , is sorted by key in decending order so that and . List the executions of the BinarySearch algorithms that would be needed to complete BinarySearch(1,9) when:  The search key is C = 12  The search key is C = 11   Assume that distinct items have distinct keys.    BinarySearch(1,9) probes and finds it to be less than 12 and then calls BinarySearch(1,4). BinarySearch(1,4) probes and finds it to be greater than 12 and then calls BinarySearch(3,4). BinarySearch(3,4) probes and finds 12.  BinarySearch(1,9) probes and finds it to be less than 11 and then calls BinarySearch(1,4). BinarySearch(1,4) probes and finds it to be greater than 11 and then calls BinarySearch(3,4). BinarySearch(3,4) probes and finds 12, which is greater than 11 and calls BinarySearch(4,4). BinarySearch(4,4) examines and since it is not equal to 11, the search ends unsuccessfully.   "
},
{
  "id": "s-faces-of-recursion-11-6",
  "level": "2",
  "url": "s-faces-of-recursion.html#s-faces-of-recursion-11-6",
  "type": "Exercise",
  "number": "8.1.8.5",
  "title": "",
  "body": "What is wrong with the following definition of ? and if .  The basis is not reached in a finite number of steps if you try to compute for a nonzero value of . "
},
{
  "id": "s-faces-of-recursion-11-7",
  "level": "2",
  "url": "s-faces-of-recursion.html#s-faces-of-recursion-11-7",
  "type": "Exercise",
  "number": "8.1.8.6",
  "title": "",
  "body": "Prove the two definitions of binomials coefficients, and , are equivalent.  Basis: The basis for the definition of binomial coefficients states that , which is the number of zero element subsets of any set; i. e., the empty set. Also is consistent with the fact that the only element subset of an element set is the whole set itself.  Induction: Now assume . We will partition the element subsets of an element set according to whether some arbitrary last element is in a subset or not. If that last element isn't included in a subset, there are as many subsets as the the element subsets of an element set, . If the last element is included, then any subset will be made up with that element and an element subset from the other elements, so there are such subsets. The sum of the two alternative counts matches the recursive definition.  "
},
{
  "id": "exercise-binomial-sum",
  "level": "2",
  "url": "s-faces-of-recursion.html#exercise-binomial-sum",
  "type": "Exercise",
  "number": "8.1.8.7",
  "title": "",
  "body": "Prove by induction that if , "
},
{
  "id": "s-Sequences",
  "level": "1",
  "url": "s-Sequences.html",
  "type": "Section",
  "number": "8.2",
  "title": "Sequences",
  "body": " Sequences  Sequences  Sequences and Ways They Are Defined  Sequence Sequence A sequence is a function from the natural numbers into some predetermined set. The image of any natural number can be written as or and is called the  term of . The variable is called the index or argument of the sequence.  For example, a sequence of integers would be a function .  Three sequences defined in different ways   The sequence defined by , , is a sequence of integers.  The sequence defined recursively by and for is a sequence of integers. The terms of can be computed either by applying the recursion formula or by iteration. For example, or   Let be the number of strings of 0's and 1's of length having no consecutive zeros. These terms define a sequence of integers.    Remarks:   A sequence is often called a discrete function .  Although it is important to keep in mind that a sequence is a function, another useful way of visualizing a sequence is as a list. For example, the sequence in the previous example could be written as . Finite sequences can appear much the same way when they are the input to or output from a computer. The index of a sequence can be thought of as a time variable. Imagine the terms of a sequence flashing on a screen every second. Then would be what you see in the second. It is convenient to use terminology like this in describing sequences. For example, the terms that precede the term of would be . They might be called the earlier terms.    A Fundamental Problem  Given the definition of any sequence, a fundamental problem that we will concern ourselves with is to devise a method for determining any specific term in a minimum amount of time. Generally, time can be equated with the number of operations needed. In counting operations, the application of a recursive formula would be considered an operation.   The terms of in are very easy to compute because of the closed form expression. No matter what term you decide to compute, only three operations need to be performed.  How to compute the terms of is not so clear. Suppose that you wanted to know . One approach would be to apply the definition recursively: The recursion equation for would be applied 100 times and 100 additions would then follow. To compute by this method, operations are needed. An iterative computation of is an improvement: Only additions are needed. This still isn't a good situation. As gets large, we take more and more time to compute . The formula is called a recurrence relation on . The process of finding a closed form expression for , one that requires no more than some fixed number of operations, is called solving the recurrence relation.  The determination of is a standard kind of problem in combinatorics. One solution is by way of a recurrence relation. In fact, many problems in combinatorics are most easily solved by first searching for a recurrence relation and then solving it. The following observation will suggest the recurrence relation that we need to determine . If , then every string of 0's and 1's with length and no two consecutive 0's is either or , where and are strings with no two consecutive 0's of length and respectively. From this observation we can see that for . The terms and are easy to determine by enumeration. Now, by iteration, any can be easily determined. For example, can be computed with five additions. A closed form expression for would be an improvement. Note that the recurrence relation for is identical to the one for . Only the basis is different.     Exercises   Prove by induction that  , is a closed form expression for the sequence in   Basis: , as defined.  Induction: Assume: for some .     The sequence of first differences of any sequence is the sequence . The sequence of second differences is the sequence of first differences of the first differences:   Consider sequence defined by , . Complete the table below and determine a recurrence relation that describes .   Let , . Complete the table below and determine a recurrence relation for .       The table below indicates that the recurrence relation that describes is , or rearranging the terms and changing the index, we get , with the basis .   The table below suggests the recurrence relation for is       Given lines ( ) on a plane such that no two lines are parallel and no three lines meet at the same point, let be the number of regions into which the lines divide the plane (including the infinite ones (see ). Describe how the recurrence relation can be derived. Given that , determine .   A general configuration of three lines   A general configuration of three lines    Imagine drawing line in one of the infinite regions that it passes through. That infinite region is divided into two infinite regions by line . As line is drawn through every one of the previous lines, you enter another region that line divides. Therefore regions are divided and the number of regions is increased by . From this observation we get .   A sample of a radioactive substance is expected to decay by 0.15 percent each hour. If  , is the weight of the sample hours into an experiment, write a recurrence relation for .     Let be the number of multiplications needed to evaluate an degree polynomial. Use the recursive definition of a polynomial expression to define recursively.   For greater than zero, , and .   Let be sequence of integers. Using short English sentences, not symbols, describe what the following propositions say about . Are the two propositions equivalent?          S is unbounded. For any real number, no matter how large, there will be a term of the sequence that will exceed that number.  S is unbounded and stays at an arbitrarily high level. For any real number, S will exceed that number at some point and stay above that level.  The second condition is stronger than the first. An example of a sequence for which the first condition is true but the second is not true is the sequence .    "
},
{
  "id": "def-sequence",
  "level": "2",
  "url": "s-Sequences.html#def-sequence",
  "type": "Definition",
  "number": "8.2.1",
  "title": "Sequence.",
  "body": "Sequence Sequence A sequence is a function from the natural numbers into some predetermined set. The image of any natural number can be written as or and is called the  term of . The variable is called the index or argument of the sequence. "
},
{
  "id": "ex-three-sequences",
  "level": "2",
  "url": "s-Sequences.html#ex-three-sequences",
  "type": "Example",
  "number": "8.2.2",
  "title": "Three sequences defined in different ways.",
  "body": "Three sequences defined in different ways   The sequence defined by , , is a sequence of integers.  The sequence defined recursively by and for is a sequence of integers. The terms of can be computed either by applying the recursion formula or by iteration. For example, or   Let be the number of strings of 0's and 1's of length having no consecutive zeros. These terms define a sequence of integers.   "
},
{
  "id": "exercises-8-2-2",
  "level": "2",
  "url": "s-Sequences.html#exercises-8-2-2",
  "type": "Exercise",
  "number": "8.2.3.1",
  "title": "",
  "body": " Prove by induction that  , is a closed form expression for the sequence in   Basis: , as defined.  Induction: Assume: for some .   "
},
{
  "id": "exercises-8-2-3",
  "level": "2",
  "url": "s-Sequences.html#exercises-8-2-3",
  "type": "Exercise",
  "number": "8.2.3.2",
  "title": "",
  "body": " The sequence of first differences of any sequence is the sequence . The sequence of second differences is the sequence of first differences of the first differences:   Consider sequence defined by , . Complete the table below and determine a recurrence relation that describes .   Let , . Complete the table below and determine a recurrence relation for .       The table below indicates that the recurrence relation that describes is , or rearranging the terms and changing the index, we get , with the basis .   The table below suggests the recurrence relation for is      "
},
{
  "id": "ex-lines-regions",
  "level": "2",
  "url": "s-Sequences.html#ex-lines-regions",
  "type": "Exercise",
  "number": "8.2.3.3",
  "title": "",
  "body": "Given lines ( ) on a plane such that no two lines are parallel and no three lines meet at the same point, let be the number of regions into which the lines divide the plane (including the infinite ones (see ). Describe how the recurrence relation can be derived. Given that , determine .   A general configuration of three lines   A general configuration of three lines    Imagine drawing line in one of the infinite regions that it passes through. That infinite region is divided into two infinite regions by line . As line is drawn through every one of the previous lines, you enter another region that line divides. Therefore regions are divided and the number of regions is increased by . From this observation we get .  "
},
{
  "id": "exercises-8-2-5",
  "level": "2",
  "url": "s-Sequences.html#exercises-8-2-5",
  "type": "Exercise",
  "number": "8.2.3.4",
  "title": "",
  "body": "A sample of a radioactive substance is expected to decay by 0.15 percent each hour. If  , is the weight of the sample hours into an experiment, write a recurrence relation for .    "
},
{
  "id": "exercises-8-2-6",
  "level": "2",
  "url": "s-Sequences.html#exercises-8-2-6",
  "type": "Exercise",
  "number": "8.2.3.5",
  "title": "",
  "body": "Let be the number of multiplications needed to evaluate an degree polynomial. Use the recursive definition of a polynomial expression to define recursively.   For greater than zero, , and .  "
},
{
  "id": "exercises-8-2-7",
  "level": "2",
  "url": "s-Sequences.html#exercises-8-2-7",
  "type": "Exercise",
  "number": "8.2.3.6",
  "title": "",
  "body": "Let be sequence of integers. Using short English sentences, not symbols, describe what the following propositions say about . Are the two propositions equivalent?          S is unbounded. For any real number, no matter how large, there will be a term of the sequence that will exceed that number.  S is unbounded and stays at an arbitrarily high level. For any real number, S will exceed that number at some point and stay above that level.  The second condition is stronger than the first. An example of a sequence for which the first condition is true but the second is not true is the sequence .  "
},
{
  "id": "s-recurrence-relations",
  "level": "1",
  "url": "s-recurrence-relations.html",
  "type": "Section",
  "number": "8.3",
  "title": "Recurrence Relations",
  "body": " Recurrence Relations   In this section we will begin our study of recurrence relations and their solutions. Our primary focus will be on the class of finite order linear recurrence relations with constant coefficients (shortened to finite order linear relations). First, we will examine closed form expressions from which these relations arise. Second, we will present an algorithm for solving them. In later sections we will consider some other common relations (8.4) and introduce two additional tools for studying recurrence relations: generating functions (8.5) and matrix methods (Chapter 12).   Definition and Terminology  Recurrence Relation Recurrence Relation Let be a sequence of numbers. A recurrence relation on is a formula that relates all but a finite number of terms of to previous terms of . That is, there is a in the domain of such that if , then is expressed in terms of some (and possibly all) of the terms that precede . If the domain of is , the terms are not defined by the recurrence formula. Their values are the initial conditions (or boundary conditions, or basis) that complete the definition of .  Some Examples of Recurrence Relations  The Fibonacci sequence is defined by the recurrence relation , with the initial conditions and . The recurrence relation is called a second-order relation because depends on the two previous terms of . Recall that the sequence in Section 8.2, , can be defined with the same recurrence relation, but with different initial conditions.  The relation is a third-order recurrence relation. If values of , , and are specified, then is completely defined.  The recurrence relation , , with has infinite order. To determine when is even, you must go back terms. Since grows unbounded with , no finite order can be given to .    Solving Recurrence Relations Recurrence Relations Solving  Sequences are often most easily defined with a recurrence relation; however, the calculation of terms by directly applying a recurrence relation can be time-consuming. The process of determining a closed form expression for the terms of a sequence from its recurrence relation is called solving the relation. There is no single technique or algorithm that can be used to solve all recurrence relations. In fact, some recurrence relations cannot be solved. The relation that defines above is one such example. Most of the recurrence relations that you are likely to encounter in the future are classified as finite order linear recurrence relations with constant coefficients. This class is the one that we will spend most of our time with in this chapter.  Order Linear Recurrence Relation  Order of a Recurrence Relation Let be a sequence of numbers with domain . An order linear recurrence relation on with constant coefficients is a recurrence relation that can be written in the form where are constants and is a numeric function that is defined for .  Note: We will shorten the name of this class of relations to order linear relations. Therefore, in further discussions, would not be considered a first-order linear relation.  Some Finite Order Linear Relations   The Fibonacci sequence is defined by the second-order linear relation because  The relation is a third-order linear relation. In this case, .  The relation can be written as . Therefore, it is a first-order linear relation.     Recurrence Relations Obtained from Solutions Recurrence Relations Obtained from Solutions  Before giving an algorithm for solving finite order linear relations, we will examine recurrence relations that arise from certain closed form expressions. The closed form expressions are selected so that we will obtain finite order linear relations from them. This approach may seem a bit contrived, but if you were to write down a few simple algebraic expressions, chances are that most of them would be similar to the ones we are about to examine.  For our first example, consider , defined by , . If , . Therefore, satisfies the first order linear relation and the initial condition serves as an initial condition for .  As a second example, consider , . Quite a bit more algebraic manipulation is required to get our result:          .   .       The recurrence relation that we have just obtained, defined for , together with the initial conditions and , define .  summarizes our results together with a few other examples that we will let the reader derive. Based on these results, we might conjecture that any closed form expression for a sequence that combines exponential expressions and polynomial expressions will be solutions of finite order linear relations. Not only is this true, but the converse is true: a finite order linear relation defines a closed form expression that is similar to the ones that were just examined. The only additional information that is needed is a set of initial conditions.   Recurrence Relations Obtained from Given Sequences  Closed Form Expression Recurrence Relation           Homogeneous Recurrence Relation Homogeneous Recurrence Relation. An order linear relation is homogeneous if for all . For each recurrence relation , the associated homogeneous relation is   First Order Homogeneous Recurrence Relations is a first-order homogeneous relation. Since it can also be written as , it should be no surprise that it arose from an expression that involves powers of 2. More generally, you would expect that the solution of would involve . Actually, the solution is , where the value of is given by the initial condition.  A Second Order Example  Consider the second-order homogeneous relation together with the initial conditions and . From our discussion above, we can predict that the solution to this relation involves terms of the form , where and are nonzero constants that must be determined. If the solution were to equal this quantity exactly, then Substitute these expressions into the recurrence relation to get Each term on the left-hand side of this equation has a factor of , which is nonzero. Dividing through by this common factor yields  Therefore, the only possible values of are 3 and 4. Equation is called the characteristic equation of the recurrence relation. The fact is that our original recurrence relation is true for any sequence of the form , where and are real numbers. This set of sequences is called the general solution of the recurrence relation. If we didn't have initial conditions for , we would stop here. The initial conditions make it possible for us to find definite values for and .   The solution of this set of simultaneous equations is and and so the solution is .  Characteristic Equation Characteristic Equation  Characteristic Roots The characteristic equation of the homogeneous order linear relation is the th degree polynomial equation The left-hand side of this equation is called the characteristic polynomial. The roots of the characteristic polynomial are called the characteristic roots of the equation.  Some characteristic equations   The characteristic equation of is .  The characteristic equation of is Note that the absence of a term means that there is not an term appearing in the characteristic equation.      Algorithm for Solving Homogeneous Finite Order Linear Relations     Write out the characteristic equation of the relation , which is .  Find all roots of the characteristic equation, the characteristic roots.  If there are distinct characteristic roots, , then the general solution of the recurrence relation is . If there are fewer than characteristic roots, then at least one root is a multiple root. If is a double root, then the term is replaced with In general, if is a root of multiplicity , then the term is replaced with .  If initial conditions are given, we get linear equations in unknowns (the from Step 3) by substitution. If possible, solve these equations to determine a final form for .      Although this algorithm is valid for all values of , there are limits to the size of for which the algorithm is feasible. Using just a pencil and paper, we can always solve second-order equations. The quadratic formula for the roots of is The solutions of are then   Although cubic and quartic formulas exist, they are too lengthy to introduce here. For this reason, the only higher-order relations ( ) that you could be expected to solve by hand are ones for which there is an easy factorization of the characteristic polynomial.  A solution using the algorithm Suppose that is defined by , with and . We can solve this recurrence relation with :   Note that we have written the recurrence relation in nonstandard form. To avoid errors in this easy step, you might consider a rearrangement of the equation to, in this case, . Therefore, the characteristic equation is .  The characteristic roots are and . These roots can be just as easily obtained by factoring the characteristic polynomial into .  The general solution of the recurrence relation is .    The simultaneous equations have the solution and . Therefore, .    Here is one rule that might come in handy: If the coefficients of the characteristic polynomial are all integers, with the constant term equal to , then the only possible rational characteristic roots are divisors of (both positive and negative).  With the aid of a computer (or possibly only a calculator), we can increase . Approximations of the characteristic roots can be obtained by any of several well-known methods, some of which are part of standard software packages. There is no general rule that specifies the values of for which numerical approximations will be feasible. The accuracy that you get will depend on the relation that you try to solve. (See Exercise 17 of this section.)  Solution of a Third Order Recurrence Relation Solve , where , , and .  The characteristic equation is .  The only rational roots that we can attempt are . By checking these, we obtain the three roots 1, 2, and .  The general solution is . The first term can simply be written .  You can solve this system by elimination to obtain , , and . Therefore,     Solution with a Double Characteristic Root Solve , where and .   Characteristic equation: .  . Therefore, there is a double characteristic root, 4.  General solution: .    Therefore .     Solution of Nonhomogeneous Finite Order Linear Relations Nonhomogeneous of Finite Order Linear Relations Solution  Our algorithm for nonhomogeneous relations will not be as complete as for the homogeneous case. This is due to the fact that different right-hand sides ( 's) call for different rules in obtaining a particular solution.   Algorithm for Solving Nonhomogeneous Finite Order Linear Relations   To solve the recurrence relation   Write the associated homogeneous relation and find its general solution (Steps (a) through (c) of ). Call this the homogeneous solution, .  Start to obtain what is called a particular solution, of the recurrence relation by taking an educated guess at the form of a particular solution. For a large class of right-hand sides, this is not really a guess, since the particular solution is often the same type of function as (see ).  Substitute your guess from Step 2 into the recurrence relation. If you made a good guess, you should be able to determine the unknown coefficients of your guess. If you made a wrong guess, it should be apparent from the result of this substitution, so go back to Step 2.  The general solution of the recurrence relation is the sum of the homogeneous and particular solutions. If no conditions are given, then you are finished. If initial conditions are given, they will translate to linear equations in unknowns and solve the system to get a complete solution.      Particular solutions for given right-hand sides   Right Hand Side, Form of Particular Solution,  Constant, Constant,  Linear Function, Linear Function,  degree polynomial, degree polynomial,  exponential function, exponential function,    Solution of a Nonhomogeneous First Order Recurrence Relation Solve , with .   The associated homogeneous relation, has the characteristic equation ; therefore, . The homogeneous solution is .  Since the right-hand side is a constant, we guess that the particular solution will be a constant, .  If we substitute into the recurrence relation, we get , or . Therefore, .  The general solution of the recurrence relation is The initial condition will give us one equation to solve in order to determine . Therefore, and .     Solution of a Nonhomogeneous Second Order Recurrence Relation Consider with and .   From , we know that . Caution:Don't apply the initial conditions to until you add !  Since the right-hand side is a linear polynomial, is linear; that is, .  Substitution into the recurrence relation yields:  Two polynomials are equal only if their coefficients are equal. Therefore,   Use the general solution and the initial conditions to get a final solution:    Therefore, .     A quick note on interest rates  When a quantity, such as a savings account balance, is increased by some fixed percent, it is most easily computed with a multiplier. In the case of an increase, the multiplier is 1.08 because any original amount , has added to it, so that the new balance is .  Another example is that if the interest rate is , the multiplier would be 1.035. This presumes that the interest is applied at the end of year for annual interest, often called simple interest . If the interest is applied monthly, and we assume a simplifed case where each month has the same length, the multiplier after every month would be . After a year passes, this multiplier would be applied 12 times, which is the same as multiplying by . That increase from 1.035 to 1.03557 is the effect of compound interest .   A Sort of Annuity Suppose you open a savings account that pays an annual interest rate of . In addition, suppose you decide to deposit one dollar when you open the account, and you intend to double your deposit each year. Let be your balance after years. can be described by the relation , with . If, instead of doubling the deposit each year, you deposited a constant amount, , the term would be replaced with . A sequence of regular deposits such as this is called a simple annuity.  Returning to the original situation,    should be of the form .  Therefore .     Therefore, .    Matching Roots Find the general solution to .   The characteristic roots of the associated homogeneous relation are and 4. Therefore, .  A function of the form will not be a particular solution of the nonhomogeneous relation since it solves the associated homogeneous relation. When the right-hand side involves an exponential function with a base that equals a characteristic root,you should multiply your guess at a particular solution by . Our guess at would then be . See for a more complete description of this rule.  Substitute into the recurrence relation for : Each term on the left-hand side has a factor of  Therefore, .  The general solution to the recurrence relation is      When the base of right-hand side is equal to a characteristic root  If the right-hand side of a nonhomogeneous relation involves an exponential with base , and is also a characteristic root of multiplicity , then multiply your guess at a particular solution as prescribed in by , where is the index of the sequence.  Examples of matching bases  If , the characteristic roots are 4 and 5. Since 5 matches the base of the right side, will take the form .  If the only characteristic root is 3, but it is a double root (multiplicity 2). Therefore, the form of the particular solution is .  If , the characteristic roots are and 4. The form of the particular solution will be .  If , the characteristic roots are 1 and 8. If the right-hand side is a polynomial, as it is in this case, then the exponential factor can be introduced. The particular solution will take the form .     We conclude this section with a comment on the situation in which the characteristic equation gives rise to complex roots. If we restrict the coefficients of our finite order linear relations to real numbers, or even to integers, we can still encounter characteristic equations whose roots are complex. Here, we will simply take the time to point out that our algorithms are still valid with complex characteristic roots, but the customary method for expressing the solutions of these relations is different. Since an understanding of these representations requires some background in complex numbers, we will simply suggest that an interested reader can refer to a more advanced treatment of recurrence relations (see also difference equations).    Exercises    Solve the following sets of recurrence relations and initial conditions:    , ,     , ,    ,    , ,       , ,            , ,     , ,     Find a closed form expression for in of Section 8.2.  The solution is . Notice that , which matches the value of computed with recursion.    Find a closed form expression for the terms of the Fibonacci sequence (see ).  The sequence was defined by = the number of strings of zeros and ones with length having no consecutive zeros ( (c)). Its recurrence relation is the same as that of the Fibonacci sequence. Determine a closed form expression for , .   The characteristic equation is , which has solutions and , It is useful to point out that and . The general solution is . Using the initial conditions, we obtain the system: and . The solution to this system is and   Therefore the final solution is     If , , then can be described with the recurrence relation . For each of the following sequences that are defined using a summation, find a closed form expression:    ,  ,  ,   ,               Let be the number of ways that the set , , can be partitioned into two nonempty subsets.   Find a recurrence relation for . (Hint: It will be a first-order linear relation.)  Solve the recurrence relation.   For each two-block partition of , there are two partitions we can create when we add , but there is one additional two-block partition to count for which one block is . Therefore,     If you were to deposit a certain amount of money at the end of each year for a number of years, this sequence of payments would be called an annuity (see ).  Find a closed form expression for the balance or value of an annuity that consists of payments of dollars at a rate of interest of . Note that for a normal annuity, the first payment is made after one year. However, assume that there is an initial balance . This will be convenient to apply our solution to the case of a loan.  With an interest rate of 5.5 percent, how much would you need to deposit into an annuity to have a value of one million dollars after 18 years?  The payment of a loan is a form of annuity in which the initial value is some negative amount (the amount of the loan) and the annuity ends when the value is raised to zero. How much could you borrow if you can afford to pay $5,000 per year for 25 years at 11 percent interest?     If is the balance after years, then the recurrence relation that we need to solve is with the initial condition . The solution can take several different forms, all algebraically equivalent. One form is  The amount that is needed is approximately $33,920 per year. In this case, the formula for the balance is . We want . Solving for gives the answer.  We can borrow approximately $42,109. In the case of a loan, we start with a negative balance, and solve to arrive at our answer.     Suppose that is a small positive number. Consider the recurrence relation , with initial conditions and . If is small enough, we might consider approximating the relation by replacing with 1 and with 0. Solve the original relation and its approximation. Let a be the solution of the approximation. Compare closed form expressions for and . Their forms are very different because the characteristic roots of the original relation were close together and the approximation resulted in one double characteristic root. If characteristic roots of a relation are relatively far apart, this problem will not occur. For example, compare the general solutions of and .     "
},
{
  "id": "def-recurrence-relation",
  "level": "2",
  "url": "s-recurrence-relations.html#def-recurrence-relation",
  "type": "Definition",
  "number": "8.3.1",
  "title": "Recurrence Relation.",
  "body": "Recurrence Relation Recurrence Relation Let be a sequence of numbers. A recurrence relation on is a formula that relates all but a finite number of terms of to previous terms of . That is, there is a in the domain of such that if , then is expressed in terms of some (and possibly all) of the terms that precede . If the domain of is , the terms are not defined by the recurrence formula. Their values are the initial conditions (or boundary conditions, or basis) that complete the definition of . "
},
{
  "id": "ex-some-recurrence-relations",
  "level": "2",
  "url": "s-recurrence-relations.html#ex-some-recurrence-relations",
  "type": "Example",
  "number": "8.3.2",
  "title": "Some Examples of Recurrence Relations.",
  "body": "Some Examples of Recurrence Relations  The Fibonacci sequence is defined by the recurrence relation , with the initial conditions and . The recurrence relation is called a second-order relation because depends on the two previous terms of . Recall that the sequence in Section 8.2, , can be defined with the same recurrence relation, but with different initial conditions.  The relation is a third-order recurrence relation. If values of , , and are specified, then is completely defined.  The recurrence relation , , with has infinite order. To determine when is even, you must go back terms. Since grows unbounded with , no finite order can be given to .  "
},
{
  "id": "def-n-th-order-rr",
  "level": "2",
  "url": "s-recurrence-relations.html#def-n-th-order-rr",
  "type": "Definition",
  "number": "8.3.3",
  "title": "<span xmlns:pf=\"https:\/\/prefigure.org\" class=\"process-math\">\\(n^{th}\\)<\/span> Order Linear Recurrence Relation.",
  "body": "Order Linear Recurrence Relation  Order of a Recurrence Relation Let be a sequence of numbers with domain . An order linear recurrence relation on with constant coefficients is a recurrence relation that can be written in the form where are constants and is a numeric function that is defined for . "
},
{
  "id": "ex-some-finite-order-rr",
  "level": "2",
  "url": "s-recurrence-relations.html#ex-some-finite-order-rr",
  "type": "Example",
  "number": "8.3.4",
  "title": "Some Finite Order Linear  Relations.",
  "body": "Some Finite Order Linear Relations   The Fibonacci sequence is defined by the second-order linear relation because  The relation is a third-order linear relation. In this case, .  The relation can be written as . Therefore, it is a first-order linear relation.   "
},
{
  "id": "sss-recurrence-relations-obtained-from-solutions-6",
  "level": "2",
  "url": "s-recurrence-relations.html#sss-recurrence-relations-obtained-from-solutions-6",
  "type": "Table",
  "number": "8.3.5",
  "title": "",
  "body": "        .   .      "
},
{
  "id": "table-reverse-solutions-rr",
  "level": "2",
  "url": "s-recurrence-relations.html#table-reverse-solutions-rr",
  "type": "Table",
  "number": "8.3.6",
  "title": "Recurrence Relations Obtained from Given Sequences",
  "body": " Recurrence Relations Obtained from Given Sequences  Closed Form Expression Recurrence Relation          "
},
{
  "id": "def-homogeneous-recurrence-relation",
  "level": "2",
  "url": "s-recurrence-relations.html#def-homogeneous-recurrence-relation",
  "type": "Definition",
  "number": "8.3.7",
  "title": "Homogeneous Recurrence Relation.",
  "body": "Homogeneous Recurrence Relation Homogeneous Recurrence Relation. An order linear relation is homogeneous if for all . For each recurrence relation , the associated homogeneous relation is  "
},
{
  "id": "ex-first-order-homogeneous-rr",
  "level": "2",
  "url": "s-recurrence-relations.html#ex-first-order-homogeneous-rr",
  "type": "Example",
  "number": "8.3.8",
  "title": "First Order Homogeneous Recurrence Relations.",
  "body": "First Order Homogeneous Recurrence Relations is a first-order homogeneous relation. Since it can also be written as , it should be no surprise that it arose from an expression that involves powers of 2. More generally, you would expect that the solution of would involve . Actually, the solution is , where the value of is given by the initial condition. "
},
{
  "id": "ex-second-order-rr",
  "level": "2",
  "url": "s-recurrence-relations.html#ex-second-order-rr",
  "type": "Example",
  "number": "8.3.9",
  "title": "A Second Order Example.",
  "body": "A Second Order Example  Consider the second-order homogeneous relation together with the initial conditions and . From our discussion above, we can predict that the solution to this relation involves terms of the form , where and are nonzero constants that must be determined. If the solution were to equal this quantity exactly, then Substitute these expressions into the recurrence relation to get Each term on the left-hand side of this equation has a factor of , which is nonzero. Dividing through by this common factor yields  Therefore, the only possible values of are 3 and 4. Equation is called the characteristic equation of the recurrence relation. The fact is that our original recurrence relation is true for any sequence of the form , where and are real numbers. This set of sequences is called the general solution of the recurrence relation. If we didn't have initial conditions for , we would stop here. The initial conditions make it possible for us to find definite values for and .   The solution of this set of simultaneous equations is and and so the solution is . "
},
{
  "id": "def-characteristic-equation",
  "level": "2",
  "url": "s-recurrence-relations.html#def-characteristic-equation",
  "type": "Definition",
  "number": "8.3.10",
  "title": "Characteristic Equation.",
  "body": "Characteristic Equation Characteristic Equation  Characteristic Roots The characteristic equation of the homogeneous order linear relation is the th degree polynomial equation The left-hand side of this equation is called the characteristic polynomial. The roots of the characteristic polynomial are called the characteristic roots of the equation. "
},
{
  "id": "ex-some-char-equations",
  "level": "2",
  "url": "s-recurrence-relations.html#ex-some-char-equations",
  "type": "Example",
  "number": "8.3.11",
  "title": "Some characteristic equations.",
  "body": "Some characteristic equations   The characteristic equation of is .  The characteristic equation of is Note that the absence of a term means that there is not an term appearing in the characteristic equation.    "
},
{
  "id": "algorithm-linear-homogeneous-recurrence-relations",
  "level": "2",
  "url": "s-recurrence-relations.html#algorithm-linear-homogeneous-recurrence-relations",
  "type": "Algorithm",
  "number": "8.3.12",
  "title": "Algorithm for Solving Homogeneous Finite Order Linear Relations.",
  "body": " Algorithm for Solving Homogeneous Finite Order Linear Relations     Write out the characteristic equation of the relation , which is .  Find all roots of the characteristic equation, the characteristic roots.  If there are distinct characteristic roots, , then the general solution of the recurrence relation is . If there are fewer than characteristic roots, then at least one root is a multiple root. If is a double root, then the term is replaced with In general, if is a root of multiplicity , then the term is replaced with .  If initial conditions are given, we get linear equations in unknowns (the from Step 3) by substitution. If possible, solve these equations to determine a final form for .     "
},
{
  "id": "ex-hrr-solution-example-1",
  "level": "2",
  "url": "s-recurrence-relations.html#ex-hrr-solution-example-1",
  "type": "Example",
  "number": "8.3.13",
  "title": "A solution using the algorithm.",
  "body": "A solution using the algorithm Suppose that is defined by , with and . We can solve this recurrence relation with :   Note that we have written the recurrence relation in nonstandard form. To avoid errors in this easy step, you might consider a rearrangement of the equation to, in this case, . Therefore, the characteristic equation is .  The characteristic roots are and . These roots can be just as easily obtained by factoring the characteristic polynomial into .  The general solution of the recurrence relation is .    The simultaneous equations have the solution and . Therefore, .   "
},
{
  "id": "ex-hrr-solution-example-2",
  "level": "2",
  "url": "s-recurrence-relations.html#ex-hrr-solution-example-2",
  "type": "Example",
  "number": "8.3.14",
  "title": "Solution of a Third Order Recurrence Relation.",
  "body": "Solution of a Third Order Recurrence Relation Solve , where , , and .  The characteristic equation is .  The only rational roots that we can attempt are . By checking these, we obtain the three roots 1, 2, and .  The general solution is . The first term can simply be written .  You can solve this system by elimination to obtain , , and . Therefore,    "
},
{
  "id": "ex-hrr-solution-example-3",
  "level": "2",
  "url": "s-recurrence-relations.html#ex-hrr-solution-example-3",
  "type": "Example",
  "number": "8.3.15",
  "title": "Solution with a Double Characteristic Root.",
  "body": "Solution with a Double Characteristic Root Solve , where and .   Characteristic equation: .  . Therefore, there is a double characteristic root, 4.  General solution: .    Therefore .   "
},
{
  "id": "algorithm-linear-nonhomogeneous-recurrence-relations",
  "level": "2",
  "url": "s-recurrence-relations.html#algorithm-linear-nonhomogeneous-recurrence-relations",
  "type": "Algorithm",
  "number": "8.3.16",
  "title": "Algorithm for Solving Nonhomogeneous Finite Order Linear Relations.",
  "body": " Algorithm for Solving Nonhomogeneous Finite Order Linear Relations   To solve the recurrence relation   Write the associated homogeneous relation and find its general solution (Steps (a) through (c) of ). Call this the homogeneous solution, .  Start to obtain what is called a particular solution, of the recurrence relation by taking an educated guess at the form of a particular solution. For a large class of right-hand sides, this is not really a guess, since the particular solution is often the same type of function as (see ).  Substitute your guess from Step 2 into the recurrence relation. If you made a good guess, you should be able to determine the unknown coefficients of your guess. If you made a wrong guess, it should be apparent from the result of this substitution, so go back to Step 2.  The general solution of the recurrence relation is the sum of the homogeneous and particular solutions. If no conditions are given, then you are finished. If initial conditions are given, they will translate to linear equations in unknowns and solve the system to get a complete solution.    "
},
{
  "id": "tab-particular-sols",
  "level": "2",
  "url": "s-recurrence-relations.html#tab-particular-sols",
  "type": "Table",
  "number": "8.3.17",
  "title": "Particular solutions for given right-hand sides",
  "body": " Particular solutions for given right-hand sides   Right Hand Side, Form of Particular Solution,  Constant, Constant,  Linear Function, Linear Function,  degree polynomial, degree polynomial,  exponential function, exponential function,   "
},
{
  "id": "ex-nhrr-solution-example-1",
  "level": "2",
  "url": "s-recurrence-relations.html#ex-nhrr-solution-example-1",
  "type": "Example",
  "number": "8.3.18",
  "title": "Solution of a Nonhomogeneous First Order Recurrence Relation.",
  "body": "Solution of a Nonhomogeneous First Order Recurrence Relation Solve , with .   The associated homogeneous relation, has the characteristic equation ; therefore, . The homogeneous solution is .  Since the right-hand side is a constant, we guess that the particular solution will be a constant, .  If we substitute into the recurrence relation, we get , or . Therefore, .  The general solution of the recurrence relation is The initial condition will give us one equation to solve in order to determine . Therefore, and .    "
},
{
  "id": "ex-nhrr-solution-example-2",
  "level": "2",
  "url": "s-recurrence-relations.html#ex-nhrr-solution-example-2",
  "type": "Example",
  "number": "8.3.19",
  "title": "Solution of a Nonhomogeneous Second Order Recurrence Relation.",
  "body": "Solution of a Nonhomogeneous Second Order Recurrence Relation Consider with and .   From , we know that . Caution:Don't apply the initial conditions to until you add !  Since the right-hand side is a linear polynomial, is linear; that is, .  Substitution into the recurrence relation yields:  Two polynomials are equal only if their coefficients are equal. Therefore,   Use the general solution and the initial conditions to get a final solution:    Therefore, .    "
},
{
  "id": "ss-solution-of-nonhomogeneous-relations-8",
  "level": "2",
  "url": "s-recurrence-relations.html#ss-solution-of-nonhomogeneous-relations-8",
  "type": "Note",
  "number": "8.3.20",
  "title": "A quick note on interest rates.",
  "body": "A quick note on interest rates  When a quantity, such as a savings account balance, is increased by some fixed percent, it is most easily computed with a multiplier. In the case of an increase, the multiplier is 1.08 because any original amount , has added to it, so that the new balance is .  Another example is that if the interest rate is , the multiplier would be 1.035. This presumes that the interest is applied at the end of year for annual interest, often called simple interest . If the interest is applied monthly, and we assume a simplifed case where each month has the same length, the multiplier after every month would be . After a year passes, this multiplier would be applied 12 times, which is the same as multiplying by . That increase from 1.035 to 1.03557 is the effect of compound interest .  "
},
{
  "id": "ex-a-novel-annuity",
  "level": "2",
  "url": "s-recurrence-relations.html#ex-a-novel-annuity",
  "type": "Example",
  "number": "8.3.21",
  "title": "A Sort of Annuity.",
  "body": "A Sort of Annuity Suppose you open a savings account that pays an annual interest rate of . In addition, suppose you decide to deposit one dollar when you open the account, and you intend to double your deposit each year. Let be your balance after years. can be described by the relation , with . If, instead of doubling the deposit each year, you deposited a constant amount, , the term would be replaced with . A sequence of regular deposits such as this is called a simple annuity.  Returning to the original situation,    should be of the form .  Therefore .     Therefore, .   "
},
{
  "id": "ex-matching-roots",
  "level": "2",
  "url": "s-recurrence-relations.html#ex-matching-roots",
  "type": "Example",
  "number": "8.3.22",
  "title": "Matching Roots.",
  "body": "Matching Roots Find the general solution to .   The characteristic roots of the associated homogeneous relation are and 4. Therefore, .  A function of the form will not be a particular solution of the nonhomogeneous relation since it solves the associated homogeneous relation. When the right-hand side involves an exponential function with a base that equals a characteristic root,you should multiply your guess at a particular solution by . Our guess at would then be . See for a more complete description of this rule.  Substitute into the recurrence relation for : Each term on the left-hand side has a factor of  Therefore, .  The general solution to the recurrence relation is     "
},
{
  "id": "obs-matching-base",
  "level": "2",
  "url": "s-recurrence-relations.html#obs-matching-base",
  "type": "Observation",
  "number": "8.3.23",
  "title": "When the base of right-hand side is equal to a characteristic root.",
  "body": "When the base of right-hand side is equal to a characteristic root  If the right-hand side of a nonhomogeneous relation involves an exponential with base , and is also a characteristic root of multiplicity , then multiply your guess at a particular solution as prescribed in by , where is the index of the sequence. "
},
{
  "id": "ex-base-match",
  "level": "2",
  "url": "s-recurrence-relations.html#ex-base-match",
  "type": "Example",
  "number": "8.3.24",
  "title": "Examples of matching bases.",
  "body": "Examples of matching bases  If , the characteristic roots are 4 and 5. Since 5 matches the base of the right side, will take the form .  If the only characteristic root is 3, but it is a double root (multiplicity 2). Therefore, the form of the particular solution is .  If , the characteristic roots are and 4. The form of the particular solution will be .  If , the characteristic roots are 1 and 8. If the right-hand side is a polynomial, as it is in this case, then the exponential factor can be introduced. The particular solution will take the form .    "
},
{
  "id": "exercises-8-3-2-2",
  "level": "2",
  "url": "s-recurrence-relations.html#exercises-8-3-2-2",
  "type": "Exercise",
  "number": "8.3.5.1",
  "title": "",
  "body": " , ,   "
},
{
  "id": "exercises-8-3-2-3",
  "level": "2",
  "url": "s-recurrence-relations.html#exercises-8-3-2-3",
  "type": "Exercise",
  "number": "8.3.5.2",
  "title": "",
  "body": " , ,   "
},
{
  "id": "exercises-8-3-2-4",
  "level": "2",
  "url": "s-recurrence-relations.html#exercises-8-3-2-4",
  "type": "Exercise",
  "number": "8.3.5.3",
  "title": "",
  "body": ",  "
},
{
  "id": "exercises-8-3-2-5",
  "level": "2",
  "url": "s-recurrence-relations.html#exercises-8-3-2-5",
  "type": "Exercise",
  "number": "8.3.5.4",
  "title": "",
  "body": " , ,   "
},
{
  "id": "exercises-8-3-2-6",
  "level": "2",
  "url": "s-recurrence-relations.html#exercises-8-3-2-6",
  "type": "Exercise",
  "number": "8.3.5.5",
  "title": "",
  "body": "  "
},
{
  "id": "exercises-8-3-2-7",
  "level": "2",
  "url": "s-recurrence-relations.html#exercises-8-3-2-7",
  "type": "Exercise",
  "number": "8.3.5.6",
  "title": "",
  "body": ", ,   "
},
{
  "id": "exercises-8-3-2-8",
  "level": "2",
  "url": "s-recurrence-relations.html#exercises-8-3-2-8",
  "type": "Exercise",
  "number": "8.3.5.7",
  "title": "",
  "body": " "
},
{
  "id": "exercises-8-3-2-9",
  "level": "2",
  "url": "s-recurrence-relations.html#exercises-8-3-2-9",
  "type": "Exercise",
  "number": "8.3.5.8",
  "title": "",
  "body": "  "
},
{
  "id": "exercises-8-3-2-10",
  "level": "2",
  "url": "s-recurrence-relations.html#exercises-8-3-2-10",
  "type": "Exercise",
  "number": "8.3.5.9",
  "title": "",
  "body": " "
},
{
  "id": "ex-annuity-sol",
  "level": "2",
  "url": "s-recurrence-relations.html#ex-annuity-sol",
  "type": "Exercise",
  "number": "8.3.5.10",
  "title": "",
  "body": " , ,   "
},
{
  "id": "exercises-8-3-2-12",
  "level": "2",
  "url": "s-recurrence-relations.html#exercises-8-3-2-12",
  "type": "Exercise",
  "number": "8.3.5.11",
  "title": "",
  "body": " , ,   "
},
{
  "id": "exercises-8-3-3",
  "level": "2",
  "url": "s-recurrence-relations.html#exercises-8-3-3",
  "type": "Exercise",
  "number": "8.3.5.12",
  "title": "",
  "body": "Find a closed form expression for in of Section 8.2.  The solution is . Notice that , which matches the value of computed with recursion.  "
},
{
  "id": "exercises-8-3-4",
  "level": "2",
  "url": "s-recurrence-relations.html#exercises-8-3-4",
  "type": "Exercise",
  "number": "8.3.5.13",
  "title": "",
  "body": " Find a closed form expression for the terms of the Fibonacci sequence (see ).  The sequence was defined by = the number of strings of zeros and ones with length having no consecutive zeros ( (c)). Its recurrence relation is the same as that of the Fibonacci sequence. Determine a closed form expression for , .   The characteristic equation is , which has solutions and , It is useful to point out that and . The general solution is . Using the initial conditions, we obtain the system: and . The solution to this system is and   Therefore the final solution is    "
},
{
  "id": "exercises-8-3-5",
  "level": "2",
  "url": "s-recurrence-relations.html#exercises-8-3-5",
  "type": "Exercise",
  "number": "8.3.5.14",
  "title": "",
  "body": "If , , then can be described with the recurrence relation . For each of the following sequences that are defined using a summation, find a closed form expression:    ,  ,  ,   ,              "
},
{
  "id": "exercises-8-3-6",
  "level": "2",
  "url": "s-recurrence-relations.html#exercises-8-3-6",
  "type": "Exercise",
  "number": "8.3.5.15",
  "title": "",
  "body": "Let be the number of ways that the set , , can be partitioned into two nonempty subsets.   Find a recurrence relation for . (Hint: It will be a first-order linear relation.)  Solve the recurrence relation.   For each two-block partition of , there are two partitions we can create when we add , but there is one additional two-block partition to count for which one block is . Therefore,    "
},
{
  "id": "exercises-8-3-7",
  "level": "2",
  "url": "s-recurrence-relations.html#exercises-8-3-7",
  "type": "Exercise",
  "number": "8.3.5.16",
  "title": "",
  "body": "If you were to deposit a certain amount of money at the end of each year for a number of years, this sequence of payments would be called an annuity (see ).  Find a closed form expression for the balance or value of an annuity that consists of payments of dollars at a rate of interest of . Note that for a normal annuity, the first payment is made after one year. However, assume that there is an initial balance . This will be convenient to apply our solution to the case of a loan.  With an interest rate of 5.5 percent, how much would you need to deposit into an annuity to have a value of one million dollars after 18 years?  The payment of a loan is a form of annuity in which the initial value is some negative amount (the amount of the loan) and the annuity ends when the value is raised to zero. How much could you borrow if you can afford to pay $5,000 per year for 25 years at 11 percent interest?     If is the balance after years, then the recurrence relation that we need to solve is with the initial condition . The solution can take several different forms, all algebraically equivalent. One form is  The amount that is needed is approximately $33,920 per year. In this case, the formula for the balance is . We want . Solving for gives the answer.  We can borrow approximately $42,109. In the case of a loan, we start with a negative balance, and solve to arrive at our answer.   "
},
{
  "id": "exercises-8-3-8",
  "level": "2",
  "url": "s-recurrence-relations.html#exercises-8-3-8",
  "type": "Exercise",
  "number": "8.3.5.17",
  "title": "",
  "body": " Suppose that is a small positive number. Consider the recurrence relation , with initial conditions and . If is small enough, we might consider approximating the relation by replacing with 1 and with 0. Solve the original relation and its approximation. Let a be the solution of the approximation. Compare closed form expressions for and . Their forms are very different because the characteristic roots of the original relation were close together and the approximation resulted in one double characteristic root. If characteristic roots of a relation are relatively far apart, this problem will not occur. For example, compare the general solutions of and .  "
},
{
  "id": "s-some-common-rrs",
  "level": "1",
  "url": "s-some-common-rrs.html",
  "type": "Section",
  "number": "8.4",
  "title": "Some Common Recurrence Relations",
  "body": " Some Common Recurrence Relations  In this section we intend to examine a variety of recurrence relations that are not finite-order linear with constant coefficients. For each part of this section, we will consider a concrete example, present a solution, and, if possible, examine a more general form of the original relation.  A First Basic Example  Consider the homogeneous first-order linear relation without constant coefficients, , , with initial condition . Upon close examination of this relation, we see that the th term is times the term, which is a property of factorial. is a solution of this relation, for if , In addition, since , the initial condition is satisfied. It should be pointed out that from a computational point of view, our solution really isn't much of an improvement since the exact calculation of takes multiplications.  If we examine a similar relation,  with , a table of values for suggests a possible solution: The exponent of 2 in is growing according to the relation with . Thus and . Note that could also be written as , for , but this is not a closed form expression.  In general, the relation for with , where is a function that is defined for all , has the solution  This product form of is not a closed form expression because as grows, the number of multiplications grow. Thus, it is really not a true solution. Often, as for above, a closed form expression can be derived from the product form.   An Analysis of the Binary Search Algorithm   Suppose you intend to use a binary search algorithm (see ) on lists of zero or more sorted items, and that the items are stored in an array, so that you have easy access to each item. A natural question to ask is How much time will it take to complete the search? When a question like this is asked, the time we refer to is often the so-called worst-case time. That is, if we were to search through items, what is the longest amount of time that we will need to complete the search? In order to make an analysis such as this independent of the computer to be used, time is measured by counting the number of steps that are executed. Each step (or sequence of steps) is assigned an absolute time, or weight; therefore, our answer will not be in seconds, but in absolute time units. If the steps in two different algorithms are assigned weights that are consistent, then analyses of the algorithms can be used to compare their relative efficiencies. There are two major steps that must be executed in a call of the binary search algorithm:   If the lower index is less than or equal to the upper index, then the middle of the list is located and its key is compared to the value that you are searching for.  In the worst case, the algorithm must be executed with a list that is roughly half as large as in the previous execution. If we assume that Step 1 takes one time unit and is the worst-case time for a list of items, then For simplicity, we will assume that even though the conditions of Step 1 must be evaluated as false if . You might wonder why is truncated in . If is odd, then for some , the middle of the list will be the item, and no matter what half of the list the search is directed to, the reduced list will have items. On the other hand, if is even, then for . The middle of the list will be the item, and the worst case will occur if we are directed to the items that come after the middle (the through items). Again the reduced list has items.   Solution to and . To determine , the easiest case is when is a power of two. If we compute , , by iteration, our results are The pattern that is established makes it clear that . This result would seem to indicate that every time you double the size of your list, the search time increases by only one unit.  A more complete solution can be obtained if we represent in binary form. For each , there exists a non-negative integer such that For example, if , ; therefore, . If satisfies (8.4.3), its binary representation requires digits. For example, = .  In general, . where . Note that in this form, is easy to describe: it is the digit binary number  Therefore, .  From the pattern that we've just established, reduces to . A formal inductive proof of this statement is possible. However, we expect that most readers would be satisfied with the argument above. Any skeptics are invited to provide the inductive proof.  For those who prefer to see a numeric example, suppose .   Our general conclusion is that the solution to and is that for , , where .  A less cumbersome statement of this fact is that . For example, .   Review of Logarithms Logarithms  Any discussion of logarithms must start by establishing a base, which can be any positive number other than 1. With the exception of , our base will be 2. We will see that the use of a different base (10 and are the other common ones) only has the effect of multiplying each logarithm by a constant. Therefore, the base that you use really isn't very important. Our choice of base 2 logarithms is convenient for the problems that we are considering.  Base 2 logarithm Logarithm, base 2 The base 2 logarithm of a positive number represents an exponent and is defined by the following equivalence for any positive real numbers . .   Plot of the logarithm, bases 2, function   Plot of the logarithm, bases 2, function    For example, because and because . A graph of the function in shows that if , the ; that is, when increases, also increases. However, if we move from to , only increases from 10 to 11. This slow rate of increase of the logarithm function is an important point to remember. An algorithm acting on pieces of data that can be executed in time units can handle significantly larger sets of data than an algorithm that can be executed in or time units. The graph of would show the same behavior.  A few more properties that we will use in subsequent discussions involving logarithms are summarized in the following theorem.  Fundamental Properties of Logarithms Logarithms Properties Let and be positive real numbers, and a real number.    Logarithms base Logarithm General Base  Logarithm, base of  If , , then for ,    How logarithms with different bases are related Let , . Then for all , . Therefore, if , base b logarithms can be computed from base 2 logarithms by dividing by the positive scaling factor . If , this scaling factor is negative.  By an analogue of , . Therefore, if we take the base 2 logarithm of both sides of this equality we get: Finally, divide both sides of the last equation by .   and .    Returning to the binary search algorithm, we can derive the final expression for using the properties of logarithms, including that the logarithm function is increasing so that inequalities are maintained when taking logarithms of numbers.     We can apply several of these properties of logarithms to get an alternate expression for :   If the time that was assigned to Step 1 of the binary search algorithm is changed, we wouldn't expect the form of the solution to be very different. If with , then .  A further generalization would be to add a coefficient to : with , where , and is not quite as simple to derive. First, if we consider values of that are powers of 2: If is not a power of 2, by reasoning that is identical to what we used to and , where .  The first term of this expression is a geometric sum, which can be written in closed form. Let be that sum: We've multiplied each term of by and aligned the identical terms in and . Now if we subtract the two equations, Therefore, .  A closed form expression for is     Analysis of Bubble Sort and Merge Sort Bubble Sort Merge Sort The efficiency of any search algorithm such as the binary search relies on fact that the search list is sorted according to a key value and that the search is based on the key value. There are several methods for sorting a list. One example is the bubble sort. You might be familiar with this one since it is a popular first sorting algorithm. A time analysis of the algorithm shows that if is the worst-case time needed to complete the bubble sort on items, then and . The solution of this relation is a quadratic function . The growth rate of a quadratic function such as this one is controlled by its squared term. Any other terms are dwarfed by it as gets large. For the bubble sort, this means that if we double the size of the list that we are to sort, changes to and so becomes . Therefore, the time needed to do a bubble sort is quadrupled. One alternative to bubble sort is the merge sort. Here is a simple version of this algorithm for sorting , . If , the list is sorted trivially. If then:   Divide into and .  Sort and using a merge sort.  Merge the sorted lists and into one sorted list. If the sort is to be done in descending order of key values, you continue to choose the higher key value from the fronts of and and place them in the back of .   Note that will always have items and will have items; thus, if is odd, gets one more item than . We will assume that the time required to perform Step 1 of the algorithm is insignificant compared to the other steps; therefore, we will assign a time value of zero to this step. Step 3 requires roughly comparisons and movements of items from and to ; thus, its time is proportional to . For this reason, we will assume that Step 3 takes time units. Since Step 2 requires time units, with the initial condition   Instead of an exact solution of these equations, we will be content with an estimate for . First, consider the case of , :   Thus, if is a power of 2, . Now if, for some , , then . This can be proved by induction on . As increases from to , increases from to and is slightly larger than . The discrepancy is small enough so that can be considered a solution of and for the purposes of comparing the merge sort with other algorithms. compares with for selected values of .   Comparison of Times for Bubble Sort and Merge Sort   n  10 45 34  50 1225 283  100 4950 665  500 124750 4483  1000 499500 9966     Derangements  Derangement  A derangement is a permutation on a set that has no fixed points . Here is a formal definition:  Derangement A derangement of a nonempty set is a permutation of (i.e., a bijection from into ) such that for all .  If , an interesting question might be How many derangements are there of ? We know that our answer is bounded above by . We can also expect our answer to be quite a bit smaller than since is the image of itself for of the permutations of .  Let be the number of derangements of . Our answer will come from discovering a recurrence relation on . Suppose that . If we are to construct a derangement of , , then . Thus, the image of can be selected in different ways. No matter which of the choices we make, we can complete the definition of in one of two ways. First, we can decide to make , leaving ways of completing the definition of , since will be a derangement of . Second, if we decide to select , each of the derangements of can be used to define . If is a derangement of such that , then define f by   Note that with our second construction of , , while in the first construction, . Therefore, no derangement of with can be constructed by both methods.  To recap our result, we see that is determined by first choosing one of images of and then constructing the remainder of in one of ways. Therefore,   This homogeneous second-order linear relation with variable coefficients, together with the initial conditions and , completely defines . Instead of deriving a solution of this relation by analytical methods, we will give an empirical derivation of an approximation of . Since the derangements of are drawn from a pool of permutations, we will see what percentage of these permutations are derangements by listing the values of , , and . The results we observe will indicate that as grows, hardly changes at all. If this quotient is computed to eight decimal places, for , . The reciprocal of this number, which seems to be tending toward, is, to eight places, 2.7182818. This number appears in so many places in mathematics that it has its own name, . An approximate solution of our recurrence relation on is then .     Exercises   Solve the following recurrence relations. Indicate whether your solution is an improvement over iteration.    , .   , .   , , .    !  , an improvement.   !, no improvement.    Prove that if , . (Hint: Consider the cases of odd and even separately.)   There are two cases, is either even or odd.  If , then   Alternatively, and    Solve as completely as possible:   , .  , .   , . (Hint: Write in octal form.)          Prove by induction that if , , and , , then .  Prove by induction on .  If , , then and , as desired.  Assume the equality is true for some and that , which implies that ,. Then    Use the substitution to solve for , with , , and .  The indicated substitution yields . Since , for all . Therefore .  Consider the sequence defined by for , with the conditions and for all . Use the substitution to solve for .   The with the suggested substitution, we have with . A closed form equation for is and so .   Solve as completely as possible:    , , .   , , .   A good approximation to the solution of this recurrence relation is based on the following observation: is a power of a power of two; that is, is , where , then . By applying this recurrence relation times we obtain . Going back to the original form of , or . We would expect that in general, is . We do not see any elementary method for arriving at an exact solution.  Suppose that is a positive integer with . Then can be written in binary form, with and is equal to the sum  . If , then we can estimate this sum to be between and . Therefore, .   Suppose Step 1 of the merge sort algorithm did take a significant amount of time. Assume it takes 0.1 time unit, independent of the value of .   Write out a new recurrence relation for that takes this factor into account.  Solve for , .  Assuming the solution for powers of 2 is a good estimate for all , compare your result to the solution in the text. As gets large, is there really much difference?      An argument can be made that the merging a pair of list with a total of items takes only comparisons, so we propose that recurrence , which is consistent with the initial condition that  If we apply the recurrence to the the first few values of we observe the pattern a   The result for powers of 2 suggests that in general would have and so the added time to split the lists is not significant for large values of .      "
},
{
  "id": "def-log-base-2",
  "level": "2",
  "url": "s-some-common-rrs.html#def-log-base-2",
  "type": "Definition",
  "number": "8.4.1",
  "title": "Base 2 logarithm.",
  "body": "Base 2 logarithm Logarithm, base 2 The base 2 logarithm of a positive number represents an exponent and is defined by the following equivalence for any positive real numbers . . "
},
{
  "id": "fig-log-2-plot",
  "level": "2",
  "url": "s-some-common-rrs.html#fig-log-2-plot",
  "type": "Figure",
  "number": "8.4.2",
  "title": "",
  "body": " Plot of the logarithm, bases 2, function   Plot of the logarithm, bases 2, function   "
},
{
  "id": "theorem-log-properties",
  "level": "2",
  "url": "s-some-common-rrs.html#theorem-log-properties",
  "type": "Theorem",
  "number": "8.4.3",
  "title": "Fundamental Properties of Logarithms.",
  "body": "Fundamental Properties of Logarithms Logarithms Properties Let and be positive real numbers, and a real number.   "
},
{
  "id": "def-logarithm-general-base",
  "level": "2",
  "url": "s-some-common-rrs.html#def-logarithm-general-base",
  "type": "Definition",
  "number": "8.4.4",
  "title": "Logarithms base <span xmlns:pf=\"https:\/\/prefigure.org\" class=\"process-math\">\\(b\\)<\/span>.",
  "body": "Logarithms base Logarithm General Base  Logarithm, base of  If , , then for ,  "
},
{
  "id": "theorem-logs-related",
  "level": "2",
  "url": "s-some-common-rrs.html#theorem-logs-related",
  "type": "Theorem",
  "number": "8.4.5",
  "title": "How logarithms with different bases are related.",
  "body": " How logarithms with different bases are related Let , . Then for all , . Therefore, if , base b logarithms can be computed from base 2 logarithms by dividing by the positive scaling factor . If , this scaling factor is negative.  By an analogue of , . Therefore, if we take the base 2 logarithm of both sides of this equality we get: Finally, divide both sides of the last equation by . "
},
{
  "id": "sss-review-of-logarithms-11",
  "level": "2",
  "url": "s-some-common-rrs.html#sss-review-of-logarithms-11",
  "type": "Note",
  "number": "8.4.6",
  "title": "",
  "body": " and . "
},
{
  "id": "table-sort-analysis",
  "level": "2",
  "url": "s-some-common-rrs.html#table-sort-analysis",
  "type": "Table",
  "number": "8.4.7",
  "title": "Comparison of Times for Bubble Sort and Merge Sort",
  "body": " Comparison of Times for Bubble Sort and Merge Sort   n  10 45 34  50 1225 283  100 4950 665  500 124750 4483  1000 499500 9966   "
},
{
  "id": "def-derangement",
  "level": "2",
  "url": "s-some-common-rrs.html#def-derangement",
  "type": "Definition",
  "number": "8.4.8",
  "title": "Derangement.",
  "body": "Derangement A derangement of a nonempty set is a permutation of (i.e., a bijection from into ) such that for all . "
},
{
  "id": "exercises-8-4-2",
  "level": "2",
  "url": "s-some-common-rrs.html#exercises-8-4-2",
  "type": "Exercise",
  "number": "8.4.5.1",
  "title": "",
  "body": " Solve the following recurrence relations. Indicate whether your solution is an improvement over iteration.    , .   , .   , , .    !  , an improvement.   !, no improvement.  "
},
{
  "id": "exercises-8-4-3",
  "level": "2",
  "url": "s-some-common-rrs.html#exercises-8-4-3",
  "type": "Exercise",
  "number": "8.4.5.2",
  "title": "",
  "body": " Prove that if , . (Hint: Consider the cases of odd and even separately.)   There are two cases, is either even or odd.  If , then   Alternatively, and   "
},
{
  "id": "exercises-8-4-4",
  "level": "2",
  "url": "s-some-common-rrs.html#exercises-8-4-4",
  "type": "Exercise",
  "number": "8.4.5.3",
  "title": "",
  "body": "Solve as completely as possible:   , .  , .   , . (Hint: Write in octal form.)         "
},
{
  "id": "exercises-8-4-5",
  "level": "2",
  "url": "s-some-common-rrs.html#exercises-8-4-5",
  "type": "Exercise",
  "number": "8.4.5.4",
  "title": "",
  "body": "Prove by induction that if , , and , , then .  Prove by induction on .  If , , then and , as desired.  Assume the equality is true for some and that , which implies that ,. Then   "
},
{
  "id": "exercises-8-4-6",
  "level": "2",
  "url": "s-some-common-rrs.html#exercises-8-4-6",
  "type": "Exercise",
  "number": "8.4.5.5",
  "title": "",
  "body": "Use the substitution to solve for , with , , and .  The indicated substitution yields . Since , for all . Therefore . "
},
{
  "id": "exercises-8-4-7",
  "level": "2",
  "url": "s-some-common-rrs.html#exercises-8-4-7",
  "type": "Exercise",
  "number": "8.4.5.6",
  "title": "",
  "body": "Consider the sequence defined by for , with the conditions and for all . Use the substitution to solve for .   The with the suggested substitution, we have with . A closed form equation for is and so .  "
},
{
  "id": "exercises-8-4-8",
  "level": "2",
  "url": "s-some-common-rrs.html#exercises-8-4-8",
  "type": "Exercise",
  "number": "8.4.5.7",
  "title": "",
  "body": "Solve as completely as possible:    , , .   , , .   A good approximation to the solution of this recurrence relation is based on the following observation: is a power of a power of two; that is, is , where , then . By applying this recurrence relation times we obtain . Going back to the original form of , or . We would expect that in general, is . We do not see any elementary method for arriving at an exact solution.  Suppose that is a positive integer with . Then can be written in binary form, with and is equal to the sum  . If , then we can estimate this sum to be between and . Therefore, .  "
},
{
  "id": "exercises-8-4-9",
  "level": "2",
  "url": "s-some-common-rrs.html#exercises-8-4-9",
  "type": "Exercise",
  "number": "8.4.5.8",
  "title": "",
  "body": "Suppose Step 1 of the merge sort algorithm did take a significant amount of time. Assume it takes 0.1 time unit, independent of the value of .   Write out a new recurrence relation for that takes this factor into account.  Solve for , .  Assuming the solution for powers of 2 is a good estimate for all , compare your result to the solution in the text. As gets large, is there really much difference?      An argument can be made that the merging a pair of list with a total of items takes only comparisons, so we propose that recurrence , which is consistent with the initial condition that  If we apply the recurrence to the the first few values of we observe the pattern a   The result for powers of 2 suggests that in general would have and so the added time to split the lists is not significant for large values of .    "
},
{
  "id": "s-generating-functions",
  "level": "1",
  "url": "s-generating-functions.html",
  "type": "Section",
  "number": "8.5",
  "title": "Generating Functions",
  "body": " Generating Functions  Generating Functions  This section contains an introduction to the topic of generating functions and how they are used to solve recurrence relations, among other problems. Methods that employ generating functions are based on the concept that you can take a problem involving sequences and translate it into a problem involving generating functions. Once you've solved the new problem, a translation back to sequences gives you a solution of the original problem.  This section covers:  The definition of a generating function.  Solution of a recurrence relation using generating functions to identify the skills needed to use generating functions.  An introduction and\/or review of the skills identified in point 2.  Some applications of generating functions.    Definition  Generating Function of a Sequence  Generating Function  The generating function of a sequence with terms , is the infinite sum The domain and codomain of generating functions will not be of any concern to us since we will only be performing algebraic operations on them.   First Examples   If , , then We can get a closed form expression for by observing that . Therefore, .  Finite sequences have generating functions. For example, the sequence of binomial coefficients , , , , has generating function by application of the binomial formula.  If , . Note that the index that is used in the summation has no significance. Also, note that the lower limit of the summation could start at 1 since .     Solution of a Recurrence Relation Using Generating Functions  We illustrate the use of generating functions by solving , , with and .   Translate the recurrence relation into an equation about generating functions.  Let , , with and . Therefore,   Solve for the generating function of the unknown sequence, .   Close examination of the three sums above shows:    since and .         Therefore,     Determine the sequence whose generating function is the one we got in Step 2.  For our example, we need to know one general fact about the closed form expression of an exponential sequence (a proof will be given later):   Now, in order to recognize in our example, we must write our closed form expression for as a sum of terms like above. Note that the denominator of can be factored: If you look at this last expression for closely, you can imagine how it could be the result of addition of two fractions, where and are two real numbers that must be determined. Starting on the right of , it should be clear that the sum, for any and , would look like the left-hand side. The process of finding values of and that make true is called the partial fractions decomposition of the left-hand side:   Therefore, and   We can apply to each term of :   is the generating function for  is the generating function for .   Therefore, .    From this example, we see that there are several skills that must be mastered in order to work with generating functions. You must be able to:   Manipulate summation expressions and their indices (in Step 2).  Solve algebraic equations and manipulate algebraic expressions, including partial function decompositions (Steps 2 and 3).  Identify sequences with their generating functions (Steps 1 and 3).   We will concentrate on the last skill first, a proficiency in the other skills is a product of doing as many exercises and reading as many examples as possible.  First, we will identify the operations on sequences and on generating functions.   Operations on Sequences  Operations on Sequences  Sequences Operations on  pop  push  Convolution of sequences and  Let and be sequences of numbers and let be a real number. Define the sum , the scalar product , the product , the convolution , the pop operation (read pop ), and the push operation (read push ) term-wise for by          If one imagines a sequence to be a matrix with one row and an infinite number of columns, and are exactly as in matrix addition and scalar multiplication. There is no obvious similarity between the other operations and matrix operations.  The pop and push operations can be understood by imagining a sequence to be an infinite stack of numbers with at the top, next, etc., as in a. The sequence is obtained by popping S(0) from the stack, leaving a stack as in b, with S(1) at the top, S(2) next, etc. The sequence is obtained by placing a zero at the top of the stack, resulting in a stack as in c. Keep these figures in mind when we discuss the pop and push operations.   Stack interpretation of pop and push operation   Stack interpretation of pop and push operation    Some Sequence Operations  If , , , and :                         Note that .  Multiple Pop and Push  Multiple Pop and Push  Multiple pop operation on  Multiple push operation on  If S is a sequence of numbers and a positive integer greater than 1, define Similarly, define   In general, and    Operations on Generating Functions Generating Functions Operations on  Operations on Generating Functions  Generating Functions Operations on  If and are generating functions and is a real number, then the sum , scalar product , product , and monomial product , are generating functions, where   The last sum is obtained by substituting for in the previous sum.    Some operations on generating functions If and then      Note: , and from .   Now we establish the connection between the operations on sequences and generating functions. Let and be sequences and let be a real number.  In words, says that the generating function of the sum of two sequences equals the sum of the generating functions of those sequences. Take the time to write out the other four identities in your own words. From the previous examples, these identities should be fairly obvious, with the possible exception of the last two. We will prove as part of the next theorem and leave the proof of to the interested reader. Note that there is no operation on generating functions that is related to sequence multiplication; that is, cannot be simplified.  Generating functions related to Pop and Push If ,     .    We prove (a) by induction and leave the proof of (b) to the reader.  Basis: Therefore, part (a) is true for .  Induction: Suppose that for some , the statement in part (a) is true: by the induction hypothesis. Now write in the last expression above as so that it fits into the finite summation:   Therefore the statement is true for .    Closed Form Expressions for Generating Functions Generating Functions Closed form expressions for  The most basic tool used to express generating functions in closed form is the closed form expression for the geometric series, which is an expression of the form . It can either be terminated or extended infinitely.  Finite Geometric Series:   Infinite Geometric Series:   Restrictions: and represent constants and the right sides of the two equations apply under the following conditions:   must not equal 1 in the finite case. Note that if .  In the infinite case, the absolute value of must be less than 1.   These restrictions don't come into play with generating functions. We could derive by noting that if , , then (See Exercise 10 of Section 8.3). An alternative derivation was used in Section 8.4. We will take the same steps to derive . Let . Then   Generating Functions involving Geometric Sums   If , , is an infinite geometric series with and .Therefore, .  If , 0, then .  If , then .  Let . Then .  Given a choice between the last form of and the previous sum of three fractions, we would prefer leaving it as a sum of three functions. As we saw in an earlier example, a partial fractions decomposition of a fraction such as the last expression requires some effort to produce.  If , then can be determined by multiplying the numerator and denominator by 1\/2 to obtain . We recognize this fraction as the sum of the infinite geometric series with and . Therefore .  If , then we expand to . Therefore ,  , , and, since there are no higher-powered terms, , . A more concise way of describing is , since is interpreted as 0 of .    lists some closed form expressions for the generating functions of some common sequences.   Closed Form Expressions of some Generating Functions    Sequence Generating Function           Another Complete Solution Solve , , with and . The solution will be derived using the same steps that were used earlier in this section, with one variation.   Translate to an equation about generating functions. First, we change the index of the recurrence relation by substituting for . The result is , . Now, if , then is the zero sequence, which has a zero generating function. Furthermore, . Therefore,  We want to now solve the following equation for : Multiply by : Expand and collect all terms involving on one side of the equation: Therefore,  Determine S from its generating function. thus a partial fraction decomposition of would be: Therefore, and . The solution of this set of equations is and . . In conclusion, since , .    An Application to Counting Let and let be the set of all strings of length zero or more that can be made using each of the elements of zero or more times. By the generalized rule of products, there are such strings that have length , , Suppose that is the set of strings of length with the property that all of the 's and 's precede all of the 's, 's, and 's. Thus , but . Let . A closed form expression for can be obtained by recognizing as the convolution of two sequences. To illustrate our point, we will consider the calculation of .  Note that if a string belongs to , it starts with characters from and is followed by characters from . Let be the number of strings of 's and 's with length and let be the number of strings of 's, 's, and 's with length . By the generalized rule of products, and . Among the strings in are the ones that start with two 's and 's and end with 's, 's, and 's. There are such strings. By the law of addition, Note that the sixth term of R is the sixth term of the convolution of with , . Think about the general situation for a while and it should be clear that . Now, our course of action will be to:   Determine the generating functions of and ,  Multiply and to obtain , and  Determine on the basis of .    , and    To recognize from , we must do a partial fractions decomposition: Therefore, and . The solution of this pair of equations is and . Since , which is the sum of the generating functions of and ,   For example, . Naturally, this equals the sum that we get from . To put this number in perspective, the total number of strings of length 6 with no restrictions is , and . Therefore approximately 13 percent of the strings of length 6 satisfy the conditions of the problem.     Extra for Experts  The remainder of this section is intended for readers who have had, or who intend to take, a course in combinatorics. We do not advise that it be included in a typical course. The method that was used in the previous example is a very powerful one and can be used to solve many problems in combinatorics. We close this section with a general description of the problems that can be solved in this way, followed by some examples.  Consider the situation in which , , , are actions that must be taken, each of which results in a well-defined outcome. For each define to be the set of possible outcomes of . We will assume that each outcome can be quantified in some way and that the quantification of the elements of is defined by the function . Thus, each outcome has a non-negative integer associated with it. Finally, define a frequency function such that is the number of elements of that have a quantification of .  Now, based on these assumptions, we can define the problems that can be solved. If a process is defined as a sequence of actions as above, and if the outcome of , which would be an element of , is quantified by then the frequency function, , for is the convolution of the frequency functions for , , , , which has a generating function equal to the product of the generating functions of the frequency functions , , , . That is,   Rolling Two Dice Suppose that you roll a die two times and add up the numbers on the top face for each roll. Since the faces on the die represent the integers 1 through 6, the sum must be between 2 and 12. How many ways can any one of these sums be obtained? Obviously, 2 can be obtained only one way, with two 1's. There are two sequences that yield a sum of 3: 1-2 and 2-1. To obtain all of the frequencies with which the numbers 2 through 12 can be obtained, we set up the situation as follows. For ; is the rolling of the die for the time. and is defined by . Since each number appears on a die exactly once, the frequency function is if , and otherwise. The process of rolling the die two times is quantified by adding up the ; that is, . The generating function for the frequency function of rolling the die two times is then   Now, to get , just read the coefficient of . For example, the coefficient of is 4, so there are four ways to roll a total of 5.  To apply this method, the crucial step is to decompose a large process in the proper way so that it fits into the general situation that we've described.  Distribution of a Committee Suppose that an organization is divided into three geographic sections, A, B, and C. Suppose that an executive committee of 11 members must be selected so that no more than 5 members from any one section are on the committee and that Sections A, B, and C must have minimums of 3, 2, and 2 members, respectively, on the committee. Looking only at the number of members from each section on the committee, how many ways can the committee be made up? One example of a valid committee would be 4 A's, 4 B's, and 3 C's.  Let be the action of deciding how many members (not who) from Section A will serve on the committee. and . The frequency function, , is defined by if , with otherwise. is then . Similarly, . Since the committee must have 11 members, our answer will be the coefficient of in , which is 10.      Exercises  What sequences have the following generating functions?   1                  What sequences have the following generating functions?                This one is a little trickier. If you divide into , you get In the final form we see that this is the generating function of the sum of two sequences. The first being and the second . So the zeroth term is and the th term is for .     Find closed form expressions for the generating functions of the following sequences:    , where for , with and .  The Fibonacci sequence: , , with .           Find closed form expressions for the generating functions of the following sequences:    for and for .   , where for , with and .   , where for , with .          .   . This can derived from the observation that is the zero generating function.     For each of the following expressions, find the partial fraction decomposition and identify the sequence having the expression as a generating function.                 Find the partial fraction decompositions and identify the sequence having the following expressions:              , which is the generating function for the sequence .   , which is the generating function for the sequence    , which is the generating function for the sequence     Given that and , what is the term of the generating function of each of the following sequences:                      Given that what is the the generating function of each of the following sequences:                A game is played by rolling a die five times. For the roll, one point is added to your score if you roll a number higher than . Otherwise, your score is zero for that roll. For example, the sequence of rolls gives you a total score of three; while a sequence of 1,2,3,4,5 gives you a score of zero. Of the possible sequences of rolls, how many give you a score of zero?, of one? of five?  Coefficients of through in     Suppose that you roll a die five times in a row and record the square of each number that you roll. How many ways could the sum of the squares of your rolls equal 40? What is the most common outcome?   There are 70 ways to score 40 points, because the coefficient of in . The most common outcome would be 67, which can be attained 170 ways. The minimum score is 5, which you get if you roll all 1's and the maximum is 180, which you get if you roll all 6's. Between 5 and 180, inclusive, there are 176 numbers of which 132 are possible scores.    "
},
{
  "id": "def-generating-function",
  "level": "2",
  "url": "s-generating-functions.html#def-generating-function",
  "type": "Definition",
  "number": "8.5.1",
  "title": "Generating Function of a Sequence.",
  "body": "Generating Function of a Sequence  Generating Function  The generating function of a sequence with terms , is the infinite sum The domain and codomain of generating functions will not be of any concern to us since we will only be performing algebraic operations on them.  "
},
{
  "id": "ex-first-gf-examples",
  "level": "2",
  "url": "s-generating-functions.html#ex-first-gf-examples",
  "type": "Example",
  "number": "8.5.2",
  "title": "First Examples.",
  "body": "First Examples   If , , then We can get a closed form expression for by observing that . Therefore, .  Finite sequences have generating functions. For example, the sequence of binomial coefficients , , , , has generating function by application of the binomial formula.  If , . Note that the index that is used in the summation has no significance. Also, note that the lower limit of the summation could start at 1 since .   "
},
{
  "id": "ss-solution-of-rr-using-generating-functions-3",
  "level": "2",
  "url": "s-generating-functions.html#ss-solution-of-rr-using-generating-functions-3",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "partial fractions decomposition "
},
{
  "id": "ops-on-sequences-2",
  "level": "2",
  "url": "s-generating-functions.html#ops-on-sequences-2",
  "type": "Definition",
  "number": "8.5.3",
  "title": "Operations on Sequences.",
  "body": "Operations on Sequences  Sequences Operations on  pop  push  Convolution of sequences and  Let and be sequences of numbers and let be a real number. Define the sum , the scalar product , the product , the convolution , the pop operation (read pop ), and the push operation (read push ) term-wise for by         "
},
{
  "id": "fig-pop-push",
  "level": "2",
  "url": "s-generating-functions.html#fig-pop-push",
  "type": "Figure",
  "number": "8.5.4",
  "title": "",
  "body": " Stack interpretation of pop and push operation   Stack interpretation of pop and push operation   "
},
{
  "id": "ex-some-sequence-operations",
  "level": "2",
  "url": "s-generating-functions.html#ex-some-sequence-operations",
  "type": "Example",
  "number": "8.5.5",
  "title": "Some Sequence Operations.",
  "body": "Some Sequence Operations  If , , , and :                        "
},
{
  "id": "def-multiple-pop-and-push",
  "level": "2",
  "url": "s-generating-functions.html#def-multiple-pop-and-push",
  "type": "Definition",
  "number": "8.5.6",
  "title": "Multiple Pop and Push.",
  "body": "Multiple Pop and Push  Multiple Pop and Push  Multiple pop operation on  Multiple push operation on  If S is a sequence of numbers and a positive integer greater than 1, define Similarly, define  "
},
{
  "id": "sss-operations-on-generating-functions-3",
  "level": "2",
  "url": "s-generating-functions.html#sss-operations-on-generating-functions-3",
  "type": "Definition",
  "number": "8.5.7",
  "title": "Operations on Generating Functions.",
  "body": "Operations on Generating Functions  Generating Functions Operations on  If and are generating functions and is a real number, then the sum , scalar product , product , and monomial product , are generating functions, where   The last sum is obtained by substituting for in the previous sum.   "
},
{
  "id": "ex-some-gf-operations",
  "level": "2",
  "url": "s-generating-functions.html#ex-some-gf-operations",
  "type": "Example",
  "number": "8.5.8",
  "title": "Some operations on generating functions.",
  "body": "Some operations on generating functions If and then      Note: , and from .  "
},
{
  "id": "gf-of-pop-push",
  "level": "2",
  "url": "s-generating-functions.html#gf-of-pop-push",
  "type": "Theorem",
  "number": "8.5.9",
  "title": "Generating functions related to Pop and Push.",
  "body": "Generating functions related to Pop and Push If ,     .    We prove (a) by induction and leave the proof of (b) to the reader.  Basis: Therefore, part (a) is true for .  Induction: Suppose that for some , the statement in part (a) is true: by the induction hypothesis. Now write in the last expression above as so that it fits into the finite summation:   Therefore the statement is true for .  "
},
{
  "id": "ex-geometric-sums",
  "level": "2",
  "url": "s-generating-functions.html#ex-geometric-sums",
  "type": "Example",
  "number": "8.5.10",
  "title": "Generating Functions involving Geometric Sums.",
  "body": "Generating Functions involving Geometric Sums   If , , is an infinite geometric series with and .Therefore, .  If , 0, then .  If , then .  Let . Then .  Given a choice between the last form of and the previous sum of three fractions, we would prefer leaving it as a sum of three functions. As we saw in an earlier example, a partial fractions decomposition of a fraction such as the last expression requires some effort to produce.  If , then can be determined by multiplying the numerator and denominator by 1\/2 to obtain . We recognize this fraction as the sum of the infinite geometric series with and . Therefore .  If , then we expand to . Therefore ,  , , and, since there are no higher-powered terms, , . A more concise way of describing is , since is interpreted as 0 of .   "
},
{
  "id": "table-gf-closed-form",
  "level": "2",
  "url": "s-generating-functions.html#table-gf-closed-form",
  "type": "Table",
  "number": "8.5.11",
  "title": "Closed Form Expressions of some Generating Functions",
  "body": " Closed Form Expressions of some Generating Functions    Sequence Generating Function          "
},
{
  "id": "ex-another-complete-solution",
  "level": "2",
  "url": "s-generating-functions.html#ex-another-complete-solution",
  "type": "Example",
  "number": "8.5.12",
  "title": "Another Complete Solution.",
  "body": "Another Complete Solution Solve , , with and . The solution will be derived using the same steps that were used earlier in this section, with one variation.   Translate to an equation about generating functions. First, we change the index of the recurrence relation by substituting for . The result is , . Now, if , then is the zero sequence, which has a zero generating function. Furthermore, . Therefore,  We want to now solve the following equation for : Multiply by : Expand and collect all terms involving on one side of the equation: Therefore,  Determine S from its generating function. thus a partial fraction decomposition of would be: Therefore, and . The solution of this set of equations is and . . In conclusion, since , .   "
},
{
  "id": "example-counting-application",
  "level": "2",
  "url": "s-generating-functions.html#example-counting-application",
  "type": "Example",
  "number": "8.5.13",
  "title": "An Application to Counting.",
  "body": "An Application to Counting Let and let be the set of all strings of length zero or more that can be made using each of the elements of zero or more times. By the generalized rule of products, there are such strings that have length , , Suppose that is the set of strings of length with the property that all of the 's and 's precede all of the 's, 's, and 's. Thus , but . Let . A closed form expression for can be obtained by recognizing as the convolution of two sequences. To illustrate our point, we will consider the calculation of .  Note that if a string belongs to , it starts with characters from and is followed by characters from . Let be the number of strings of 's and 's with length and let be the number of strings of 's, 's, and 's with length . By the generalized rule of products, and . Among the strings in are the ones that start with two 's and 's and end with 's, 's, and 's. There are such strings. By the law of addition, Note that the sixth term of R is the sixth term of the convolution of with , . Think about the general situation for a while and it should be clear that . Now, our course of action will be to:   Determine the generating functions of and ,  Multiply and to obtain , and  Determine on the basis of .    , and    To recognize from , we must do a partial fractions decomposition: Therefore, and . The solution of this pair of equations is and . Since , which is the sum of the generating functions of and ,   For example, . Naturally, this equals the sum that we get from . To put this number in perspective, the total number of strings of length 6 with no restrictions is , and . Therefore approximately 13 percent of the strings of length 6 satisfy the conditions of the problem.   "
},
{
  "id": "ex-dice-roll",
  "level": "2",
  "url": "s-generating-functions.html#ex-dice-roll",
  "type": "Example",
  "number": "8.5.14",
  "title": "Rolling Two Dice.",
  "body": "Rolling Two Dice Suppose that you roll a die two times and add up the numbers on the top face for each roll. Since the faces on the die represent the integers 1 through 6, the sum must be between 2 and 12. How many ways can any one of these sums be obtained? Obviously, 2 can be obtained only one way, with two 1's. There are two sequences that yield a sum of 3: 1-2 and 2-1. To obtain all of the frequencies with which the numbers 2 through 12 can be obtained, we set up the situation as follows. For ; is the rolling of the die for the time. and is defined by . Since each number appears on a die exactly once, the frequency function is if , and otherwise. The process of rolling the die two times is quantified by adding up the ; that is, . The generating function for the frequency function of rolling the die two times is then   Now, to get , just read the coefficient of . For example, the coefficient of is 4, so there are four ways to roll a total of 5.  To apply this method, the crucial step is to decompose a large process in the proper way so that it fits into the general situation that we've described. "
},
{
  "id": "ex-committee-distribution",
  "level": "2",
  "url": "s-generating-functions.html#ex-committee-distribution",
  "type": "Example",
  "number": "8.5.15",
  "title": "Distribution of a Committee.",
  "body": "Distribution of a Committee Suppose that an organization is divided into three geographic sections, A, B, and C. Suppose that an executive committee of 11 members must be selected so that no more than 5 members from any one section are on the committee and that Sections A, B, and C must have minimums of 3, 2, and 2 members, respectively, on the committee. Looking only at the number of members from each section on the committee, how many ways can the committee be made up? One example of a valid committee would be 4 A's, 4 B's, and 3 C's.  Let be the action of deciding how many members (not who) from Section A will serve on the committee. and . The frequency function, , is defined by if , with otherwise. is then . Similarly, . Since the committee must have 11 members, our answer will be the coefficient of in , which is 10.   "
},
{
  "id": "s-generating-functions-10-2",
  "level": "2",
  "url": "s-generating-functions.html#s-generating-functions-10-2",
  "type": "Exercise",
  "number": "8.5.7.1",
  "title": "",
  "body": "What sequences have the following generating functions?   1                 "
},
{
  "id": "s-generating-functions-10-3",
  "level": "2",
  "url": "s-generating-functions.html#s-generating-functions-10-3",
  "type": "Exercise",
  "number": "8.5.7.2",
  "title": "",
  "body": "What sequences have the following generating functions?                This one is a little trickier. If you divide into , you get In the final form we see that this is the generating function of the sum of two sequences. The first being and the second . So the zeroth term is and the th term is for .    "
},
{
  "id": "s-generating-functions-10-4",
  "level": "2",
  "url": "s-generating-functions.html#s-generating-functions-10-4",
  "type": "Exercise",
  "number": "8.5.7.3",
  "title": "",
  "body": "Find closed form expressions for the generating functions of the following sequences:    , where for , with and .  The Fibonacci sequence: , , with .          "
},
{
  "id": "s-generating-functions-10-5",
  "level": "2",
  "url": "s-generating-functions.html#s-generating-functions-10-5",
  "type": "Exercise",
  "number": "8.5.7.4",
  "title": "",
  "body": "Find closed form expressions for the generating functions of the following sequences:    for and for .   , where for , with and .   , where for , with .          .   . This can derived from the observation that is the zero generating function.    "
},
{
  "id": "s-generating-functions-10-6",
  "level": "2",
  "url": "s-generating-functions.html#s-generating-functions-10-6",
  "type": "Exercise",
  "number": "8.5.7.5",
  "title": "",
  "body": "For each of the following expressions, find the partial fraction decomposition and identify the sequence having the expression as a generating function.                "
},
{
  "id": "s-generating-functions-10-7",
  "level": "2",
  "url": "s-generating-functions.html#s-generating-functions-10-7",
  "type": "Exercise",
  "number": "8.5.7.6",
  "title": "",
  "body": "Find the partial fraction decompositions and identify the sequence having the following expressions:              , which is the generating function for the sequence .   , which is the generating function for the sequence    , which is the generating function for the sequence    "
},
{
  "id": "s-generating-functions-10-8",
  "level": "2",
  "url": "s-generating-functions.html#s-generating-functions-10-8",
  "type": "Exercise",
  "number": "8.5.7.7",
  "title": "",
  "body": "Given that and , what is the term of the generating function of each of the following sequences:                     "
},
{
  "id": "s-generating-functions-10-9",
  "level": "2",
  "url": "s-generating-functions.html#s-generating-functions-10-9",
  "type": "Exercise",
  "number": "8.5.7.8",
  "title": "",
  "body": "Given that what is the the generating function of each of the following sequences:               "
},
{
  "id": "s-generating-functions-10-10",
  "level": "2",
  "url": "s-generating-functions.html#s-generating-functions-10-10",
  "type": "Exercise",
  "number": "8.5.7.9",
  "title": "",
  "body": "A game is played by rolling a die five times. For the roll, one point is added to your score if you roll a number higher than . Otherwise, your score is zero for that roll. For example, the sequence of rolls gives you a total score of three; while a sequence of 1,2,3,4,5 gives you a score of zero. Of the possible sequences of rolls, how many give you a score of zero?, of one? of five?  Coefficients of through in    "
},
{
  "id": "s-generating-functions-10-11",
  "level": "2",
  "url": "s-generating-functions.html#s-generating-functions-10-11",
  "type": "Exercise",
  "number": "8.5.7.10",
  "title": "",
  "body": "Suppose that you roll a die five times in a row and record the square of each number that you roll. How many ways could the sum of the squares of your rolls equal 40? What is the most common outcome?   There are 70 ways to score 40 points, because the coefficient of in . The most common outcome would be 67, which can be attained 170 ways. The minimum score is 5, which you get if you roll all 1's and the maximum is 180, which you get if you roll all 6's. Between 5 and 180, inclusive, there are 176 numbers of which 132 are possible scores.  "
},
{
  "id": "s-graphs-introduction",
  "level": "1",
  "url": "s-graphs-introduction.html",
  "type": "Section",
  "number": "9.1",
  "title": "Graphs - General Introduction",
  "body": " Graphs - General Introduction  Definitions  Recall that we introduced directed graphs in Chapter 6 as a tool to visualize relations on a set. Here is a formal definition.  Simple Directed Graph  Graph Simple Directed Directed Graph Edges of a directed graph  A simple directed graph consists of a nonempty set of vertices , , and a set of edges , , that is a subset of the set .  Some Terminology and Comments Each edge is an ordered pair of elements from the vertex set. The first entry is the initial vertex of the edge and the second entry is the terminal vertex . Despite the set terminology in this definition, we often think of a graph as a picture, an aid in visualizing a situation. In Chapter 6, we introduced this concept to help understand relations on sets. Although those relations were principally of a mathematical nature, it remains true that when we see a graph, it tells us how the elements of a set are related to one another. We have chosen not to allow a graph with an empty vertex set, the so-called empty graph. There are both advantages and disadvantages to allowing the empty graph, so you may encounter it in other references.  A Simple Directed Graph is an example of a simple directed graph. In set terms, this graph is , where and . Note how each edge is labeled either 0 or 1. There are often reasons for labeling even simple graphs. Some labels are to help make a graph easier to discuss; others are more significant. We will discuss the significance of the labels on this graph later.   A directed graph   A directed graph     There are cases where the order of the vertices is not significant and so we use a different mathematical model for this situation:  Simple Undirected Graph  Simple Undirected Graph  Graph Simple Undirected Edges of an undirected graph  A simple undirected graph consists of a nonempty set , called a vertex set, and a set of two-element subsets of , called the edge set.  Henceforth, we will refer to simple undirected graphs as undirected graphs . When drawing an undirected graph, the two-element subsets are drawn as undirected lines or arcs connecting the vertices. It is customary to not allow self loops in undirected graphs since isn't a two element subset of vertices.  On Empty Graphs Empty Graph  It may occur to some readers that a graph could be empty, in the sense that it has empty vertex and edge sets. We might refer to this graph as the empty graph . However, there doesn't seem to be a universally agreed upon definition of an empty graph. In some works, a graph with any number of vertices and no edges is called an empty graph. To avoid this dilemma, we have defined both directed and undirected graphs to have nonempty vertex sets. For convenience, we've relaxed this rule in our definition of a and allowed for an empty binary tree.   An Undirected Graph A network of computers can be described easily using a graph. describes a network of five computers, , , , , and . An edge between any two vertices indicates that direct two-way communication is possible between the two computers. Note that the edges of this graph are not directed. This is due to the fact that the relation that is being displayed is symmetric (i.e., if can communicate with , then can communicate with ). Although directed edges could be used here, it would simply clutter the graph.    Communications Map   Trefoil image     Island Road Map     This undirected graph, in set terms, is and  There are several other situations for which this graph can serve as a model. One of them is to interpret the vertices as cities and the edges as roads, an abstraction of a map such as the one in . Another interpretation is as an abstraction of the floor plan of a house. See . Vertex represents the outside of the house; all others represent rooms. Two vertices are connected if there is a door between them.   Complete Undirected Graph Complete Undirected Graph. A complete undirected graph with vertices.  A complete undirected graph on vertices is an undirected graph with the property that each pair of distinct vertices are connected to one another. Such a graph is usually denoted by .  In certain cases there may be a need for more than one edge between two vertices, and we need to expand the class of directed graphs.  Multigraph  Multigraph  Graph Multigraph  A multigraph is a set of vertices with a set of edges that can contain more than one edge between the vertices.  One important point to keep in mind is that if we identify a graph as being a multigraph, it isn't necessary that there are two or more edges between some of the vertices. It is only just allowed . In other words, every simple graph is a multigraph. This is analogous to how a rectangle is a more general geometric figure than a square, but a square is still considered a rectangle.  A Multigraph A common occurrence of a multigraph is a road map. The cities and towns on the map can be thought of as vertices, while the roads are the edges. It is not uncommon to have more than one road connecting two cities. In order to give clear travel directions, we name or number roads so that there is no ambiguity. We use the same method to describe the edges of the multigraph in . There is no question what is; however, referring to the edge would be ambiguous.   A directed multigraph   A directed multigraph     A Labeled Graph A flowchart is a common example of a simple graph that requires labels for its vertices and some of its edges. is one such example that illustrates how many problems are solved.   A flow chart - an example of a labeled graph   A flow chart - an example of a labeled graph    At the start of the problem-solving process, we are at the vertex labeled Start and at the end (if we are lucky enough to have solved the problem) we will be at the vertex labeled End. The sequence of vertices that we pass through as we move from Start to End is called a path. The Start vertex is called the initial vertex of the path, while the End is called the final, or terminal, vertex. Suppose that the problem is solved after two attempts; then the path that was taken is . An alternate path description would be to list the edges that were used: . This second method of describing a path has the advantage of being applicable for multigraphs. On the graph in , the vertex list does not clearly describe a path between 1 and 3, but is unambiguous.   A Summary of Path Notation and Terminology  Path, in a graph  Circuit, in a graph  If and are two vertices of a graph, then a path between and describes a motion from to along edges of the graph. Vertex is called the initial vertex of the path and is called the terminal vertex. A path between and can always be described by its edge list, the list of edges that were used: , where: (1) the initial vertex of is ; (2) the terminal vertex of is the initial vertex of , ; and (3) the terminal vertex of is . The number of edges in the edge list is the path length . A path on a simple graph can also be described by a vertex list. A path of length will have a list of vertices , , , where, for , is an edge on the graph. A circuit is a path that terminates at its initial vertex.  Suppose that a path between two vertices has an edge list . A subpath of this graph is any portion of the path described by one or more consecutive edges in the edge list. For example, is a subpath of . Any path is its own subpath; however, we call it an improper subpath of itself. All other nonempty subpaths are called proper subpaths.  A path or circuit is simple if it contains no proper subpath that is a circuit. This is the same as saying that a path or circuit is simple if it does not visit any vertex more than once except for the common initial and terminal vertex in the circuit. In the problem-solving method described in , the path that you take is simple only if you reach a solution on the first try.    Subgraphs  Intuitively, you could probably predict what the term subgraph means. A graph contained within a graph, right? But since a graph involves two sets, vertices and edges, does it involve a subset of both of these sets, or just one of them? The answer is it could be either. There are different types of subgraphs. The two that we will define below will meet most of our future needs in discussing the theory of graphs.  Subgraph  Subgraph  Induced Subgraph  Spanning Subgraph  Let be a graph of any kind: directed, directed multigraph, or undirected. is a subgraph of if , and only if and the vertices of are in . You create a subgraph of by removing zero or more vertices and all edges that include the removed vertices and then you possibly remove some other edges. If the only removed edges are those that include the removed vertices, then we say that is an induced subgraph . Finally, is a spanning subgraph of if , or, in other words, no vertices are removed from , only edges.  Some subgraphs  Consider the graph, , in the top left of . The other three graphs in that figure are all subgraphs of . The graph in the top right was created by first removing vertex 5 and all edges connecting it. In addition, we have removed the edge . That removed edge disqualifies the graph from being an induced subgraph. The graphs in the bottom left and right are both spanning subgraphs. The one on the bottom right is a tree, and is referred to as a spanning subtree. Spanning subtrees will be discussed in the chapter on .   A few subgraphs   A few subgraphs      One set of subgraphs of any graph is the connected components of a graph. For simplicity, we will define them for undirected graphs. Given a graph , consider the relation is connected to on . We interpret this relation so that each vertex is connected to itself, and any two distinct vertices are related if there is a path along edges of the graph from one to the other. It shouldn't be too difficult to convince yourself that this is an equivalence relation on .  Connected Component  Connected Component  Given a graph , let be the relation is connected to on . Then the connected components of are the induced subgraphs of each with a vertex set that is an equivalence class with respect to .  If you ignore the duplicate names of vertices in the four graphs of , and consider the whole figure as one large graph, then there are four connected components in that graph. It's as simple as that! It's harder to describe precisely than to understand the concept.   From the examples we've seen so far, we can see that although a graph can be defined, in short, as a collection of vertices and edges, an integral part of most graphs is the labeling of the vertices and edges that allows us to interpret the graph as a model for some situation. We continue with a few more examples to illustrate this point.  A Graph as a Model for a Set of Strings Suppose that you would like to mechanically describe the set of strings of 0's and 1's having no consecutive 1's. One way to visualize a string of this kind is with the graph in . Consider any path starting at vertex . If the label on each graph is considered to be the output to a printer, then the output will have no consecutive 1's. For example, the path that is described by the vertex list would result in an output of . Conversely, any string with no consecutive 1's determines a path starting at s.  A Tournament Graph Suppose that four teams compete in a round-robin sporting event; that is, each team meets every other team once, and each game is played until a winner is determined. If the teams are named A, B, C, and D, we can define the relation on the set of teams by if beat . For one set of results, the graph of might look like .   Round-robin tournament graph with four vertices   Round-robin tournament graph with four vertices     There are many types of tournaments and they all can be modeled by different types of graphs.  Tournament Graph Tournament Graph   A tournament graph is a directed graph with the property that no edge connects a vertex to itself, and between any two vertices there is at most one edge.  A complete (or round-robin) tournament graph is a tournament graph with the property that between any two distinct vertices there is exactly one edge.  A single-elimination tournament graph is a tournament graph with the properties that: (i) one vertex (the champion) has no edge terminating at it and at least one edge initiating from it; (ii) every other vertex is the terminal vertex of exactly one edge; and (iii) there is a path from the champion vertex to every other vertex.    Graph of a Single Elimination Tournament The major league baseball championship is decided with a single-elimination tournament, where each game is actually a series of games. From 1969 to 1994, the two divisional champions in the American League (East and West) competed in a series of games. The loser is eliminated and the winner competed against the winner of the National League series (which is decided as in the American League). The tournament graph of the 1983 championship is in   A single elimination tournament graph   A single elimination tournament graph      Graph Isomorphisms  Next, we establish the relation is isomorphic to, a form of equality on graphs. The graphs in obviously share some similarities, such as the number of vertices and the number of edges. It happens that they are even more similar than just that. If the letters , , , and in the left graph are replaced with the numbers 1,3,4, and 2, respectively, and the vertices are moved around so that they have the same position as the graph on the right, you get the graph on the right.   Isomorphic Graphs   Isomorphic Graphs    Here is a more precise definition that reflects the fact that the actual positioning (or embedding) of vertices isn't an essential part of a graph.  Isomorphic Graphs Isomorphic Graphs Two graphs and are isomorphic if there exists a bijection such that if and only if . For multigraphs, we add that the number of edges connecting to must equal the number of edges from to .  The most significant local characteristic of a vertex within a graph is its degree. Collectively, the degrees can partially characterize a graph.  Degree of a vertex Degree degree, indegree and outdegree of vertex   Let be a vertex of an undirected graph. The degree of , denoted , is the number of edges that connect to the other vertices in the graph.  If is a vertex of a directed graph, then the outdegree of , denoted , is the number of edges of the graph that initiate at . The indegree of , denoted , is the number of edges that terminate at .    Degree Sequence of a Graph Degree Sequence of a Graph  The degree sequence of a simple undirected graph is the non-increasing sequence of its vertex degrees.  Some degrees   An undirected graph      The degrees of vertices 1 through 5 in are 2, 3, 4, 1, and 2, respectively. The degree sequence of the graph is .  In a tournament graph, is the number of wins for and is the number of losses. In a complete (round-robin) tournament graph with vertices, for each vertex.    Graphic Sequence Graphic Sequence  A finite nonincreasing sequence of integers is graphic if there exists a simple undirected graph with vertices having the sequence as its degree sequence.  For example, is graphic because the degrees of the graph in match these numbers. There is no connection between the vertex number and its degree in this graph.   A graph that shows that is a graphic sequence.   A graph that shows that is a graphic sequence.    See for more details on what are also referred to as graphical degree sequences , including an algorithm for determining whether or not a sequence is graphic.   Next Steps  A Prospectus for the Rest of the Chapter The question Once you have a graph, what do you do with it? might come to mind. The following list of common questions and comments about graphs is a partial list that will give you an overview of the remainder of the chapter.   How can a graph be represented as a data structure for use on a computer? We will discuss some common data structures that are used to represent graphs in Section 9.2.  Given two vertices in a graph, does there exist a path between them? The existence of a path between any or all pairs of vertices in a graph will be discussed in Section 9.3. A related question is: How many paths of a certain type or length are there between two vertices?  Is there a path (or circuit) that passes through every vertex (or uses every edge) exactly once? Paths of this kind are called traversals. We will discuss traversals in Section 9.4.  Suppose that a cost is associated with the use of each vertex and\/or edge in a path. What is the cheapest path, circuit, or traversal of a given kind? Problems of this kind will be discussed in Section 9.5.  Given the specifications of a graph, or the graph itself, what is the best way to draw the graph? The desire for neatness alone makes this a reasonable question, but there are other motivations. Another goal might be to avoid having edges of the graph cross one another. This is discussed in Section 9.6.      Exercises  What is the significance of the fact that there is a path connecting vertex with every other vertex in , as it applies to various situations that it models?  In , computer can communicate with all other computers. In , there are direct roads to and from city to all other cities.  Draw a graph similar to that represents the set of strings of 0's and 1's containing no more than two consecutive 1's in any part of the string.   Solution to exercise 3 of Section 9.1   A directed graph with four vertices labeled start, zero, one and two. The edges (One,Zero),(Start,Zero),(Two,Zero) and (Zero,Zero) labeled 0; and edges (One,Two),(Start,One) and(Zero,One) labeled 1.    Besides the start vertex, you need three vertices to capture the states of having most recently recorded zero, one or two consecutive 1's.   Draw a directed graph that models the set of strings of 0's and 1's (zero or more of each) where all of the 1's must appear consecutively.   Solution to exercise 3 of Section 9.1   Solution to exercise 3 of Section 9.1     In the NCAA final-four basketball tournament, the East champion plays the West champion, and the champions from the Mideast and Midwest play. The winners of the two games play for the national championship. How many different single-elimination tournament graphs could occur?  There are eight different tournament graphs that could occur. This follows from the rule of products. There are two ways the edge connecting East and West can be directed. The same for the edge connecting Mideast and Midwest. The for each of the pair of outcomes for these two games, there are two orientations in the national championship game.   What is the maximum number of edges in an undirected graph with eight vertices? The maximum number of edges would be .  Which of the graphs in are isomorphic? What is the correspondence between their vertices?   Which graphs are isomorphic to one another?   Graph for exercise 6 of section 9.1       (a) and (f) are isomorphic  (d) and (e) are isomorphic  (c) and (g) are isomorphic  (h) is not isomorphic to any others. It is the only graph with two vertices of degree three.       How many edges does a complete tournament graph with vertices have?  How many edges does a single-elimination tournament graph with vertices have?      , each vertex except the champion vertex has an indegree of 1 and the champion vertex has an indegree of zero.    Draw complete undirected graphs with 1, 2, 3, 4, 5 and 6 vertices. How many edges does a , a complete undirected graph with vertices, have?   The first three complete graphs   Solution to exercise 8 of Section 9.1     The next three complete graphs   Solution to exercise 8 of Section 9.1    In general, a complete graph with vertices has edges   Determine whether the following sequences are graphic. Explain your logic.                  Not graphic - if the degree of a graph with seven vertices is 6, it is connected to all other vertices and so there cannot be a vertex with degree zero.  Graphic. One graph with this degree sequence is a cycle of length 6.  Not Graphic. The number of vertices with odd degree is odd, which is impossible.  Graphic. A \"wheel graph\" with one vertex connected to all other and the others connected to one another in a cycle has this degree sequence.  Graphic. Pairs of vertices connected only to one another.  Not Graphic. With two vertices having maximal degree, 5, every vertex would need to have a degree of 2 or more, so the 1 in this sequence makes it non-graphic.       Based on observations you might have made in exercise 9, describe as many characteristics as you can about graphic sequences of length .  Consider the two graphs in . Notice that they have the same degree sequences, . Explain why the two graphs are not isomorphic.    Two graphs with the same degree sequences   Two graphs with the same degree sequences     A graphic sequence of length is the degree sequence of at least one graph containing vertices, and the sum of entries in the degree sequence is two times the number of edges.  The two graphs in the second part are not isomorphic for several reasons, some of which we haven't formally defined such as the fact that the graph on the left is not connected and the one on the right is connected. Using just the definition of a graph isomorphism. Assume we can map the left graph to the right graph with an isomorphism, . Let be any vertex on the left. Then is some vertex on the right. If we call the two vertices that are connected to the left and , then and must be the two vertices that are connected to , but notice that while and are connected with and edge, and are not connected, which contradicts the definition of a graph isomorphism.   Draw a plan for the rooms of a house so that models connectedness of the rooms. That is, is an edge if and only if a door connects rooms and .  How many subgraphs are there of a , . How many of them are spanning graphs? Assume the vertices are distinguishable. For example, if and we remove one edge from the , we count three possible subgraphs depending on which edge is removed even though all three are isomorphic and would not be different if the vertices were indistinguishable.  The number of spanning subgraphs is raised to the since every edge can either be included or not included in a spanning subgraph. The number of subgraphs is more complicated. We can count the number of subgraphs containing vertices by observing that there are subsets of the vertices that we can include. Then for each of them there are spanning subgraphs of the vertices. Therefore, our answer is Notice that the lower limit of the sum is because we can have a graph with no vertices. This sum doesn't seem to simplify. See The On-Line Encyclopedia of Integer Sequences for more information about this sequence.    "
},
{
  "id": "def-simple-directed-graph",
  "level": "2",
  "url": "s-graphs-introduction.html#def-simple-directed-graph",
  "type": "Definition",
  "number": "9.1.1",
  "title": "Simple Directed Graph.",
  "body": "Simple Directed Graph  Graph Simple Directed Directed Graph Edges of a directed graph  A simple directed graph consists of a nonempty set of vertices , , and a set of edges , , that is a subset of the set . "
},
{
  "id": "s-graphs-introduction-2-4",
  "level": "2",
  "url": "s-graphs-introduction.html#s-graphs-introduction-2-4",
  "type": "Note",
  "number": "9.1.2",
  "title": "Some Terminology and Comments.",
  "body": "Some Terminology and Comments Each edge is an ordered pair of elements from the vertex set. The first entry is the initial vertex of the edge and the second entry is the terminal vertex . Despite the set terminology in this definition, we often think of a graph as a picture, an aid in visualizing a situation. In Chapter 6, we introduced this concept to help understand relations on sets. Although those relations were principally of a mathematical nature, it remains true that when we see a graph, it tells us how the elements of a set are related to one another. We have chosen not to allow a graph with an empty vertex set, the so-called empty graph. There are both advantages and disadvantages to allowing the empty graph, so you may encounter it in other references. "
},
{
  "id": "ex-9-1",
  "level": "2",
  "url": "s-graphs-introduction.html#ex-9-1",
  "type": "Example",
  "number": "9.1.3",
  "title": "A Simple Directed Graph.",
  "body": "A Simple Directed Graph is an example of a simple directed graph. In set terms, this graph is , where and . Note how each edge is labeled either 0 or 1. There are often reasons for labeling even simple graphs. Some labels are to help make a graph easier to discuss; others are more significant. We will discuss the significance of the labels on this graph later.   A directed graph   A directed graph    "
},
{
  "id": "def-undirected-graph",
  "level": "2",
  "url": "s-graphs-introduction.html#def-undirected-graph",
  "type": "Definition",
  "number": "9.1.5",
  "title": "Simple Undirected Graph.",
  "body": "Simple Undirected Graph  Simple Undirected Graph  Graph Simple Undirected Edges of an undirected graph  A simple undirected graph consists of a nonempty set , called a vertex set, and a set of two-element subsets of , called the edge set. "
},
{
  "id": "s-graphs-introduction-2-9",
  "level": "2",
  "url": "s-graphs-introduction.html#s-graphs-introduction-2-9",
  "type": "Note",
  "number": "9.1.6",
  "title": "On Empty Graphs.",
  "body": "On Empty Graphs Empty Graph  It may occur to some readers that a graph could be empty, in the sense that it has empty vertex and edge sets. We might refer to this graph as the empty graph . However, there doesn't seem to be a universally agreed upon definition of an empty graph. In some works, a graph with any number of vertices and no edges is called an empty graph. To avoid this dilemma, we have defined both directed and undirected graphs to have nonempty vertex sets. For convenience, we've relaxed this rule in our definition of a and allowed for an empty binary tree.  "
},
{
  "id": "ex-undirected-1",
  "level": "2",
  "url": "s-graphs-introduction.html#ex-undirected-1",
  "type": "Example",
  "number": "9.1.7",
  "title": "An Undirected Graph.",
  "body": "An Undirected Graph A network of computers can be described easily using a graph. describes a network of five computers, , , , , and . An edge between any two vertices indicates that direct two-way communication is possible between the two computers. Note that the edges of this graph are not directed. This is due to the fact that the relation that is being displayed is symmetric (i.e., if can communicate with , then can communicate with ). Although directed edges could be used here, it would simply clutter the graph.    Communications Map   Trefoil image     Island Road Map     This undirected graph, in set terms, is and  There are several other situations for which this graph can serve as a model. One of them is to interpret the vertices as cities and the edges as roads, an abstraction of a map such as the one in . Another interpretation is as an abstraction of the floor plan of a house. See . Vertex represents the outside of the house; all others represent rooms. Two vertices are connected if there is a door between them.  "
},
{
  "id": "def-complete-undirected-graph",
  "level": "2",
  "url": "s-graphs-introduction.html#def-complete-undirected-graph",
  "type": "Definition",
  "number": "9.1.10",
  "title": "Complete Undirected Graph.",
  "body": "Complete Undirected Graph Complete Undirected Graph. A complete undirected graph with vertices.  A complete undirected graph on vertices is an undirected graph with the property that each pair of distinct vertices are connected to one another. Such a graph is usually denoted by . "
},
{
  "id": "def-multigraph",
  "level": "2",
  "url": "s-graphs-introduction.html#def-multigraph",
  "type": "Definition",
  "number": "9.1.11",
  "title": "Multigraph.",
  "body": "Multigraph  Multigraph  Graph Multigraph  A multigraph is a set of vertices with a set of edges that can contain more than one edge between the vertices. "
},
{
  "id": "ex-multigraph-9-1",
  "level": "2",
  "url": "s-graphs-introduction.html#ex-multigraph-9-1",
  "type": "Example",
  "number": "9.1.12",
  "title": "A Multigraph.",
  "body": "A Multigraph A common occurrence of a multigraph is a road map. The cities and towns on the map can be thought of as vertices, while the roads are the edges. It is not uncommon to have more than one road connecting two cities. In order to give clear travel directions, we name or number roads so that there is no ambiguity. We use the same method to describe the edges of the multigraph in . There is no question what is; however, referring to the edge would be ambiguous.   A directed multigraph   A directed multigraph    "
},
{
  "id": "ex-labeled-graph-9-1",
  "level": "2",
  "url": "s-graphs-introduction.html#ex-labeled-graph-9-1",
  "type": "Example",
  "number": "9.1.14",
  "title": "A Labeled Graph.",
  "body": "A Labeled Graph A flowchart is a common example of a simple graph that requires labels for its vertices and some of its edges. is one such example that illustrates how many problems are solved.   A flow chart - an example of a labeled graph   A flow chart - an example of a labeled graph    At the start of the problem-solving process, we are at the vertex labeled Start and at the end (if we are lucky enough to have solved the problem) we will be at the vertex labeled End. The sequence of vertices that we pass through as we move from Start to End is called a path. The Start vertex is called the initial vertex of the path, while the End is called the final, or terminal, vertex. Suppose that the problem is solved after two attempts; then the path that was taken is . An alternate path description would be to list the edges that were used: . This second method of describing a path has the advantage of being applicable for multigraphs. On the graph in , the vertex list does not clearly describe a path between 1 and 3, but is unambiguous.  "
},
{
  "id": "s-graphs-introduction-2-17",
  "level": "2",
  "url": "s-graphs-introduction.html#s-graphs-introduction-2-17",
  "type": "Note",
  "number": "9.1.16",
  "title": "A Summary of Path Notation and Terminology.",
  "body": "A Summary of Path Notation and Terminology  Path, in a graph  Circuit, in a graph  If and are two vertices of a graph, then a path between and describes a motion from to along edges of the graph. Vertex is called the initial vertex of the path and is called the terminal vertex. A path between and can always be described by its edge list, the list of edges that were used: , where: (1) the initial vertex of is ; (2) the terminal vertex of is the initial vertex of , ; and (3) the terminal vertex of is . The number of edges in the edge list is the path length . A path on a simple graph can also be described by a vertex list. A path of length will have a list of vertices , , , where, for , is an edge on the graph. A circuit is a path that terminates at its initial vertex.  Suppose that a path between two vertices has an edge list . A subpath of this graph is any portion of the path described by one or more consecutive edges in the edge list. For example, is a subpath of . Any path is its own subpath; however, we call it an improper subpath of itself. All other nonempty subpaths are called proper subpaths.  A path or circuit is simple if it contains no proper subpath that is a circuit. This is the same as saying that a path or circuit is simple if it does not visit any vertex more than once except for the common initial and terminal vertex in the circuit. In the problem-solving method described in , the path that you take is simple only if you reach a solution on the first try.  "
},
{
  "id": "def-subgraph",
  "level": "2",
  "url": "s-graphs-introduction.html#def-subgraph",
  "type": "Definition",
  "number": "9.1.17",
  "title": "Subgraph.",
  "body": "Subgraph  Subgraph  Induced Subgraph  Spanning Subgraph  Let be a graph of any kind: directed, directed multigraph, or undirected. is a subgraph of if , and only if and the vertices of are in . You create a subgraph of by removing zero or more vertices and all edges that include the removed vertices and then you possibly remove some other edges. If the only removed edges are those that include the removed vertices, then we say that is an induced subgraph . Finally, is a spanning subgraph of if , or, in other words, no vertices are removed from , only edges. "
},
{
  "id": "ex-subgraphs",
  "level": "2",
  "url": "s-graphs-introduction.html#ex-subgraphs",
  "type": "Example",
  "number": "9.1.18",
  "title": "Some subgraphs.",
  "body": "Some subgraphs  Consider the graph, , in the top left of . The other three graphs in that figure are all subgraphs of . The graph in the top right was created by first removing vertex 5 and all edges connecting it. In addition, we have removed the edge . That removed edge disqualifies the graph from being an induced subgraph. The graphs in the bottom left and right are both spanning subgraphs. The one on the bottom right is a tree, and is referred to as a spanning subtree. Spanning subtrees will be discussed in the chapter on .   A few subgraphs   A few subgraphs     "
},
{
  "id": "def-connected-component",
  "level": "2",
  "url": "s-graphs-introduction.html#def-connected-component",
  "type": "Definition",
  "number": "9.1.20",
  "title": "Connected Component.",
  "body": "Connected Component  Connected Component  Given a graph , let be the relation is connected to on . Then the connected components of are the induced subgraphs of each with a vertex set that is an equivalence class with respect to . "
},
{
  "id": "ex-connected-components",
  "level": "2",
  "url": "s-graphs-introduction.html#ex-connected-components",
  "type": "Example",
  "number": "9.1.21",
  "title": "",
  "body": "If you ignore the duplicate names of vertices in the four graphs of , and consider the whole figure as one large graph, then there are four connected components in that graph. It's as simple as that! It's harder to describe precisely than to understand the concept.  "
},
{
  "id": "ex-string-model-9-1",
  "level": "2",
  "url": "s-graphs-introduction.html#ex-string-model-9-1",
  "type": "Example",
  "number": "9.1.22",
  "title": "A Graph as a Model for a Set of Strings.",
  "body": "A Graph as a Model for a Set of Strings Suppose that you would like to mechanically describe the set of strings of 0's and 1's having no consecutive 1's. One way to visualize a string of this kind is with the graph in . Consider any path starting at vertex . If the label on each graph is considered to be the output to a printer, then the output will have no consecutive 1's. For example, the path that is described by the vertex list would result in an output of . Conversely, any string with no consecutive 1's determines a path starting at s. "
},
{
  "id": "ex-tournament-graph-9-1",
  "level": "2",
  "url": "s-graphs-introduction.html#ex-tournament-graph-9-1",
  "type": "Example",
  "number": "9.1.23",
  "title": "A Tournament Graph.",
  "body": "A Tournament Graph Suppose that four teams compete in a round-robin sporting event; that is, each team meets every other team once, and each game is played until a winner is determined. If the teams are named A, B, C, and D, we can define the relation on the set of teams by if beat . For one set of results, the graph of might look like .   Round-robin tournament graph with four vertices   Round-robin tournament graph with four vertices    "
},
{
  "id": "def-tournament-graph",
  "level": "2",
  "url": "s-graphs-introduction.html#def-tournament-graph",
  "type": "Definition",
  "number": "9.1.25",
  "title": "Tournament Graph.",
  "body": "Tournament Graph Tournament Graph   A tournament graph is a directed graph with the property that no edge connects a vertex to itself, and between any two vertices there is at most one edge.  A complete (or round-robin) tournament graph is a tournament graph with the property that between any two distinct vertices there is exactly one edge.  A single-elimination tournament graph is a tournament graph with the properties that: (i) one vertex (the champion) has no edge terminating at it and at least one edge initiating from it; (ii) every other vertex is the terminal vertex of exactly one edge; and (iii) there is a path from the champion vertex to every other vertex.   "
},
{
  "id": "ex-single-elimination-9-1",
  "level": "2",
  "url": "s-graphs-introduction.html#ex-single-elimination-9-1",
  "type": "Example",
  "number": "9.1.26",
  "title": "Graph of a Single Elimination Tournament.",
  "body": "Graph of a Single Elimination Tournament The major league baseball championship is decided with a single-elimination tournament, where each game is actually a series of games. From 1969 to 1994, the two divisional champions in the American League (East and West) competed in a series of games. The loser is eliminated and the winner competed against the winner of the National League series (which is decided as in the American League). The tournament graph of the 1983 championship is in   A single elimination tournament graph   A single elimination tournament graph    "
},
{
  "id": "fig-isomorphic-graphs-9-1",
  "level": "2",
  "url": "s-graphs-introduction.html#fig-isomorphic-graphs-9-1",
  "type": "Figure",
  "number": "9.1.28",
  "title": "",
  "body": " Isomorphic Graphs   Isomorphic Graphs   "
},
{
  "id": "def-isomorphic-graphs",
  "level": "2",
  "url": "s-graphs-introduction.html#def-isomorphic-graphs",
  "type": "Definition",
  "number": "9.1.29",
  "title": "Isomorphic Graphs.",
  "body": "Isomorphic Graphs Isomorphic Graphs Two graphs and are isomorphic if there exists a bijection such that if and only if . For multigraphs, we add that the number of edges connecting to must equal the number of edges from to . "
},
{
  "id": "def-degree-of-a-vertex",
  "level": "2",
  "url": "s-graphs-introduction.html#def-degree-of-a-vertex",
  "type": "Definition",
  "number": "9.1.30",
  "title": "Degree of a vertex.",
  "body": "Degree of a vertex Degree degree, indegree and outdegree of vertex   Let be a vertex of an undirected graph. The degree of , denoted , is the number of edges that connect to the other vertices in the graph.  If is a vertex of a directed graph, then the outdegree of , denoted , is the number of edges of the graph that initiate at . The indegree of , denoted , is the number of edges that terminate at .   "
},
{
  "id": "def-degree-sequence",
  "level": "2",
  "url": "s-graphs-introduction.html#def-degree-sequence",
  "type": "Definition",
  "number": "9.1.31",
  "title": "Degree Sequence of a Graph.",
  "body": "Degree Sequence of a Graph Degree Sequence of a Graph  The degree sequence of a simple undirected graph is the non-increasing sequence of its vertex degrees. "
},
{
  "id": "ex-degrees-9-1",
  "level": "2",
  "url": "s-graphs-introduction.html#ex-degrees-9-1",
  "type": "Example",
  "number": "9.1.32",
  "title": "Some degrees.",
  "body": "Some degrees   An undirected graph      The degrees of vertices 1 through 5 in are 2, 3, 4, 1, and 2, respectively. The degree sequence of the graph is .  In a tournament graph, is the number of wins for and is the number of losses. In a complete (round-robin) tournament graph with vertices, for each vertex.   "
},
{
  "id": "def-graphic-sequence",
  "level": "2",
  "url": "s-graphs-introduction.html#def-graphic-sequence",
  "type": "Definition",
  "number": "9.1.34",
  "title": "Graphic Sequence.",
  "body": "Graphic Sequence Graphic Sequence  A finite nonincreasing sequence of integers is graphic if there exists a simple undirected graph with vertices having the sequence as its degree sequence. "
},
{
  "id": "fig-degree-sequence-example",
  "level": "2",
  "url": "s-graphs-introduction.html#fig-degree-sequence-example",
  "type": "Figure",
  "number": "9.1.35",
  "title": "",
  "body": " A graph that shows that is a graphic sequence.   A graph that shows that is a graphic sequence.   "
},
{
  "id": "s-graphs-introduction-4-13",
  "level": "2",
  "url": "s-graphs-introduction.html#s-graphs-introduction-4-13",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "graphical degree sequences "
},
{
  "id": "list-graph-prospectus",
  "level": "2",
  "url": "s-graphs-introduction.html#list-graph-prospectus",
  "type": "List",
  "number": "9.1.36",
  "title": "A Prospectus for the Rest of the Chapter",
  "body": "A Prospectus for the Rest of the Chapter The question Once you have a graph, what do you do with it? might come to mind. The following list of common questions and comments about graphs is a partial list that will give you an overview of the remainder of the chapter.   How can a graph be represented as a data structure for use on a computer? We will discuss some common data structures that are used to represent graphs in Section 9.2.  Given two vertices in a graph, does there exist a path between them? The existence of a path between any or all pairs of vertices in a graph will be discussed in Section 9.3. A related question is: How many paths of a certain type or length are there between two vertices?  Is there a path (or circuit) that passes through every vertex (or uses every edge) exactly once? Paths of this kind are called traversals. We will discuss traversals in Section 9.4.  Suppose that a cost is associated with the use of each vertex and\/or edge in a path. What is the cheapest path, circuit, or traversal of a given kind? Problems of this kind will be discussed in Section 9.5.  Given the specifications of a graph, or the graph itself, what is the best way to draw the graph? The desire for neatness alone makes this a reasonable question, but there are other motivations. Another goal might be to avoid having edges of the graph cross one another. This is discussed in Section 9.6.   "
},
{
  "id": "exercises-9-1-2",
  "level": "2",
  "url": "s-graphs-introduction.html#exercises-9-1-2",
  "type": "Exercise",
  "number": "9.1.5.1",
  "title": "",
  "body": "What is the significance of the fact that there is a path connecting vertex with every other vertex in , as it applies to various situations that it models?  In , computer can communicate with all other computers. In , there are direct roads to and from city to all other cities. "
},
{
  "id": "exercises-9-1-3",
  "level": "2",
  "url": "s-graphs-introduction.html#exercises-9-1-3",
  "type": "Exercise",
  "number": "9.1.5.2",
  "title": "",
  "body": "Draw a graph similar to that represents the set of strings of 0's and 1's containing no more than two consecutive 1's in any part of the string.   Solution to exercise 3 of Section 9.1   A directed graph with four vertices labeled start, zero, one and two. The edges (One,Zero),(Start,Zero),(Two,Zero) and (Zero,Zero) labeled 0; and edges (One,Two),(Start,One) and(Zero,One) labeled 1.    Besides the start vertex, you need three vertices to capture the states of having most recently recorded zero, one or two consecutive 1's.  "
},
{
  "id": "exercises-9-1-4",
  "level": "2",
  "url": "s-graphs-introduction.html#exercises-9-1-4",
  "type": "Exercise",
  "number": "9.1.5.3",
  "title": "",
  "body": "Draw a directed graph that models the set of strings of 0's and 1's (zero or more of each) where all of the 1's must appear consecutively.   Solution to exercise 3 of Section 9.1   Solution to exercise 3 of Section 9.1    "
},
{
  "id": "exercises-9-1-5",
  "level": "2",
  "url": "s-graphs-introduction.html#exercises-9-1-5",
  "type": "Exercise",
  "number": "9.1.5.4",
  "title": "",
  "body": "In the NCAA final-four basketball tournament, the East champion plays the West champion, and the champions from the Mideast and Midwest play. The winners of the two games play for the national championship. How many different single-elimination tournament graphs could occur?  There are eight different tournament graphs that could occur. This follows from the rule of products. There are two ways the edge connecting East and West can be directed. The same for the edge connecting Mideast and Midwest. The for each of the pair of outcomes for these two games, there are two orientations in the national championship game.  "
},
{
  "id": "exercises-9-1-6",
  "level": "2",
  "url": "s-graphs-introduction.html#exercises-9-1-6",
  "type": "Exercise",
  "number": "9.1.5.5",
  "title": "",
  "body": "What is the maximum number of edges in an undirected graph with eight vertices? The maximum number of edges would be . "
},
{
  "id": "exercises-9-1-7",
  "level": "2",
  "url": "s-graphs-introduction.html#exercises-9-1-7",
  "type": "Exercise",
  "number": "9.1.5.6",
  "title": "",
  "body": "Which of the graphs in are isomorphic? What is the correspondence between their vertices?   Which graphs are isomorphic to one another?   Graph for exercise 6 of section 9.1       (a) and (f) are isomorphic  (d) and (e) are isomorphic  (c) and (g) are isomorphic  (h) is not isomorphic to any others. It is the only graph with two vertices of degree three.    "
},
{
  "id": "exercises-9-1-8",
  "level": "2",
  "url": "s-graphs-introduction.html#exercises-9-1-8",
  "type": "Exercise",
  "number": "9.1.5.7",
  "title": "",
  "body": "  How many edges does a complete tournament graph with vertices have?  How many edges does a single-elimination tournament graph with vertices have?      , each vertex except the champion vertex has an indegree of 1 and the champion vertex has an indegree of zero.   "
},
{
  "id": "exercises-9-1-9",
  "level": "2",
  "url": "s-graphs-introduction.html#exercises-9-1-9",
  "type": "Exercise",
  "number": "9.1.5.8",
  "title": "",
  "body": "Draw complete undirected graphs with 1, 2, 3, 4, 5 and 6 vertices. How many edges does a , a complete undirected graph with vertices, have?   The first three complete graphs   Solution to exercise 8 of Section 9.1     The next three complete graphs   Solution to exercise 8 of Section 9.1    In general, a complete graph with vertices has edges  "
},
{
  "id": "exercises-9-1-10",
  "level": "2",
  "url": "s-graphs-introduction.html#exercises-9-1-10",
  "type": "Exercise",
  "number": "9.1.5.9",
  "title": "",
  "body": "Determine whether the following sequences are graphic. Explain your logic.                  Not graphic - if the degree of a graph with seven vertices is 6, it is connected to all other vertices and so there cannot be a vertex with degree zero.  Graphic. One graph with this degree sequence is a cycle of length 6.  Not Graphic. The number of vertices with odd degree is odd, which is impossible.  Graphic. A \"wheel graph\" with one vertex connected to all other and the others connected to one another in a cycle has this degree sequence.  Graphic. Pairs of vertices connected only to one another.  Not Graphic. With two vertices having maximal degree, 5, every vertex would need to have a degree of 2 or more, so the 1 in this sequence makes it non-graphic.    "
},
{
  "id": "exercises-9-1-11",
  "level": "2",
  "url": "s-graphs-introduction.html#exercises-9-1-11",
  "type": "Exercise",
  "number": "9.1.5.10",
  "title": "",
  "body": "  Based on observations you might have made in exercise 9, describe as many characteristics as you can about graphic sequences of length .  Consider the two graphs in . Notice that they have the same degree sequences, . Explain why the two graphs are not isomorphic.    Two graphs with the same degree sequences   Two graphs with the same degree sequences     A graphic sequence of length is the degree sequence of at least one graph containing vertices, and the sum of entries in the degree sequence is two times the number of edges.  The two graphs in the second part are not isomorphic for several reasons, some of which we haven't formally defined such as the fact that the graph on the left is not connected and the one on the right is connected. Using just the definition of a graph isomorphism. Assume we can map the left graph to the right graph with an isomorphism, . Let be any vertex on the left. Then is some vertex on the right. If we call the two vertices that are connected to the left and , then and must be the two vertices that are connected to , but notice that while and are connected with and edge, and are not connected, which contradicts the definition of a graph isomorphism.  "
},
{
  "id": "exercise-house-9-1",
  "level": "2",
  "url": "s-graphs-introduction.html#exercise-house-9-1",
  "type": "Exercise",
  "number": "9.1.5.11",
  "title": "",
  "body": "Draw a plan for the rooms of a house so that models connectedness of the rooms. That is, is an edge if and only if a door connects rooms and . "
},
{
  "id": "exercise-subgraphs",
  "level": "2",
  "url": "s-graphs-introduction.html#exercise-subgraphs",
  "type": "Exercise",
  "number": "9.1.5.12",
  "title": "",
  "body": "How many subgraphs are there of a , . How many of them are spanning graphs? Assume the vertices are distinguishable. For example, if and we remove one edge from the , we count three possible subgraphs depending on which edge is removed even though all three are isomorphic and would not be different if the vertices were indistinguishable.  The number of spanning subgraphs is raised to the since every edge can either be included or not included in a spanning subgraph. The number of subgraphs is more complicated. We can count the number of subgraphs containing vertices by observing that there are subsets of the vertices that we can include. Then for each of them there are spanning subgraphs of the vertices. Therefore, our answer is Notice that the lower limit of the sum is because we can have a graph with no vertices. This sum doesn't seem to simplify. See The On-Line Encyclopedia of Integer Sequences for more information about this sequence.  "
},
{
  "id": "s-data-structures-for-graphs",
  "level": "1",
  "url": "s-data-structures-for-graphs.html",
  "type": "Section",
  "number": "9.2",
  "title": "Data Structures for Graphs",
  "body": " Data Structures for Graphs  Graph Data Structures  In this section, we will describe data structures that are commonly used to represent graphs. In addition we will introduce the basic syntax for graphs in Sage.  Basic Data Structures   Data Structures for Graphs  Assume that we have a graph with vertices that can be indexed by the integers . Here are three different data structures that can be employed to represent graphs.    Adjacency Matrix: As we saw in Chapter 6, the information about edges in a graph can be summarized with an adjacency matrix, , where if and only if vertex is connected to vertex in the graph. Note that this is the same as the adjacency matrix for a relation.  Edge Dictionary: For each vertex in our graph, we maintain a list of edges that initiate at that vertex. If represents the graph's edge information, then we denote by the list of vertices that are terminal vertices of edges initiating at vertex . The exact syntax that would be used can vary. We will use Sage\/Python syntax in our examples.  Edge List: Note that in creating either of the first two data structures, we would presume that a list of edges for the graph exists. A simple way to represent the edges is to maintain this list of ordered pairs, or two element sets, depending on whether the graph is intended to be directed or undirected. We will not work with this data structure here, other than in the first example.    A Very Small Example  We consider the representation of the following graph:  Graph for a Very Small Example   Example Graph to illustrate data structures    The adjacency matrix that represents the graph would be .  The same graph could be represented with the edge dictionary   {1:[2,4],2:[3,4],3:[3],4:[1]} .  Notice the general form of each item in the dictionary: vertex:[list of vertices] .  Finally, a list of edges [(1,2),(1,4),(2,3),(2,4),(3,3),(4,1)] also describes the same graph.   A natural question to ask is: Which data structure should be used in a given situation? For small graphs, it really doesn't make much difference. For larger matrices the edge count would be a consideration. If is large and the number of edges is relatively small, it might use less memory to maintain an edge dictionary or list of edges instead of building an matrix. Some software for working with graphs will make the decision for you.  NCAA Basketball Consider the tournament graph representing a NCAA Division 1 men's (or women's) college basketball season in the United States. There are approximately 350 teams in Division 1. Suppose we constructed the graph with an edge from team A to team B if A beat B at least once in the season; and we label the edge with the number of wins. Since the average team plays around 30 games in a season, most of which will be against other Division I teams, we could expect around edges in the graph. This would be somewhat reduced by games with lower division teams and cases where two or more wins over the same team produces one edge. Since 5,250 is much smaller than entries in an adjacency matrix, an edge dictionary or edge list would be more compact than an adjacency matrix. Even if we were to use software to create an adjacency matrix, many programs will identify the fact that a matrix such as the one in this example would be sparse and would leave data in list form and use sparse array methods to work with it.    Sage Graphs  SageMath Note Graphs  The most common way to define a graph in Sage is to use an edge dictionary. Here is how the graph in is generated and then displayed. Notice that we simply wrap the function DiGraph() around the same dictionary expression we identified earlier.   You can get the adjacency matrix of a graph with the adjacency_matrix method.   You can also define a graph based on its adjacency matrix.   The edge list of any directed graph can be easily retrieved. If you replace edges with edge_iterator , you can iterate through the edge list. The third coordinate of the items in the edge is the label of the edge, which is None in this case.   Replacing the wrapper DiGraph() with Graph() creates an undirected graph.   There are many special graphs and graph families that are available in Sage through the graphs module. They are referenced with the prefix graphs. followed by the name and zero or more parameters inside parentheses. Here are a couple of them, first a complete graph with five vertices.   Here is a wheel graph, named for an obvious pattern of vertices and edges. We assign a name to it first and then show the graph without labeling the vertices.   There are dozens of graph methods, one of which determines the degree sequence of a graph. In this case, it's the wheel graph above.   The degree sequence method is defined within the graphs module, but the prefix graphs. is not needed because the value of w inherits the graphs methods.    Exercises   Estimate the number of vertices and edges in each of the following graphs. Would the graph be considered sparse, so that an adjacency matrix would be inefficient?   Vertices: Cities of the world that are served by at least one airline. Edges: Pairs of cities that are connected by a regular direct flight.  Vertices: ASCII characters. Edges: connect characters that differ in their binary code by exactly two bits.  Vertices: All English words. Edges: An edge connects word to word if is a prefix of .   A rough estimate of the number of vertices in the world airline graph would be the number of cities with population greater than or equal to 100,000. This is estimated to be around 4,100. There are many smaller cities that have airports, but some of the metropolitan areas with clusters of large cities are served by only a few airports. 4,000-5,000 is probably a good guess. As for edges, that's a bit more difficult to estimate. It's certainly not a complete graph. Looking at some medium sized airports such as Manchester, NH, the average number of cities that you can go to directly is in the 50-100 range. So a very rough estimate would be . This is far less than , so an edge list or dictionary of some kind would be more efficient.  The number of ASCII characters is 128. Each character would be connected to others and so there are edges. Comparing this to the , an array is probably the best choice.  The Oxford English Dictionary as approximately a half-million words, although many are obsolete. The number of edges is probably of the same order of magnitude as the number of words, so an edge list or dictionary is probably the best choice.   Each edge of a graph is colored with one of the four colors red, blue, yellow, or green. How could you represent the edges in this graph using a variation of the adjacency matrix structure?  One representation would be with a matrix having entries in the set with each of the numbers corresponding with a color. The numbers themselves could represented with pairs of bits, so a graph with vertices could be represented with bits.    Directed graphs , each with vertex set are represented by the matrices below. Which graphs are isomorphic to one another?     Each graph is isomorphic to itself. In addition, are isomorphic; and are isomorphic to one another.    The following Sage command verifies that the wheel graph with four vertices is isomorphic to the complete graph with four vertices.   A list of all graphs in this the graphs database is available via tab completion. Type \"graphs.\" and then hit the tab key to see which graphs are available. This can be done using the Sage application or SageMathCloud, but not sage cells. Find some other pairs of isomorphic graphs in the database.   One pair is a graphs.CompleteBipartite(2,2) and graphs.CyclicGraph(4). A second pair is graphs.CompleteBipartite(4,1) and graphs.StarGraph(4). No doubt, there are many more.    "
},
{
  "id": "ss-graph-data-structures-2",
  "level": "2",
  "url": "s-data-structures-for-graphs.html#ss-graph-data-structures-2",
  "type": "List",
  "number": "9.2.1",
  "title": "Data Structures for Graphs",
  "body": " Data Structures for Graphs  Assume that we have a graph with vertices that can be indexed by the integers . Here are three different data structures that can be employed to represent graphs.    Adjacency Matrix: As we saw in Chapter 6, the information about edges in a graph can be summarized with an adjacency matrix, , where if and only if vertex is connected to vertex in the graph. Note that this is the same as the adjacency matrix for a relation.  Edge Dictionary: For each vertex in our graph, we maintain a list of edges that initiate at that vertex. If represents the graph's edge information, then we denote by the list of vertices that are terminal vertices of edges initiating at vertex . The exact syntax that would be used can vary. We will use Sage\/Python syntax in our examples.  Edge List: Note that in creating either of the first two data structures, we would presume that a list of edges for the graph exists. A simple way to represent the edges is to maintain this list of ordered pairs, or two element sets, depending on whether the graph is intended to be directed or undirected. We will not work with this data structure here, other than in the first example.   "
},
{
  "id": "ex-data-structure-sample",
  "level": "2",
  "url": "s-data-structures-for-graphs.html#ex-data-structure-sample",
  "type": "Example",
  "number": "9.2.2",
  "title": "A Very Small Example.",
  "body": "A Very Small Example  We consider the representation of the following graph:  Graph for a Very Small Example   Example Graph to illustrate data structures    The adjacency matrix that represents the graph would be .  The same graph could be represented with the edge dictionary   {1:[2,4],2:[3,4],3:[3],4:[1]} .  Notice the general form of each item in the dictionary: vertex:[list of vertices] .  Finally, a list of edges [(1,2),(1,4),(2,3),(2,4),(3,3),(4,1)] also describes the same graph.  "
},
{
  "id": "ex-ncaa-bb",
  "level": "2",
  "url": "s-data-structures-for-graphs.html#ex-ncaa-bb",
  "type": "Example",
  "number": "9.2.4",
  "title": "NCAA Basketball.",
  "body": "NCAA Basketball Consider the tournament graph representing a NCAA Division 1 men's (or women's) college basketball season in the United States. There are approximately 350 teams in Division 1. Suppose we constructed the graph with an edge from team A to team B if A beat B at least once in the season; and we label the edge with the number of wins. Since the average team plays around 30 games in a season, most of which will be against other Division I teams, we could expect around edges in the graph. This would be somewhat reduced by games with lower division teams and cases where two or more wins over the same team produces one edge. Since 5,250 is much smaller than entries in an adjacency matrix, an edge dictionary or edge list would be more compact than an adjacency matrix. Even if we were to use software to create an adjacency matrix, many programs will identify the fact that a matrix such as the one in this example would be sparse and would leave data in list form and use sparse array methods to work with it. "
},
{
  "id": "exercises-9-2-2",
  "level": "2",
  "url": "s-data-structures-for-graphs.html#exercises-9-2-2",
  "type": "Exercise",
  "number": "9.2.3.1",
  "title": "",
  "body": " Estimate the number of vertices and edges in each of the following graphs. Would the graph be considered sparse, so that an adjacency matrix would be inefficient?   Vertices: Cities of the world that are served by at least one airline. Edges: Pairs of cities that are connected by a regular direct flight.  Vertices: ASCII characters. Edges: connect characters that differ in their binary code by exactly two bits.  Vertices: All English words. Edges: An edge connects word to word if is a prefix of .   A rough estimate of the number of vertices in the world airline graph would be the number of cities with population greater than or equal to 100,000. This is estimated to be around 4,100. There are many smaller cities that have airports, but some of the metropolitan areas with clusters of large cities are served by only a few airports. 4,000-5,000 is probably a good guess. As for edges, that's a bit more difficult to estimate. It's certainly not a complete graph. Looking at some medium sized airports such as Manchester, NH, the average number of cities that you can go to directly is in the 50-100 range. So a very rough estimate would be . This is far less than , so an edge list or dictionary of some kind would be more efficient.  The number of ASCII characters is 128. Each character would be connected to others and so there are edges. Comparing this to the , an array is probably the best choice.  The Oxford English Dictionary as approximately a half-million words, although many are obsolete. The number of edges is probably of the same order of magnitude as the number of words, so an edge list or dictionary is probably the best choice.  "
},
{
  "id": "exercises-9-2-3",
  "level": "2",
  "url": "s-data-structures-for-graphs.html#exercises-9-2-3",
  "type": "Exercise",
  "number": "9.2.3.2",
  "title": "",
  "body": "Each edge of a graph is colored with one of the four colors red, blue, yellow, or green. How could you represent the edges in this graph using a variation of the adjacency matrix structure?  One representation would be with a matrix having entries in the set with each of the numbers corresponding with a color. The numbers themselves could represented with pairs of bits, so a graph with vertices could be represented with bits.  "
},
{
  "id": "exercises-9-2-4",
  "level": "2",
  "url": "s-data-structures-for-graphs.html#exercises-9-2-4",
  "type": "Exercise",
  "number": "9.2.3.3",
  "title": "",
  "body": " Directed graphs , each with vertex set are represented by the matrices below. Which graphs are isomorphic to one another?     Each graph is isomorphic to itself. In addition, are isomorphic; and are isomorphic to one another.  "
},
{
  "id": "exercises-9-2-5",
  "level": "2",
  "url": "s-data-structures-for-graphs.html#exercises-9-2-5",
  "type": "Exercise",
  "number": "9.2.3.4",
  "title": "",
  "body": " The following Sage command verifies that the wheel graph with four vertices is isomorphic to the complete graph with four vertices.   A list of all graphs in this the graphs database is available via tab completion. Type \"graphs.\" and then hit the tab key to see which graphs are available. This can be done using the Sage application or SageMathCloud, but not sage cells. Find some other pairs of isomorphic graphs in the database.   One pair is a graphs.CompleteBipartite(2,2) and graphs.CyclicGraph(4). A second pair is graphs.CompleteBipartite(4,1) and graphs.StarGraph(4). No doubt, there are many more.  "
},
{
  "id": "s-Connectivity",
  "level": "1",
  "url": "s-Connectivity.html",
  "type": "Section",
  "number": "9.3",
  "title": "Connectivity",
  "body": " Connectivity  Connectivity in Graphs   This section is devoted to a question that, when posed in relation to the graphs that we have examined, seems trivial. That question is: Given two vertices, and , of a graph, is there a path from to ? If , this question is interpreted as asking whether there is a circuit of positive length starting at . Of course, for the graphs we have seen up to now, this question can be answered after a brief examination.   Preliminaries  There are two situations under which a question of this kind is nontrivial. One is where the graph is very large and an examination of the graph could take a considerable amount of time. Anyone who has tried to solve a maze may have run into a similar problem. The second interesting situation is when we want to pose the question to a machine. If only the information on the edges between the vertices is part of the data structure for the graph, how can you put that information together to determine whether two vertices can be connected by a path?  Connectivity Terminology Let and be vertices of a directed graph. Vertex is connected to vertex if there is a path from to . Two vertices are strongly connected if they are connected in both directions to one another. A graph is connected if, for each pair of distinct vertices, and , is connected to or is connected to . A graph is strongly connected if every pair of its vertices is strongly connected. For an undirected graph, in which edges can be used in either direction, the notions of strongly connected and connected are the same.   Maximal Path Theorem  If a graph has vertices and vertex is connected to vertex , then there exists a path from to of length no more than . (Indirect): Suppose is connected to , but the shortest path from to has length , where . A vertex list for a path of length will have vertices. This path can be represented as , where and . Note that since there are only vertices in the graph and vertices are listed in the path after , we can apply the pigeonhole principle and be assured that there must be some duplication in the last vertices of the vertex list, which represents a circuit in the path. This means that our path of minimum length can be reduced, which is a contradiction.    Adjacency Matrix Method Adjacency Matrix Method  Adjacency Matrix Method   Suppose that the information about edges in a graph is stored in an adjacency matrix, . The relation, , that defines is if there is an edge connecting to . Recall that the composition of with itself, , is defined by if there exists a vertex such that and ; that is, is connected to by a path of length 2. We could prove by induction that the relation , , is defined by if and only if there is a path of length from to . Since the transitive closure, , is the union of ,  , we can answer our connectivity question by determining the transitive closure of , which can be done most easily by keeping our relation in matrix form. is significant in our calculations because it tells us that we need only go as far as to determine the matrix of the transitive closure.    The main advantage of the adjacency matrix method is that the transitive closure matrix can answer all questions about the existence of paths between any vertices. If is the matrix of the transitive closure, is connected to if and only if . A directed graph is connected if or for each . A directed graph is strongly connected if its transitive closure matrix has no zeros.  A disadvantage of the adjacency matrix method is that the transitive closure matrix tells us whether a path exists, but not what the path is. The next algorithm will solve this problem.   Breadth-First Search Breadth-First Search  We will describe the Breadth-First Search Algorithm first with an example.  The football team at Mediocre State University (MSU) has had a bad year, 2 wins and 9 losses. Thirty days after the end of the football season, the university trustees are meeting to decide whether to rehire the head coach; things look bad for him. However, on the day of the meeting, the coach issues the following press release with results from the past year:  Press Release: MSU completes successful season  The Mediocre State University football team compared favorably with national champion Enormous State University this season.   Mediocre State defeated Local A and M.  Local A and M defeated City College.  City College defeated Corn State U.  ... (25 results later)  Tough Tech defeated Enormous State University (ESU).   ...and ESU went on to win the national championship!   The trustees were so impressed that they rehired the coach with a raise! How did the coach come up with such a list?  In reality, such lists exist occasionally and have appeared in newspapers from time to time. Of course they really don't prove anything since each team that defeated MSU in our example above can produce a similar, shorter chain of results. Since college football records are readily available, the coach could have found this list by trial and error. All that he needed to start with was that his team won at least one game. Since ESU lost one game, there was some hope of producing the chain.  The problem of finding this list is equivalent to finding a path in the tournament graph for last year's football season that initiates at MSU and ends at ESU. Such a graph is far from complete and is likely to be represented using edge lists. To make the coach's problem interesting, let's imagine that only the winner of any game remembers the result of the game. The coach's problem has now taken on the flavor of a maze. To reach ESU, he must communicate with the various teams along the path. One way that the coach could have discovered his list in time is by sending the following messages to the coaches of the two teams that MSU defeated during the season:  When this example was first written, we commented that ties should be ignored. Most recent NCAA rules call for a tiebreaker in college football and so ties are no longer an issue. Email was also not common and we described the process in terms of letters, not email messages. Another change is that the coach could also have asked the MSU math department to use Mathematica or Sage to find the path!  The Coach's Letter  Dear Football Coach:  Please follow these directions exactly.   If you are the coach at ESU, contact the coach at MSU now and tell him who sent you this message.  If you are not the coach at ESU and this is the first message of this type that you have received, then:   Remember from whom you received this message.  Forward a copy of this message, signed by you, to each of the coaches whose teams you defeated during the past year.  Ignore this message if you have received one like it already.    Signed,  Coach of MSU    Observations  From the conditions of this message, it should be clear that if everyone cooperates and if coaches participate within a day of receiving the message:   If a path of length exists from MSU to ESU, then the coach will know about it in days.  By making a series of phone calls, the coach can construct a path that he wants by first calling the coach who defeated ESU (the person who sent ESU's coach that message). This coach will know who sent him a letter, and so on. Therefore, the vertex list of the desired path is constructed in reverse order.  If a total of football games were played, no more than messages will be sent out.  If a day passes without any message being sent out, no path from MSU to ESU exists.  This method could be extended to construct a list of all teams that a given team can be connected to. Simply imagine a series of letters like the one above sent by each football coach and targeted at every other coach.    The general problem of finding a path between two vertices in a graph, if one exists, can be solved exactly as we solved the problem above. The following algorithm, commonly called a breadth-first search, uses a stack.   Stacks Stack A stack is a fundamental data structure in computer science. A common analogy used to describe stacks is a stack of plates. If you put a plate on the top of a stack and then want to use a plate, it's natural to use that top plate. So the last plate in is the first plate out. Last in, first out is the short description of the rule for stacks. This is contrast with a queue which uses a First in, first out rule.   Breadth-first Search  Breadth-first Search  A broadcasting algorithm for finding a path between vertex and vertex of a graph having vertices. Each item of a list , consists of a Boolean field and an integer field . The sets , , called depth sets, have the property that if , then the shortest path from vertex to vertex is of length . In Step 5, a stack is used to put the vertex list for the path from the vertex to vertex in the proper order. That stack is the output of the algorithm.   Set the value equal to False,      while ) and   for each k in :  for each edge (k,t):  If == False:       if :    while :  Push onto    Push onto  Push onto       Notes on Breadth-first Search   If is the starting vertex and we initialize , then the algorithm will find the shortest circuit containing . If that is of interest, we would also not initialize the depth of to  This algorithm will produce one path from vertex to vertex , if one exists, and that path will be as short as possible. If more than one path of this length exists, then the one that is produced depends on the order in which the edges are examined and the order in which the elements of are examined in Step 4.  The condition is analogous to the condition that no mail is sent in a given stage of the process, in which case MSU cannot be connected to ESU.  This algorithm can be easily revised to find paths to all vertices that can be reached from vertex . Step 5 would be put off until a specific path to a vertex is needed since the information in contains an efficient list of all paths. The algorithm can also be extended further to find paths between any two vertices.    A simple example  Consider the graph below. The existence of a path from vertex 2 to vertex 3 is not difficult to determine by examination. After a few seconds, you should be able to find two paths of length four. will produce one of them.   A simple example of breadth-first search   Example Graph for breadth-first search    Suppose that the edges from each vertex are sorted in ascending order by terminal vertex. For example, the edges from vertex 3 would be in the order . In addition, assume that in the body of Step 4 of the algorithm, the elements of are used in ascending order. Then at the end of Step 4, the value of will be Therefore, the path is produced by the algorithm. Note that if we wanted a path from 2 to 5, the information in produces the path (2, 1, 5) since and . A shortest circuit that initiates at vertex 2 is also available by noting that , , and ; thus the circuit is the output of the algorithm.   Graph Measurements  If we were to perform a breadth first search from each vertex in a graph, we could proceed to determine several key measurements relating to the general connectivity of that graph. From each vertex , the distance from to any other vertex , , is number of edges in the shortest path from to . This number is also the index of the depth set to which belongs in a breath-first search starting at . where is the family of depth sets starting at .  If the vector of from-values is known from the breath-first search, then the distance can be determined recursively as follows:    Computing Distances   Graph Measurements Example   An undirected graph with 12 vertices having dictionary representation {1:[6,7,10],2:[7,9,12],3:[10],4:[6,8,10],5:[9,11],6:[7,11],7:[10,12],9:[11]}    Consider . If we perform a breadth first search of this graph starting at vertex 2, for example, we get the following from data telling us from what vertex each vertex is reached. For example, 4.from has a value of 6. We can compute :    Once we know distances between any two vertices, we can determine the eccentricity of each vertex; and the graph's diameter, radius and center. First, we define these terms precisely.     The eccentricity of a vertex     The diameter of graph     The radius of graph     The center of graph    Eccentricity of a Vertex Eccentricity of a vertex  The maximum distance from a vertex to all other vertices, .    Diameter of a Graph Diameter of a Graph  The maximum eccentricity of vertices in a graph, denoted .    Radius of a Graph Radius of a graph  The minimum eccentricity of vertices in a graph, denoted .    Center of a Graph Center of a graph  The set of vertices with minimal eccentricity,    Measurements from distance matrices  If we compute all distances between vertices, we can summarize the results in a distance matrix, where the entry in row , column is the distance from vertex to vertex . For the graph in , that matrix is If we scan the matrix, we can see that the maximum distance is the distance between vertices 3 and 5, which is 5 and is the diameter of the graph. If we focus on individual rows and identify the maximum values, which are the eccentricities, their minimum is 3, which the graph's radius. This eccentricity value is attained by vertices in the set , which is the center of the graph.    SageMath Note - Graph Searching SageMath Note Search in a Graph  The following sequence of Sage cells illustrates how searching can be done in graphs.  Generate a random undirected graph with 18 vertices. For each pair of vertices, an edge is included between them with probability 0.2. Since there are potential edges, we expect that there will be approximately edges. The random number generation is seeded first so that the result will always be the same in spite of the random graph function. Changing or removing that first line will let you experiment with different graphs.   Count the number of edges. In this case the number is a bit less than expected.   Find a shortest path from vertex 0 to vertex 8.   Generate a list of vertices that would be reached in a breadth-first search. The expression Gr.breadth_first_search(0) creates an iterator that is convenient for programming. Wrapping list( ) around the expression shows the order in which the vertices are visited with the depth set indicated in the second coordinates.     Exercises  Apply to find a path from 5 to 1 in . What would be the final value of ? Assume that the terminal vertices in edge lists and elements of the depth sets are put into ascending order, as we assumed in .       Apply to find a path from to in the road graph in using the edge list in that example. Assume that the elements of the depth sets are put into ascending order.      In a simple undirected graph with no self-loops, what is the maximum number of edges you can have, keeping the graph unconnected? What is the minimum number of edges that will assure that the graph is connected? If the number of vertices is , there can be vertices with one vertex not connected to any of the others. One more edge and connectivity is assured.   Use a broadcasting algorithm to determine the shortest path from vertex to vertex in the graphs shown in the below. List the depth sets and the stack that is created.   Shortest paths from to ?   Graphs for exercise 9-3-4     , , , , and . We can construct a path from to using the data , , and to get the path .  , , , , and . We can construct a path from to using the data , , and to get the path .     For each of the following graphs, determine the eccentricities of each vertex, and the diameter, radius, and center of the graph.    An undirected graph with 6 vertices.    An undirected graph with 6 vertices.      An undirected graph with 6 vertices.    An undirected graph with 6 vertices.      The eccentricity of each vertex is 2; and the diameter and radius are both 2 as well. All vertices are part of the center.  The corners (1,3,10 and 10) have eccentricities 5. The two central vertices, 5 and 8, which are in the center of the graph have eccentricity 3. All other vertices have eccentricity 4. The diameter is 5. The radius is 3.  Vertices 1, 2 and 5 have eccentricity 2 and make up the center of this graph. Verticies 7 and 8 have eccentricity 4, and all other vertices have eccentricity 3. The diameter is 4. The radius is 2.  The eccentricity of each vertex is 4; and the diameter and radius are both 4 as well. All vertices are part of the center.       The terms diameter, radius and center are familiar ones in the context of circles. Compare their usage in circles and graphs. How are they similar and how are they different?  Eccentricity might be less familiar. How is is used in geometry, and does it have a compatible use in graph theory?     Whereas for any circle, the relationship between radius and diameter in a graph is as exact. The best we can say is that in a graph.  The term eccentricity as it applies to ellipses is a measurement of the whole ellipse, while in a graph the eccentricity is defined for individual vertices.     Prove (by induction on ) that if the relation on vertices of a graph is defined by if there is an edge connecting to , then , , is defined by if there is a path of length from to .  Basis: Is the relation , defined by if there is a path of length 1 from ? Yes, since if and only if an edge, which is a path of length , connects to .  Induction: Assume that if and only if there is a path of length from to . We must show that if and only if there is a path of length from to .  By the induction hypothesis, there is a path of length from . And by the basis, there is a path of length one from to . If we combine these two paths, we obtain a path of length from to . Of course, if we start with a path of length from to , we have a path of length from to some vertex and a path of length 1 from to . Therefore, .  For each of the following distance matrices of graphs, identify the diameter, radius and center. Assume the graphs vertices are the numbers 1 through for an matrix.       , and the center is .  , and the center is .      "
},
{
  "id": "ss-connectivity-prelim-3",
  "level": "2",
  "url": "s-Connectivity.html#ss-connectivity-prelim-3",
  "type": "Note",
  "number": "9.3.1",
  "title": "Connectivity Terminology.",
  "body": "Connectivity Terminology Let and be vertices of a directed graph. Vertex is connected to vertex if there is a path from to . Two vertices are strongly connected if they are connected in both directions to one another. A graph is connected if, for each pair of distinct vertices, and , is connected to or is connected to . A graph is strongly connected if every pair of its vertices is strongly connected. For an undirected graph, in which edges can be used in either direction, the notions of strongly connected and connected are the same.  "
},
{
  "id": "theorem-9-3-1",
  "level": "2",
  "url": "s-Connectivity.html#theorem-9-3-1",
  "type": "Theorem",
  "number": "9.3.2",
  "title": "Maximal Path Theorem.",
  "body": "Maximal Path Theorem  If a graph has vertices and vertex is connected to vertex , then there exists a path from to of length no more than . (Indirect): Suppose is connected to , but the shortest path from to has length , where . A vertex list for a path of length will have vertices. This path can be represented as , where and . Note that since there are only vertices in the graph and vertices are listed in the path after , we can apply the pigeonhole principle and be assured that there must be some duplication in the last vertices of the vertex list, which represents a circuit in the path. This means that our path of minimum length can be reduced, which is a contradiction.  "
},
{
  "id": "ss-adjacency-matrix-method-3",
  "level": "2",
  "url": "s-Connectivity.html#ss-adjacency-matrix-method-3",
  "type": "Algorithm",
  "number": "9.3.3",
  "title": "Adjacency Matrix Method.",
  "body": "Adjacency Matrix Method   Suppose that the information about edges in a graph is stored in an adjacency matrix, . The relation, , that defines is if there is an edge connecting to . Recall that the composition of with itself, , is defined by if there exists a vertex such that and ; that is, is connected to by a path of length 2. We could prove by induction that the relation , , is defined by if and only if there is a path of length from to . Since the transitive closure, , is the union of ,  , we can answer our connectivity question by determining the transitive closure of , which can be done most easily by keeping our relation in matrix form. is significant in our calculations because it tells us that we need only go as far as to determine the matrix of the transitive closure.   "
},
{
  "id": "ss-breadth-first-search-5",
  "level": "2",
  "url": "s-Connectivity.html#ss-breadth-first-search-5",
  "type": "List",
  "number": "9.3.4",
  "title": "Press Release: MSU completes successful season",
  "body": "Press Release: MSU completes successful season  The Mediocre State University football team compared favorably with national champion Enormous State University this season.   Mediocre State defeated Local A and M.  Local A and M defeated City College.  City College defeated Corn State U.  ... (25 results later)  Tough Tech defeated Enormous State University (ESU).   ...and ESU went on to win the national championship!  "
},
{
  "id": "ss-breadth-first-search-9",
  "level": "2",
  "url": "s-Connectivity.html#ss-breadth-first-search-9",
  "type": "Note",
  "number": "9.3.5",
  "title": "",
  "body": "When this example was first written, we commented that ties should be ignored. Most recent NCAA rules call for a tiebreaker in college football and so ties are no longer an issue. Email was also not common and we described the process in terms of letters, not email messages. Another change is that the coach could also have asked the MSU math department to use Mathematica or Sage to find the path! "
},
{
  "id": "ss-breadth-first-search-10",
  "level": "2",
  "url": "s-Connectivity.html#ss-breadth-first-search-10",
  "type": "List",
  "number": "9.3.6",
  "title": "The Coach’s Letter",
  "body": "The Coach's Letter  Dear Football Coach:  Please follow these directions exactly.   If you are the coach at ESU, contact the coach at MSU now and tell him who sent you this message.  If you are not the coach at ESU and this is the first message of this type that you have received, then:   Remember from whom you received this message.  Forward a copy of this message, signed by you, to each of the coaches whose teams you defeated during the past year.  Ignore this message if you have received one like it already.    Signed,  Coach of MSU   "
},
{
  "id": "ss-breadth-first-search-11",
  "level": "2",
  "url": "s-Connectivity.html#ss-breadth-first-search-11",
  "type": "List",
  "number": "9.3.7",
  "title": "Observations",
  "body": "Observations  From the conditions of this message, it should be clear that if everyone cooperates and if coaches participate within a day of receiving the message:   If a path of length exists from MSU to ESU, then the coach will know about it in days.  By making a series of phone calls, the coach can construct a path that he wants by first calling the coach who defeated ESU (the person who sent ESU's coach that message). This coach will know who sent him a letter, and so on. Therefore, the vertex list of the desired path is constructed in reverse order.  If a total of football games were played, no more than messages will be sent out.  If a day passes without any message being sent out, no path from MSU to ESU exists.  This method could be extended to construct a list of all teams that a given team can be connected to. Simply imagine a series of letters like the one above sent by each football coach and targeted at every other coach.   "
},
{
  "id": "alg-breadth-first",
  "level": "2",
  "url": "s-Connectivity.html#alg-breadth-first",
  "type": "Algorithm",
  "number": "9.3.8",
  "title": "Breadth-first Search.",
  "body": "Breadth-first Search  Breadth-first Search  A broadcasting algorithm for finding a path between vertex and vertex of a graph having vertices. Each item of a list , consists of a Boolean field and an integer field . The sets , , called depth sets, have the property that if , then the shortest path from vertex to vertex is of length . In Step 5, a stack is used to put the vertex list for the path from the vertex to vertex in the proper order. That stack is the output of the algorithm.   Set the value equal to False,      while ) and   for each k in :  for each edge (k,t):  If == False:       if :    while :  Push onto    Push onto  Push onto      "
},
{
  "id": "ss-breadth-first-search-15",
  "level": "2",
  "url": "s-Connectivity.html#ss-breadth-first-search-15",
  "type": "List",
  "number": "9.3.9",
  "title": "Notes on Breadth-first Search",
  "body": "Notes on Breadth-first Search   If is the starting vertex and we initialize , then the algorithm will find the shortest circuit containing . If that is of interest, we would also not initialize the depth of to  This algorithm will produce one path from vertex to vertex , if one exists, and that path will be as short as possible. If more than one path of this length exists, then the one that is produced depends on the order in which the edges are examined and the order in which the elements of are examined in Step 4.  The condition is analogous to the condition that no mail is sent in a given stage of the process, in which case MSU cannot be connected to ESU.  This algorithm can be easily revised to find paths to all vertices that can be reached from vertex . Step 5 would be put off until a specific path to a vertex is needed since the information in contains an efficient list of all paths. The algorithm can also be extended further to find paths between any two vertices.   "
},
{
  "id": "ex-search-example",
  "level": "2",
  "url": "s-Connectivity.html#ex-search-example",
  "type": "Example",
  "number": "9.3.10",
  "title": "A simple example.",
  "body": "A simple example  Consider the graph below. The existence of a path from vertex 2 to vertex 3 is not difficult to determine by examination. After a few seconds, you should be able to find two paths of length four. will produce one of them.   A simple example of breadth-first search   Example Graph for breadth-first search    Suppose that the edges from each vertex are sorted in ascending order by terminal vertex. For example, the edges from vertex 3 would be in the order . In addition, assume that in the body of Step 4 of the algorithm, the elements of are used in ascending order. Then at the end of Step 4, the value of will be Therefore, the path is produced by the algorithm. Note that if we wanted a path from 2 to 5, the information in produces the path (2, 1, 5) since and . A shortest circuit that initiates at vertex 2 is also available by noting that , , and ; thus the circuit is the output of the algorithm. "
},
{
  "id": "e-computing-distances",
  "level": "2",
  "url": "s-Connectivity.html#e-computing-distances",
  "type": "Example",
  "number": "9.3.12",
  "title": "Computing Distances.",
  "body": "Computing Distances   Graph Measurements Example   An undirected graph with 12 vertices having dictionary representation {1:[6,7,10],2:[7,9,12],3:[10],4:[6,8,10],5:[9,11],6:[7,11],7:[10,12],9:[11]}    Consider . If we perform a breadth first search of this graph starting at vertex 2, for example, we get the following from data telling us from what vertex each vertex is reached. For example, 4.from has a value of 6. We can compute :   "
},
{
  "id": "e-distance-matrix",
  "level": "2",
  "url": "s-Connectivity.html#e-distance-matrix",
  "type": "Example",
  "number": "9.3.14",
  "title": "Measurements from distance matrices.",
  "body": "Measurements from distance matrices  If we compute all distances between vertices, we can summarize the results in a distance matrix, where the entry in row , column is the distance from vertex to vertex . For the graph in , that matrix is If we scan the matrix, we can see that the maximum distance is the distance between vertices 3 and 5, which is 5 and is the diameter of the graph. If we focus on individual rows and identify the maximum values, which are the eccentricities, their minimum is 3, which the graph's radius. This eccentricity value is attained by vertices in the set , which is the center of the graph.  "
},
{
  "id": "exercises-9-3-2",
  "level": "2",
  "url": "s-Connectivity.html#exercises-9-3-2",
  "type": "Exercise",
  "number": "9.3.6.1",
  "title": "",
  "body": "Apply to find a path from 5 to 1 in . What would be the final value of ? Assume that the terminal vertices in edge lists and elements of the depth sets are put into ascending order, as we assumed in .      "
},
{
  "id": "exercises-9-3-3",
  "level": "2",
  "url": "s-Connectivity.html#exercises-9-3-3",
  "type": "Exercise",
  "number": "9.3.6.2",
  "title": "",
  "body": "Apply to find a path from to in the road graph in using the edge list in that example. Assume that the elements of the depth sets are put into ascending order.     "
},
{
  "id": "exercises-9-3-4",
  "level": "2",
  "url": "s-Connectivity.html#exercises-9-3-4",
  "type": "Exercise",
  "number": "9.3.6.3",
  "title": "",
  "body": "In a simple undirected graph with no self-loops, what is the maximum number of edges you can have, keeping the graph unconnected? What is the minimum number of edges that will assure that the graph is connected? If the number of vertices is , there can be vertices with one vertex not connected to any of the others. One more edge and connectivity is assured. "
},
{
  "id": "exercises-9-3-5",
  "level": "2",
  "url": "s-Connectivity.html#exercises-9-3-5",
  "type": "Exercise",
  "number": "9.3.6.4",
  "title": "",
  "body": " Use a broadcasting algorithm to determine the shortest path from vertex to vertex in the graphs shown in the below. List the depth sets and the stack that is created.   Shortest paths from to ?   Graphs for exercise 9-3-4     , , , , and . We can construct a path from to using the data , , and to get the path .  , , , , and . We can construct a path from to using the data , , and to get the path .   "
},
{
  "id": "exercises-9-3-6",
  "level": "2",
  "url": "s-Connectivity.html#exercises-9-3-6",
  "type": "Exercise",
  "number": "9.3.6.5",
  "title": "",
  "body": " For each of the following graphs, determine the eccentricities of each vertex, and the diameter, radius, and center of the graph.    An undirected graph with 6 vertices.    An undirected graph with 6 vertices.      An undirected graph with 6 vertices.    An undirected graph with 6 vertices.      The eccentricity of each vertex is 2; and the diameter and radius are both 2 as well. All vertices are part of the center.  The corners (1,3,10 and 10) have eccentricities 5. The two central vertices, 5 and 8, which are in the center of the graph have eccentricity 3. All other vertices have eccentricity 4. The diameter is 5. The radius is 3.  Vertices 1, 2 and 5 have eccentricity 2 and make up the center of this graph. Verticies 7 and 8 have eccentricity 4, and all other vertices have eccentricity 3. The diameter is 4. The radius is 2.  The eccentricity of each vertex is 4; and the diameter and radius are both 4 as well. All vertices are part of the center.    "
},
{
  "id": "exercises-9-3-7",
  "level": "2",
  "url": "s-Connectivity.html#exercises-9-3-7",
  "type": "Exercise",
  "number": "9.3.6.6",
  "title": "",
  "body": "  The terms diameter, radius and center are familiar ones in the context of circles. Compare their usage in circles and graphs. How are they similar and how are they different?  Eccentricity might be less familiar. How is is used in geometry, and does it have a compatible use in graph theory?     Whereas for any circle, the relationship between radius and diameter in a graph is as exact. The best we can say is that in a graph.  The term eccentricity as it applies to ellipses is a measurement of the whole ellipse, while in a graph the eccentricity is defined for individual vertices.    "
},
{
  "id": "exercises-9-3-8",
  "level": "2",
  "url": "s-Connectivity.html#exercises-9-3-8",
  "type": "Exercise",
  "number": "9.3.6.7",
  "title": "",
  "body": "Prove (by induction on ) that if the relation on vertices of a graph is defined by if there is an edge connecting to , then , , is defined by if there is a path of length from to .  Basis: Is the relation , defined by if there is a path of length 1 from ? Yes, since if and only if an edge, which is a path of length , connects to .  Induction: Assume that if and only if there is a path of length from to . We must show that if and only if there is a path of length from to .  By the induction hypothesis, there is a path of length from . And by the basis, there is a path of length one from to . If we combine these two paths, we obtain a path of length from to . Of course, if we start with a path of length from to , we have a path of length from to some vertex and a path of length 1 from to . Therefore, . "
},
{
  "id": "exercises-9-3-9",
  "level": "2",
  "url": "s-Connectivity.html#exercises-9-3-9",
  "type": "Exercise",
  "number": "9.3.6.8",
  "title": "",
  "body": "For each of the following distance matrices of graphs, identify the diameter, radius and center. Assume the graphs vertices are the numbers 1 through for an matrix.       , and the center is .  , and the center is .    "
},
{
  "id": "s-traversals",
  "level": "1",
  "url": "s-traversals.html",
  "type": "Section",
  "number": "9.4",
  "title": "Traversals: Eulerian and Hamiltonian Graphs",
  "body": " Traversals: Eulerian and Hamiltonian Graphs  Traversals of Graphs  The subject of graph traversals has a long history. In fact, the solution by Leonhard Euler (Switzerland, 1707-83) of the Koenigsberg Bridge Problem is considered by many to represent the birth of graph theory.   Eulerian Graphs    A map of Koenigsberg, circa 1735   A map of Koenigsberg     A multigraph for the bridges of Koenigsberg     A map of the Prussian city of Koenigsberg (circa 1735) in shows that there were seven bridges connecting the four land masses that made up the city. The legend of this problem states that the citizens of Koenigsberg searched in vain for a walking tour that passed over each bridge exactly once. No one could design such a tour and the search was abruptly abandoned with the publication of Euler's Theorem.  Euler's Theorem: Koenigsberg Case Euler's Theorem Koenigsberg Case  No walking tour of Koenigsberg can be designed so that each bridge is used exactly once.  The map of Koenigsberg can be represented as an undirected multigraph, as in . The four land masses are the vertices and each edge represents a bridge.  The desired tour is then a path that uses each edge once and only once. Since the path can start and end at two different vertices, there are two remaining vertices that must be intermediate vertices in the path. If is an intermediate vertex, then every time that you visit , you must use two of its incident edges, one to enter and one to exit. Therefore, there must be an even number of edges connecting to the other vertices. Since every vertex in the Koenigsberg graph has an odd number of edges, no tour of the type that is desired is possible.    As is typical of most mathematicians, Euler wasn't satisfied with solving only the Koenigsberg problem. His original theorem, which is paraphrased below, concerned the existence of paths and circuits like those sought in Koenigsberg. These paths and circuits have become associated with Euler's name.  Eulerian Paths, Circuits, Graphs Eulerian Paths, Circuits, Graphs An Eulerian path through a graph is a path whose edge list contains each edge of the graph exactly once. If the path is a circuit, then it is called an Eulerian circuit. An Eulerian graph is a graph that possesses an Eulerian circuit.  Notice that if a graph has an Eulerian path that is not a circuit it is generally not considered an Eulerian graph, although some authors will call it such. So in any reference you read, be sure to check that definition that is used!  An Eulerian Graph Without tracing any paths, we can be sure that the graph below has an Eulerian circuit because all vertices have an even degree. This follows from the following theorem.   An Eulerian graph   An Eulerian graph     Euler's Theorem: General Case  Euler's Theorem  An undirected graph has an Eulerian path if and only if it is connected and has either zero or two vertices with an odd degree. If no vertex has an odd degree, then the graph is Eulerian.  It can be proven by induction that the number of vertices in an undirected graph that have an odd degree must be even. We will leave the proof of this fact to the reader as an exercise. The necessity of having either zero or two vertices of odd degree is clear from the proof of the Koenigsberg case of this theorem. Therefore, we will concentrate on proving that this condition is sufficient to ensure that a graph has an Eulerian path. Let be the number of vertices with odd degree.  Phase 1. If , start at any vertex, , and travel along any path, not using any edge twice. Since each vertex has an even degree, this path can always be continued past each vertex that you reach except . The result is a circuit that includes . If , let be either one of the vertices of odd degree. Trace any path starting at using up edges until you can go no further, as in the case. This time, the path that you obtain must end at the other vertex of odd degree that we will call . At the end of Phase 1, we have an initial path that may or may not be Eulerian. If it is not Eulerian, Phase 2 can be repeated until all of the edges have been used. Since the number of unused edges is decreased in any use of Phase 2, an Eulerian path must be obtained in a finite number of steps.  Phase 2. As we enter this phase, we have constructed a path that uses a proper subset of the edges in our graph. We will refer to this path as the current path. Let be the vertices of our graph, the edges, and the edges that have been used in the current path. Consider the graph . Note that every vertex in has an even degree. Select any edge, , from Let and be the vertices that connects. Trace a new path starting at whose first edge is . We can be sure that at least one vertex of the new path is also in the current path since is connected. Starting at , there exists a path in to any vertex in the current path. At some point along this path, which we can consider the start of the new path, we will have intersected the current path. Since the degree of each vertex in is even, any path that we start at can be continued until it is a circuit. Now, we simply augment the current path with this circuit. As we travel along the current path, the first time that we intersect the new path, we travel along it (see ). Once we complete the circuit that is the new path, we resume the traversal of the current path.   Path Augmentation Plan   Path Augmentation Plan    If the result of this phase is an Eulerian path, then we are finished; otherwise, repeat this phase.   Complete Eulerian Graphs The complete undirected graphs , . .., are Eulerian. If , then is not Eulerian.   Hamiltonian Graphs  To search for a path that uses every vertex of a graph exactly once seems to be a natural next problem after you have considered Eulerian graphs.The Irish mathematician Sir William Rowan Hamilton (1805-65) is given credit for first defining such paths. He is also credited with discovering the quaternions, for which he was honored by the Irish government with a postage stamp in 2005.   Irish stamp honoring Sir William Rowan Hamilton   Irish stamp honoring Sir William Rowan Hamilton    Hamiltonian Path, Circuit, and Graphs Hamiltonian Paths, Circuits, and Graphs  A Hamiltonian path through a graph is a path whose vertex list contains each vertex of the graph exactly once, except if the path is a circuit, in which case the initial vertex appears a second time as the terminal vertex. If the path is a circuit, then it is called a Hamiltonian circuit. A Hamiltonian graph is a graph that possesses a Hamiltonian circuit.  The Original Hamiltonian Graph shows a graph that is Hamiltonian. In fact, it is the graph that Hamilton used as an example to pose the question of existence of Hamiltonian paths in 1859. In its original form, the puzzle that was posed to readers was called Around the World. The vertices were labeled with names of major cities of the world and the object was to complete a tour of these cities. The graph is also referred to as the dodecahedron graph, where vertices correspond with the corners of a dodecahedron and the edges are the edges of the solid that connect the corners.    A Dodecahedron   A Dodecahedron     The Dodecahedron Graph   The Dodecahedron Graph      Unfortunately, a simple condition doesn't exist that characterizes a Hamiltonian graph. An obvious necessary condition is that the graph be connected; however, there is a connected undirected graph with four vertices that is not Hamiltonian. Can you draw such a graph?  What Is Possible and What Is Impossible? The search for a Hamiltonian path in a graph is typical of many simple-sounding problems in graph theory that have proven to be very difficult to solve. Although there are simple algorithms for conducting the search, they are impractical for large problems because they take such a long time to complete as graph size increases. Currently, every algorithm to search for a Hamiltonian path in a graph takes a time that grows at a rate that is greater than any polynomial as a function of the number of vertices. Rates of this type are called super-polynomial . That is, if is the time it takes to search a graph of vertices, and is any polynomial, then for all but possibly a finite number of positive values for .  It is an unproven but widely held belief that no faster algorithm exists to search for Hamiltonian paths in general graphs. To sum up, the problem of determining whether a graph is Hamiltonian is theoretically possible; however, for large graphs we consider it a practical impossibility. Many of the problems we will discuss in the next section, particularly the Traveling Salesman Problem, are thought to be impossible in the same sense.  The -cube  N-cube the -cube Let , and let be the set of strings of 0's and 1's with length . The -cube is the undirected graph with a vertex for each string in and an edge connecting each pair of strings that differ in exactly one position. The -cube is normally denoted .  The -cube is among the graphs that are defined within the graphs package of SageMath and is created with the expression graphs.CubeGraph(n) .   Analog-to-digital Conversion and the Gray Code  Analog-to-digital Conversion  Gray Code  A common problem encountered in engineering is that of analog-to-digital (a-d) conversion, where the reading on a dial, for example, must be converted to a numerical value. In order for this conversion to be done reliably and quickly, one must solve an interesting problem in graph theory. Before this problem is posed, we will make the connection between a-d conversion and the graph problem using a simple example. Suppose a dial can be turned in any direction, and that the positions will be converted to one of the numbers zero through seven as depicted in . The angles from 0 to 360 are divided into eight equal parts, and each part is assigned a number starting with 0 and increasing clockwise. If the dial points in any of these sectors the conversion is to the number of that sector. If the dial is on the boundary, then we will be satisfied with the conversion to either of the numbers in the bordering sectors. This conversion can be thought of as giving an approximate angle of the dial, for if the dial is in sector , then the angle that the dial makes with east is approximately .   Analog-Digital Dial   Analog-Digital Dial    Now that the desired conversion has been identified, we will describe a solution that has one major error in it, and then identify how this problem can be rectified. All digital computers represent numbers in binary form, as a sequence of 0's and 1's called bits, short for binary digits. The binary representations of numbers 0 through 7 are:   The way that we could send those bits to a computer is by coating parts of the back of the dial with a metallic substance, as in . For each of the three concentric circles on the dial there is a small magnet. If a magnet lies under a part of the dial that has been coated with metal, then it will turn a switch ON, whereas the switch stays OFF when no metal is detected above a magnet. Notice how every ON\/OFF combination of the three switches is possible given the way the back of the dial is coated.  If the dial is placed so that the magnets are in the middle of a sector, we expect this method to work well. There is a problem on certain boundaries, however. If the dial is turned so that the magnets are between sectors three and four, for example, then it is unclear what the result will be. This is due to the fact that each magnet will have only a fraction of the required metal above it to turn its switch ON. Due to expected irregularities in the coating of the dial, we can be safe in saying that for each switch either ON or OFF could be the result, and so if the dial is between sectors three and four, any number could be indicated. This problem does not occur between every sector. For example, between sectors 0 and 1, there is only one switch that cannot be predicted. No matter what the outcome is for the units switch in this case, the indicated sector must be either 0 or 1. This consistent with the original objective that a positioning of the dial on a boundary of two sectors should produce the number of either sector.   Coating scheme for the Analog-Digital Dial   Analog-Digital Dial    Is there a way to coat the sectors on the back of the dial so that each of the eight patterns corresponding to the numbers 0 to 7 appears once, and so that between any two adjacent sectors there is only one switch that will have a questionable setting? What we are describing here is a Hamiltonian circuit of the -cube ( ). If one can draw a path along the edges in the 3-cube that starts at any vertex, passes through every other vertex once, and returns to the start, then that sequence of bit patterns can be used to coat the back of the dial so that between every sector there is only one questionable switch. Such a path is not difficult to find, as we will see below.   The 3-cube   The 3-cube    Many A-D conversion problems require many more sectors and switches than this example, and the same kinds of problems can occur. The solution would be to find a path within a much larger yet similar graph. For example, there might be 1,024 sectors with 10 switches, resulting in a graph with 1,024 vertices. Fortunately, our solution will apply to the -cube for any positive value of .  A Hamiltonian circuit of the -cube can be described recursively. The circuit itself, called the Gray Code, is not the only Hamiltonian circuit of the -cube, but it is the easiest to describe. The standard way to write the Gray Code is as a column of strings, where the last string is followed by the first string to complete the circuit.  Basis for the Gray Code ( ): The Gray Code for the 1-cube is . Note that the edge between 0 and 1 is used twice in this circuit. That doesn't violate any rules for Hamiltonian circuits, but can only happen if a graph has two vertices.  Recursive definition of the Gray Code: Given the Gray Code for the -cube, , then is obtained by (1) listing with each string prefixed with 0, and then (2) reversing the list of strings in with each string prefixed with 1. Symbolically, the recursion can be expressed as follows, where is the reverse of list .   The Gray Codes for the 2-cube and 3-cube are   One question might come to mind at this point. If the coatings of the dial no longer in the sequence from 0 to 7, how would you interpret the patterns that are on the back of the dial as numbers from 0 to 7? In Chapter 14 we will see that if the Gray Code is used, this decoding is quite easy.   Applications of the Gray Code One application of the Gray code was discussed in the Introduction to this book. Another application is in statistics. In a statistical analysis, there is often a variable that depends on several factors, but exactly which factors are significant may not be obvious. For each subset of factors, there would be certain quantities to be calculated. One such quantity is the multiple correlation coefficient for a subset. If the correlation coefficient for a given subset, , is known, then the value for any subset that is obtained by either deleting or adding an element to can be obtained quickly. To calculate the correlation coefficient for each set, we simply travel along , where is the number of factors being studied. The first vertex will always be the string of 0's, which represents the empty set. For each vertex that you visit, the set that it corresponds to contains the factor if the character is a 1.   The 3-cube and its generalization, the -cube, play a role in the design of a multiprocessor called a hypercube. A multiprocessor is a computer that consists of several independent processors that can operate simultaneously and are connected to one another by a network of connections. In a hypercube with processors, the processors are numbered 0 to . Two processors are connected if their binary representations differ in exactly one bit. The hypercube has proven to be the best possible network for certain problems requiring the use of a supercomputer.    Exercises  Locate a map of New York City and draw a graph that represents its land masses, bridges and tunnels. Is there an Eulerian path through New York? You can do the same with any other city that has at least two land masses. Using a recent road map, it appears that an Eulerian circuit exists in New York City, not including the small islands that belong to the city. Lowell, Massachusetts, is located at the confluence of the Merrimack and Concord rivers and has several canals flowing through it. No Eulerian path exists for Lowell.  Which of the following drawings can be drawn without removing your pencil from the paper and without drawing any line twice?                  You should be able to draw this without raising your pen\/pencil from the paper starting at the middle of the dog's eyes and ending at the top of the nose. Those points are the only ones were an odd number of lines meet.  There are ten points on the drawing there three lines meet, so there is no way to draw this figure without raising your pen\/pencil from the paper.  All junctions of lines have four lines and so you should be able to draw this without raising your pen\/pencil from the paper starting anywhere.     Write out the Gray Code for the 4-cube. Gray Code for the 4-cube:   Find a Hamiltonian circuit for the dodecahedron graph in . There are many different Hamiltonian circuits. Here is one of them: . Normally, starting at 14 instead of 1 and following the sames sequence of edges, wouldn't be considered different. Reversing the cycle , probably would be considered different, depending exactly one how you define different.   The Euler Construction Company has been contracted to construct an extra bridge in Koenigsberg so that an Eulerian path through the town exists. Can this be done, and if so, where should the bridge be built? Any bridge between two land masses will be sufficient. To get an Eulerian circuit, you must add a second bridge that connects the two land masses that were not connected by the first bridge.  Consider the graphs in . Determine which of the graphs have an Eulerian path, and find an Eulerian path for the graphs that have one.   Graphs for exercise 6   graphs for exercise 9-4-6      No Eulerian path since more than two vertices have odd degree.  There is an Eulerian circuit since the graph is connected and all vertices have even degree.  There is an Eulerian path between vertices 4 and 7 since the graph is connected and those two vertices are the only ones with odd degree.  This graph is not connected since no even numbered vertex is connected to an odd numbered vertex. Therefore, although all vertices have degree four it is not Eulerian    Formulate Euler's theorem for directed graphs.  Let be a directed graph. has an Eulerian circuit if and only if is connected and for all . There exists an Eulerian path from if and only if is connected, , , and for all other vertices in the indegree and outdegree are equal.   Prove that the number of vertices in an undirected graph with odd degree must be even. You could prove this by induction on the number of edges, but an easier way would be to consider the degree sequence and use something you know about the sum of the entries.  We know that since each edge contributes to the sum of the degrees of vertices in any graph, the sum of entries in its degree sequence must be even. In order for the sum of integers to be even there has to be an even number odd numbers in the sequence, which is what we wanted to prove. An induction proof is a little longer. We would start with the basis, which is to observe that if there are zero edges, there are zero vertices of odd degree. Then we assume that for some , all undirected graphs with edges have and even number of vertices of odd degree. Imagine that we now have an undirected graph with edges. Temporarily remove any edge and that gives us an undirected graph with edges which, by the induction hypothesis has an even number of vertices of odd degree. Now reintroduce the edge we removed. Depending on the degrees of the two vertices it connects in the reduced graph, we end up turning two odd vertices into even ones, turning two even vertices into odd ones, or turning and odd and even vertex inot an even and odd vertex. In any case the number of odd vertices changes by on even number, which completes the proof.     Under what conditions will a round-robin tournament graph be Eulerian?  Prove that every round-robin tournament graph has a Hamiltonian path.   A round-robin tournament graph is rarely Eulerian. It will be Eulerian if it has an odd number of vertices and each vertex (team) wins exactly as many times as it loses. Every round-robin tournament graph has a Hamiltonian path. This can be proven by induction on the number of vertices.  For what values of is the -cube Eulerian?   Since every vertex on an -cube has degree , the -cubes for which is even are Eulerian.   A particular set of dominoes has 21 tiles: . Is it possible to lay all 21 tiles in a line so that each adjacent pair of tile ends matches (that is, each 1 abuts a 1, and so on)?  No, such a line does not exist. The dominoes with two different numbers correspond with edges in a . See corresponding dominos and edges in . Dominos with two equal numbers could be held back and inserted into the line created with the other dominoes if such a line exists. For example, if were part of the line, could be inserted between those two dominoes. The line we want exists if and only if there exists an Eulerian path in a . Since all six vertices of a have odd degree no such path exists.   Correspondence between a line of dominos and a path in a    Four dominos lay end-to-end with numbers on abutting ends matching. They correspond with four connecting edges in a .     Let be the graph below.      An undirected graph     Explain why it’s not possible to find an Eulerian circuit in .  Remove two edges from so the resulting graph has an Eulerian circuit. Then list the vertices of an Eulerian circuit in in the order in which they are visited by the circuit.    There is no Eulerian circuit in because there are two vertices of odd degree.  If we remove the two edges and from the resulting graph has an Eulerian circuit. If we start at , one Eulerian circuit, which is not unique, is      "
},
{
  "id": "fig-konigsberg-map",
  "level": "2",
  "url": "s-traversals.html#fig-konigsberg-map",
  "type": "Figure",
  "number": "9.4.1",
  "title": "",
  "body": " A map of Koenigsberg, circa 1735   A map of Koenigsberg   "
},
{
  "id": "fig-konigsberg-multigraph",
  "level": "2",
  "url": "s-traversals.html#fig-konigsberg-multigraph",
  "type": "Figure",
  "number": "9.4.2",
  "title": "",
  "body": " A multigraph for the bridges of Koenigsberg   "
},
{
  "id": "th-euler-theorem-koenigsberg-case",
  "level": "2",
  "url": "s-traversals.html#th-euler-theorem-koenigsberg-case",
  "type": "Theorem",
  "number": "9.4.3",
  "title": "Euler’s Theorem: Koenigsberg Case.",
  "body": "Euler's Theorem: Koenigsberg Case Euler's Theorem Koenigsberg Case  No walking tour of Koenigsberg can be designed so that each bridge is used exactly once.  The map of Koenigsberg can be represented as an undirected multigraph, as in . The four land masses are the vertices and each edge represents a bridge.  The desired tour is then a path that uses each edge once and only once. Since the path can start and end at two different vertices, there are two remaining vertices that must be intermediate vertices in the path. If is an intermediate vertex, then every time that you visit , you must use two of its incident edges, one to enter and one to exit. Therefore, there must be an even number of edges connecting to the other vertices. Since every vertex in the Koenigsberg graph has an odd number of edges, no tour of the type that is desired is possible.   "
},
{
  "id": "def-eulerian-paths-circuits-graphs",
  "level": "2",
  "url": "s-traversals.html#def-eulerian-paths-circuits-graphs",
  "type": "Definition",
  "number": "9.4.4",
  "title": "Eulerian Paths, Circuits, Graphs.",
  "body": "Eulerian Paths, Circuits, Graphs Eulerian Paths, Circuits, Graphs An Eulerian path through a graph is a path whose edge list contains each edge of the graph exactly once. If the path is a circuit, then it is called an Eulerian circuit. An Eulerian graph is a graph that possesses an Eulerian circuit. "
},
{
  "id": "ex-an-eulerian-graph",
  "level": "2",
  "url": "s-traversals.html#ex-an-eulerian-graph",
  "type": "Example",
  "number": "9.4.5",
  "title": "An Eulerian Graph.",
  "body": "An Eulerian Graph Without tracing any paths, we can be sure that the graph below has an Eulerian circuit because all vertices have an even degree. This follows from the following theorem.   An Eulerian graph   An Eulerian graph    "
},
{
  "id": "theorem-euler-theorem-general",
  "level": "2",
  "url": "s-traversals.html#theorem-euler-theorem-general",
  "type": "Theorem",
  "number": "9.4.7",
  "title": "Euler’s Theorem: General Case.",
  "body": "Euler's Theorem: General Case  Euler's Theorem  An undirected graph has an Eulerian path if and only if it is connected and has either zero or two vertices with an odd degree. If no vertex has an odd degree, then the graph is Eulerian.  It can be proven by induction that the number of vertices in an undirected graph that have an odd degree must be even. We will leave the proof of this fact to the reader as an exercise. The necessity of having either zero or two vertices of odd degree is clear from the proof of the Koenigsberg case of this theorem. Therefore, we will concentrate on proving that this condition is sufficient to ensure that a graph has an Eulerian path. Let be the number of vertices with odd degree.  Phase 1. If , start at any vertex, , and travel along any path, not using any edge twice. Since each vertex has an even degree, this path can always be continued past each vertex that you reach except . The result is a circuit that includes . If , let be either one of the vertices of odd degree. Trace any path starting at using up edges until you can go no further, as in the case. This time, the path that you obtain must end at the other vertex of odd degree that we will call . At the end of Phase 1, we have an initial path that may or may not be Eulerian. If it is not Eulerian, Phase 2 can be repeated until all of the edges have been used. Since the number of unused edges is decreased in any use of Phase 2, an Eulerian path must be obtained in a finite number of steps.  Phase 2. As we enter this phase, we have constructed a path that uses a proper subset of the edges in our graph. We will refer to this path as the current path. Let be the vertices of our graph, the edges, and the edges that have been used in the current path. Consider the graph . Note that every vertex in has an even degree. Select any edge, , from Let and be the vertices that connects. Trace a new path starting at whose first edge is . We can be sure that at least one vertex of the new path is also in the current path since is connected. Starting at , there exists a path in to any vertex in the current path. At some point along this path, which we can consider the start of the new path, we will have intersected the current path. Since the degree of each vertex in is even, any path that we start at can be continued until it is a circuit. Now, we simply augment the current path with this circuit. As we travel along the current path, the first time that we intersect the new path, we travel along it (see ). Once we complete the circuit that is the new path, we resume the traversal of the current path.   Path Augmentation Plan   Path Augmentation Plan    If the result of this phase is an Eulerian path, then we are finished; otherwise, repeat this phase.  "
},
{
  "id": "ex-complete-eulerian",
  "level": "2",
  "url": "s-traversals.html#ex-complete-eulerian",
  "type": "Example",
  "number": "9.4.9",
  "title": "Complete Eulerian Graphs.",
  "body": "Complete Eulerian Graphs The complete undirected graphs , . .., are Eulerian. If , then is not Eulerian. "
},
{
  "id": "fig-hamilton-stamp",
  "level": "2",
  "url": "s-traversals.html#fig-hamilton-stamp",
  "type": "Figure",
  "number": "9.4.10",
  "title": "",
  "body": " Irish stamp honoring Sir William Rowan Hamilton   Irish stamp honoring Sir William Rowan Hamilton   "
},
{
  "id": "def-hamiltonian-path-circuit-graph",
  "level": "2",
  "url": "s-traversals.html#def-hamiltonian-path-circuit-graph",
  "type": "Definition",
  "number": "9.4.11",
  "title": "Hamiltonian Path, Circuit, and Graphs.",
  "body": "Hamiltonian Path, Circuit, and Graphs Hamiltonian Paths, Circuits, and Graphs  A Hamiltonian path through a graph is a path whose vertex list contains each vertex of the graph exactly once, except if the path is a circuit, in which case the initial vertex appears a second time as the terminal vertex. If the path is a circuit, then it is called a Hamiltonian circuit. A Hamiltonian graph is a graph that possesses a Hamiltonian circuit. "
},
{
  "id": "ex-dodec-graph",
  "level": "2",
  "url": "s-traversals.html#ex-dodec-graph",
  "type": "Example",
  "number": "9.4.12",
  "title": "The Original Hamiltonian Graph.",
  "body": "The Original Hamiltonian Graph shows a graph that is Hamiltonian. In fact, it is the graph that Hamilton used as an example to pose the question of existence of Hamiltonian paths in 1859. In its original form, the puzzle that was posed to readers was called Around the World. The vertices were labeled with names of major cities of the world and the object was to complete a tour of these cities. The graph is also referred to as the dodecahedron graph, where vertices correspond with the corners of a dodecahedron and the edges are the edges of the solid that connect the corners.    A Dodecahedron   A Dodecahedron     The Dodecahedron Graph   The Dodecahedron Graph     "
},
{
  "id": "ss-hamiltonian-graphs-6",
  "level": "2",
  "url": "s-traversals.html#ss-hamiltonian-graphs-6",
  "type": "Problem",
  "number": "9.4.15",
  "title": "",
  "body": "Unfortunately, a simple condition doesn't exist that characterizes a Hamiltonian graph. An obvious necessary condition is that the graph be connected; however, there is a connected undirected graph with four vertices that is not Hamiltonian. Can you draw such a graph? "
},
{
  "id": "ss-hamiltonian-graphs-7",
  "level": "2",
  "url": "s-traversals.html#ss-hamiltonian-graphs-7",
  "type": "Note",
  "number": "9.4.16",
  "title": "What Is Possible and What Is Impossible?",
  "body": "What Is Possible and What Is Impossible? The search for a Hamiltonian path in a graph is typical of many simple-sounding problems in graph theory that have proven to be very difficult to solve. Although there are simple algorithms for conducting the search, they are impractical for large problems because they take such a long time to complete as graph size increases. Currently, every algorithm to search for a Hamiltonian path in a graph takes a time that grows at a rate that is greater than any polynomial as a function of the number of vertices. Rates of this type are called super-polynomial . That is, if is the time it takes to search a graph of vertices, and is any polynomial, then for all but possibly a finite number of positive values for .  It is an unproven but widely held belief that no faster algorithm exists to search for Hamiltonian paths in general graphs. To sum up, the problem of determining whether a graph is Hamiltonian is theoretically possible; however, for large graphs we consider it a practical impossibility. Many of the problems we will discuss in the next section, particularly the Traveling Salesman Problem, are thought to be impossible in the same sense. "
},
{
  "id": "def-n-cube",
  "level": "2",
  "url": "s-traversals.html#def-n-cube",
  "type": "Definition",
  "number": "9.4.17",
  "title": "The <span xmlns:pf=\"https:\/\/prefigure.org\" class=\"process-math\">\\(n\\)<\/span>-cube.",
  "body": "The -cube  N-cube the -cube Let , and let be the set of strings of 0's and 1's with length . The -cube is the undirected graph with a vertex for each string in and an edge connecting each pair of strings that differ in exactly one position. The -cube is normally denoted . "
},
{
  "id": "ex-intro-a-to-d",
  "level": "2",
  "url": "s-traversals.html#ex-intro-a-to-d",
  "type": "Example",
  "number": "9.4.18",
  "title": "Analog-to-digital Conversion and the Gray Code.",
  "body": "Analog-to-digital Conversion and the Gray Code  Analog-to-digital Conversion  Gray Code  A common problem encountered in engineering is that of analog-to-digital (a-d) conversion, where the reading on a dial, for example, must be converted to a numerical value. In order for this conversion to be done reliably and quickly, one must solve an interesting problem in graph theory. Before this problem is posed, we will make the connection between a-d conversion and the graph problem using a simple example. Suppose a dial can be turned in any direction, and that the positions will be converted to one of the numbers zero through seven as depicted in . The angles from 0 to 360 are divided into eight equal parts, and each part is assigned a number starting with 0 and increasing clockwise. If the dial points in any of these sectors the conversion is to the number of that sector. If the dial is on the boundary, then we will be satisfied with the conversion to either of the numbers in the bordering sectors. This conversion can be thought of as giving an approximate angle of the dial, for if the dial is in sector , then the angle that the dial makes with east is approximately .   Analog-Digital Dial   Analog-Digital Dial    Now that the desired conversion has been identified, we will describe a solution that has one major error in it, and then identify how this problem can be rectified. All digital computers represent numbers in binary form, as a sequence of 0's and 1's called bits, short for binary digits. The binary representations of numbers 0 through 7 are:   The way that we could send those bits to a computer is by coating parts of the back of the dial with a metallic substance, as in . For each of the three concentric circles on the dial there is a small magnet. If a magnet lies under a part of the dial that has been coated with metal, then it will turn a switch ON, whereas the switch stays OFF when no metal is detected above a magnet. Notice how every ON\/OFF combination of the three switches is possible given the way the back of the dial is coated.  If the dial is placed so that the magnets are in the middle of a sector, we expect this method to work well. There is a problem on certain boundaries, however. If the dial is turned so that the magnets are between sectors three and four, for example, then it is unclear what the result will be. This is due to the fact that each magnet will have only a fraction of the required metal above it to turn its switch ON. Due to expected irregularities in the coating of the dial, we can be safe in saying that for each switch either ON or OFF could be the result, and so if the dial is between sectors three and four, any number could be indicated. This problem does not occur between every sector. For example, between sectors 0 and 1, there is only one switch that cannot be predicted. No matter what the outcome is for the units switch in this case, the indicated sector must be either 0 or 1. This consistent with the original objective that a positioning of the dial on a boundary of two sectors should produce the number of either sector.   Coating scheme for the Analog-Digital Dial   Analog-Digital Dial    Is there a way to coat the sectors on the back of the dial so that each of the eight patterns corresponding to the numbers 0 to 7 appears once, and so that between any two adjacent sectors there is only one switch that will have a questionable setting? What we are describing here is a Hamiltonian circuit of the -cube ( ). If one can draw a path along the edges in the 3-cube that starts at any vertex, passes through every other vertex once, and returns to the start, then that sequence of bit patterns can be used to coat the back of the dial so that between every sector there is only one questionable switch. Such a path is not difficult to find, as we will see below.   The 3-cube   The 3-cube    Many A-D conversion problems require many more sectors and switches than this example, and the same kinds of problems can occur. The solution would be to find a path within a much larger yet similar graph. For example, there might be 1,024 sectors with 10 switches, resulting in a graph with 1,024 vertices. Fortunately, our solution will apply to the -cube for any positive value of .  A Hamiltonian circuit of the -cube can be described recursively. The circuit itself, called the Gray Code, is not the only Hamiltonian circuit of the -cube, but it is the easiest to describe. The standard way to write the Gray Code is as a column of strings, where the last string is followed by the first string to complete the circuit.  Basis for the Gray Code ( ): The Gray Code for the 1-cube is . Note that the edge between 0 and 1 is used twice in this circuit. That doesn't violate any rules for Hamiltonian circuits, but can only happen if a graph has two vertices.  Recursive definition of the Gray Code: Given the Gray Code for the -cube, , then is obtained by (1) listing with each string prefixed with 0, and then (2) reversing the list of strings in with each string prefixed with 1. Symbolically, the recursion can be expressed as follows, where is the reverse of list .   The Gray Codes for the 2-cube and 3-cube are   One question might come to mind at this point. If the coatings of the dial no longer in the sequence from 0 to 7, how would you interpret the patterns that are on the back of the dial as numbers from 0 to 7? In Chapter 14 we will see that if the Gray Code is used, this decoding is quite easy.  "
},
{
  "id": "ss-hamiltonian-graphs-12",
  "level": "2",
  "url": "s-traversals.html#ss-hamiltonian-graphs-12",
  "type": "Example",
  "number": "9.4.22",
  "title": "Applications of the Gray Code.",
  "body": "Applications of the Gray Code One application of the Gray code was discussed in the Introduction to this book. Another application is in statistics. In a statistical analysis, there is often a variable that depends on several factors, but exactly which factors are significant may not be obvious. For each subset of factors, there would be certain quantities to be calculated. One such quantity is the multiple correlation coefficient for a subset. If the correlation coefficient for a given subset, , is known, then the value for any subset that is obtained by either deleting or adding an element to can be obtained quickly. To calculate the correlation coefficient for each set, we simply travel along , where is the number of factors being studied. The first vertex will always be the string of 0's, which represents the empty set. For each vertex that you visit, the set that it corresponds to contains the factor if the character is a 1.  "
},
{
  "id": "exercises-9-4-2",
  "level": "2",
  "url": "s-traversals.html#exercises-9-4-2",
  "type": "Exercise",
  "number": "9.4.3.1",
  "title": "",
  "body": "Locate a map of New York City and draw a graph that represents its land masses, bridges and tunnels. Is there an Eulerian path through New York? You can do the same with any other city that has at least two land masses. Using a recent road map, it appears that an Eulerian circuit exists in New York City, not including the small islands that belong to the city. Lowell, Massachusetts, is located at the confluence of the Merrimack and Concord rivers and has several canals flowing through it. No Eulerian path exists for Lowell. "
},
{
  "id": "exercises-9-4-3",
  "level": "2",
  "url": "s-traversals.html#exercises-9-4-3",
  "type": "Exercise",
  "number": "9.4.3.2",
  "title": "",
  "body": "Which of the following drawings can be drawn without removing your pencil from the paper and without drawing any line twice?                  You should be able to draw this without raising your pen\/pencil from the paper starting at the middle of the dog's eyes and ending at the top of the nose. Those points are the only ones were an odd number of lines meet.  There are ten points on the drawing there three lines meet, so there is no way to draw this figure without raising your pen\/pencil from the paper.  All junctions of lines have four lines and so you should be able to draw this without raising your pen\/pencil from the paper starting anywhere.    "
},
{
  "id": "exercises-9-4-4",
  "level": "2",
  "url": "s-traversals.html#exercises-9-4-4",
  "type": "Exercise",
  "number": "9.4.3.3",
  "title": "",
  "body": "Write out the Gray Code for the 4-cube. Gray Code for the 4-cube:  "
},
{
  "id": "exercises-9-4-5",
  "level": "2",
  "url": "s-traversals.html#exercises-9-4-5",
  "type": "Exercise",
  "number": "9.4.3.4",
  "title": "",
  "body": "Find a Hamiltonian circuit for the dodecahedron graph in . There are many different Hamiltonian circuits. Here is one of them: . Normally, starting at 14 instead of 1 and following the sames sequence of edges, wouldn't be considered different. Reversing the cycle , probably would be considered different, depending exactly one how you define different.  "
},
{
  "id": "exercises-9-4-6",
  "level": "2",
  "url": "s-traversals.html#exercises-9-4-6",
  "type": "Exercise",
  "number": "9.4.3.5",
  "title": "",
  "body": "The Euler Construction Company has been contracted to construct an extra bridge in Koenigsberg so that an Eulerian path through the town exists. Can this be done, and if so, where should the bridge be built? Any bridge between two land masses will be sufficient. To get an Eulerian circuit, you must add a second bridge that connects the two land masses that were not connected by the first bridge. "
},
{
  "id": "exercises-9-4-7",
  "level": "2",
  "url": "s-traversals.html#exercises-9-4-7",
  "type": "Exercise",
  "number": "9.4.3.6",
  "title": "",
  "body": "Consider the graphs in . Determine which of the graphs have an Eulerian path, and find an Eulerian path for the graphs that have one.   Graphs for exercise 6   graphs for exercise 9-4-6      No Eulerian path since more than two vertices have odd degree.  There is an Eulerian circuit since the graph is connected and all vertices have even degree.  There is an Eulerian path between vertices 4 and 7 since the graph is connected and those two vertices are the only ones with odd degree.  This graph is not connected since no even numbered vertex is connected to an odd numbered vertex. Therefore, although all vertices have degree four it is not Eulerian   "
},
{
  "id": "exercises-9-4-8",
  "level": "2",
  "url": "s-traversals.html#exercises-9-4-8",
  "type": "Exercise",
  "number": "9.4.3.7",
  "title": "",
  "body": "Formulate Euler's theorem for directed graphs.  Let be a directed graph. has an Eulerian circuit if and only if is connected and for all . There exists an Eulerian path from if and only if is connected, , , and for all other vertices in the indegree and outdegree are equal.  "
},
{
  "id": "exercises-9-4-9",
  "level": "2",
  "url": "s-traversals.html#exercises-9-4-9",
  "type": "Exercise",
  "number": "9.4.3.8",
  "title": "",
  "body": "Prove that the number of vertices in an undirected graph with odd degree must be even. You could prove this by induction on the number of edges, but an easier way would be to consider the degree sequence and use something you know about the sum of the entries.  We know that since each edge contributes to the sum of the degrees of vertices in any graph, the sum of entries in its degree sequence must be even. In order for the sum of integers to be even there has to be an even number odd numbers in the sequence, which is what we wanted to prove. An induction proof is a little longer. We would start with the basis, which is to observe that if there are zero edges, there are zero vertices of odd degree. Then we assume that for some , all undirected graphs with edges have and even number of vertices of odd degree. Imagine that we now have an undirected graph with edges. Temporarily remove any edge and that gives us an undirected graph with edges which, by the induction hypothesis has an even number of vertices of odd degree. Now reintroduce the edge we removed. Depending on the degrees of the two vertices it connects in the reduced graph, we end up turning two odd vertices into even ones, turning two even vertices into odd ones, or turning and odd and even vertex inot an even and odd vertex. In any case the number of odd vertices changes by on even number, which completes the proof.  "
},
{
  "id": "exercises-9-4-10",
  "level": "2",
  "url": "s-traversals.html#exercises-9-4-10",
  "type": "Exercise",
  "number": "9.4.3.9",
  "title": "",
  "body": "  Under what conditions will a round-robin tournament graph be Eulerian?  Prove that every round-robin tournament graph has a Hamiltonian path.   A round-robin tournament graph is rarely Eulerian. It will be Eulerian if it has an odd number of vertices and each vertex (team) wins exactly as many times as it loses. Every round-robin tournament graph has a Hamiltonian path. This can be proven by induction on the number of vertices. "
},
{
  "id": "exercises-9-4-11",
  "level": "2",
  "url": "s-traversals.html#exercises-9-4-11",
  "type": "Exercise",
  "number": "9.4.3.10",
  "title": "",
  "body": "For what values of is the -cube Eulerian?   Since every vertex on an -cube has degree , the -cubes for which is even are Eulerian.  "
},
{
  "id": "exercises-9-4-12",
  "level": "2",
  "url": "s-traversals.html#exercises-9-4-12",
  "type": "Exercise",
  "number": "9.4.3.11",
  "title": "",
  "body": "A particular set of dominoes has 21 tiles: . Is it possible to lay all 21 tiles in a line so that each adjacent pair of tile ends matches (that is, each 1 abuts a 1, and so on)?  No, such a line does not exist. The dominoes with two different numbers correspond with edges in a . See corresponding dominos and edges in . Dominos with two equal numbers could be held back and inserted into the line created with the other dominoes if such a line exists. For example, if were part of the line, could be inserted between those two dominoes. The line we want exists if and only if there exists an Eulerian path in a . Since all six vertices of a have odd degree no such path exists.   Correspondence between a line of dominos and a path in a    Four dominos lay end-to-end with numbers on abutting ends matching. They correspond with four connecting edges in a .    "
},
{
  "id": "exercises-9-4-13",
  "level": "2",
  "url": "s-traversals.html#exercises-9-4-13",
  "type": "Exercise",
  "number": "9.4.3.12",
  "title": "",
  "body": "Let be the graph below.      An undirected graph     Explain why it’s not possible to find an Eulerian circuit in .  Remove two edges from so the resulting graph has an Eulerian circuit. Then list the vertices of an Eulerian circuit in in the order in which they are visited by the circuit.    There is no Eulerian circuit in because there are two vertices of odd degree.  If we remove the two edges and from the resulting graph has an Eulerian circuit. If we start at , one Eulerian circuit, which is not unique, is    "
},
{
  "id": "s-graph-optimization",
  "level": "1",
  "url": "s-graph-optimization.html",
  "type": "Section",
  "number": "9.5",
  "title": "Graph Optimization",
  "body": " Graph Optimization  Graph Optimization   The common thread that connects all of the problems in this section is the desire to optimize (maximize or minimize) a quantity that is associated with a graph. We will concentrate most of our attention on two of these problems, the Traveling Salesman Problem and the Maximum Flow Problem. At the close of this section, we will discuss some other common optimization problems.   Weighted Graphs  Weighted Graph Weighted Graph A weighted graph, , is a graph together with a weight function . If , is the weight on edge .  As you will see in our examples, is often a cost associated with the edge ; therefore, most weights will be positive.  A Distance Graph Let be the set of six capital cities in New England: Boston, Augusta, Hartford, Providence, Concord, and Montpelier. Let be the set of all possible undirected edges between these cities. So is a complete unordered graph. An example of a weight function on this graph is .  Many road maps that you could get at gas stations at one time defined distance functions as in the following table.   Distances between capital cities in New England   -- Augusta Boston Concord Hartford Montpelier Providence  Augusta, ME -- 165 148 266 190 208  Boston, MA 165 -- 75 103 192 43  Concord, NH 148 75 -- 142 117 109  Hartford, CT 266 103 142 -- 204 70  Montpelier, VT 190 192 117 204 -- 223  Providence, RI 208 43 109 70 223 --      The Traveling Salesman Problem Traveling Salesman Problem, The  The Traveling Salesman Problem is, given a weighted graph, to find a circuit that visits every vertex at least once and minimizes the sum of the weights, . Any such circuit is called an optimal path.  Some statements of the Traveling Salesman Problem require that the circuit be Hamiltonian. In many applications, the graph in question will be complete and this restriction presents no problem. If the weight on each edge is constant, for example, , then an optimal path would be any Hamiltonian circuit.  The problem of a Boston salesman The Traveling Salesman Problem gets its name from the situation of a salesman who wants to minimize the number of miles that he travels in visiting his customers. For example, if a salesman from Boston must visit the other capital cities of New England, then the problem is to find a circuit in the weighted graph of . Note that distance and cost are clearly related in this case. In addition, tolls and traffic congestion might also be taken into account.  The search for an efficient algorithm that solves the Traveling Salesman has occupied researchers for years. If the graph in question is complete, there are different circuits. As gets large, it is impossible to check every possible circuit. The most efficient algorithms for solving the Traveling Salesman Problem take an amount of time that is proportional to . Since this quantity grows so quickly, we can't expect to have the time to solve the Traveling Salesman Problem for large values of . Most of the useful algorithms that have been developed have to be heuristic; that is, they find a circuit that should be close to the optimal one. One such algorithm is the closest neighbor algorithm, one of the earliest attempts at solving the Traveling Salesman Problem. The general idea behind this algorithm is, starting at any vertex, to visit the closest neighbor to the starting point. At each vertex, the next vertex that is visited is the closest one that has not been reached. This shortsighted approach typifies heuristic algorithms called greedy algorithms, which attempt to solve a minimization (maximization) problem by minimizing (maximizing) the quantity associated with only the first step.  The Closest Neighbor Algorithm Closest Neighbor Algorithm  Let be a complete weighted graph with . The closest neighbor circuit through G starting at is , defined by the steps:    .  For to     the closest vertex in to  ##  ## In case of a tie for closest, may be chosen arbitrarily.     the only element of   The cost of the closest neighbor circuit is    A small example The closest neighbor circuit starting at 1 in is , with a cost of 29. The optimal path is , with a cost of 27.   A small example   A small weighted graph     Although the closest neighbor circuit is often not optimal, we may be satisfied if it is close to optimal. If and are the costs of optimal and closest neighbor circuits in a graph, then it is always the case that or . We can assess how good the closest neighbor algorithm is by determining how small the quantity gets. If it is always near 1, then the algorithm is good. However, if there are graphs for which it is large, then the algorithm may be discarded. Note that in , . A 7 percent increase in cost may or may not be considered significant, depending on the situation.  The One-way Street A salesman must make stops at vertices A, B, and C, which are all on the same one-way street. The graph in is weighted by the function equal to the time it takes to drive from vertex to vertex .   Traveling a one-way street   A small directed weighted graph    Note that if is down the one-way street from , then . The values of , and are 20 and 32, respectively. Verify that is 32 by using the closest neighbor algorithm. The value of is significant in this case since our salesman would spend 60 percent more time on the road if he used the closest neighbor algorithm.   A more general result relating to the closest neighbor algorithm presumes that the graph in question is complete and that the weight function satisfies the conditions    for all , in the vertex set, and   for all , , in the vertex set.   The first condition is called the symmetry condition and the second is the triangle inequality.  If is a complete weighted graph on vertices that satisfies the symmetry and triangle inequality conditions, then   If , then this theorem says that can be no larger than twice the size of ; however, it doesn't say that the closest neighbor circuit will necessarily be that far from an optimal circuit. The quantity is called an upper bound for the ratio . It tells us only that things can't be any worse than the upper bound. Certainly, there are many graphs with eight vertices such that the optimal and closest neighbor circuits are the same. What is left unstated in this theorem is whether there are graphs for which the quantities are equal. If there are such graphs, we say that the upper bound is sharp.  The value of in Example is 1.6, which is greater than ; however, the weight function in this example does not satisfy the conditions of the theorem.   The Unit Square Problem Suppose a robot is programmed to weld joints on square metal plates. Each plate must be welded at prescribed points on the square. To minimize the time it takes to complete the job, the total distance that a robot's arm moves should be minimized. Let be the distance between and . Assume that before each plate can be welded, the arm must be positioned at a certain point . Given a list of points, we want to put them in order so that is as small as possible.  The type of problem that is outlined in the example above is of such importance that it is one of the most studied version of the Traveling Salesman Problem. What follows is the usual statement of the problem. Let , and let , the unit square. Given pairs of real numbers in that represent the vertices of a , find a circuit of the graph that minimizes the sum of the distances traveled in traversing the circuit.  Since the problem calls for a circuit, it doesn't matter which vertex we start at; assume that we will start at . Once the problem is solved, we can always change our starting position. A function can most efficiently describe a circuit in this problem. Every bijection with describes a circuit There are such bijections. Since a circuit and its reversal have the same associated cost, there are cases to consider. An examination of all possible cases is not feasible for large values of .  One popular heuristic algorithm is the strip algorithm:  The Strip Algorithm  Given points in the unit square:  Phase 1:  Divide the square into vertical strips, as in . Let d be the width of each strip. If a point lies on a boundary between two strips, consider it part of the left-hand strip.  Starting from the left, find the first strip that contains one of the points. Locate the starting point by selecting the first point that is encountered in that strip as you travel from bottom to top. We will assume that the first point is  Alternate traveling up and down the strips that contain vertices until all of the vertices have been reached.  Return to the starting point.   Phase 2:  Shift all strips units to the right (creating a small strip on the left).  Repeat Steps 1.2 through 1.4 of Phase 1 with the new strips.   When the two phases are complete, choose the shorter of the two circuits obtained.     The Strip Algorithm   The Strip Algorithm    Step may need a bit more explanation. How do you travel up or down a strip? In most cases, the vertices in a strip will be vertically distributed so that the order in which they are visited is obvious. In some cases, however, the order might not be clear, as in the third strip in Phase I of . Within a strip, the order in which you visit the points (if you are going up the strip) is determined thusly: precedes if or if and . In traveling down a strip, replace with .  The selection of strips was made in . It balances the problems that arise if the number of strips is too small or too large. If the square is divided into too few strips, some strips may be packed with vertices so that visiting them would require excessive horizontal motion. If too many strips are used, excessive vertical motion tends to be the result. An update on what is known about this algorithm is contained in .  Since the construction of a circuit in the square consists of sorting the given points, it should come as no surprise that the strip algorithm requires a time that is roughly a multiple of time units when points are to be visited.  The worst case that has been encountered with this algorithm is one in which the circuit obtained has a total distance of approximately (see Sopowit et al.).   Networks and the Maximum Flow Problem Networks  Network Network  A network is a simple weighted directed graph that contains two distinguished vertices called the source and the sink with the properties that the indegree of the source and outdegree of the sink are both zero, and source is connected to sink. The weight function on a network is the capacity function, which has positive weights.  An example of a real situation that can be represented by a network is a city's water system. A reservoir would be the source, while a distribution point in the city to all of the users would be the sink. The system of pumps and pipes that carries the water from source to sink makes up the remaining network. We can assume that the water that passes through a pipe in one minute is controlled by a pump and the maximum rate is determined by the size of the pipe and the strength of the pump. This maximum rate of flow through a pipe is called its capacity and is the information that the weight function of a network contains.  A City Water System Consider the system that is illustrated in . The numbers that appear next to each pipe indicate the capacity of that pipe in thousands of gallons per minute. This map can be drawn in the form of a network, as in .   City Water System   City Water System     Flow Diagram for a City's Water Network   City Water Network    Although the material passing through this network is water, networks can also represent the flow of other materials, such as automobiles, electricity, bits, telephone calls, or patients in a health system.   The Maximum Flow Problem  The Maximum Flow Problem is derived from the objective of moving the maximum amount of water or other material from the source to the sink. To measure this amount, we define a flow as a function such that (1) the flow of material through any edge is nonnegative and no larger than its capacity: , for all ; and (2) for each vertex other than the source and sink, the total amount of material that is directed into a vertex is equal to the total amount that is directed out: The summation on the left of represents the sum of the flows through each edge in that has as a terminal vertex. The right-hand side indicates that you should add all of the flows through edges that initiate at .    Flow out of Source equals Flow in Sink   If is a flow, then   Subtract the right-hand side of from the left-hand side. The result is: Now sum up these differences for each vertex in . The result is   Now observe that if an edge connects two vertices in , its flow appears as both a positive and a negative term in . This means that the only positive terms that are not cancelled out are the flows into the sink. In addition, the only negative terms that remain are the flows out of the source. Therefore,   The Value of a Flow Value of a Flow  The value of flow  The two values flow into the sink and flow out of the source were proved to be equal in and this common value is called the value of the flow . It is denoted by . The value of a flow represents the amount of material that passes through the network with that flow.  Since the Maximum Flow Problem consists of maximizing the amount of material that passes through a given network, it is equivalent to finding a flow with the largest possible value. Any such flow is called a maximal flow Maximal flow .  For the network in , one flow is , defined by , , , , and . The value of , , is 45. Since the total flow into the sink can be no larger than 50 ( ), we can tell that is not very far from the solution. Can you improve on at all? The sum of the capacities into the sink can't always be obtained by a flow. The same is true for the sum of the capacities out of the source. In this case, the sum of the capacities out of the source is 60, which obviously can't be reached in this network.  A solution of the Maximum Flow Problem for this network is the maximal flow , where , , , , and , with . This solution is not unique. In fact, there is an infinite number of maximal flows for this problem.  There have been several algorithms developed to solve the Maximal Flow Problem. One of these is the Ford and Fulkerson Algorithm (FFA). The FFA consists of repeatedly finding paths in a network called flow augmenting paths until no improvement can be made in the flow that has been obtained.  Flow Augmenting Path Flow Augmenting Path  Given a flow in a network , a flow augmenting path with respect to is a simple path from the source to the sink using edges both in their forward and their reverse directions such that for each edge in the path, if is used in its forward direction and if is used in the reverse direction.  Augmenting City Water Flow  For in , a flow augmenting path would be since , , and .  These positive differences represent unused capacities, and the smallest value represents the amount of flow that can be added to each edge in the path. Note that by adding 5 to each edge in our path, we obtain , which is maximal. If an edge with a positive flow is used in its reverse direction, it is contributing a movement of material that is counterproductive to the objective of maximizing flow. This is why the algorithm directs us to decrease the flow through that edge.  The Ford and Fulkerson Algorithm    Define the flow function by for each edge .  i = 0.  Repeat:   If possible, find a flow augmenting path with respect to .  If a flow augmenting path exists, then:   Determine  Define by       until no flow augmenting path exists.  Terminate with a maximal flow     Notes on the Ford and Fulkerson Algorithm   It should be clear that every flow augmenting path leads to a flow of increased value and that none of the capacities of the network can be violated.  The depth-first search should be used to find flow augmenting paths since it is far more efficient than the breadth-first search in this situation. The depth-first search differs from the breadth-first algorithm in that you sequentially visit vertices until you reach a dead end and then backtrack.  There have been networks discovered for which the FFA does not terminate in a finite number of steps. These examples all have irrational capacities. It has been proven that if all capacities are positive integers, the FFA terminates in a finite number of steps. See Ford and Fulkerson, Even, or Berge for details.  When you use the FFA to solve the Maximum Flow Problem by hand it is convenient to label each edge of the network with the fraction .    Depth-First Search for a Flow Augmenting Path   This is a depth-first search for the Sink Initiating at the Source. Let be the set of directed edges that can be used in producing a flow augmenting path. Add to the network a vertex called start and the edge   vertex set of the network.  source Move along the edge   while is not equal to start or sink:   if an edge in exists that takes you from to another vertex in :  .         A flow augmenting path going against the flow Consider the network in , where the current flow, , is indicated by a labeling of the edges.   Current Flow   Current Flow    The path is a flow augmenting path that allows us to increase the flow by one unit. Note that is used in the reverse direction, which is allowed because . The value of the new flow that we obtain is 8. This flow must be maximal since the capacities out of the source add up to 8. This maximal flow is defined by .   Updated Flow   Updated Flow      Other Graph Optimization Problems   The Minimum Spanning Tree Problem: Given a weighted graph, , find a subset of with the properties that is connected and the sum of the weights of edges in is as small as possible. We will discuss this problem in Chapter 10.  The Minimum Matching Problem: Given an undirected weighted graph, , with an even number of vertices, pair up the vertices so that each pair is connected by an edge and the sum of these edges is as small as possible. A unit square version of this problem has been studied extensively. See for details on what is known about this version of the problem.  Center of a Graph The Graph Center Problem: Given a connected, undirected, weighted graph, find a vertex (called a center) in the graph with the property that the distance from the center to every other vertex is as small as possible. As small as possible is normally interpreted as minimizing the maximum distance from the center to a vertex.     Exercises  Find the closest neighbor circuit through the six capitals of New England starting at Boston. If you start at a different city, will you get a different circuit? The circuit would be Boston, Providence, Hartford, Concord, Montpelier, Augusta, Boston. It does matter where you start. If you start in Concord, for example, your mileage will be higher.  Is the estimate in sharp for ? For ? For there is only one circuit and so the ratio in the theorem will be exactly . The upper bound given by the theorem is , so the estimate isn't sharp. For the theorem bounds the ratio to also be . This doesn't appear to be sharp either. However, the actual maximum ration seems to be at least . This can be seen by considering the graph in , where we imagine to be a very small positive real number. If we start at vertex , the nearest neighbor circuit in this graph is with a total cost of . However, the circuit has a total cost of . The ratio can be made as close to as we want.   Updated Flow   A complete graph with vertices labeled A, B, C and D. Edges are AB with weight 1+epsilon, AC with weight 2-epsilon, AD with weight 1, BC with weight 2-epsilon, BD with weight 1, CD with weight 1+epsilon.     Given the following sets of points in the unit square, find the shortest circuit that visits all the points and find the circuit that is obtained with the strip algorithm.             Optimal cost . Phase 1 cost . Phase 2 cost .  Optimal cost Phase 1 cost . Phase 2 cost .    There are 4 points; so we will divide the unit square into two strips.  Optimal Path:  Phase I Path:   Phase II Path:      There are 5 points; so we will divide the unit square into three strips.  Optimal Path:  Phase I Path:  Phase II Path:      For , locate points in the unit square for which the strip algorithm works poorly.  Consider the network whose maximum capacities are shown on the following graph.    Figure for Exercise 9-5-5     A function is partially defined on the edges of this network by: , , , and . Define on the rest of the other edges so that is a flow. What is the value of ?  Find a flow augmenting path with respect to for this network. What is the value of the augmented flow?  Is the augmented flow a maximum flow? Explain.    , , , , and .  There are three possible flow-augmenting paths. with flow increase of 1. with flow increase of 1, and with flow increase of 2.  The new flow is never maximal, since another flow-augmenting path will always exist. For example, if is used above, the new flow can be augmented by 2 units with .    Given the following network with capacity function and flow function , find a maximal flow function. The labels on the edges of the network are of the form , where is the capacity of edge and is the used capacity for flow .    Figure for Exercise 9-5-6     Find maximal flows for the following networks.     Figure for Exercise 9-5-7a      Figure for Exercise 9-5-7b       Figure for Exercise 9-5-7c    Value of maximal flow .  Value of maximal flow .  Value of maximal flow . See for one way to got this flow.     Step Flow-augmenting path Flow added  1 2  2 3  3 4  4 1  5 2  6 2       Find two maximal flows for the network in other than the one found in the text.  Describe the set of all maximal flows for the same network.  Prove that if a network has two maximal flows, then it has an infinite number of maximal flows.     Discuss reasons that the closest neighbor algorithm is not used in the unit square version of the Traveling Salesman Problem. Count the number of comparisons of distances that must be done. To locate the closest neighbor among the list of other points on the unit square requires a time proportional to . Therefore the time required for the closest-neighbor algorithm with points is proportional to , which is proportional to . Since the strip algorithm takes a time proportional to , it is much faster for large values of .  Explore the possibility of solving the Traveling Salesman Problem in the unit box : .  Devise a closest neighbor algorithm for matching points in the unit square.   "
},
{
  "id": "def-weighted-graph",
  "level": "2",
  "url": "s-graph-optimization.html#def-weighted-graph",
  "type": "Definition",
  "number": "9.5.1",
  "title": "Weighted Graph.",
  "body": "Weighted Graph Weighted Graph A weighted graph, , is a graph together with a weight function . If , is the weight on edge . "
},
{
  "id": "ex-distance-graph",
  "level": "2",
  "url": "s-graph-optimization.html#ex-distance-graph",
  "type": "Example",
  "number": "9.5.2",
  "title": "A Distance Graph.",
  "body": "A Distance Graph Let be the set of six capital cities in New England: Boston, Augusta, Hartford, Providence, Concord, and Montpelier. Let be the set of all possible undirected edges between these cities. So is a complete unordered graph. An example of a weight function on this graph is .  Many road maps that you could get at gas stations at one time defined distance functions as in the following table.   Distances between capital cities in New England   -- Augusta Boston Concord Hartford Montpelier Providence  Augusta, ME -- 165 148 266 190 208  Boston, MA 165 -- 75 103 192 43  Concord, NH 148 75 -- 142 117 109  Hartford, CT 266 103 142 -- 204 70  Montpelier, VT 190 192 117 204 -- 223  Providence, RI 208 43 109 70 223 --    "
},
{
  "id": "ex-boston-salesman",
  "level": "2",
  "url": "s-graph-optimization.html#ex-boston-salesman",
  "type": "Example",
  "number": "9.5.4",
  "title": "The problem of a Boston salesman.",
  "body": "The problem of a Boston salesman The Traveling Salesman Problem gets its name from the situation of a salesman who wants to minimize the number of miles that he travels in visiting his customers. For example, if a salesman from Boston must visit the other capital cities of New England, then the problem is to find a circuit in the weighted graph of . Note that distance and cost are clearly related in this case. In addition, tolls and traffic congestion might also be taken into account. "
},
{
  "id": "alg-closest-neighbor",
  "level": "2",
  "url": "s-graph-optimization.html#alg-closest-neighbor",
  "type": "Algorithm",
  "number": "9.5.5",
  "title": "The Closest Neighbor Algorithm.",
  "body": "The Closest Neighbor Algorithm Closest Neighbor Algorithm  Let be a complete weighted graph with . The closest neighbor circuit through G starting at is , defined by the steps:    .  For to     the closest vertex in to  ##  ## In case of a tie for closest, may be chosen arbitrarily.     the only element of   The cost of the closest neighbor circuit is   "
},
{
  "id": "ex-tsp-small-example",
  "level": "2",
  "url": "s-graph-optimization.html#ex-tsp-small-example",
  "type": "Example",
  "number": "9.5.6",
  "title": "A small example.",
  "body": "A small example The closest neighbor circuit starting at 1 in is , with a cost of 29. The optimal path is , with a cost of 27.   A small example   A small weighted graph    "
},
{
  "id": "ex-one-way-street",
  "level": "2",
  "url": "s-graph-optimization.html#ex-one-way-street",
  "type": "Example",
  "number": "9.5.8",
  "title": "The One-way Street.",
  "body": "The One-way Street A salesman must make stops at vertices A, B, and C, which are all on the same one-way street. The graph in is weighted by the function equal to the time it takes to drive from vertex to vertex .   Traveling a one-way street   A small directed weighted graph    Note that if is down the one-way street from , then . The values of , and are 20 and 32, respectively. Verify that is 32 by using the closest neighbor algorithm. The value of is significant in this case since our salesman would spend 60 percent more time on the road if he used the closest neighbor algorithm.  "
},
{
  "id": "th-cn-theorem-9-5",
  "level": "2",
  "url": "s-graph-optimization.html#th-cn-theorem-9-5",
  "type": "Theorem",
  "number": "9.5.10",
  "title": "",
  "body": "If is a complete weighted graph on vertices that satisfies the symmetry and triangle inequality conditions, then "
},
{
  "id": "ss-traveling-salesman-problem-15",
  "level": "2",
  "url": "s-graph-optimization.html#ss-traveling-salesman-problem-15",
  "type": "Observation",
  "number": "9.5.11",
  "title": "",
  "body": " If , then this theorem says that can be no larger than twice the size of ; however, it doesn't say that the closest neighbor circuit will necessarily be that far from an optimal circuit. The quantity is called an upper bound for the ratio . It tells us only that things can't be any worse than the upper bound. Certainly, there are many graphs with eight vertices such that the optimal and closest neighbor circuits are the same. What is left unstated in this theorem is whether there are graphs for which the quantities are equal. If there are such graphs, we say that the upper bound is sharp.  The value of in Example is 1.6, which is greater than ; however, the weight function in this example does not satisfy the conditions of the theorem.  "
},
{
  "id": "ex-unit-square",
  "level": "2",
  "url": "s-graph-optimization.html#ex-unit-square",
  "type": "Example",
  "number": "9.5.12",
  "title": "The Unit Square Problem.",
  "body": "The Unit Square Problem Suppose a robot is programmed to weld joints on square metal plates. Each plate must be welded at prescribed points on the square. To minimize the time it takes to complete the job, the total distance that a robot's arm moves should be minimized. Let be the distance between and . Assume that before each plate can be welded, the arm must be positioned at a certain point . Given a list of points, we want to put them in order so that is as small as possible. "
},
{
  "id": "alg-strip-algorithm",
  "level": "2",
  "url": "s-graph-optimization.html#alg-strip-algorithm",
  "type": "Heuristic",
  "number": "9.5.13",
  "title": "The Strip Algorithm.",
  "body": "The Strip Algorithm  Given points in the unit square:  Phase 1:  Divide the square into vertical strips, as in . Let d be the width of each strip. If a point lies on a boundary between two strips, consider it part of the left-hand strip.  Starting from the left, find the first strip that contains one of the points. Locate the starting point by selecting the first point that is encountered in that strip as you travel from bottom to top. We will assume that the first point is  Alternate traveling up and down the strips that contain vertices until all of the vertices have been reached.  Return to the starting point.   Phase 2:  Shift all strips units to the right (creating a small strip on the left).  Repeat Steps 1.2 through 1.4 of Phase 1 with the new strips.   When the two phases are complete, choose the shorter of the two circuits obtained.   "
},
{
  "id": "fig-strip-alg-tsp",
  "level": "2",
  "url": "s-graph-optimization.html#fig-strip-alg-tsp",
  "type": "Figure",
  "number": "9.5.14",
  "title": "",
  "body": " The Strip Algorithm   The Strip Algorithm   "
},
{
  "id": "def-network",
  "level": "2",
  "url": "s-graph-optimization.html#def-network",
  "type": "Definition",
  "number": "9.5.15",
  "title": "Network.",
  "body": "Network Network  A network is a simple weighted directed graph that contains two distinguished vertices called the source and the sink with the properties that the indegree of the source and outdegree of the sink are both zero, and source is connected to sink. The weight function on a network is the capacity function, which has positive weights. "
},
{
  "id": "ex-city-water",
  "level": "2",
  "url": "s-graph-optimization.html#ex-city-water",
  "type": "Example",
  "number": "9.5.16",
  "title": "A City Water System.",
  "body": "A City Water System Consider the system that is illustrated in . The numbers that appear next to each pipe indicate the capacity of that pipe in thousands of gallons per minute. This map can be drawn in the form of a network, as in .   City Water System   City Water System     Flow Diagram for a City's Water Network   City Water Network    Although the material passing through this network is water, networks can also represent the flow of other materials, such as automobiles, electricity, bits, telephone calls, or patients in a health system.  "
},
{
  "id": "problem-maximal-flow",
  "level": "2",
  "url": "s-graph-optimization.html#problem-maximal-flow",
  "type": "Problem",
  "number": "9.5.19",
  "title": "The Maximum Flow Problem.",
  "body": "The Maximum Flow Problem  The Maximum Flow Problem is derived from the objective of moving the maximum amount of water or other material from the source to the sink. To measure this amount, we define a flow as a function such that (1) the flow of material through any edge is nonnegative and no larger than its capacity: , for all ; and (2) for each vertex other than the source and sink, the total amount of material that is directed into a vertex is equal to the total amount that is directed out: The summation on the left of represents the sum of the flows through each edge in that has as a terminal vertex. The right-hand side indicates that you should add all of the flows through edges that initiate at .   "
},
{
  "id": "theorem-flow-inout",
  "level": "2",
  "url": "s-graph-optimization.html#theorem-flow-inout",
  "type": "Theorem",
  "number": "9.5.20",
  "title": "Flow out of Source equals Flow in Sink.",
  "body": "Flow out of Source equals Flow in Sink   If is a flow, then   Subtract the right-hand side of from the left-hand side. The result is: Now sum up these differences for each vertex in . The result is   Now observe that if an edge connects two vertices in , its flow appears as both a positive and a negative term in . This means that the only positive terms that are not cancelled out are the flows into the sink. In addition, the only negative terms that remain are the flows out of the source. Therefore,  "
},
{
  "id": "def-value-of-flow",
  "level": "2",
  "url": "s-graph-optimization.html#def-value-of-flow",
  "type": "Definition",
  "number": "9.5.21",
  "title": "The Value of a Flow.",
  "body": "The Value of a Flow Value of a Flow  The value of flow  The two values flow into the sink and flow out of the source were proved to be equal in and this common value is called the value of the flow . It is denoted by . The value of a flow represents the amount of material that passes through the network with that flow. "
},
{
  "id": "ss-networks-and-flows-9",
  "level": "2",
  "url": "s-graph-optimization.html#ss-networks-and-flows-9",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "maximal flow "
},
{
  "id": "def-flow-augmenting-path",
  "level": "2",
  "url": "s-graph-optimization.html#def-flow-augmenting-path",
  "type": "Definition",
  "number": "9.5.22",
  "title": "Flow Augmenting Path.",
  "body": "Flow Augmenting Path Flow Augmenting Path  Given a flow in a network , a flow augmenting path with respect to is a simple path from the source to the sink using edges both in their forward and their reverse directions such that for each edge in the path, if is used in its forward direction and if is used in the reverse direction. "
},
{
  "id": "example-water-augmenting",
  "level": "2",
  "url": "s-graph-optimization.html#example-water-augmenting",
  "type": "Example",
  "number": "9.5.23",
  "title": "Augmenting City Water Flow.",
  "body": "Augmenting City Water Flow  For in , a flow augmenting path would be since , , and .  These positive differences represent unused capacities, and the smallest value represents the amount of flow that can be added to each edge in the path. Note that by adding 5 to each edge in our path, we obtain , which is maximal. If an edge with a positive flow is used in its reverse direction, it is contributing a movement of material that is counterproductive to the objective of maximizing flow. This is why the algorithm directs us to decrease the flow through that edge. "
},
{
  "id": "alg-ford-fulkerson",
  "level": "2",
  "url": "s-graph-optimization.html#alg-ford-fulkerson",
  "type": "Algorithm",
  "number": "9.5.24",
  "title": "The Ford and Fulkerson Algorithm.",
  "body": "The Ford and Fulkerson Algorithm    Define the flow function by for each edge .  i = 0.  Repeat:   If possible, find a flow augmenting path with respect to .  If a flow augmenting path exists, then:   Determine  Define by       until no flow augmenting path exists.  Terminate with a maximal flow    "
},
{
  "id": "ss-networks-and-flows-16",
  "level": "2",
  "url": "s-graph-optimization.html#ss-networks-and-flows-16",
  "type": "List",
  "number": "9.5.25",
  "title": "Notes on the Ford and Fulkerson Algorithm",
  "body": "Notes on the Ford and Fulkerson Algorithm   It should be clear that every flow augmenting path leads to a flow of increased value and that none of the capacities of the network can be violated.  The depth-first search should be used to find flow augmenting paths since it is far more efficient than the breadth-first search in this situation. The depth-first search differs from the breadth-first algorithm in that you sequentially visit vertices until you reach a dead end and then backtrack.  There have been networks discovered for which the FFA does not terminate in a finite number of steps. These examples all have irrational capacities. It has been proven that if all capacities are positive integers, the FFA terminates in a finite number of steps. See Ford and Fulkerson, Even, or Berge for details.  When you use the FFA to solve the Maximum Flow Problem by hand it is convenient to label each edge of the network with the fraction .   "
},
{
  "id": "alg-depth-first-search",
  "level": "2",
  "url": "s-graph-optimization.html#alg-depth-first-search",
  "type": "Algorithm",
  "number": "9.5.26",
  "title": "Depth-First Search for a Flow Augmenting Path.",
  "body": "Depth-First Search for a Flow Augmenting Path   This is a depth-first search for the Sink Initiating at the Source. Let be the set of directed edges that can be used in producing a flow augmenting path. Add to the network a vertex called start and the edge   vertex set of the network.  source Move along the edge   while is not equal to start or sink:   if an edge in exists that takes you from to another vertex in :  .        "
},
{
  "id": "ex-fap-1",
  "level": "2",
  "url": "s-graph-optimization.html#ex-fap-1",
  "type": "Example",
  "number": "9.5.27",
  "title": "A flow augmenting path going against the flow.",
  "body": "A flow augmenting path going against the flow Consider the network in , where the current flow, , is indicated by a labeling of the edges.   Current Flow   Current Flow    The path is a flow augmenting path that allows us to increase the flow by one unit. Note that is used in the reverse direction, which is allowed because . The value of the new flow that we obtain is 8. This flow must be maximal since the capacities out of the source add up to 8. This maximal flow is defined by .   Updated Flow   Updated Flow    "
},
{
  "id": "exercises-9-5-2",
  "level": "2",
  "url": "s-graph-optimization.html#exercises-9-5-2",
  "type": "Exercise",
  "number": "9.5.5.1",
  "title": "",
  "body": "Find the closest neighbor circuit through the six capitals of New England starting at Boston. If you start at a different city, will you get a different circuit? The circuit would be Boston, Providence, Hartford, Concord, Montpelier, Augusta, Boston. It does matter where you start. If you start in Concord, for example, your mileage will be higher. "
},
{
  "id": "exercises-9-5-3",
  "level": "2",
  "url": "s-graph-optimization.html#exercises-9-5-3",
  "type": "Exercise",
  "number": "9.5.5.2",
  "title": "",
  "body": "Is the estimate in sharp for ? For ? For there is only one circuit and so the ratio in the theorem will be exactly . The upper bound given by the theorem is , so the estimate isn't sharp. For the theorem bounds the ratio to also be . This doesn't appear to be sharp either. However, the actual maximum ration seems to be at least . This can be seen by considering the graph in , where we imagine to be a very small positive real number. If we start at vertex , the nearest neighbor circuit in this graph is with a total cost of . However, the circuit has a total cost of . The ratio can be made as close to as we want.   Updated Flow   A complete graph with vertices labeled A, B, C and D. Edges are AB with weight 1+epsilon, AC with weight 2-epsilon, AD with weight 1, BC with weight 2-epsilon, BD with weight 1, CD with weight 1+epsilon.    "
},
{
  "id": "exercises-9-5-4",
  "level": "2",
  "url": "s-graph-optimization.html#exercises-9-5-4",
  "type": "Exercise",
  "number": "9.5.5.3",
  "title": "",
  "body": "Given the following sets of points in the unit square, find the shortest circuit that visits all the points and find the circuit that is obtained with the strip algorithm.             Optimal cost . Phase 1 cost . Phase 2 cost .  Optimal cost Phase 1 cost . Phase 2 cost .    There are 4 points; so we will divide the unit square into two strips.  Optimal Path:  Phase I Path:   Phase II Path:      There are 5 points; so we will divide the unit square into three strips.  Optimal Path:  Phase I Path:  Phase II Path:     "
},
{
  "id": "exercises-9-5-5",
  "level": "2",
  "url": "s-graph-optimization.html#exercises-9-5-5",
  "type": "Exercise",
  "number": "9.5.5.4",
  "title": "",
  "body": "For , locate points in the unit square for which the strip algorithm works poorly. "
},
{
  "id": "exercises-9-5-6",
  "level": "2",
  "url": "s-graph-optimization.html#exercises-9-5-6",
  "type": "Exercise",
  "number": "9.5.5.5",
  "title": "",
  "body": "Consider the network whose maximum capacities are shown on the following graph.    Figure for Exercise 9-5-5     A function is partially defined on the edges of this network by: , , , and . Define on the rest of the other edges so that is a flow. What is the value of ?  Find a flow augmenting path with respect to for this network. What is the value of the augmented flow?  Is the augmented flow a maximum flow? Explain.    , , , , and .  There are three possible flow-augmenting paths. with flow increase of 1. with flow increase of 1, and with flow increase of 2.  The new flow is never maximal, since another flow-augmenting path will always exist. For example, if is used above, the new flow can be augmented by 2 units with .   "
},
{
  "id": "exercises-9-5-7",
  "level": "2",
  "url": "s-graph-optimization.html#exercises-9-5-7",
  "type": "Exercise",
  "number": "9.5.5.6",
  "title": "",
  "body": "Given the following network with capacity function and flow function , find a maximal flow function. The labels on the edges of the network are of the form , where is the capacity of edge and is the used capacity for flow .    Figure for Exercise 9-5-6    "
},
{
  "id": "exercises-9-5-8",
  "level": "2",
  "url": "s-graph-optimization.html#exercises-9-5-8",
  "type": "Exercise",
  "number": "9.5.5.7",
  "title": "",
  "body": "Find maximal flows for the following networks.     Figure for Exercise 9-5-7a      Figure for Exercise 9-5-7b       Figure for Exercise 9-5-7c    Value of maximal flow .  Value of maximal flow .  Value of maximal flow . See for one way to got this flow.     Step Flow-augmenting path Flow added  1 2  2 3  3 4  4 1  5 2  6 2    "
},
{
  "id": "exercises-9-5-9",
  "level": "2",
  "url": "s-graph-optimization.html#exercises-9-5-9",
  "type": "Exercise",
  "number": "9.5.5.8",
  "title": "",
  "body": "  Find two maximal flows for the network in other than the one found in the text.  Describe the set of all maximal flows for the same network.  Prove that if a network has two maximal flows, then it has an infinite number of maximal flows.   "
},
{
  "id": "exercises-9-5-10",
  "level": "2",
  "url": "s-graph-optimization.html#exercises-9-5-10",
  "type": "Exercise",
  "number": "9.5.5.9",
  "title": "",
  "body": " Discuss reasons that the closest neighbor algorithm is not used in the unit square version of the Traveling Salesman Problem. Count the number of comparisons of distances that must be done. To locate the closest neighbor among the list of other points on the unit square requires a time proportional to . Therefore the time required for the closest-neighbor algorithm with points is proportional to , which is proportional to . Since the strip algorithm takes a time proportional to , it is much faster for large values of . "
},
{
  "id": "exercises-9-5-11",
  "level": "2",
  "url": "s-graph-optimization.html#exercises-9-5-11",
  "type": "Exercise",
  "number": "9.5.5.10",
  "title": "",
  "body": "Explore the possibility of solving the Traveling Salesman Problem in the unit box : . "
},
{
  "id": "exercises-9-5-12",
  "level": "2",
  "url": "s-graph-optimization.html#exercises-9-5-12",
  "type": "Exercise",
  "number": "9.5.5.11",
  "title": "",
  "body": "Devise a closest neighbor algorithm for matching points in the unit square. "
},
{
  "id": "s-planarity-and-colorings",
  "level": "1",
  "url": "s-planarity-and-colorings.html",
  "type": "Section",
  "number": "9.6",
  "title": "Planarity and Colorings",
  "body": " Planarity and Colorings  The topics in this section are related to how graphs are drawn.  Planarity: Can a given graph be drawn in a plane so that no edges intersect? Certainly, it is natural to avoid intersections, but up to now we haven't gone out of our way to do so.  Colorings: Suppose that each vertex in an undirected graph is to be colored so that no two vertices that are connected by an edge have the same color. How many colors are needed? This question is motivated by the problem of drawing a map so that no two bordering countries are colored the same. A similar question can be asked for coloring edges.   Planar Graphs  Planar Graph\/Plane Graph  Planar Graph  Plane Graph  Planar Embedding of a Graph  A graph is planar if it can be drawn in a plane so that no edges cross. A drawing of a graph on the plane such that there are no edge crossings is called a planar embedding of the graph, or a plane graph for short.  A Planar Graph The graph in (a) is planar but the drawing of it is not a plane graph. The drawing of the same graph in (b) is a planar graph.   A planar graph and a planar embedding that graph.   On the left of the figure is a graph labeled (a) that has two edges crossing and so it is not a plane graph. That graph is planar and to the right a graph labeled (b) is redrawn and is planar embedding, having no edge crossings. The graph is defined in SageMath as Graph({1:[2,3],2:[3,4]}).      In discussing planarity, we need only consider simple undirected graphs with no self-loops. All other graphs can be treated as such since all of the edges that relate any two vertices can be considered as one bundle that clearly can be drawn in a plane.  Can you think of a graph that is not planar? How would you prove that it isn't planar? Proving the nonexistence of something is usually more difficult than proving its existence. This case is no exception. Intuitively, we would expect that sparse graphs would be planar and dense graphs would be nonplanar. will verify that dense graphs are indeed nonplanar.  The topic of planarity is a result of trying to restrict a graph to two dimensions. Is there an analogous topic for three dimensions? What graphs can be drawn in one dimension?   Path Graph  Path Graph  a path graph of length  A path graph of length , denoted , is an undirected graph with vertices having edges , .  Graphs in other dimensions If a graph has only a finite number of vertices, it can always be drawn in three dimensions with no edge crossings. Is this also true for all graphs with an infinite number of vertices? The only one-dimensional graphs are graphs consisting of a single vertex, and path graphs, as shown in .    One dimensional graphs   One dimensional graphs    Three Utilities Puzzle A discussion of planarity is not complete without mentioning the famous Three Utilities Puzzle. The object of the puzzle is to supply three houses, A, B, and C, with the three utilities, gas, electric, and water. The constraint that makes this puzzle impossible to solve is that no utility lines may intersect. There is no planar embedding of the graph in , which is commonly denoted . This graph is one of two fundamental nonplanar graphs. The Kuratowski Reduction Theorem states that if a graph is nonplanar then it contains either a or a . Containment is in the sense that if you start with a nonplanar graph you can always perform a sequence of edge deletions and contractions (shrinking an edge so that the two vertices connecting it coincide) to produce one of the two graphs.   The Three Utilities Puzzle   The Three Utilities Puzzle    Regions of a Planar Graph A planar graph divides the plane into one or more regions. Two points on the plane lie in the same region if you can draw a curve connecting the two points that does not pass through an edge. One of these regions will be of infinite area. Each point on the plane is either a vertex, a point on an edge, or a point in a region. A remarkable fact about the geography of planar graphs is the following theorem that is attributed to Euler.  Experiment: Jot down a graph right now and count the number of vertices, regions, and edges that you have. If is not 2, then your graph is either nonplanar or not connected.  Euler's Formula Euler's Formula   If is a connected planar graph with regions, vertices, and edges, then   We prove Euler's Formula by Induction on , for .  Basis: If , then must be a graph with one vertex, ; and there is one infinite region, . Therefore, , and the basis is true.  Induction: Suppose that has edges, , and that all connected planar graphs with less than edges satisfy . Select any edge that is part of the boundary of the infinite region and call it . Let be the graph obtained from by deleting . illustrates the two different possibilities we need to consider: either is connected or it has two connected components, and .   Two cases in the proof of Euler's Formula   Two cases in the proof of Euler's Formula    If is connected, the induction hypothesis can be applied to it. If has vertices, regions and edges, then and in terms of the corresponding numbers for ,  For the case where is connected, If is not connected, it must consist of two connected components, and , since we started with a connected graph, . We can apply the induction hypothesis to each of the two components to complete the proof. We leave it to the students to do this, with the reminder that in counting regions, and will share the same infinite region.   A Bound on Edges of a Planar Graph  If is a connected planar graph with vertices, , and edges, then  (Outline of a Proof)   Let be the number of regions in . For each region, count the number of edges that comprise its border. The sum of these counts must be at least . Recall that we are working with simple graphs here, so a region made by two edges connecting the same two vertices is not possible.  Based on (a), infer that the number of edges in must be at least .    Substitute for in Euler's Formula to obtain an inequality that is equivalent to    One implication of is that the number of edges in a connected planar graph will never be larger than three times its number of vertices (as long as it has at least three vertices). Since the maximum number of edges in a graph with vertices is a quadratic function of , as increases, planar graphs are more and more sparse.  The following theorem will be useful as we turn to graph coloring.  A Vertex of Degree Five  If is a connected planar graph, then it has a vertex with degree 5 or less.  (by contradiction): We can assume that has at least seven vertices, for otherwise the degree of any vertex is at most 5. Suppose that is a connected planar graph and each vertex has a degree of 6 or more. Then, since each edge contributes to the degree of two vertices, . However, states that the , which is a contradiction.   Graph Coloring   A 3-coloring of Euler Island   A 3-coloring of Euler Island    The map of Euler Island in shows that there are seven towns on the island. Suppose that a cartographer must produce a colored map in which no two towns that share a boundary have the same color. To keep costs down, she wants to minimize the number of different colors that appear on the map. How many colors are sufficient? For Euler Island, the answer is three. Although it might not be obvious, this is a graph problem. We can represent the map with a graph, where the vertices are countries and an edge between two vertices indicates that the two corresponding countries share a boundary of positive length. The graph corresponding to the map of Euler Island is .   The graph of Euler Island   Graph({1:[2,5],2:[3,5,6,4,7], 3:[4],4:[7],7:[6],5:[6]})    The problem of coloring Euler Island motivates a more general problem.  Graph Coloring  Graph Coloring Chromatic Number  the chromatic number of  Given an undirected graph , find a coloring function  from into a set of colors such that and has the smallest possible cardinality. The cardinality of is called the chromatic number of , .   A coloring function onto an -element set is called an -coloring.  In terms of this general problem, the chromatic number of the graph of Euler Island is three. To see that no more than three colors are needed, we need only display a 3-coloring: , , and . This coloring is not unique. The next smallest set of colors would be of two colors, and you should be able to convince yourself that no 2-coloring exists for this graph.   In the mid-nineteenth century, it became clear that the typical planar graph had a chromatic number of no more than 4. At that point, mathematicians attacked the Four-Color Conjecture, which is that if is any planar graph, then its chromatic number is no more than 4. Although the conjecture is quite easy to state, it took over 100 years, until 1976, to prove the conjecture in the affirmative.  The Four-Color Theorem Four-Color Theorem  If is a planar graph, then .   A proof of the Four-Color Theorem is beyond the scope of this text, but we can prove a theorem that is only 25 percent inferior.  The Five-Color Theorem Five-Color Theorem  If is a planar graph, then .   The number 5 is not a sharp upper bound for because of the Four-Color Theorem.  This is a proof by Induction on the Number of Vertices in the Graph.  Basis: Clearly, a graph with one vertex has a chromatic number of 1.  Induction: Assume that all planar graphs with vertices have a chromatic number of 5 or less. Let be a planar graph. By , there exists a vertex with . Let be the planar graph obtained by deleting and all edges that connect to other vertices in . By the induction hypothesis, has a 5-coloring. Assume that the colors used are red, white, blue, green, and yellow.  If , then we can produce a 5-coloring of by selecting a color that is not used in coloring the vertices that are connected to with an edge in .  If , then we can use the same approach if the five vertices that are adjacent to are not all colored differently. We are now left with the possibility that , , , , and are all connected to by an edge and they are all colored differently. Assume that they are colored red, white blue, yellow, and green, respectively, as in .    Figure used in the proof of the five color theorem    Starting at in , suppose we try to construct a path that passes through only red and blue vertices. This can either be accomplished or it can't be accomplished. If it can't be done, consider all paths that start at , and go through only red and blue vertices. If we exchange the colors of the vertices in these paths, including we still have a 5-coloring of . Since is now blue, we can color the central vertex, , red.  Finally, suppose that is connected to using only red and blue vertices. Then a path from to by using red and blue vertices followed by the edges and completes a circuit that either encloses or encloses and . Therefore, no path from to exists using only white and yellow vertices. We can then repeat the same process as in the previous paragraph with and , which will allow us to color white.   Bipartite Graph Bipartite Graph.  A bipartite graph is a graph that has a 2-coloring. Equivalently, a graph is bipartite if its vertices can be partitioned into two nonempty subsets so that no edge connects vertices from the same subset.   A Few Examples   The graph of the Three Utilities Puzzle is bipartite. The vertices are partitioned into the utilities and the homes. A 2-coloring of the graph is to color the utilities red and the homes blue.  For , the -cube is bipartite. A coloring would be to color all strings with an even number of 1's red and the strings with an odd number of 1's blue. By the definition of the -cube, two strings that have the same color couldn't be connected since they would need to differ in at least two positions.  Let be a set of 64 vertices, one for each square on a chess board. We can index the elements of by = the square on the row , column . Connect vertices in according to whether or not you can move a knight from one square to another. Using our indexing of ,  is a bipartite graph. The usual coloring of a chessboard is valid 2-coloring.    How can you recognize whether a graph is bipartite? There is a nice equivalent condition for a graph to be bipartite.  No Odd Circuits in a Bipartite Graph An undirected graph is bipartite if and only if it has no circuit of odd length.  ( ) Let be a bipartite graph that is partitioned into two sets, R(ed) and B(lue) that define a 2-coloring. Consider any circuit in . If we specify a direction in the circuit and define on the vertices of the circuit by Note that is a bijection. Hence the number of red vertices in the circuit equals the number of blue vertices, and so the length of the circuit must be even.  ( ) Assume that has no circuit of odd length. For each component of , select any vertex and color it red. Then for every other vertex in the component, find the path of shortest distance from to . If the length of the path is odd, color blue, and if it is even, color red. We claim that this method defines a 2-coloring of . Suppose that it does not define a 2-coloring. Then let and be two vertices with identical colors that are connected with an edge. By the way that we colored , neither nor could equal . We can now construct a circuit with an odd length in . First, we start at and follow the shortest path to . Then follow the edge , and finally, follow the reverse of a shortest path from to . Since and have the same color, the first and third segments of this circuit have lengths that are both odd or even, and the sum of their lengths must be even. The addition of the single edge shows us that this circuit has an odd length. This contradicts our premise.    Exercises  Use Euler's formula to prove by contradiction that a is nonplanar. This shows that a is the largest complete graph that is planar. A has 10 edges. If a is planar, the number of regions into which the plane is divided must be 7, by Euler's formala ( ). If we re-count the edges of the graph by counting the number edges bordering the regions we get a count of at least . But we've counted each edge twice this way and the count must be even. This implies that the number of edges is at least 11, which a contradiction.  Use Euler's formula to prove by contradiction that a is nonplanar. Don't forget !  What are the chromatic numbers of the following graphs?   What are the chromatic numbers?   What are the chromatic numbers?     4  3  3  3  2  4   A connected planar graph has vertices and divides the plane into regions. How many edges does it have?  What is , ? The chromatic number is since every vertex is connected to every other vertex.  Suppose that all of the vertices of connected planar graph have degree 3 and that there are 20 vertices. How many edges and regions does this graph have?  Complete the proof of . Suppose that is not connected. Then is made up of 2 components that are planar graphs with less than edges, and . For let be the number of vertices, regions and edges in . By the induction hypothesis, for .  One of the regions, the infinite one, is common to both graphs. Therefore, when we add edge back to the graph, we have , , and .   Use the outline of a proof of to write a complete proof. Be sure to point out where the premise is essential.  Let with , and let be the set of all undirected edges between distinct vertices in . Prove that either or is nonplanar. Since , either has at least elements. Assume that it is that is larger. Since is greater than , would be nonplanar. Of course, if is larger, then would be nonplanar by the same reasoning. Can you find a graph with ten vertices such that it is planar and its complement is also planar?  Design an algorithm to determine whether a graph is bipartite.  Prove that a bipartite graph with an odd number of vertices greater than or equal to 3 has no Hamiltonian circuit. Suppose that is bipartite (with colors red and blue), is odd, and is a Hamiltonian circuit. If is red, then would also be red. But then would not be in , a contradiction.  Prove that any graph with a finite number of vertices can be drawn in three dimensions so that no edges intersect.  Suppose you had to color the edges of an undirected graph so that for each vertex, the edges that it is connected to have different colors. How can this problem be transformed into a vertex coloring problem? Draw a graph with one vertex for each edge, If two edges in the original graph meet at the same vertex, then draw an edge connecting the corresponding vertices in the new graph.    Suppose the edges of a are colored either red or blue. Prove that there will be either a red (a subset of the vertex set with three vertices connected by red edges) or a blue or both.  Suppose six people are selected at random. Prove that either there exists a subset of three of them with the property that any two people in the subset can communicate in a common language, or there exist three people, no two of whom can communicate in a common language.    Mesh Graph Let be a positive integer, and let be positive integers greater than or equal to two. The mesh graph  has vertices of the form where . Two vertices and are adjacent if and only if . In other words, two adjacent vertices must differ in only one coordinate and by a difference of 1.    What is the chromatic number of ?  For what pairs does have a Hamiltonian circuit?   For what triples does have a Hamiltonian circuit?     Further Reading  Wilson, R., Four Colors Suffice - How the Map Problem Was Solved Princeton, NJ: Princeton U. Press, 2013.   "
},
{
  "id": "def-planar-graph",
  "level": "2",
  "url": "s-planarity-and-colorings.html#def-planar-graph",
  "type": "Definition",
  "number": "9.6.1",
  "title": "Planar Graph\/Plane Graph.",
  "body": "Planar Graph\/Plane Graph  Planar Graph  Plane Graph  Planar Embedding of a Graph  A graph is planar if it can be drawn in a plane so that no edges cross. A drawing of a graph on the plane such that there are no edge crossings is called a planar embedding of the graph, or a plane graph for short. "
},
{
  "id": "ex-planar-graph",
  "level": "2",
  "url": "s-planarity-and-colorings.html#ex-planar-graph",
  "type": "Example",
  "number": "9.6.2",
  "title": "A Planar Graph.",
  "body": "A Planar Graph The graph in (a) is planar but the drawing of it is not a plane graph. The drawing of the same graph in (b) is a planar graph.   A planar graph and a planar embedding that graph.   On the left of the figure is a graph labeled (a) that has two edges crossing and so it is not a plane graph. That graph is planar and to the right a graph labeled (b) is redrawn and is planar embedding, having no edge crossings. The graph is defined in SageMath as Graph({1:[2,3],2:[3,4]}).    "
},
{
  "id": "def-def-path-graph",
  "level": "2",
  "url": "s-planarity-and-colorings.html#def-def-path-graph",
  "type": "Definition",
  "number": "9.6.4",
  "title": "Path Graph.",
  "body": "Path Graph  Path Graph  a path graph of length  A path graph of length , denoted , is an undirected graph with vertices having edges , . "
},
{
  "id": "ss-planarity-6",
  "level": "2",
  "url": "s-planarity-and-colorings.html#ss-planarity-6",
  "type": "Observation",
  "number": "9.6.5",
  "title": "Graphs in other dimensions.",
  "body": "Graphs in other dimensions If a graph has only a finite number of vertices, it can always be drawn in three dimensions with no edge crossings. Is this also true for all graphs with an infinite number of vertices? The only one-dimensional graphs are graphs consisting of a single vertex, and path graphs, as shown in .  "
},
{
  "id": "fig-path-graphs",
  "level": "2",
  "url": "s-planarity-and-colorings.html#fig-path-graphs",
  "type": "Figure",
  "number": "9.6.6",
  "title": "",
  "body": " One dimensional graphs   One dimensional graphs   "
},
{
  "id": "fig-utilities-puzzle",
  "level": "2",
  "url": "s-planarity-and-colorings.html#fig-utilities-puzzle",
  "type": "Figure",
  "number": "9.6.7",
  "title": "",
  "body": " The Three Utilities Puzzle   The Three Utilities Puzzle   "
},
{
  "id": "ss-planarity-11",
  "level": "2",
  "url": "s-planarity-and-colorings.html#ss-planarity-11",
  "type": "Activity",
  "number": "9.6.1",
  "title": "",
  "body": "Experiment: Jot down a graph right now and count the number of vertices, regions, and edges that you have. If is not 2, then your graph is either nonplanar or not connected. "
},
{
  "id": "theorem-euler-formula",
  "level": "2",
  "url": "s-planarity-and-colorings.html#theorem-euler-formula",
  "type": "Theorem",
  "number": "9.6.8",
  "title": "Euler’s Formula.",
  "body": "Euler's Formula Euler's Formula   If is a connected planar graph with regions, vertices, and edges, then   We prove Euler's Formula by Induction on , for .  Basis: If , then must be a graph with one vertex, ; and there is one infinite region, . Therefore, , and the basis is true.  Induction: Suppose that has edges, , and that all connected planar graphs with less than edges satisfy . Select any edge that is part of the boundary of the infinite region and call it . Let be the graph obtained from by deleting . illustrates the two different possibilities we need to consider: either is connected or it has two connected components, and .   Two cases in the proof of Euler's Formula   Two cases in the proof of Euler's Formula    If is connected, the induction hypothesis can be applied to it. If has vertices, regions and edges, then and in terms of the corresponding numbers for ,  For the case where is connected, If is not connected, it must consist of two connected components, and , since we started with a connected graph, . We can apply the induction hypothesis to each of the two components to complete the proof. We leave it to the students to do this, with the reminder that in counting regions, and will share the same infinite region.  "
},
{
  "id": "theorem-edge-bound",
  "level": "2",
  "url": "s-planarity-and-colorings.html#theorem-edge-bound",
  "type": "Theorem",
  "number": "9.6.10",
  "title": "A Bound on Edges of a Planar Graph.",
  "body": "A Bound on Edges of a Planar Graph  If is a connected planar graph with vertices, , and edges, then  (Outline of a Proof)   Let be the number of regions in . For each region, count the number of edges that comprise its border. The sum of these counts must be at least . Recall that we are working with simple graphs here, so a region made by two edges connecting the same two vertices is not possible.  Based on (a), infer that the number of edges in must be at least .    Substitute for in Euler's Formula to obtain an inequality that is equivalent to   "
},
{
  "id": "ss-planarity-14",
  "level": "2",
  "url": "s-planarity-and-colorings.html#ss-planarity-14",
  "type": "Remark",
  "number": "9.6.11",
  "title": "",
  "body": "One implication of is that the number of edges in a connected planar graph will never be larger than three times its number of vertices (as long as it has at least three vertices). Since the maximum number of edges in a graph with vertices is a quadratic function of , as increases, planar graphs are more and more sparse. "
},
{
  "id": "theorem-degree-5",
  "level": "2",
  "url": "s-planarity-and-colorings.html#theorem-degree-5",
  "type": "Theorem",
  "number": "9.6.12",
  "title": "A Vertex of Degree Five.",
  "body": "A Vertex of Degree Five  If is a connected planar graph, then it has a vertex with degree 5 or less.  (by contradiction): We can assume that has at least seven vertices, for otherwise the degree of any vertex is at most 5. Suppose that is a connected planar graph and each vertex has a degree of 6 or more. Then, since each edge contributes to the degree of two vertices, . However, states that the , which is a contradiction. "
},
{
  "id": "fig-euler-island",
  "level": "2",
  "url": "s-planarity-and-colorings.html#fig-euler-island",
  "type": "Figure",
  "number": "9.6.13",
  "title": "",
  "body": " A 3-coloring of Euler Island   A 3-coloring of Euler Island   "
},
{
  "id": "fig-eulerislandgraph",
  "level": "2",
  "url": "s-planarity-and-colorings.html#fig-eulerislandgraph",
  "type": "Figure",
  "number": "9.6.14",
  "title": "",
  "body": " The graph of Euler Island   Graph({1:[2,5],2:[3,5,6,4,7], 3:[4],4:[7],7:[6],5:[6]})   "
},
{
  "id": "def-graph-coloring",
  "level": "2",
  "url": "s-planarity-and-colorings.html#def-graph-coloring",
  "type": "Definition",
  "number": "9.6.15",
  "title": "Graph Coloring.",
  "body": "Graph Coloring  Graph Coloring Chromatic Number  the chromatic number of  Given an undirected graph , find a coloring function  from into a set of colors such that and has the smallest possible cardinality. The cardinality of is called the chromatic number of , . "
},
{
  "id": "theorem-four-color-theorem",
  "level": "2",
  "url": "s-planarity-and-colorings.html#theorem-four-color-theorem",
  "type": "Theorem",
  "number": "9.6.16",
  "title": "The Four-Color Theorem.",
  "body": "The Four-Color Theorem Four-Color Theorem  If is a planar graph, then .  "
},
{
  "id": "theorem-five-color-theorem",
  "level": "2",
  "url": "s-planarity-and-colorings.html#theorem-five-color-theorem",
  "type": "Theorem",
  "number": "9.6.17",
  "title": "The Five-Color Theorem.",
  "body": "The Five-Color Theorem Five-Color Theorem  If is a planar graph, then .   The number 5 is not a sharp upper bound for because of the Four-Color Theorem.  This is a proof by Induction on the Number of Vertices in the Graph.  Basis: Clearly, a graph with one vertex has a chromatic number of 1.  Induction: Assume that all planar graphs with vertices have a chromatic number of 5 or less. Let be a planar graph. By , there exists a vertex with . Let be the planar graph obtained by deleting and all edges that connect to other vertices in . By the induction hypothesis, has a 5-coloring. Assume that the colors used are red, white, blue, green, and yellow.  If , then we can produce a 5-coloring of by selecting a color that is not used in coloring the vertices that are connected to with an edge in .  If , then we can use the same approach if the five vertices that are adjacent to are not all colored differently. We are now left with the possibility that , , , , and are all connected to by an edge and they are all colored differently. Assume that they are colored red, white blue, yellow, and green, respectively, as in .    Figure used in the proof of the five color theorem    Starting at in , suppose we try to construct a path that passes through only red and blue vertices. This can either be accomplished or it can't be accomplished. If it can't be done, consider all paths that start at , and go through only red and blue vertices. If we exchange the colors of the vertices in these paths, including we still have a 5-coloring of . Since is now blue, we can color the central vertex, , red.  Finally, suppose that is connected to using only red and blue vertices. Then a path from to by using red and blue vertices followed by the edges and completes a circuit that either encloses or encloses and . Therefore, no path from to exists using only white and yellow vertices. We can then repeat the same process as in the previous paragraph with and , which will allow us to color white.  "
},
{
  "id": "def-bipartite-graph",
  "level": "2",
  "url": "s-planarity-and-colorings.html#def-bipartite-graph",
  "type": "Definition",
  "number": "9.6.19",
  "title": "Bipartite Graph.",
  "body": "Bipartite Graph Bipartite Graph.  A bipartite graph is a graph that has a 2-coloring. Equivalently, a graph is bipartite if its vertices can be partitioned into two nonempty subsets so that no edge connects vertices from the same subset.  "
},
{
  "id": "ex-bipartite",
  "level": "2",
  "url": "s-planarity-and-colorings.html#ex-bipartite",
  "type": "Example",
  "number": "9.6.20",
  "title": "A Few Examples.",
  "body": "A Few Examples   The graph of the Three Utilities Puzzle is bipartite. The vertices are partitioned into the utilities and the homes. A 2-coloring of the graph is to color the utilities red and the homes blue.  For , the -cube is bipartite. A coloring would be to color all strings with an even number of 1's red and the strings with an odd number of 1's blue. By the definition of the -cube, two strings that have the same color couldn't be connected since they would need to differ in at least two positions.  Let be a set of 64 vertices, one for each square on a chess board. We can index the elements of by = the square on the row , column . Connect vertices in according to whether or not you can move a knight from one square to another. Using our indexing of ,  is a bipartite graph. The usual coloring of a chessboard is valid 2-coloring.   "
},
{
  "id": "theorem-no-odd",
  "level": "2",
  "url": "s-planarity-and-colorings.html#theorem-no-odd",
  "type": "Theorem",
  "number": "9.6.21",
  "title": "No Odd Circuits in a Bipartite Graph.",
  "body": "No Odd Circuits in a Bipartite Graph An undirected graph is bipartite if and only if it has no circuit of odd length.  ( ) Let be a bipartite graph that is partitioned into two sets, R(ed) and B(lue) that define a 2-coloring. Consider any circuit in . If we specify a direction in the circuit and define on the vertices of the circuit by Note that is a bijection. Hence the number of red vertices in the circuit equals the number of blue vertices, and so the length of the circuit must be even.  ( ) Assume that has no circuit of odd length. For each component of , select any vertex and color it red. Then for every other vertex in the component, find the path of shortest distance from to . If the length of the path is odd, color blue, and if it is even, color red. We claim that this method defines a 2-coloring of . Suppose that it does not define a 2-coloring. Then let and be two vertices with identical colors that are connected with an edge. By the way that we colored , neither nor could equal . We can now construct a circuit with an odd length in . First, we start at and follow the shortest path to . Then follow the edge , and finally, follow the reverse of a shortest path from to . Since and have the same color, the first and third segments of this circuit have lengths that are both odd or even, and the sum of their lengths must be even. The addition of the single edge shows us that this circuit has an odd length. This contradicts our premise. "
},
{
  "id": "exercises-9-6-2",
  "level": "2",
  "url": "s-planarity-and-colorings.html#exercises-9-6-2",
  "type": "Exercise",
  "number": "9.6.3.1",
  "title": "",
  "body": "Use Euler's formula to prove by contradiction that a is nonplanar. This shows that a is the largest complete graph that is planar. A has 10 edges. If a is planar, the number of regions into which the plane is divided must be 7, by Euler's formala ( ). If we re-count the edges of the graph by counting the number edges bordering the regions we get a count of at least . But we've counted each edge twice this way and the count must be even. This implies that the number of edges is at least 11, which a contradiction. "
},
{
  "id": "exercises-9-6-3",
  "level": "2",
  "url": "s-planarity-and-colorings.html#exercises-9-6-3",
  "type": "Exercise",
  "number": "9.6.3.2",
  "title": "",
  "body": "Use Euler's formula to prove by contradiction that a is nonplanar. Don't forget ! "
},
{
  "id": "exercises-9-6-4",
  "level": "2",
  "url": "s-planarity-and-colorings.html#exercises-9-6-4",
  "type": "Exercise",
  "number": "9.6.3.3",
  "title": "",
  "body": "What are the chromatic numbers of the following graphs?   What are the chromatic numbers?   What are the chromatic numbers?     4  3  3  3  2  4  "
},
{
  "id": "exercises-9-6-5",
  "level": "2",
  "url": "s-planarity-and-colorings.html#exercises-9-6-5",
  "type": "Exercise",
  "number": "9.6.3.4",
  "title": "",
  "body": "A connected planar graph has vertices and divides the plane into regions. How many edges does it have? "
},
{
  "id": "exercises-9-6-6",
  "level": "2",
  "url": "s-planarity-and-colorings.html#exercises-9-6-6",
  "type": "Exercise",
  "number": "9.6.3.5",
  "title": "",
  "body": "What is , ? The chromatic number is since every vertex is connected to every other vertex. "
},
{
  "id": "exercises-9-6-7",
  "level": "2",
  "url": "s-planarity-and-colorings.html#exercises-9-6-7",
  "type": "Exercise",
  "number": "9.6.3.6",
  "title": "",
  "body": "Suppose that all of the vertices of connected planar graph have degree 3 and that there are 20 vertices. How many edges and regions does this graph have? "
},
{
  "id": "exercises-9-6-8",
  "level": "2",
  "url": "s-planarity-and-colorings.html#exercises-9-6-8",
  "type": "Exercise",
  "number": "9.6.3.7",
  "title": "",
  "body": "Complete the proof of . Suppose that is not connected. Then is made up of 2 components that are planar graphs with less than edges, and . For let be the number of vertices, regions and edges in . By the induction hypothesis, for .  One of the regions, the infinite one, is common to both graphs. Therefore, when we add edge back to the graph, we have , , and .  "
},
{
  "id": "exercises-9-6-9",
  "level": "2",
  "url": "s-planarity-and-colorings.html#exercises-9-6-9",
  "type": "Exercise",
  "number": "9.6.3.8",
  "title": "",
  "body": "Use the outline of a proof of to write a complete proof. Be sure to point out where the premise is essential. "
},
{
  "id": "exercises-9-6-10",
  "level": "2",
  "url": "s-planarity-and-colorings.html#exercises-9-6-10",
  "type": "Exercise",
  "number": "9.6.3.9",
  "title": "",
  "body": "Let with , and let be the set of all undirected edges between distinct vertices in . Prove that either or is nonplanar. Since , either has at least elements. Assume that it is that is larger. Since is greater than , would be nonplanar. Of course, if is larger, then would be nonplanar by the same reasoning. Can you find a graph with ten vertices such that it is planar and its complement is also planar? "
},
{
  "id": "exercises-9-6-11",
  "level": "2",
  "url": "s-planarity-and-colorings.html#exercises-9-6-11",
  "type": "Exercise",
  "number": "9.6.3.10",
  "title": "",
  "body": "Design an algorithm to determine whether a graph is bipartite. "
},
{
  "id": "exercises-9-6-12",
  "level": "2",
  "url": "s-planarity-and-colorings.html#exercises-9-6-12",
  "type": "Exercise",
  "number": "9.6.3.11",
  "title": "",
  "body": "Prove that a bipartite graph with an odd number of vertices greater than or equal to 3 has no Hamiltonian circuit. Suppose that is bipartite (with colors red and blue), is odd, and is a Hamiltonian circuit. If is red, then would also be red. But then would not be in , a contradiction. "
},
{
  "id": "exercises-9-6-13",
  "level": "2",
  "url": "s-planarity-and-colorings.html#exercises-9-6-13",
  "type": "Exercise",
  "number": "9.6.3.12",
  "title": "",
  "body": "Prove that any graph with a finite number of vertices can be drawn in three dimensions so that no edges intersect. "
},
{
  "id": "exercises-9-6-14",
  "level": "2",
  "url": "s-planarity-and-colorings.html#exercises-9-6-14",
  "type": "Exercise",
  "number": "9.6.3.13",
  "title": "",
  "body": "Suppose you had to color the edges of an undirected graph so that for each vertex, the edges that it is connected to have different colors. How can this problem be transformed into a vertex coloring problem? Draw a graph with one vertex for each edge, If two edges in the original graph meet at the same vertex, then draw an edge connecting the corresponding vertices in the new graph. "
},
{
  "id": "exercises-9-6-15",
  "level": "2",
  "url": "s-planarity-and-colorings.html#exercises-9-6-15",
  "type": "Exercise",
  "number": "9.6.3.14",
  "title": "",
  "body": "  Suppose the edges of a are colored either red or blue. Prove that there will be either a red (a subset of the vertex set with three vertices connected by red edges) or a blue or both.  Suppose six people are selected at random. Prove that either there exists a subset of three of them with the property that any two people in the subset can communicate in a common language, or there exist three people, no two of whom can communicate in a common language.   "
},
{
  "id": "exercises-9-6-16",
  "level": "2",
  "url": "s-planarity-and-colorings.html#exercises-9-6-16",
  "type": "Exercise",
  "number": "9.6.3.15",
  "title": "",
  "body": "Mesh Graph Let be a positive integer, and let be positive integers greater than or equal to two. The mesh graph  has vertices of the form where . Two vertices and are adjacent if and only if . In other words, two adjacent vertices must differ in only one coordinate and by a difference of 1.    What is the chromatic number of ?  For what pairs does have a Hamiltonian circuit?   For what triples does have a Hamiltonian circuit?   "
},
{
  "id": "s-what-is-a-tree",
  "level": "1",
  "url": "s-what-is-a-tree.html",
  "type": "Section",
  "number": "10.1",
  "title": "What Is a Tree?",
  "body": " What Is a Tree?  What Is a Tree?  Definition  What distinguishes trees from other types of connected graphs is the absence of certain paths called cycles. Recall that a path is a sequence of consecutive edges in a graph, and a circuit is a path that begins and ends at the same vertex.  Cycle  Cycle A cycle with edges. A cycle is a circuit whose edge list contains no duplicates. It is customary to use to denote a cycle with edges.  The simplest example of a cycle in an undirected graph is a pair of vertices with two edges connecting them. Since trees are cycle-free, we can rule out all multigraphs having at least one pair of vertices connected with two or more edges from consideration as trees.  Trees can either be undirected or directed graphs. We will concentrate on the undirected variety in this chapter.  Tree  Tree  An undirected graph is a tree if it is connected and contains no cycles.  Some trees and non-trees   Some trees and some non-trees   Some trees and some non-trees     Graphs i, ii and iii in are all trees, while graphs iv, v, and vi are not trees.   A is a tree. However, if , a is not a tree.  In a loose sense, a botanical tree is a mathematical tree. There are usually no cycles in the branch structure of a botanical tree.  The structures of some chemical compounds are modeled by a tree. For example, butane consists of four carbon atoms and ten hydrogen atoms, where an edge between two atoms represents a bond between them. A bond is a force that keeps two atoms together. The same set of atoms can be linked together in a different tree structure to give us the compound isobutane . There are some compounds whose graphs are not trees. One example is benzene .      Butane   Structure of Butane     Isobutane   Structure of Isobutane     Benzene   Structure of Benzene      One type of graph that is not a tree, but is closely related, is a forest.  Forest  Forest. A forest is an undirected graph whose components are all trees.  A forest The top half of can be viewed as a forest of three trees. Graph (vi) in this figure is also a forest.   Conditions for a Graph to be a Tree  We will now examine several conditions that are equivalent to the one that defines a tree. The following theorem will be used as a tool in proving that the conditions are equivalent.   Let be an undirected graph with no self-loops, and let . If two different simple paths exist between and , then there exists a cycle in .  Let and be two different simple paths from to . The first step we will take is to delete from and the initial edges that are identical. That is, if , , , and delete the first edges of both paths. Once this is done, both paths start at the same vertex, call it , and both still end at . Now we construct a cycle by starting at and following what is left of until we first meet what is left of . If this first meeting occurs at vertex , then the remainder of the cycle is completed by following the portion of the reverse of that starts at and ends at .    Equivalent Conditions for a Graph to be a Tree  Let be an undirected graph with no self-loops and . The following are all equivalent:    is a tree.  For each pair of distinct vertices in , there exists a unique simple path between them.   is connected, and if , then is disconnected.   contains no cycles, but by adding one edge, you create a cycle.   is connected and .     Proof Strategy. Most of this theorem can be proven by proving the following chain of implications: , , , and . Once these implications have been demonstrated, the transitive closure of on establishes the equivalence of the first four conditions. The proof that Statement 5 is equivalent to the first four can be done by induction, which we will leave to the reader.  (Indirect). Assume that is a tree and that there exists a pair of vertices between which there is either no path or there are at least two distinct paths. Both of these possibilities contradict the premise that is a tree. If no path exists, is disconnected, and if two paths exist, a cycle can be obtained by .  . We now use Statement 2 as a premise. Since each pair of vertices in are connected by exactly one path, is connected. Now if we select any edge in , it connects two vertices, and . By (2), there is no simple path connecting to other than . Therefore, no path at all can exist between and in . Hence is disconnected.  . Now we will assume that Statement 3 is true. We must show that has no cycles and that adding an edge to creates a cycle. We will use an indirect proof for this part. Since (4) is a conjunction, by DeMorgan's Law its negation is a disjunction and we must consider two cases. First, suppose that has a cycle. Then the deletion of any edge in the cycle keeps the graph connected, which contradicts (3). The second case is that the addition of an edge to does not create a cycle. Then there are two distinct paths between the vertices that the new edge connects. By , a cycle can then be created, which is a contradiction.  Assume that contains no cycles and that the addition of an edge creates a cycle. All that we need to prove to verify that is a tree is that is connected. If it is not connected, then select any two vertices that are not connected. If we add an edge to connect them, the fact that a cycle is created implies that a second path between the two vertices can be found which is in the original graph, which is a contradiction.    The usual definition of a directed tree is based on whether the associated undirected graph, which is created by erasing its directional arrows, is a tree. In Section 10.3 we will introduce the rooted tree, which is a special type of directed tree.    Exercises  Given the following vertex sets, draw all possible undirected trees that connect them.         .   The number of trees are: (a) 1, (b) 3, and (c) 16. The trees that connect are:    Solution to exercise 10-1-1     Are all trees planar? If they are, can you explain why? If they are not, you should be able to find a nonplanar tree.   Prove that if is a simple undirected graph with no self-loops, then is a tree if and only if is connected and .  Use induction on .    Prove that if is a tree and , then is a forest of two trees.  Prove that if ) and are disjoint trees and is an edge that connects a vertex in to a vertex in , then is a tree.     Prove that any tree with at least two vertices has at least two vertices of degree 1.  Prove that if a tree has vertices, , and is not a path graph, , then it has at least three vertices of degree 1.    Assume that is a tree with , and all but possibly one vertex in has degree two or more.   The proof of this part is similar to part a in that we can infer , using the fact that a non-chain tree has at least one vertex of degree three or more.     "
},
{
  "id": "def-cycle",
  "level": "2",
  "url": "s-what-is-a-tree.html#def-cycle",
  "type": "Definition",
  "number": "10.1.1",
  "title": "Cycle.",
  "body": "Cycle  Cycle A cycle with edges. A cycle is a circuit whose edge list contains no duplicates. It is customary to use to denote a cycle with edges. "
},
{
  "id": "def-tree",
  "level": "2",
  "url": "s-what-is-a-tree.html#def-tree",
  "type": "Definition",
  "number": "10.1.2",
  "title": "Tree.",
  "body": "Tree  Tree  An undirected graph is a tree if it is connected and contains no cycles. "
},
{
  "id": "ex-some-trees",
  "level": "2",
  "url": "s-what-is-a-tree.html#ex-some-trees",
  "type": "Example",
  "number": "10.1.3",
  "title": "Some trees and non-trees.",
  "body": "Some trees and non-trees   Some trees and some non-trees   Some trees and some non-trees     Graphs i, ii and iii in are all trees, while graphs iv, v, and vi are not trees.   A is a tree. However, if , a is not a tree.  In a loose sense, a botanical tree is a mathematical tree. There are usually no cycles in the branch structure of a botanical tree.  The structures of some chemical compounds are modeled by a tree. For example, butane consists of four carbon atoms and ten hydrogen atoms, where an edge between two atoms represents a bond between them. A bond is a force that keeps two atoms together. The same set of atoms can be linked together in a different tree structure to give us the compound isobutane . There are some compounds whose graphs are not trees. One example is benzene .      Butane   Structure of Butane     Isobutane   Structure of Isobutane     Benzene   Structure of Benzene     "
},
{
  "id": "def-forest",
  "level": "2",
  "url": "s-what-is-a-tree.html#def-forest",
  "type": "Definition",
  "number": "10.1.8",
  "title": "Forest.",
  "body": "Forest  Forest. A forest is an undirected graph whose components are all trees. "
},
{
  "id": "ex-a-forest",
  "level": "2",
  "url": "s-what-is-a-tree.html#ex-a-forest",
  "type": "Example",
  "number": "10.1.9",
  "title": "A forest.",
  "body": "A forest The top half of can be viewed as a forest of three trees. Graph (vi) in this figure is also a forest. "
},
{
  "id": "lemma-two-paths",
  "level": "2",
  "url": "s-what-is-a-tree.html#lemma-two-paths",
  "type": "Lemma",
  "number": "10.1.10",
  "title": "",
  "body": " Let be an undirected graph with no self-loops, and let . If two different simple paths exist between and , then there exists a cycle in .  Let and be two different simple paths from to . The first step we will take is to delete from and the initial edges that are identical. That is, if , , , and delete the first edges of both paths. Once this is done, both paths start at the same vertex, call it , and both still end at . Now we construct a cycle by starting at and following what is left of until we first meet what is left of . If this first meeting occurs at vertex , then the remainder of the cycle is completed by following the portion of the reverse of that starts at and ends at .   "
},
{
  "id": "theorem-tree-conditions",
  "level": "2",
  "url": "s-what-is-a-tree.html#theorem-tree-conditions",
  "type": "Theorem",
  "number": "10.1.11",
  "title": "Equivalent Conditions for a Graph to be a Tree.",
  "body": "Equivalent Conditions for a Graph to be a Tree  Let be an undirected graph with no self-loops and . The following are all equivalent:    is a tree.  For each pair of distinct vertices in , there exists a unique simple path between them.   is connected, and if , then is disconnected.   contains no cycles, but by adding one edge, you create a cycle.   is connected and .     Proof Strategy. Most of this theorem can be proven by proving the following chain of implications: , , , and . Once these implications have been demonstrated, the transitive closure of on establishes the equivalence of the first four conditions. The proof that Statement 5 is equivalent to the first four can be done by induction, which we will leave to the reader.  (Indirect). Assume that is a tree and that there exists a pair of vertices between which there is either no path or there are at least two distinct paths. Both of these possibilities contradict the premise that is a tree. If no path exists, is disconnected, and if two paths exist, a cycle can be obtained by .  . We now use Statement 2 as a premise. Since each pair of vertices in are connected by exactly one path, is connected. Now if we select any edge in , it connects two vertices, and . By (2), there is no simple path connecting to other than . Therefore, no path at all can exist between and in . Hence is disconnected.  . Now we will assume that Statement 3 is true. We must show that has no cycles and that adding an edge to creates a cycle. We will use an indirect proof for this part. Since (4) is a conjunction, by DeMorgan's Law its negation is a disjunction and we must consider two cases. First, suppose that has a cycle. Then the deletion of any edge in the cycle keeps the graph connected, which contradicts (3). The second case is that the addition of an edge to does not create a cycle. Then there are two distinct paths between the vertices that the new edge connects. By , a cycle can then be created, which is a contradiction.  Assume that contains no cycles and that the addition of an edge creates a cycle. All that we need to prove to verify that is a tree is that is connected. If it is not connected, then select any two vertices that are not connected. If we add an edge to connect them, the fact that a cycle is created implies that a second path between the two vertices can be found which is in the original graph, which is a contradiction.   "
},
{
  "id": "exercise-trees",
  "level": "2",
  "url": "s-what-is-a-tree.html#exercise-trees",
  "type": "Exercise",
  "number": "10.1.3.1",
  "title": "",
  "body": "Given the following vertex sets, draw all possible undirected trees that connect them.         .   The number of trees are: (a) 1, (b) 3, and (c) 16. The trees that connect are:    Solution to exercise 10-1-1    "
},
{
  "id": "exercises-10-1-3",
  "level": "2",
  "url": "s-what-is-a-tree.html#exercises-10-1-3",
  "type": "Exercise",
  "number": "10.1.3.2",
  "title": "",
  "body": "Are all trees planar? If they are, can you explain why? If they are not, you should be able to find a nonplanar tree.  "
},
{
  "id": "exercises-10-1-4",
  "level": "2",
  "url": "s-what-is-a-tree.html#exercises-10-1-4",
  "type": "Exercise",
  "number": "10.1.3.3",
  "title": "",
  "body": "Prove that if is a simple undirected graph with no self-loops, then is a tree if and only if is connected and .  Use induction on . "
},
{
  "id": "exercises-10-1-5",
  "level": "2",
  "url": "s-what-is-a-tree.html#exercises-10-1-5",
  "type": "Exercise",
  "number": "10.1.3.4",
  "title": "",
  "body": "  Prove that if is a tree and , then is a forest of two trees.  Prove that if ) and are disjoint trees and is an edge that connects a vertex in to a vertex in , then is a tree.   "
},
{
  "id": "exercises-10-1-6",
  "level": "2",
  "url": "s-what-is-a-tree.html#exercises-10-1-6",
  "type": "Exercise",
  "number": "10.1.3.5",
  "title": "",
  "body": " Prove that any tree with at least two vertices has at least two vertices of degree 1.  Prove that if a tree has vertices, , and is not a path graph, , then it has at least three vertices of degree 1.    Assume that is a tree with , and all but possibly one vertex in has degree two or more.   The proof of this part is similar to part a in that we can infer , using the fact that a non-chain tree has at least one vertex of degree three or more.   "
},
{
  "id": "s-spanning-trees",
  "level": "1",
  "url": "s-spanning-trees.html",
  "type": "Section",
  "number": "10.2",
  "title": "Spanning Trees",
  "body": " Spanning Trees  Spanning Trees  Motivation  The topic of spanning trees is motivated by a graph-optimization problem.  A graph of Atlantis University ( ) shows that there are four campuses in the system. A new secure communications system is being installed and the objective is to allow for communication between any two campuses; to achieve this objective, the university must buy direct lines between certain pairs of campuses. Let be the graph with a vertex for each campus and an edge for each direct line. Total communication is equivalent to being a connected graph. This is due to the fact that two campuses can communicate over any number of lines. To minimize costs, the university wants to buy a minimum number of lines.   Atlantis University Graph   Atlantis University Graph    The solutions to this problem are all trees. Any graph that satisfies the requirements of the university must be connected, and if a cycle does exist, any line in the cycle can be deleted, reducing the cost. Each of the sixteen trees that can be drawn to connect the vertices North, South, East, and West (see ) solves the problem as it is stated. Note that in each case, three direct lines must be purchased. There are two considerations that can help reduce the number of solutions that would be considered.   Objective 1: Given that the cost of each line depends on certain factors, such as the distance between the campuses, select a tree whose cost is as low as possible.  Objective 2: Suppose that communication over multiple lines is noisier as the number of lines increases. Select a tree with the property that the maximum number of lines that any pair of campuses must use to communicate with is as small as possible.   Typically, these objectives are not compatible; that is, you cannot always simultaneously achieve these objectives. In the case of the Atlantis university system, the solution with respect to Objective 1 is indicated with solid lines in . There are four solutions to the problem with respect to Objective 2: any tree in which one campus is directly connected to the other three. One solution with respect to Objective 2 is indicated with dotted lines in . After satisfying the conditions of Objective 2, it would seem reasonable to select the cheapest of the four trees.   Definition  Spanning Tree  Spanning Tree  Let be a connected undirected graph. A spanning tree for is a spanning subgraph of that is a tree.     If is a spanning tree, .  The significance of a spanning tree is that it is a minimal spanning set. A smaller set would not span the graph, while a larger set would have a cycle, which has an edge that is superfluous.    For the remainder of this section, we will discuss two of the many topics that relate to spanning trees. The first is the problem of finding Minimal Spanning Trees, which addresses Objective 1 above. The second is the problem of finding Minimum Diameter Spanning Trees, which addresses Objective 2.  Minimal Spanning Tree  Minimal Spanning Tree  Given a weighted connected undirected graph , a minimal spanning tree is a spanning tree for which is as small as possible.   Prim's Algorithm  Unlike many of the graph-optimization problems that we've examined, a solution to this problem can be obtained efficiently. It is a situation in which a greedy algorithm works.  Bridge  Bridge  Let be an undirected graph and let be a partition of . A bridge between and is an edge in that connects a vertex in to a vertex in .   Let be a weighted connected undirected graph. Let be partitioned into two sets and . If is a bridge of least weight between and , then there exists a minimal spanning tree for that includes .  Suppose that no minimal spanning tree including exists. Let be a minimal spanning tree. If we add to , a cycle is created, and this cycle must contain another bridge, , between and . Since , we can delete and the new tree, which includes must also be a minimal spanning tree.  Some Bridges The bridges between the vertex sets and in are the edges and . According to the theorem above, a minimal spanning tree that includes exists. By examination, you should be able to see that this is true. Is it true that only the bridges of minimal weight can be part of a minimal spanning tree?   Bridges between two sets   Bridges between two sets     essentially tells us that a minimal spanning tree can be constructed recursively by continually adding minimally weighted bridges to a set of edges.  Prim's Algorithm Prim's Algorithm  Let be a connected, weighted, undirected graph, and let be an arbitrary vertex in . The following steps lead to a minimal spanning tree for . and will be sets of vertices and is a set of edges.   (Initialize) ; ; .  (Build the tree) While :   Find , a bridge of minimum weight between and .   ; ;   Terminate with a minimal spanning tree .       If more than one minimal spanning tree exists, then the one that is obtained depends on and the means by which is selected in Step 2.  Warning: If two minimally weighted bridges exist between and , do not try to speed up the algorithm by adding both of them to '.  That yields a minimal spanning tree can be proven by induction with the use of .  If it is not known whether is connected, can be revised to handle this possibility. The key change (in Step 2.1) would be to determine whether any bridge at all exists between and . The condition of the while loop in Step 2 must also be changed somewhat.    A Small Example Consider the graph in . If we apply starting at , we obtain the following edge list in the order given: . The total of the weights of these edges is 20. The method that we have used (in Step 2.1) to select a bridge when more than one minimally weighted bridge exists is to order all bridges alphabetically by the vertex in and then, if further ties exist, by the vertex in . The first vertex in that order is selected in Step 2.1 of the algorithm.   A small weighted graph   A weighted graph     Minimum Diameter Spanning Tree Minimum Diameter Spanning Tree  Given a connected undirected graph , find a spanning tree of such that the longest path in is as short as possible.  The Case for Complete Graphs The Minimum Diameter Spanning Tree Problem is trivial to solve in a . Select any vertex and construct the spanning tree whose edge set is the set of edges that connect to the other vertices in the . illustrates a solution for .   Minimum diameter spanning tree for    Minimum diameter spanning tree for K_5     For incomplete graphs, a two-stage algorithm is needed. In short, the first step is to locate a center of the graph. The maximum distance from a center to any other vertex is as small as possible. Once a center is located, a breadth-first search of the graph is used to construct the spanning tree.    Exercises  Suppose that after Atlantis University's phone system is in place, a fifth campus is established and that a transmission line can be bought to connect the new campus to any old campus. Is this larger system the most economical one possible with respect to Objective 1? Can you always satisfy Objective 2?  It might not be most economical with respect to Objective 1. You should be able to find an example to illustrate this claim. The new system can always be made most economical with respect to Objective 2 if the old system were designed with that objective in mind.  Construct a minimal spanning tree for the capital cities in New England (see ).   Show that the answer to the question posed in is no.  In the figure below, is not a minimal bridge between , but it is part of the minimal spanning tree for this graph.    Solution to exercise 10-2-3     Find a minimal spanning tree for the following graphs.    Figure for exercise-10-2-4a      Figure for exercise-10-2-4b      Figure for exercise-10-2-4c     Find a minimum diameter spanning tree for the following graphs.    Figure for exercise-10-2-5a      Figure for exercise-10-2-5b    Edges in one solution are:  Vertices 8 and 9 are centers of the graph. Starting from vertex 8, a minimum diameter spanning tree is The diameter of the tree is 7.  In each of the following parts justify your answer with either a proof or a counterexample.   Suppose a weighted undirected graph had distinct edge weights. Is it possible that no minimal spanning tree includes the edge of minimal weight?  Suppose a weighted undirected graph had distinct edge weights. Is it possible that every minimal spanning tree includes the edge of maximal weight? If true, under what conditions would it happen?     "
},
{
  "id": "fig-atlantis-10",
  "level": "2",
  "url": "s-spanning-trees.html#fig-atlantis-10",
  "type": "Figure",
  "number": "10.2.1",
  "title": "",
  "body": " Atlantis University Graph   Atlantis University Graph   "
},
{
  "id": "def-spanning-tree",
  "level": "2",
  "url": "s-spanning-trees.html#def-spanning-tree",
  "type": "Definition",
  "number": "10.2.2",
  "title": "Spanning Tree.",
  "body": "Spanning Tree  Spanning Tree  Let be a connected undirected graph. A spanning tree for is a spanning subgraph of that is a tree.  "
},
{
  "id": "s-spanning-trees-4-3",
  "level": "2",
  "url": "s-spanning-trees.html#s-spanning-trees-4-3",
  "type": "Note",
  "number": "10.2.3",
  "title": "",
  "body": "  If is a spanning tree, .  The significance of a spanning tree is that it is a minimal spanning set. A smaller set would not span the graph, while a larger set would have a cycle, which has an edge that is superfluous.   "
},
{
  "id": "def-min-spanning-tree",
  "level": "2",
  "url": "s-spanning-trees.html#def-min-spanning-tree",
  "type": "Definition",
  "number": "10.2.4",
  "title": "Minimal Spanning Tree.",
  "body": "Minimal Spanning Tree  Minimal Spanning Tree  Given a weighted connected undirected graph , a minimal spanning tree is a spanning tree for which is as small as possible. "
},
{
  "id": "def-bridge",
  "level": "2",
  "url": "s-spanning-trees.html#def-bridge",
  "type": "Definition",
  "number": "10.2.5",
  "title": "Bridge.",
  "body": "Bridge  Bridge  Let be an undirected graph and let be a partition of . A bridge between and is an edge in that connects a vertex in to a vertex in . "
},
{
  "id": "theorem-10-2-1",
  "level": "2",
  "url": "s-spanning-trees.html#theorem-10-2-1",
  "type": "Theorem",
  "number": "10.2.6",
  "title": "",
  "body": " Let be a weighted connected undirected graph. Let be partitioned into two sets and . If is a bridge of least weight between and , then there exists a minimal spanning tree for that includes .  Suppose that no minimal spanning tree including exists. Let be a minimal spanning tree. If we add to , a cycle is created, and this cycle must contain another bridge, , between and . Since , we can delete and the new tree, which includes must also be a minimal spanning tree. "
},
{
  "id": "ex-some-bridges",
  "level": "2",
  "url": "s-spanning-trees.html#ex-some-bridges",
  "type": "Example",
  "number": "10.2.7",
  "title": "Some Bridges.",
  "body": "Some Bridges The bridges between the vertex sets and in are the edges and . According to the theorem above, a minimal spanning tree that includes exists. By examination, you should be able to see that this is true. Is it true that only the bridges of minimal weight can be part of a minimal spanning tree?   Bridges between two sets   Bridges between two sets    "
},
{
  "id": "alg-prim",
  "level": "2",
  "url": "s-spanning-trees.html#alg-prim",
  "type": "Algorithm",
  "number": "10.2.9",
  "title": "Prim’s Algorithm.",
  "body": "Prim's Algorithm Prim's Algorithm  Let be a connected, weighted, undirected graph, and let be an arbitrary vertex in . The following steps lead to a minimal spanning tree for . and will be sets of vertices and is a set of edges.   (Initialize) ; ; .  (Build the tree) While :   Find , a bridge of minimum weight between and .   ; ;   Terminate with a minimal spanning tree .    "
},
{
  "id": "s-spanning-trees-5-8",
  "level": "2",
  "url": "s-spanning-trees.html#s-spanning-trees-5-8",
  "type": "Note",
  "number": "10.2.10",
  "title": "",
  "body": "  If more than one minimal spanning tree exists, then the one that is obtained depends on and the means by which is selected in Step 2.  Warning: If two minimally weighted bridges exist between and , do not try to speed up the algorithm by adding both of them to '.  That yields a minimal spanning tree can be proven by induction with the use of .  If it is not known whether is connected, can be revised to handle this possibility. The key change (in Step 2.1) would be to determine whether any bridge at all exists between and . The condition of the while loop in Step 2 must also be changed somewhat.   "
},
{
  "id": "ex-an-example",
  "level": "2",
  "url": "s-spanning-trees.html#ex-an-example",
  "type": "Example",
  "number": "10.2.11",
  "title": "A Small Example.",
  "body": "A Small Example Consider the graph in . If we apply starting at , we obtain the following edge list in the order given: . The total of the weights of these edges is 20. The method that we have used (in Step 2.1) to select a bridge when more than one minimally weighted bridge exists is to order all bridges alphabetically by the vertex in and then, if further ties exist, by the vertex in . The first vertex in that order is selected in Step 2.1 of the algorithm.   A small weighted graph   A weighted graph    "
},
{
  "id": "def-min-diameter-spanning-tree",
  "level": "2",
  "url": "s-spanning-trees.html#def-min-diameter-spanning-tree",
  "type": "Definition",
  "number": "10.2.13",
  "title": "Minimum Diameter Spanning Tree.",
  "body": "Minimum Diameter Spanning Tree Minimum Diameter Spanning Tree  Given a connected undirected graph , find a spanning tree of such that the longest path in is as short as possible. "
},
{
  "id": "ex-min-diameter-k5",
  "level": "2",
  "url": "s-spanning-trees.html#ex-min-diameter-k5",
  "type": "Example",
  "number": "10.2.14",
  "title": "The Case for Complete Graphs.",
  "body": "The Case for Complete Graphs The Minimum Diameter Spanning Tree Problem is trivial to solve in a . Select any vertex and construct the spanning tree whose edge set is the set of edges that connect to the other vertices in the . illustrates a solution for .   Minimum diameter spanning tree for    Minimum diameter spanning tree for K_5    "
},
{
  "id": "exercises-10-2-2",
  "level": "2",
  "url": "s-spanning-trees.html#exercises-10-2-2",
  "type": "Exercise",
  "number": "10.2.4.1",
  "title": "",
  "body": "Suppose that after Atlantis University's phone system is in place, a fifth campus is established and that a transmission line can be bought to connect the new campus to any old campus. Is this larger system the most economical one possible with respect to Objective 1? Can you always satisfy Objective 2?  It might not be most economical with respect to Objective 1. You should be able to find an example to illustrate this claim. The new system can always be made most economical with respect to Objective 2 if the old system were designed with that objective in mind. "
},
{
  "id": "exercises-10-2-3",
  "level": "2",
  "url": "s-spanning-trees.html#exercises-10-2-3",
  "type": "Exercise",
  "number": "10.2.4.2",
  "title": "",
  "body": "Construct a minimal spanning tree for the capital cities in New England (see ).  "
},
{
  "id": "exercises-10-2-4",
  "level": "2",
  "url": "s-spanning-trees.html#exercises-10-2-4",
  "type": "Exercise",
  "number": "10.2.4.3",
  "title": "",
  "body": "Show that the answer to the question posed in is no.  In the figure below, is not a minimal bridge between , but it is part of the minimal spanning tree for this graph.    Solution to exercise 10-2-3    "
},
{
  "id": "exercises-10-2-5",
  "level": "2",
  "url": "s-spanning-trees.html#exercises-10-2-5",
  "type": "Exercise",
  "number": "10.2.4.4",
  "title": "",
  "body": "Find a minimal spanning tree for the following graphs.    Figure for exercise-10-2-4a      Figure for exercise-10-2-4b      Figure for exercise-10-2-4c    "
},
{
  "id": "exercises-10-2-6",
  "level": "2",
  "url": "s-spanning-trees.html#exercises-10-2-6",
  "type": "Exercise",
  "number": "10.2.4.5",
  "title": "",
  "body": "Find a minimum diameter spanning tree for the following graphs.    Figure for exercise-10-2-5a      Figure for exercise-10-2-5b    Edges in one solution are:  Vertices 8 and 9 are centers of the graph. Starting from vertex 8, a minimum diameter spanning tree is The diameter of the tree is 7. "
},
{
  "id": "exercises-10-2-7",
  "level": "2",
  "url": "s-spanning-trees.html#exercises-10-2-7",
  "type": "Exercise",
  "number": "10.2.4.6",
  "title": "",
  "body": "In each of the following parts justify your answer with either a proof or a counterexample.   Suppose a weighted undirected graph had distinct edge weights. Is it possible that no minimal spanning tree includes the edge of minimal weight?  Suppose a weighted undirected graph had distinct edge weights. Is it possible that every minimal spanning tree includes the edge of maximal weight? If true, under what conditions would it happen?   "
},
{
  "id": "s-rooted-trees",
  "level": "1",
  "url": "s-rooted-trees.html",
  "type": "Section",
  "number": "10.3",
  "title": "Rooted Trees",
  "body": " Rooted Trees  Rooted Trees  In the next two sections, we will discuss rooted trees. Our primary foci will be on general rooted trees and on a special case, ordered binary trees.  Definition and Terminology   A Rooted Tree   A Rooted Tree    Informal Definition and Terminology genealogical terms  What differentiates rooted trees from undirected trees is that a rooted tree contains a distinguished vertex, called the root. Consider the tree in . Vertex has been designated the root of the tree. If we choose any other vertex in the tree, such as , we know that there is a unique path from to . The vertices on this path, , are described in genealogical terms:   is a child of (so is )  is 's parent.  , , and are 's ancestors.  , , and are descendants of .   These genealogical relationships are often easier to visualize if the tree is rewritten so that children are positioned below their parents, as in .  With this format, it is easy to see that each vertex in the tree can be thought of as the root of a tree that contains, in addition to itself, all of its descendants. For example, is the root of a tree that contains , , , and . Furthermore, is the root of a tree that contains , , and . Finally, and are roots of trees that contain only themselves. From this observation, we can give a formal definition of a rooted tree.    A Rooted Tree, redrawn   A Rooted Tree, redrawn    One can formally define the genealogical terms above. We define child here since it's used in our formal definition of a rooted tree and leave the rest of the definitions as an exercise.  Child of a Root  Child of a Root Parent of a vertex  Given a rooted tree with root , a child of is a vertex that is connected to by an edge of the tree. We refer to the root as the parent of each of its children.    Rooted Tree  Rooted Tree    A single vertex with no children is a rooted tree with root .  Recursion: Let , , be disjoint rooted trees with roots , , , respectively, and let be a vertex that does not belong to any of these trees. Then a rooted tree, rooted at , is obtained by making the parent of the vertices , , and . We call subtrees of the larger tree.     The level of a vertex Level of a vertex of a rooted tree is the number of edges that separate the vertex from the root. The level of the root is zero. The depth of a tree is the maximum level of the vertices in the tree. The depth of a tree in is three, which is the level of the vertices and . The vertices , , , , , , and have level two. , , and are at level one and has level zero.  A Decision Tree is a rooted tree with Start as the root. It is an example of what is called a decision tree.  Tree Structure of Data One of the keys to working with large amounts of information is to organize it in a consistent, logical way. A data structure is a scheme for organizing data. A simple example of a data structure might be the information a college admissions department might keep on their applicants. Items might look something like this: This structure is called a flat file .  A spreadsheet can be used to arrange data in this way. Although a flat file structure is often adequate, there are advantages to clustering some the information. For example the applicant information might be broken into four parts: name, contact information, high school, and application data: The first item in each ApplicantItem is a list , with each item in that list being a single field of the original flat file. The third item is simply the single high school item from the flat file. The application data is a list and one of its items, is itself a list with the recommendation data for each recommendation the applicant has.  The organization of this data can be visualized with a rooted tree such as the one in .   Applicant Data in a Rooted Tree   Applicant Data in a Rooted Tree    In general, you can represent a data item, , as a rooted tree with as the root and a subtree for each field. Those fields that are more than just one item are roots of further subtrees, while individual items have no further children in the tree.    Kruskal's Algorithm  An alternate algorithm for constructing a minimal spanning tree uses a forest of rooted trees. First we will describe the algorithm in its simplest terms. Afterward, we will describe how rooted trees are used to implement the algorithm. Finally, we will demonstrate the SageMath implementation of the algorithm. In all versions of this algorithm, assume that is a weighted undirected graph with and .  Kruskal's Algorithm - Informal Version    Sort the edges of in ascending order according to weight. That is, .  Go down the list from Step 1 and add edges to a set (initially empty) of edges so that the set does not form a cycle. When an edge that would create a cycle is encountered, ignore it. Continue examining edges until either edges have been selected or you have come to the end of the edge list. If edges are selected, these edges make up a minimal spanning tree for . If fewer than edges are selected, is not connected.     Step 1 can be accomplished using one of any number of standard sorting routines. Using the most efficient sorting routine, the time required to perform this step is proportional to . The second step of the algorithm, also of time complexity, is the one that uses a forest of rooted trees to test for whether an edge should be added to the spanning set.  Kruskal's Algorithm Kruskal's Algorithm    Sort the edges of in ascending order according to weight. That is, .    Initialize each vertex in V to be the root of its own rooted tree.  Go down the list of edges until either a spanning tree is completed or the edge list has been exhausted. For each edge , we can determine whether e can be added to the spanning set without forming a cycle by determining whether the root of tree is equal to the root of tree. If the two roots are equal, then ignore e. If the roots are different, then we can add e to the spanning set. In addition, we merge the trees that and belong to. This is accomplished by either making root the parent of root or vice versa.       Since we start the Kruskal's algorithm with trees and each addition of an edge decreases the number of trees by one, we end the algorithm with one rooted tree, provided a spanning tree exists.  The rooted tree that we develop in the algorithm is not the spanning tree itself.     SageMath Note - Implementation of Kruskal's Algorithm SageMath Note Kruskal's Algorithm  Kruskal's algorithm has been implemented in Sage. We illustrate how the spanning tree for a weighted graph in can be generated. First, we create such a graph  We will create a graph using a list of triples of the form . The method tells Sage to consider the labels as weights.    Weighed graph, SageMath output   SageMath Output - Weighted Graph    Next, we load the kruskal function and use it to generate the list of edges in a spanning tree of .   To see the resulting tree with the same embedding as , we generate a graph from the spanning tree edges. Next, we set the positions of the vertices to be the same as in the graph. Finally, we plot the tree.    Spanning tree, SageMath output   SageMath Output - Spanning tree for weighted graph      Exercises   Suppose that an undirected tree has diameter and that you would like to select a vertex of the tree as a root so that the resulting rooted tree has the smallest depth possible. How would such a root be selected and what would be the depth of the tree (in terms of )?  Locate any simple path of length and locate the vertex in position on the path. The tree rooted at that vertex will have a depth of , which is minimal.  Use Kruskal's algorithm to find a minimal spanning tree for the following graphs. In addition to the spanning tree, find the final rooted tree in the algorithm. When you merge two trees in the algorithm, make the root with the lower number the root of the new tree.    Figure for exercise 10-3-2a      Figure for exercise 10-3-2b     Suppose that information on buildings is arranged in records with five fields: the name of the building, its location, its owner, its height, and its floor space. The location and owner fields are records that include all of the information that you would expect, such as street, city, and state, together with the owner's name (first, middle, last) in the owner field. Draw a rooted tree to describe this type of record     Solution to exercise 10-3-3     Step through Kruskal's Algorithm by hand to verify that the example of a minimal spanning tree using Sage in is correct.    "
},
{
  "id": "fig-rooted-tree-10-3",
  "level": "2",
  "url": "s-rooted-trees.html#fig-rooted-tree-10-3",
  "type": "Figure",
  "number": "10.3.1",
  "title": "",
  "body": " A Rooted Tree   A Rooted Tree   "
},
{
  "id": "list-rooted-tree-terms",
  "level": "2",
  "url": "s-rooted-trees.html#list-rooted-tree-terms",
  "type": "List",
  "number": "10.3.2",
  "title": "Informal Definition and Terminology",
  "body": "Informal Definition and Terminology genealogical terms  What differentiates rooted trees from undirected trees is that a rooted tree contains a distinguished vertex, called the root. Consider the tree in . Vertex has been designated the root of the tree. If we choose any other vertex in the tree, such as , we know that there is a unique path from to . The vertices on this path, , are described in genealogical terms:   is a child of (so is )  is 's parent.  , , and are 's ancestors.  , , and are descendants of .   These genealogical relationships are often easier to visualize if the tree is rewritten so that children are positioned below their parents, as in .  With this format, it is easy to see that each vertex in the tree can be thought of as the root of a tree that contains, in addition to itself, all of its descendants. For example, is the root of a tree that contains , , , and . Furthermore, is the root of a tree that contains , , and . Finally, and are roots of trees that contain only themselves. From this observation, we can give a formal definition of a rooted tree.  "
},
{
  "id": "fig-rooted-tree-10-3-redrawn",
  "level": "2",
  "url": "s-rooted-trees.html#fig-rooted-tree-10-3-redrawn",
  "type": "Figure",
  "number": "10.3.3",
  "title": "",
  "body": " A Rooted Tree, redrawn   A Rooted Tree, redrawn   "
},
{
  "id": "def-child-of-root",
  "level": "2",
  "url": "s-rooted-trees.html#def-child-of-root",
  "type": "Definition",
  "number": "10.3.4",
  "title": "Child of a Root.",
  "body": "Child of a Root  Child of a Root Parent of a vertex  Given a rooted tree with root , a child of is a vertex that is connected to by an edge of the tree. We refer to the root as the parent of each of its children.   "
},
{
  "id": "def-rooted-tree",
  "level": "2",
  "url": "s-rooted-trees.html#def-rooted-tree",
  "type": "Definition",
  "number": "10.3.5",
  "title": "Rooted Tree.",
  "body": "Rooted Tree  Rooted Tree    A single vertex with no children is a rooted tree with root .  Recursion: Let , , be disjoint rooted trees with roots , , , respectively, and let be a vertex that does not belong to any of these trees. Then a rooted tree, rooted at , is obtained by making the parent of the vertices , , and . We call subtrees of the larger tree.    "
},
{
  "id": "ss-rooted-trees-8",
  "level": "2",
  "url": "s-rooted-trees.html#ss-rooted-trees-8",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "level of a vertex "
},
{
  "id": "ex-decision-tree",
  "level": "2",
  "url": "s-rooted-trees.html#ex-decision-tree",
  "type": "Example",
  "number": "10.3.6",
  "title": "A Decision Tree.",
  "body": "A Decision Tree is a rooted tree with Start as the root. It is an example of what is called a decision tree. "
},
{
  "id": "ex-data-structures",
  "level": "2",
  "url": "s-rooted-trees.html#ex-data-structures",
  "type": "Example",
  "number": "10.3.7",
  "title": "Tree Structure of Data.",
  "body": "Tree Structure of Data One of the keys to working with large amounts of information is to organize it in a consistent, logical way. A data structure is a scheme for organizing data. A simple example of a data structure might be the information a college admissions department might keep on their applicants. Items might look something like this: This structure is called a flat file .  A spreadsheet can be used to arrange data in this way. Although a flat file structure is often adequate, there are advantages to clustering some the information. For example the applicant information might be broken into four parts: name, contact information, high school, and application data: The first item in each ApplicantItem is a list , with each item in that list being a single field of the original flat file. The third item is simply the single high school item from the flat file. The application data is a list and one of its items, is itself a list with the recommendation data for each recommendation the applicant has.  The organization of this data can be visualized with a rooted tree such as the one in .   Applicant Data in a Rooted Tree   Applicant Data in a Rooted Tree    In general, you can represent a data item, , as a rooted tree with as the root and a subtree for each field. Those fields that are more than just one item are roots of further subtrees, while individual items have no further children in the tree.  "
},
{
  "id": "ss-kruskal-algorithm-3",
  "level": "2",
  "url": "s-rooted-trees.html#ss-kruskal-algorithm-3",
  "type": "Algorithm",
  "number": "10.3.9",
  "title": "Kruskal’s Algorithm - Informal Version.",
  "body": "Kruskal's Algorithm - Informal Version    Sort the edges of in ascending order according to weight. That is, .  Go down the list from Step 1 and add edges to a set (initially empty) of edges so that the set does not form a cycle. When an edge that would create a cycle is encountered, ignore it. Continue examining edges until either edges have been selected or you have come to the end of the edge list. If edges are selected, these edges make up a minimal spanning tree for . If fewer than edges are selected, is not connected.    "
},
{
  "id": "ss-kruskal-algorithm-5",
  "level": "2",
  "url": "s-rooted-trees.html#ss-kruskal-algorithm-5",
  "type": "Algorithm",
  "number": "10.3.10",
  "title": "Kruskal’s Algorithm.",
  "body": "Kruskal's Algorithm Kruskal's Algorithm    Sort the edges of in ascending order according to weight. That is, .    Initialize each vertex in V to be the root of its own rooted tree.  Go down the list of edges until either a spanning tree is completed or the edge list has been exhausted. For each edge , we can determine whether e can be added to the spanning set without forming a cycle by determining whether the root of tree is equal to the root of tree. If the two roots are equal, then ignore e. If the roots are different, then we can add e to the spanning set. In addition, we merge the trees that and belong to. This is accomplished by either making root the parent of root or vice versa.    "
},
{
  "id": "ss-kruskal-algorithm-6",
  "level": "2",
  "url": "s-rooted-trees.html#ss-kruskal-algorithm-6",
  "type": "Note",
  "number": "10.3.11",
  "title": "",
  "body": "  Since we start the Kruskal's algorithm with trees and each addition of an edge decreases the number of trees by one, we end the algorithm with one rooted tree, provided a spanning tree exists.  The rooted tree that we develop in the algorithm is not the spanning tree itself.   "
},
{
  "id": "sage_graph",
  "level": "2",
  "url": "s-rooted-trees.html#sage_graph",
  "type": "Figure",
  "number": "10.3.12",
  "title": "",
  "body": " Weighed graph, SageMath output   SageMath Output - Weighted Graph   "
},
{
  "id": "sage_spanning_tree",
  "level": "2",
  "url": "s-rooted-trees.html#sage_spanning_tree",
  "type": "Figure",
  "number": "10.3.13",
  "title": "",
  "body": " Spanning tree, SageMath output   SageMath Output - Spanning tree for weighted graph   "
},
{
  "id": "exercises-10-3-2",
  "level": "2",
  "url": "s-rooted-trees.html#exercises-10-3-2",
  "type": "Exercise",
  "number": "10.3.4.1",
  "title": "",
  "body": " Suppose that an undirected tree has diameter and that you would like to select a vertex of the tree as a root so that the resulting rooted tree has the smallest depth possible. How would such a root be selected and what would be the depth of the tree (in terms of )?  Locate any simple path of length and locate the vertex in position on the path. The tree rooted at that vertex will have a depth of , which is minimal. "
},
{
  "id": "exercises-10-3-3",
  "level": "2",
  "url": "s-rooted-trees.html#exercises-10-3-3",
  "type": "Exercise",
  "number": "10.3.4.2",
  "title": "",
  "body": "Use Kruskal's algorithm to find a minimal spanning tree for the following graphs. In addition to the spanning tree, find the final rooted tree in the algorithm. When you merge two trees in the algorithm, make the root with the lower number the root of the new tree.    Figure for exercise 10-3-2a      Figure for exercise 10-3-2b    "
},
{
  "id": "exercises-10-3-4",
  "level": "2",
  "url": "s-rooted-trees.html#exercises-10-3-4",
  "type": "Exercise",
  "number": "10.3.4.3",
  "title": "",
  "body": "Suppose that information on buildings is arranged in records with five fields: the name of the building, its location, its owner, its height, and its floor space. The location and owner fields are records that include all of the information that you would expect, such as street, city, and state, together with the owner's name (first, middle, last) in the owner field. Draw a rooted tree to describe this type of record     Solution to exercise 10-3-3    "
},
{
  "id": "exercises-10-3-5",
  "level": "2",
  "url": "s-rooted-trees.html#exercises-10-3-5",
  "type": "Exercise",
  "number": "10.3.4.4",
  "title": "",
  "body": "Step through Kruskal's Algorithm by hand to verify that the example of a minimal spanning tree using Sage in is correct.  "
},
{
  "id": "s-binary-trees",
  "level": "1",
  "url": "s-binary-trees.html",
  "type": "Section",
  "number": "10.4",
  "title": "Binary Trees",
  "body": " Binary Trees  Binary Trees  Definition of a Binary Tree  An ordered rooted tree is a rooted tree whose subtrees are put into a definite order and are, themselves, ordered rooted trees. An empty tree and a single vertex with no descendants (no subtrees) are ordered rooted trees.  Distinct Ordered Rooted Trees The trees in are identical rooted trees, with root 1, but as ordered trees, they are different.   Two different ordered rooted trees   Two different ordered rooted trees     If a tree rooted at has subtrees, we would refer to them as the first, second,..., subtrees. There is a subtle difference between certain ordered trees and binary trees, which we define next.  Binary Tree  Binary Tree    A tree consisting of no vertices (the empty tree) is a binary tree  A vertex together with two subtrees that are both binary trees is a binary tree. The subtrees are called the left and right subtrees of the binary tree.    The difference between binary trees and ordered trees is that every vertex of a binary tree has exactly two subtrees (one or both of which may be empty), while a vertex of an ordered tree may have any number of subtrees. But there is another significant difference between the two types of structures. The two trees in would be considered identical as ordered trees. However, they are different binary trees. Tree (a) has an empty right subtree and Tree (b) has an empty left subtree.   Two different binary trees   Two different binary trees    Terminology and General Facts about Binary Trees Full binary tree   Leaf of a binary tree A vertex of a binary tree with two empty subtrees is called a leaf Leaf, of a binary tree . All other vertices are called internal vertices .  The number of leaves in a binary tree can vary from one up to roughly half the number of vertices in the tree (see Exercise 4 of this section).  The maximum number of vertices at level of a binary tree is , (see Exercise 6 of this section).  Full binary tree A full binary tree is a tree for which each vertex has either zero or two empty subtrees. In other words, each vertex has either two or zero children. See of this section for a general fact about full binary trees.     Traversals of Binary Trees Traversals of Binary Trees  The traversal of a binary tree consists of visiting each vertex of the tree in some prescribed order. Unlike graph traversals, the consecutive vertices that are visited are not always connected with an edge. The most common binary tree traversals are differentiated by the order in which the root and its subtrees are visited. The three traversals are best described recursively and are:  Preorder Traversal:  Visit the root of the tree.  Preorder traverse the left subtree.  Preorder traverse the right subtree.    Inorder Traversal:  Inorder traverse the left subtree.  Visit the root of the tree.  Inorder traverse the right subtree.    Postorder Traversal:  Postorder traverse the left subtree.  Postorder traverse the right subtree.  Visit the root of the tree.     Any traversal of an empty tree consists of doing nothing.  Traversal Examples For the tree in , the orders in which the vertices are visited are:  A-B-D-E-C-F-G, for the preorder traversal.  D-B-E-A-F-C-G, for the inorder traversal.  D-E-B-F-G-C-A, for the postorder traversal.    A Complete Binary Tree to Level 2   A Complete Binary Tree to Level 2     Binary Tree Sort. Given a collection of integers (or other objects than can be ordered), one technique for sorting is a binary tree sort. If the integers are , , , , we first execute the following algorithm that creates a binary tree:  Binary Sort Tree Creation    Insert into the root of the tree.  For k := 2 to n \/\/ insert into the tree   r =  inserted = false  while not(inserted):  if :  if has a left child:  r = left child of  else:  make the left child of   inserted = true  else:  if has a right child:  r = right child of  else:  make the right child of   inserted = true       If the integers to be sorted are 25, 17, 9, 20, 33, 13, and 30, then the tree that is created is the one in . The inorder traversal of this tree is 9, 13, 17, 20, 25, 30, 33, the integers in ascending order. In general, the inorder traversal of the tree that is constructed in the algorithm above will produce a sorted list. The preorder and postorder traversals of the tree have no meaning here.   A Binary Sorting Tree   Binary Sorting Tree     Expression Trees Expression Tree  A convenient way to visualize an algebraic expression is by its expression tree. Consider the expression Since it is customary to put a precedence on multiplication\/divisions, is evaluated as . Consecutive multiplication\/divisions or addition\/subtractions are evaluated from left to right. We can analyze further by noting that it is the sum of two simpler expressions and . The first of these expressions can be broken down further into the difference of the expressions and . When we decompose any expression into , the expression tree of that expression is the binary tree whose root contains the operation and whose left and right subtrees are the trees of the left and right expressions, respectively. Additionally, a simple variable or a number has an expression tree that is a single vertex containing the variable or number. The evolution of the expression tree for expression appears in .   Building an Expression Tree   Building an Expression Tree    Some Expression Trees   If we intend to apply the addition and subtraction operations in first, we would parenthesize the expression to . Its expression tree appears in (a).  The expression trees for and for appear in (b) and (c).    Expression Tree Examples   Expression Tree Examples     The three traversals of an operation tree are all significant. A binary operation applied to a pair of numbers can be written in three ways. One is the familiar infix form, such as for the sum of and . Another form is prefix, in which the same sum is written . The final form is postfix, in which the sum is written . Algebraic expressions involving the four standard arithmetic operations in prefix and postfix form are defined as follows:   Prefix and postfix forms of an algebraic expression   Prefix  A variable or number is a prefix expression  Any operation followed by a pair of prefix expressions is a prefix expression.  Postfix  A variable or number is a postfix expression  Any pair of postfix expressions followed by an operation is a postfix expression.    The connection between traversals of an expression tree and these forms is simple:   The preorder traversal of an expression tree will result in the prefix form of the expression.  The postorder traversal of an expression tree will result in the postfix form of the expression.  The inorder traversal of an operation tree will not, in general, yield the proper infix form of the expression. If an expression requires parentheses in infix form, an inorder traversal of its expression tree has the effect of removing the parentheses.    Traversing an Expression Tree The preorder traversal of the tree in is , which is the prefix version of expression . The postorder traversal is . Note that since the original form of needed no parentheses, the inorder traversal, , is the correct infix version.    Counting Binary Trees Counting Binary Trees  We close this section with a formula for the number of different binary trees with vertices. The formula is derived using generating functions. Although the complete details are beyond the scope of this text, we will supply an overview of the derivation in order to illustrate how generating functions are used in advanced combinatorics.  Let be the number of different binary trees of size ( vertices), . By our definition of a binary tree, . Now consider any positive integer , . A binary tree of size has two subtrees, the sizes of which add up to . The possibilities can be broken down into cases:   Case 0: Left subtree has size 0; right subtree has size .  Case 1: Left subtree has size 1; right subtree has size .    Case : Left subtree has size ; right subtree has size .    Case : Left subtree has size ; right subtree has size 0.   In the general Case , we can count the number of possibilities by multiplying the number of ways that the left subtree can be filled, , by the number of ways that the right subtree can be filled. . Since the sum of these products equals , we obtain the recurrence relation for :   Now take the generating function of both sides of this recurrence relation: or  Recall that If we abbreviate to , we get Using the quadratic equation we find two solutions:   The gap in our derivation occurs here since we don't presume a knowledge of calculus. If we expand as an extended power series, we find  The coefficients after the first one are all negative and there is a singularity at 0 because of the term. However if we do the same with we get  Further analysis leads to a closed form expression for , which is This sequence of numbers is often called the Catalan numbers . For more information on the Catalan numbers, see the entry A000108 in The On-Line Encyclopedia of Integer Sequences .   SageMath Note - Power Series SageMath Note Power Series  It may be of interest to note how the extended power series expansions of and are determined using Sage. In Sage, one has the capability of being very specific about how algebraic expressions should be interpreted by specifying the underlying ring. This can make working with various algebraic expressions a bit more confusing to the beginner. Here is how to get a Laurent expansion for above.   The first Sage expression above declares a structure called a ring that contains power series. We are not using that whole structure, just a specific element, G1 . So the important thing about this first input is that it establishes z as being a variable associated with power series over the integers. When the second expression defines the value of G1 in terms of z , it is automatically converted to a power series.  The expansion of uses identical code, and its coefficients are the values of .   In Chapter 16 we will introduce rings and will be able to take further advantage of Sage's capabilities in this area.    Exercises  Draw the expression trees for the following expressions:                    Solution to exercise 10-4-1-A      Solution to exercise 10-4-1-B     Draw the expression trees for         Write out the preorder, inorder, and postorder traversals of the trees in Exercise 1 above.      Verify the formula for , by drawing all binary trees with three or fewer vertices.     Draw a binary tree with seven vertices and only one leaf. Your answer won't be unique. How many different possible answers are there?  Draw a binary tree with seven vertices and as many leaves as possible.   There are different possible answers to part (a). The answer to (b) is unique.    Solution to exercise 10-4-5     Prove that the maximum number of vertices at level of a binary tree is and that a tree with that many vertices at level must have vertices.   Prove that if is a full binary tree, then the number of leaves of is one more than the number of internal vertices (non-leaves).  Solution 1:  Basis: A binary tree consisting of a single vertex, which is a leaf, satisfies the equation  Induction:Assume that for some , all full binary trees with or fewer vertices have one more leaf than internal vertices. Now consider any full binary tree with vertices. Let and be the left and right subtrees of the tree which, by the definition of a full binary tree, must both be full. If and are the numbers of internal vertices in and , and and are the numbers of leaves, then and . Therefore, in the whole tree,   Solution 2: Imagine building a full binary tree starting with a single vertex. By continuing to add leaves in pairs so that the tree stays full, we can build any full binary tree. Our starting tree satisfies the condition that the number of leaves is one more than the number of internal vertices . By adding a pair of leaves to a full binary tree, an old leaf becomes an internal vertex, increasing the number of internal vertices by one. Although we lose a leaf, the two added leaves create a net increase of one leaf. Therefore, the desired equality is maintained.    There is a one to one correspondence between ordered rooted trees and binary trees. If you start with an ordered rooted tree, , you can build a binary tree with an empty right subtree by placing the the root of at the root of . Then for every vertex from that has been placed in , place it's leftmost child (if there is one) as 's left child in . Make 's next sibling (if there is one) in the right child in .  An ordered rooted tree with root .   An ordered rooted tree with root specifed by the dictionary of children {r:[a,d,c],a:[b,c],e:[f,g,h]}.      Blue (left) and Red (right) links added to the ordered rooted tree with root r.   An ordered rooted tree with root r specifed by the dictionary of children {r:[a,d,c],a:[b,c],e:[f,g,h]} with colored edges added to indicate the correspondence with a binary tree.    Binary tree corresponding to the ordered rooted tree.   The binary tree corresponding with the ordered rooted tree rooted at r specified by the dictionary of children {r:[a,d,c],a:[b,c],e:[f,g,h]}.      Why will have no right children in this correspondence?  Draw the binary tree that is produced by the ordered rooted tree in .  The left subtree of the binary tree in is one of 5 different binary trees with three vertices. Draw each of them and also the ordered rooted tree that each corresponds with.  What does this correspondence tell us about how the numbers of different binary trees and ordered rooted trees are related?    What binary tree does this correspond with?   An ordered rooted tree.    What ordered rooted tree does this correspond with?   A binary tree rooted at r with dictionary of left child- right child values {r:[a,nil],a:[b,c],b:[nil,nil],c:[nil,nil]}.       The root of is the root of the corresponding ordered rooted tree, which as no siblings.    Two columns of five graphs      Two columns of five graphs     The number of ordered rooted trees with vertices is equal to the number of binary trees with vertices,     "
},
{
  "id": "s-binary-trees-3-2",
  "level": "2",
  "url": "s-binary-trees.html#s-binary-trees-3-2",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "ordered rooted tree "
},
{
  "id": "ex-ordered-rooted",
  "level": "2",
  "url": "s-binary-trees.html#ex-ordered-rooted",
  "type": "Example",
  "number": "10.4.1",
  "title": "Distinct Ordered Rooted Trees.",
  "body": "Distinct Ordered Rooted Trees The trees in are identical rooted trees, with root 1, but as ordered trees, they are different.   Two different ordered rooted trees   Two different ordered rooted trees    "
},
{
  "id": "def-binary-tree",
  "level": "2",
  "url": "s-binary-trees.html#def-binary-tree",
  "type": "Definition",
  "number": "10.4.3",
  "title": "Binary Tree.",
  "body": "Binary Tree  Binary Tree    A tree consisting of no vertices (the empty tree) is a binary tree  A vertex together with two subtrees that are both binary trees is a binary tree. The subtrees are called the left and right subtrees of the binary tree.   "
},
{
  "id": "fig-diff-binary-trees",
  "level": "2",
  "url": "s-binary-trees.html#fig-diff-binary-trees",
  "type": "Figure",
  "number": "10.4.4",
  "title": "",
  "body": " Two different binary trees   Two different binary trees   "
},
{
  "id": "s-binary-trees-3-8",
  "level": "2",
  "url": "s-binary-trees.html#s-binary-trees-3-8",
  "type": "List",
  "number": "10.4.5",
  "title": "Terminology and General Facts about Binary Trees",
  "body": "Terminology and General Facts about Binary Trees Full binary tree   Leaf of a binary tree A vertex of a binary tree with two empty subtrees is called a leaf Leaf, of a binary tree . All other vertices are called internal vertices .  The number of leaves in a binary tree can vary from one up to roughly half the number of vertices in the tree (see Exercise 4 of this section).  The maximum number of vertices at level of a binary tree is , (see Exercise 6 of this section).  Full binary tree A full binary tree is a tree for which each vertex has either zero or two empty subtrees. In other words, each vertex has either two or zero children. See of this section for a general fact about full binary trees.   "
},
{
  "id": "ex-traversals-example",
  "level": "2",
  "url": "s-binary-trees.html#ex-traversals-example",
  "type": "Example",
  "number": "10.4.6",
  "title": "Traversal Examples.",
  "body": "Traversal Examples For the tree in , the orders in which the vertices are visited are:  A-B-D-E-C-F-G, for the preorder traversal.  D-B-E-A-F-C-G, for the inorder traversal.  D-E-B-F-G-C-A, for the postorder traversal.    A Complete Binary Tree to Level 2   A Complete Binary Tree to Level 2    "
},
{
  "id": "alg-bin-sort-maketree",
  "level": "2",
  "url": "s-binary-trees.html#alg-bin-sort-maketree",
  "type": "Algorithm",
  "number": "10.4.8",
  "title": "Binary Sort Tree Creation.",
  "body": "Binary Sort Tree Creation    Insert into the root of the tree.  For k := 2 to n \/\/ insert into the tree   r =  inserted = false  while not(inserted):  if :  if has a left child:  r = left child of  else:  make the left child of   inserted = true  else:  if has a right child:  r = right child of  else:  make the right child of   inserted = true      "
},
{
  "id": "fig-sort-tree",
  "level": "2",
  "url": "s-binary-trees.html#fig-sort-tree",
  "type": "Figure",
  "number": "10.4.9",
  "title": "",
  "body": " A Binary Sorting Tree   Binary Sorting Tree   "
},
{
  "id": "fig-expression-tree-build",
  "level": "2",
  "url": "s-binary-trees.html#fig-expression-tree-build",
  "type": "Figure",
  "number": "10.4.10",
  "title": "",
  "body": " Building an Expression Tree   Building an Expression Tree   "
},
{
  "id": "ex-expression-tree-examples",
  "level": "2",
  "url": "s-binary-trees.html#ex-expression-tree-examples",
  "type": "Example",
  "number": "10.4.11",
  "title": "Some Expression Trees.",
  "body": "Some Expression Trees   If we intend to apply the addition and subtraction operations in first, we would parenthesize the expression to . Its expression tree appears in (a).  The expression trees for and for appear in (b) and (c).    Expression Tree Examples   Expression Tree Examples    "
},
{
  "id": "ss-expression-trees-7",
  "level": "2",
  "url": "s-binary-trees.html#ss-expression-trees-7",
  "type": "List",
  "number": "10.4.13",
  "title": "Prefix and postfix forms of an algebraic expression",
  "body": " Prefix and postfix forms of an algebraic expression   Prefix  A variable or number is a prefix expression  Any operation followed by a pair of prefix expressions is a prefix expression.  Postfix  A variable or number is a postfix expression  Any pair of postfix expressions followed by an operation is a postfix expression.   "
},
{
  "id": "ex-expression-tree-traversal-example",
  "level": "2",
  "url": "s-binary-trees.html#ex-expression-tree-traversal-example",
  "type": "Example",
  "number": "10.4.14",
  "title": "Traversing an Expression Tree.",
  "body": "Traversing an Expression Tree The preorder traversal of the tree in is , which is the prefix version of expression . The postorder traversal is . Note that since the original form of needed no parentheses, the inorder traversal, , is the correct infix version.  "
},
{
  "id": "ss-counting-binary-trees-11",
  "level": "2",
  "url": "s-binary-trees.html#ss-counting-binary-trees-11",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Catalan numbers "
},
{
  "id": "ss-sage-note-powerseries-5",
  "level": "2",
  "url": "s-binary-trees.html#ss-sage-note-powerseries-5",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "ring "
},
{
  "id": "exercises-10-4-2",
  "level": "2",
  "url": "s-binary-trees.html#exercises-10-4-2",
  "type": "Exercise",
  "number": "10.4.6.1",
  "title": "",
  "body": "Draw the expression trees for the following expressions:                    Solution to exercise 10-4-1-A      Solution to exercise 10-4-1-B    "
},
{
  "id": "exercises-10-4-3",
  "level": "2",
  "url": "s-binary-trees.html#exercises-10-4-3",
  "type": "Exercise",
  "number": "10.4.6.2",
  "title": "",
  "body": "Draw the expression trees for        "
},
{
  "id": "exercises-10-4-4",
  "level": "2",
  "url": "s-binary-trees.html#exercises-10-4-4",
  "type": "Exercise",
  "number": "10.4.6.3",
  "title": "",
  "body": "Write out the preorder, inorder, and postorder traversals of the trees in Exercise 1 above.    "
},
{
  "id": "exercises-10-4-5",
  "level": "2",
  "url": "s-binary-trees.html#exercises-10-4-5",
  "type": "Exercise",
  "number": "10.4.6.4",
  "title": "",
  "body": " Verify the formula for , by drawing all binary trees with three or fewer vertices.  "
},
{
  "id": "exercises-10-4-6",
  "level": "2",
  "url": "s-binary-trees.html#exercises-10-4-6",
  "type": "Exercise",
  "number": "10.4.6.5",
  "title": "",
  "body": "  Draw a binary tree with seven vertices and only one leaf. Your answer won't be unique. How many different possible answers are there?  Draw a binary tree with seven vertices and as many leaves as possible.   There are different possible answers to part (a). The answer to (b) is unique.    Solution to exercise 10-4-5    "
},
{
  "id": "exercises-10-4-7",
  "level": "2",
  "url": "s-binary-trees.html#exercises-10-4-7",
  "type": "Exercise",
  "number": "10.4.6.6",
  "title": "",
  "body": "Prove that the maximum number of vertices at level of a binary tree is and that a tree with that many vertices at level must have vertices.  "
},
{
  "id": "exercise-full-tree",
  "level": "2",
  "url": "s-binary-trees.html#exercise-full-tree",
  "type": "Exercise",
  "number": "10.4.6.7",
  "title": "",
  "body": "Prove that if is a full binary tree, then the number of leaves of is one more than the number of internal vertices (non-leaves).  Solution 1:  Basis: A binary tree consisting of a single vertex, which is a leaf, satisfies the equation  Induction:Assume that for some , all full binary trees with or fewer vertices have one more leaf than internal vertices. Now consider any full binary tree with vertices. Let and be the left and right subtrees of the tree which, by the definition of a full binary tree, must both be full. If and are the numbers of internal vertices in and , and and are the numbers of leaves, then and . Therefore, in the whole tree,   Solution 2: Imagine building a full binary tree starting with a single vertex. By continuing to add leaves in pairs so that the tree stays full, we can build any full binary tree. Our starting tree satisfies the condition that the number of leaves is one more than the number of internal vertices . By adding a pair of leaves to a full binary tree, an old leaf becomes an internal vertex, increasing the number of internal vertices by one. Although we lose a leaf, the two added leaves create a net increase of one leaf. Therefore, the desired equality is maintained.  "
},
{
  "id": "exercises-10-4-9",
  "level": "2",
  "url": "s-binary-trees.html#exercises-10-4-9",
  "type": "Exercise",
  "number": "10.4.6.8",
  "title": "",
  "body": " There is a one to one correspondence between ordered rooted trees and binary trees. If you start with an ordered rooted tree, , you can build a binary tree with an empty right subtree by placing the the root of at the root of . Then for every vertex from that has been placed in , place it's leftmost child (if there is one) as 's left child in . Make 's next sibling (if there is one) in the right child in .  An ordered rooted tree with root .   An ordered rooted tree with root specifed by the dictionary of children {r:[a,d,c],a:[b,c],e:[f,g,h]}.      Blue (left) and Red (right) links added to the ordered rooted tree with root r.   An ordered rooted tree with root r specifed by the dictionary of children {r:[a,d,c],a:[b,c],e:[f,g,h]} with colored edges added to indicate the correspondence with a binary tree.    Binary tree corresponding to the ordered rooted tree.   The binary tree corresponding with the ordered rooted tree rooted at r specified by the dictionary of children {r:[a,d,c],a:[b,c],e:[f,g,h]}.      Why will have no right children in this correspondence?  Draw the binary tree that is produced by the ordered rooted tree in .  The left subtree of the binary tree in is one of 5 different binary trees with three vertices. Draw each of them and also the ordered rooted tree that each corresponds with.  What does this correspondence tell us about how the numbers of different binary trees and ordered rooted trees are related?    What binary tree does this correspond with?   An ordered rooted tree.    What ordered rooted tree does this correspond with?   A binary tree rooted at r with dictionary of left child- right child values {r:[a,nil],a:[b,c],b:[nil,nil],c:[nil,nil]}.       The root of is the root of the corresponding ordered rooted tree, which as no siblings.    Two columns of five graphs      Two columns of five graphs     The number of ordered rooted trees with vertices is equal to the number of binary trees with vertices,   "
},
{
  "id": "s-Operations",
  "level": "1",
  "url": "s-Operations.html",
  "type": "Section",
  "number": "11.1",
  "title": "Operations",
  "body": " Operations  Operations  One of the first mathematical skills that we all learn is how to add a pair of positive integers. A young child soon recognizes that something is wrong if a sum has two values, particularly if his or her sum is different from the teacher's. In addition, it is unlikely that a child would consider assigning a non-positive value to the sum of two positive integers. In other words, at an early age we probably know that the sum of two positive integers is unique and belongs to the set of positive integers. This is what characterizes all binary operations on a set.  What is an Operation?  Binary Operation Binary Operation. generic symbol for a binary operation Let be a nonempty set. A binary operation on is a rule that assigns to each ordered pair of elements of a unique element of . In other words, a binary operation is a function from into .  Some common binary operations Union and intersection are both binary operations on the power set of any universe. Addition and multiplication are binary operators on the natural numbers. Addition and multiplication are binary operations on the set of 2 by 2 real matrices, . Division is a binary operation on some sets of numbers, such as the positive reals. But on the integers ( ) and even on the real numbers is not defined), division is not a binary operation.   We stress that the image of each ordered pair must be in . This requirement disqualifies subtraction on the natural numbers from consideration as a binary operation, since is not a natural number. Subtraction is a binary operation on the integers.  On Notation. Despite the fact that a binary operation is a function, symbols, not letters, are used to name them. The most commonly used symbol for a binary operation is an asterisk, . We will also use a diamond, , when a second symbol is needed.    If is a binary operation on and , there are three common ways of denoting the image of the pair . They are: We are all familiar with infix form. For example, is how everyone is taught to write the sum of 2 and 3. But notice how was just described in the previous sentence! The word sum preceded 2 and 3. Orally, prefix form is quite natural to us. The prefix and postfix forms are superior to infix form in some respects. In Chapter 10, we saw that algebraic expressions with more than one operation didn't need parentheses if they were in prefix or postfix form. However, due to our familiarity with infix form, we will use it throughout most of the remainder of this book.  Some operations, such as negation of numbers and complementation of sets, are not binary, but unary operators.  Unary Operation Unary Operation. Let be a nonempty set. A unary operator on is a rule that assigns to each element of a unique element of . In other words, a unary operator is a function from into .   Properties of Operations Properties of Operations  Whenever an operation on a set is encountered, there are several properties that should immediately come to mind. To effectively make use of an operation, you should know which of these properties it has. By now, you should be familiar with most of these properties. We will list the most common ones here to refresh your memory and define them for the first time in a general setting.  First we list properties of a single binary operation.  Commutative Property Commutative Property Let be a binary operation on a set . We say that is commutative if and only if for all .  Associative Property Associative Property Let be a binary operation on a set . We say that is associative if and only if for all .  Identity Property Identity Property Let be a binary operation on a set . We say that  has an identity if and only if there exists an element, , in such that for all .  The next property presumes that has the identity property.  Inverse Property Inverse Property Let be a binary operation on a set . We say that has the inverse property if and only if for each , there exists such that . We call an inverse of .  Idempotent Property Idempotent Property Let be a binary operation on a set . We say that is idempotent if and only if for all .  Now we list properties that apply to two binary operations.  Left Distributive Property Left Distributive Property Let and be binary operations on a set . We say that is left distributive over if and only if for all .  Right Distributive Property Right Distributive Property Let and be binary operations on a set . We say that is right distributive over if and only if for all .  Distributive Property Distributive Property Let and be binary operations on a set . We say that is distributive over if and only if is both left and right distributive over .  There is one significant property of unary operations.  Involution Property Involution Property Let be a unary operation on . We say that has the involution property if for all .  Finally, a property of sets, as they relate to operations.  Closure Property Closure Property Let be a subset of and let be a binary operation on . We say that is closed under if implies that .  In other words, is closed under if by operating on elements of with , you can't get new elements that are outside of .  Some examples of closure and non-closure   The odd integers are closed under multiplication, but not under addition.  Let be a proposition over and let be the set of propositions over that imply . That is; if . Then is closed under both conjunction and disjunction.  The set of positive integers that are multiples of 5 is closed under both addition and multiplication.  It is important to realize that the properties listed above depend on both the set and the operation(s). Statements such as Multiplication is commutative. or The positive integers are closed. are meaningless on their own. Naturally, if we have established a context in which the missing set or operation is clearly implied, then they would have meaning.   Operation Tables Operation Tables  If the set on which a binary operation is defined is small, a table is often a good way of describing the operation. For example, we might want to define on by The table for is   The top row and left column of an operation table are the column and row headings, respectively. To determine , find the entry in the row labeled and the column labeled . The following operation table serves to define on .   Note that , yet . Thus, is not commutative. Commutativity is easy to identify in a table: the table must be symmetric with respect to the diagonal going from the top left to lower right.    Exercises  Determine the properties that the following operations have on the positive integers.  addition  multiplication   defined by   defined by   defined by      Commutative, and associative. Notice that zero is the identity for addition, but it is not a positive integer.  Commutative, associative, and has an identity (1)  Commutative, associative, has an identity (1), and is idempotent  Commutative, associative, and idempotent  None. Notice that , while ; and , while .    Determine the properties that the following operations have on given sets.  Intersection on the set of subsets of .   defined on the positive integers by .   defined on the integers by   defined on the positive real numbers by .  Concatenation on the set of all strings of zeros and ones.     Let be an operation on a set and . Prove that if and are both closed under , then is also closed under , but need not be.  Similarly, . Therefore, . The set of positive integers is closed under addition, and so is the set of negative integers, but . Therefore, their union, the nonzero integers, is not closed under addition.  How can you pick out the identity of an operation from its table?  Define by , the absolute value of . Which properties does have on the set of natural numbers, ?     is commutative since for all   is not associative. Take , , and , then , and .  Zero is the identity for on , since  Each element of inverts itself since .   is not idempotent, since, for , .    Which pairs of operations in Exercise 1 are distributive over one another?    "
},
{
  "id": "def-binary-operation",
  "level": "2",
  "url": "s-Operations.html#def-binary-operation",
  "type": "Definition",
  "number": "11.1.1",
  "title": "Binary Operation.",
  "body": "Binary Operation Binary Operation. generic symbol for a binary operation Let be a nonempty set. A binary operation on is a rule that assigns to each ordered pair of elements of a unique element of . In other words, a binary operation is a function from into . "
},
{
  "id": "ex-some-binary-operations",
  "level": "2",
  "url": "s-Operations.html#ex-some-binary-operations",
  "type": "Example",
  "number": "11.1.2",
  "title": "Some common binary operations.",
  "body": "Some common binary operations Union and intersection are both binary operations on the power set of any universe. Addition and multiplication are binary operators on the natural numbers. Addition and multiplication are binary operations on the set of 2 by 2 real matrices, . Division is a binary operation on some sets of numbers, such as the positive reals. But on the integers ( ) and even on the real numbers is not defined), division is not a binary operation. "
},
{
  "id": "ss-definition-of-operations-4",
  "level": "2",
  "url": "s-Operations.html#ss-definition-of-operations-4",
  "type": "Note",
  "number": "11.1.3",
  "title": "",
  "body": " We stress that the image of each ordered pair must be in . This requirement disqualifies subtraction on the natural numbers from consideration as a binary operation, since is not a natural number. Subtraction is a binary operation on the integers.  On Notation. Despite the fact that a binary operation is a function, symbols, not letters, are used to name them. The most commonly used symbol for a binary operation is an asterisk, . We will also use a diamond, , when a second symbol is needed.   "
},
{
  "id": "def-unary-operation",
  "level": "2",
  "url": "s-Operations.html#def-unary-operation",
  "type": "Definition",
  "number": "11.1.4",
  "title": "Unary Operation.",
  "body": "Unary Operation Unary Operation. Let be a nonempty set. A unary operator on is a rule that assigns to each element of a unique element of . In other words, a unary operator is a function from into . "
},
{
  "id": "def-commutative-property",
  "level": "2",
  "url": "s-Operations.html#def-commutative-property",
  "type": "Definition",
  "number": "11.1.5",
  "title": "Commutative Property.",
  "body": "Commutative Property Commutative Property Let be a binary operation on a set . We say that is commutative if and only if for all . "
},
{
  "id": "def-associative-property",
  "level": "2",
  "url": "s-Operations.html#def-associative-property",
  "type": "Definition",
  "number": "11.1.6",
  "title": "Associative Property.",
  "body": "Associative Property Associative Property Let be a binary operation on a set . We say that is associative if and only if for all . "
},
{
  "id": "def-identity-property",
  "level": "2",
  "url": "s-Operations.html#def-identity-property",
  "type": "Definition",
  "number": "11.1.7",
  "title": "Identity Property.",
  "body": "Identity Property Identity Property Let be a binary operation on a set . We say that  has an identity if and only if there exists an element, , in such that for all . "
},
{
  "id": "def-inverse-property",
  "level": "2",
  "url": "s-Operations.html#def-inverse-property",
  "type": "Definition",
  "number": "11.1.8",
  "title": "Inverse Property.",
  "body": "Inverse Property Inverse Property Let be a binary operation on a set . We say that has the inverse property if and only if for each , there exists such that . We call an inverse of . "
},
{
  "id": "def-idempotent-property",
  "level": "2",
  "url": "s-Operations.html#def-idempotent-property",
  "type": "Definition",
  "number": "11.1.9",
  "title": "Idempotent Property.",
  "body": "Idempotent Property Idempotent Property Let be a binary operation on a set . We say that is idempotent if and only if for all . "
},
{
  "id": "def-left-distributive-property",
  "level": "2",
  "url": "s-Operations.html#def-left-distributive-property",
  "type": "Definition",
  "number": "11.1.10",
  "title": "Left Distributive Property.",
  "body": "Left Distributive Property Left Distributive Property Let and be binary operations on a set . We say that is left distributive over if and only if for all . "
},
{
  "id": "def-right-distributive-property",
  "level": "2",
  "url": "s-Operations.html#def-right-distributive-property",
  "type": "Definition",
  "number": "11.1.11",
  "title": "Right Distributive Property.",
  "body": "Right Distributive Property Right Distributive Property Let and be binary operations on a set . We say that is right distributive over if and only if for all . "
},
{
  "id": "def-distributive-property",
  "level": "2",
  "url": "s-Operations.html#def-distributive-property",
  "type": "Definition",
  "number": "11.1.12",
  "title": "Distributive Property.",
  "body": "Distributive Property Distributive Property Let and be binary operations on a set . We say that is distributive over if and only if is both left and right distributive over . "
},
{
  "id": "def-involution-property",
  "level": "2",
  "url": "s-Operations.html#def-involution-property",
  "type": "Definition",
  "number": "11.1.13",
  "title": "Involution Property.",
  "body": "Involution Property Involution Property Let be a unary operation on . We say that has the involution property if for all . "
},
{
  "id": "def-closure-property",
  "level": "2",
  "url": "s-Operations.html#def-closure-property",
  "type": "Definition",
  "number": "11.1.14",
  "title": "Closure Property.",
  "body": "Closure Property Closure Property Let be a subset of and let be a binary operation on . We say that is closed under if implies that . "
},
{
  "id": "ex-closure",
  "level": "2",
  "url": "s-Operations.html#ex-closure",
  "type": "Example",
  "number": "11.1.15",
  "title": "Some examples of closure and non-closure.",
  "body": "Some examples of closure and non-closure   The odd integers are closed under multiplication, but not under addition.  Let be a proposition over and let be the set of propositions over that imply . That is; if . Then is closed under both conjunction and disjunction.  The set of positive integers that are multiples of 5 is closed under both addition and multiplication. "
},
{
  "id": "exercises-11-1-2",
  "level": "2",
  "url": "s-Operations.html#exercises-11-1-2",
  "type": "Exercise",
  "number": "11.1.4.1",
  "title": "",
  "body": "Determine the properties that the following operations have on the positive integers.  addition  multiplication   defined by   defined by   defined by      Commutative, and associative. Notice that zero is the identity for addition, but it is not a positive integer.  Commutative, associative, and has an identity (1)  Commutative, associative, has an identity (1), and is idempotent  Commutative, associative, and idempotent  None. Notice that , while ; and , while .   "
},
{
  "id": "exercises-11-1-3",
  "level": "2",
  "url": "s-Operations.html#exercises-11-1-3",
  "type": "Exercise",
  "number": "11.1.4.2",
  "title": "",
  "body": "Determine the properties that the following operations have on given sets.  Intersection on the set of subsets of .   defined on the positive integers by .   defined on the integers by   defined on the positive real numbers by .  Concatenation on the set of all strings of zeros and ones.    "
},
{
  "id": "exercises-11-1-4",
  "level": "2",
  "url": "s-Operations.html#exercises-11-1-4",
  "type": "Exercise",
  "number": "11.1.4.3",
  "title": "",
  "body": "Let be an operation on a set and . Prove that if and are both closed under , then is also closed under , but need not be.  Similarly, . Therefore, . The set of positive integers is closed under addition, and so is the set of negative integers, but . Therefore, their union, the nonzero integers, is not closed under addition. "
},
{
  "id": "exercises-11-1-5",
  "level": "2",
  "url": "s-Operations.html#exercises-11-1-5",
  "type": "Exercise",
  "number": "11.1.4.4",
  "title": "",
  "body": "How can you pick out the identity of an operation from its table? "
},
{
  "id": "exercises-11-1-6",
  "level": "2",
  "url": "s-Operations.html#exercises-11-1-6",
  "type": "Exercise",
  "number": "11.1.4.5",
  "title": "",
  "body": "Define by , the absolute value of . Which properties does have on the set of natural numbers, ?     is commutative since for all   is not associative. Take , , and , then , and .  Zero is the identity for on , since  Each element of inverts itself since .   is not idempotent, since, for , .   "
},
{
  "id": "exercises-11-1-7",
  "level": "2",
  "url": "s-Operations.html#exercises-11-1-7",
  "type": "Exercise",
  "number": "11.1.4.6",
  "title": "",
  "body": "Which pairs of operations in Exercise 1 are distributive over one another?  "
},
{
  "id": "s-algebraic-systems",
  "level": "1",
  "url": "s-algebraic-systems.html",
  "type": "Section",
  "number": "11.2",
  "title": "Algebraic Systems",
  "body": " Algebraic Systems  Algebraic Systems   An algebraic system is a mathematical system consisting of a set called the domain and one or more operations on the domain. If is the domain and are the operations, denotes the mathematical system. If the context is clear, this notation is abbreviated to .   Monoids at Two Levels  Consider the following two examples of algebraic systems.   The concatenation of and Let be the set of all finite strings of 0's and 1's including the null (or empty) string, . An algebraic system is obtained by adding the operation of concatenation. The concatenation of two strings is simply the linking of the two strings together in the order indicated. The concatenation of strings with is denoted . For example, and . Note that concatenation is an associative operation and that is the identity for concatenation.  A note on notation: There isn't a standard symbol for concatenation. We have chosen to be consistent with the notation used in Python and Sage for the concatenation.  Let be any nonempty set and let * be any operation on that is associative and has an identity in . Any such system is called a monoid . We introduce monoids briefly here, but will discuss them further in   Our second example might seem strange, but we include it to illustrate a point. The algebraic system is a special case of . Most of us are much more comfortable with than with . No doubt, the reason is that the elements in are more concrete. We know what they look like and exactly how they are combined. The description of is so vague that we don't even know what the elements are, much less how they are combined. Why would anyone want to study ? The reason is related to this question: What theorems are of interest in an algebraic system? Answering this question is one of our main objectives in this chapter. Certain properties of algebraic systems are called algebraic properties, and any theorem that says something about the algebraic properties of a system would be of interest. The ability to identify what is algebraic and what isn't is one of the skills that you should learn from this chapter.  Now, back to the question of why we study . Our answer is to illustrate the usefulness of with a theorem about .  A Monoid Theorem  If , are elements of and , then .        The power of this theorem is that it can be applied to any algebraic system that describes. Since is one such system, we can apply to any two strings that commute. For example, 01 and 0101. Although a special case of this theorem could have been proven for , it would not have been any easier to prove, and it would not have given us any insight into other special cases of .  More Concrete Monoids Consider the set of real matrices, , with the operation of matrix multiplication. In this context, can be interpreted as saying that if , then . One pair of matrices that this theorem applies to is and .  For another pair of concrete monoids, we start with a universal set - although we could be a little less specific an imaging to be any nonempty set. The power set of with intersection, and the power set of with union are both monoids. What the identities of these monoids? Are they really the same monoid? We will answer this last question in .    Levels of Abstraction Levels of Abstraction   One of the fundamental tools in mathematics is abstraction. There are three levels of abstraction that we will identify for algebraic systems: concrete, axiomatic, and universal.   The Concrete Level  Almost all of the mathematics that you have done in the past was at the concrete level. As a rule, if you can give examples of a few typical elements of the domain and describe how the operations act on them, you are describing a concrete algebraic system. Two examples of concrete systems are and . A few others are:  The integers with addition. Of course, addition isn't the only standard operation that we could include. Technically, if we were to add multiplication, we would have a different system.  The subsets of the natural numbers, with union, intersection, and complementation.  The complex numbers with addition and multiplication.     The Axiomatic Level  The next level of abstraction is the axiomatic level. At this level, the elements of the domain are not specified, but certain axioms are stated about the number of operations and their properties. The system that we called is an axiomatic system. Some combinations of axioms are so common that a name is given to any algebraic system to which they apply. Any system with the properties of is called a monoid. The study of would be called monoid theory. The assumptions that we made about , associativity and the existence of an identity, are called the monoid axioms. One of your few brushes with the axiomatic level may have been in your elementary algebra course. Many algebra texts identify the properties of the real numbers with addition and multiplication as the field axioms. As we will see in Chapter 16, Rings and Fields, the real numbers share these axioms with other concrete systems, all of which are called fields.   The Universal Level  The final level of abstraction is the universal level. There are certain concepts, called universal algebra concepts, that can be applied to the study of all algebraic systems. Although a purely universal approach to algebra would be much too abstract for our purposes, defining concepts at this level should make it easier to organize the various algebraic theories in your own mind. In this chapter, we will consider the concepts of isomorphism, subsystem, and direct product.    Groups  To illustrate the axiomatic level and the universal concepts, we will consider yet another kind of axiomatic system, the group. In Chapter 5 we noted that the simplest equation in matrix algebra that we are often called upon to solve is , where and are known square matrices and is an unknown matrix. To solve this equation, we need the associative, identity, and inverse laws. We call the systems that have these properties groups.  Group  Group a group with elements and binary operation A group consists of a nonempty set and a binary operation on satisfying the properties   is associative on : for all .  There exists an identity element, , such that for all .  For all , there exists an inverse; that is, there exists such that .    A group is usually denoted by its set's name, , or occasionally by to emphasize the operation. At the concrete level, most sets have a standard operation associated with them that will form a group. As we will see below, the integers with addition is a group. Therefore, in group theory always stands for .  Generic Symbols At the axiomatic and universal levels, there are often symbols that have a special meaning attached to them. In group theory, the letter is used to denote the identity element of whatever group is being discussed. A little later, we will prove that the inverse of a group element, , is unique and its inverse is usually denoted and is read inverse. When a concrete group is discussed, these symbols are dropped in favor of concrete symbols. These concrete symbols may or may not be similar to the generic symbols. For example, the identity element of the group of integers is 0, and the inverse of is denoted by , the additive inverse of .  The asterisk could also be considered a generic symbol since it is used to denote operations on the axiomatic level.   Some concrete groups   The integers with addition is a group. We know that addition is associative. Zero is the identity for addition: for all integers . The additive inverse of any integer is obtained by negating it. Thus the inverse of is .  The integers with multiplication is not a group. Although multiplication is associative and 1 is the identity for multiplication, not all integers have a multiplicative inverse in . For example, the multiplicative inverse of 10 is , but is not an integer.  The power set of any set with the operation of symmetric difference, , is a group. If and are sets, then . We will leave it to the reader to prove that is associative over . The identity of the group is the empty set: . Every set is its own inverse since . Note that is not a group with union or intersection.    Abelian Group Abelian Group A group is abelian if its operation is commutative.  Abel Most of the groups that we will discuss in this book will be abelian. The term abelian is used to honor the Norwegian mathematician N. Abel (1802-29), who helped develop group theory.   Norwegian Stamp honoring Abel   Norwegian Stamp honoring Abel      Exercises  Discuss the analogy between the terms generic and concrete for algebraic systems and the terms generic and trade for prescription drugs. The terms generic and trade for prescription drugs are analogous to generic and concrete algebraic systems. Generic aspirin, for example, has no name, whereas Bayer, Tylenol, Bufferin, and Anacin are all trade or specific types of aspirins. The same can be said of a generic group where is a nonempty set and is a binary operation on , When examples of typical domain elements can be given along with descriptions of how operations act on them, such as * or , then the system is concrete (has a specific name, as with the aspirin). Generic is a way to describe a general algebraic system, whereas a concrete system has a name or symbols making it distinguishable from other systems.  Discuss the connection between groups and monoids. Is every monoid a group? Is every group a monoid?  Which of the following are groups?   with concatenation (see ).   with matrix addition.   with matrix multiplication.  The positive real numbers, , with multiplication.  The nonzero real numbers, , with multiplication.   with multiplication.  The positive integers with the operation defined by .   The systems in parts b, d, e, and f are groups.  Prove that, , defined by is an associative operation on .  The following problem supplies an example of a non-abelian group. A rook matrix is a matrix that has only 0's and 1's as entries such that each row has exactly one 1 and each column has exactly one 1. The term rook matrix is derived from the fact that each rook matrix represents the placement of rooks on an chessboard such that none of the rooks can attack one another. A rook in chess can move only vertically or horizontally, but not diagonally. Let be the set of rook matrices. There are six rook matrices:   List the rook matrices. They form a group, under matrix multiplication. Write out the multiplication table. Is the group abelian?  Write out the multiplication table for . This is another group. Is it abelian?  How many rook matrices are there? How many rook matrices are there?  Elements are , and , the group is abelian. Operation table is    This group is non-abelian since, for example, and .  4! = 24, .    For each of the following sets, identify the standard operation that results in a group. What is the identity of each group?  The set of all matrices with real entries and nonzero determinants.  The set of matrices with rational entries.     Let . Let be defined (partially) by for all . Write a complete table for so that is a group.  The identity is . , , , and is abelian. (This group is commonly called the Klein-4 group.)  Consider the following set of six algebraic expressions, each defining a function on the set of real numbers excluding the numbers 0 and 1. We can operate on any two of these expressions using function composition. For example, Therefore, . Complete the following operation table for function composition on .  Partially completed operation table for   Partially completed operation table for the composition of function x, 1\/x,...     Is a monoid? Is it a group?  Yes, this is a group. You might see some similarities with the group of three by three rook matrices.   "
},
{
  "id": "ss-monoids-3",
  "level": "2",
  "url": "s-algebraic-systems.html#ss-monoids-3",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "monoid "
},
{
  "id": "theorem-first-monoid",
  "level": "2",
  "url": "s-algebraic-systems.html#theorem-first-monoid",
  "type": "Theorem",
  "number": "11.2.1",
  "title": "A Monoid Theorem.",
  "body": "A Monoid Theorem  If , are elements of and , then .       "
},
{
  "id": "ex-another-monoid",
  "level": "2",
  "url": "s-algebraic-systems.html#ex-another-monoid",
  "type": "Example",
  "number": "11.2.2",
  "title": "More Concrete Monoids.",
  "body": "More Concrete Monoids Consider the set of real matrices, , with the operation of matrix multiplication. In this context, can be interpreted as saying that if , then . One pair of matrices that this theorem applies to is and .  For another pair of concrete monoids, we start with a universal set - although we could be a little less specific an imaging to be any nonempty set. The power set of with intersection, and the power set of with union are both monoids. What the identities of these monoids? Are they really the same monoid? We will answer this last question in .  "
},
{
  "id": "def-Group",
  "level": "2",
  "url": "s-algebraic-systems.html#def-Group",
  "type": "Definition",
  "number": "11.2.3",
  "title": "Group.",
  "body": "Group  Group a group with elements and binary operation A group consists of a nonempty set and a binary operation on satisfying the properties   is associative on : for all .  There exists an identity element, , such that for all .  For all , there exists an inverse; that is, there exists such that .   "
},
{
  "id": "ss-groups-5",
  "level": "2",
  "url": "s-algebraic-systems.html#ss-groups-5",
  "type": "Note",
  "number": "11.2.4",
  "title": "Generic Symbols.",
  "body": "Generic Symbols At the axiomatic and universal levels, there are often symbols that have a special meaning attached to them. In group theory, the letter is used to denote the identity element of whatever group is being discussed. A little later, we will prove that the inverse of a group element, , is unique and its inverse is usually denoted and is read inverse. When a concrete group is discussed, these symbols are dropped in favor of concrete symbols. These concrete symbols may or may not be similar to the generic symbols. For example, the identity element of the group of integers is 0, and the inverse of is denoted by , the additive inverse of .  The asterisk could also be considered a generic symbol since it is used to denote operations on the axiomatic level.  "
},
{
  "id": "ex-some-groups",
  "level": "2",
  "url": "s-algebraic-systems.html#ex-some-groups",
  "type": "Example",
  "number": "11.2.5",
  "title": "Some concrete groups.",
  "body": "Some concrete groups   The integers with addition is a group. We know that addition is associative. Zero is the identity for addition: for all integers . The additive inverse of any integer is obtained by negating it. Thus the inverse of is .  The integers with multiplication is not a group. Although multiplication is associative and 1 is the identity for multiplication, not all integers have a multiplicative inverse in . For example, the multiplicative inverse of 10 is , but is not an integer.  The power set of any set with the operation of symmetric difference, , is a group. If and are sets, then . We will leave it to the reader to prove that is associative over . The identity of the group is the empty set: . Every set is its own inverse since . Note that is not a group with union or intersection.   "
},
{
  "id": "def-abelian-group",
  "level": "2",
  "url": "s-algebraic-systems.html#def-abelian-group",
  "type": "Definition",
  "number": "11.2.6",
  "title": "Abelian Group.",
  "body": "Abelian Group Abelian Group A group is abelian if its operation is commutative. "
},
{
  "id": "fig-stamp-abel",
  "level": "2",
  "url": "s-algebraic-systems.html#fig-stamp-abel",
  "type": "Figure",
  "number": "11.2.7",
  "title": "",
  "body": " Norwegian Stamp honoring Abel   Norwegian Stamp honoring Abel   "
},
{
  "id": "exercises-11-2-2",
  "level": "2",
  "url": "s-algebraic-systems.html#exercises-11-2-2",
  "type": "Exercise",
  "number": "11.2.4.1",
  "title": "",
  "body": "Discuss the analogy between the terms generic and concrete for algebraic systems and the terms generic and trade for prescription drugs. The terms generic and trade for prescription drugs are analogous to generic and concrete algebraic systems. Generic aspirin, for example, has no name, whereas Bayer, Tylenol, Bufferin, and Anacin are all trade or specific types of aspirins. The same can be said of a generic group where is a nonempty set and is a binary operation on , When examples of typical domain elements can be given along with descriptions of how operations act on them, such as * or , then the system is concrete (has a specific name, as with the aspirin). Generic is a way to describe a general algebraic system, whereas a concrete system has a name or symbols making it distinguishable from other systems. "
},
{
  "id": "exercises-11-2-3",
  "level": "2",
  "url": "s-algebraic-systems.html#exercises-11-2-3",
  "type": "Exercise",
  "number": "11.2.4.2",
  "title": "",
  "body": "Discuss the connection between groups and monoids. Is every monoid a group? Is every group a monoid? "
},
{
  "id": "exercises-11-2-4",
  "level": "2",
  "url": "s-algebraic-systems.html#exercises-11-2-4",
  "type": "Exercise",
  "number": "11.2.4.3",
  "title": "",
  "body": "Which of the following are groups?   with concatenation (see ).   with matrix addition.   with matrix multiplication.  The positive real numbers, , with multiplication.  The nonzero real numbers, , with multiplication.   with multiplication.  The positive integers with the operation defined by .   The systems in parts b, d, e, and f are groups. "
},
{
  "id": "exercises-11-2-5",
  "level": "2",
  "url": "s-algebraic-systems.html#exercises-11-2-5",
  "type": "Exercise",
  "number": "11.2.4.4",
  "title": "",
  "body": "Prove that, , defined by is an associative operation on . "
},
{
  "id": "ex-rook-matrices",
  "level": "2",
  "url": "s-algebraic-systems.html#ex-rook-matrices",
  "type": "Exercise",
  "number": "11.2.4.5",
  "title": "",
  "body": "The following problem supplies an example of a non-abelian group. A rook matrix is a matrix that has only 0's and 1's as entries such that each row has exactly one 1 and each column has exactly one 1. The term rook matrix is derived from the fact that each rook matrix represents the placement of rooks on an chessboard such that none of the rooks can attack one another. A rook in chess can move only vertically or horizontally, but not diagonally. Let be the set of rook matrices. There are six rook matrices:   List the rook matrices. They form a group, under matrix multiplication. Write out the multiplication table. Is the group abelian?  Write out the multiplication table for . This is another group. Is it abelian?  How many rook matrices are there? How many rook matrices are there?  Elements are , and , the group is abelian. Operation table is    This group is non-abelian since, for example, and .  4! = 24, .   "
},
{
  "id": "exercises-11-2-7",
  "level": "2",
  "url": "s-algebraic-systems.html#exercises-11-2-7",
  "type": "Exercise",
  "number": "11.2.4.6",
  "title": "",
  "body": "For each of the following sets, identify the standard operation that results in a group. What is the identity of each group?  The set of all matrices with real entries and nonzero determinants.  The set of matrices with rational entries.    "
},
{
  "id": "exercises-11-2-8",
  "level": "2",
  "url": "s-algebraic-systems.html#exercises-11-2-8",
  "type": "Exercise",
  "number": "11.2.4.7",
  "title": "",
  "body": "Let . Let be defined (partially) by for all . Write a complete table for so that is a group.  The identity is . , , , and is abelian. (This group is commonly called the Klein-4 group.) "
},
{
  "id": "exercises-11-2-9",
  "level": "2",
  "url": "s-algebraic-systems.html#exercises-11-2-9",
  "type": "Exercise",
  "number": "11.2.4.8",
  "title": "",
  "body": "Consider the following set of six algebraic expressions, each defining a function on the set of real numbers excluding the numbers 0 and 1. We can operate on any two of these expressions using function composition. For example, Therefore, . Complete the following operation table for function composition on .  Partially completed operation table for   Partially completed operation table for the composition of function x, 1\/x,...     Is a monoid? Is it a group?  Yes, this is a group. You might see some similarities with the group of three by three rook matrices. "
},
{
  "id": "s-general-properties-of-groups",
  "level": "1",
  "url": "s-general-properties-of-groups.html",
  "type": "Section",
  "number": "11.3",
  "title": "Some General Properties of Groups",
  "body": " Some General Properties of Groups  Some General Properties of Groups  In this section, we will present some of the most basic theorems of group theory. Keep in mind that each of these theorems tells us something about every group. We will illustrate this point with concrete examples at the close of the section.   First Theorems  Identities are Unique The identity of a group is unique.  One difficulty that students often encounter is how to get started in proving a theorem like this. The difficulty is certainly not in the theorem's complexity. It's too terse! Before actually starting the proof, we rephrase the theorem so that the implication it states is clear.  Identities are Unique - Rephrased  If is a group and is an identity of , then no other element of is an identity of .  (Indirect): Suppose that , , and is an identity of . We will show that , which is a contradiction, completing the proof.   Next we justify the phrase ... the inverse of an element of a group.  Inverses are Unique  The inverse of any element of a group is unique.  The same problem is encountered here as in the previous theorem. We will leave it to the reader to rephrase this theorem. The proof is also left to the reader to write out in detail. Here is a hint: If and are both inverses of , then you can prove that . If you have difficulty with this proof, note that we have already proven it in a concrete setting in Chapter 5.  As mentioned above, the significance of is that we can refer to the inverse of an element without ambiguity. The notation for the inverse of is usually (note the exception below).  Some Inverses  In any group, is the inverse of the identity , which always is .   is the inverse of , which is always equal to (see below).   is the inverse of .  In a concrete group with an operation that is based on addition, the inverse of is usually written . For example, the inverse of in the group is written . In the group of matrices over the real numbers under matrix addition, the inverse of is written , which equals .    Inverse of Inverse Theorem  If a is an element of group , then .  Again, we rephrase the theorem to make it clear how to proceed.  Inverse of Inverse Theorem (Rephrased)  If has inverse and has inverse , then .       The next theorem gives us a formula for the inverse of . This formula should be familiar. In Chapter 5 we saw that if and are invertible matrices, then .  Inverse of a Product  If and are elements of group , then .  Let . We will prove that inverts . Since we know that the inverse is unique, we will have proved the theorem. Similarly, ; therefore,    Cancellation Laws  Cancellation in Groups  If , , and are elements of group , then   We will prove the left cancellation law. The right law can be proved in exactly the same way. Starting with , we can operate on both and on the left with : Applying the associative property to both sides we get    Linear Equations in a Group  Linear Equations in a Group  If is a group and , the equation has a unique solution, . In addition, the equation has a unique solution, .  We prove the theorem only for , since the second statement is proven identically. By the cancellation law, we can conclude that .  If and are two solutions of the equation , then and, by the cancellation law, . This verifies that is the only solution of .   Our proof of was analogous to solving the concrete equation in the following way: Therefore, by cancelling 4,   Exponents  If is an element of a group , then we establish the notation that In addition, we allow negative exponents and define, for example, Although this should be clear, proving exponentiation properties requires a more precise recursive definition.  Exponentiation in Groups  Exponentiation in Groups  For , define recursively by and if . Also, if , .  Some concrete exponentiations   In the group of positive real numbers with multiplication, and  In a group with addition, we use a different form of notation, reflecting the fact that in addition repeated terms are multiples, not powers. For example, in , is written as , is written as , etc. The inverse of a multiple of such as is written as .    Although we define, for example, , we need to be able to extract the single factor on the left. The following lemma justifies doing precisely that.   Let be a group. If and , then , and hence .  (By induction): If ,   Now assume the formula of the lemma is true for some .    Based on the definitions for exponentiation above, there are several properties that can be proven. They are all identical to the exponentiation properties from elementary algebra.  Properties of Exponentiation  If a is an element of a group , and and are integers,   and hence        We will leave the proofs of these properties to the reader. All three parts can be done by induction. For example the proof of the second part would start by defining the proposition , , to be . The basis is .    Our final theorem is the only one that contains a hypothesis about the group in question. The theorem only applies to finite groups.   If is a finite group, , and is an element of , then there exists a positive integer such that and .  Consider the list . Since there are elements of in this list, there must be some duplication. Suppose that , with . Let . Then Furthermore, since , .  Consider the concrete group . All of the theorems that we have stated in this section except for the last one say something about . Among the facts that we conclude from the theorems about are:  Since the inverse of 5 is , the inverse of is 5.  The inverse of is .  The solution of is .  .  (twenty-eight 3s).      Exercises  Let be a group and be an element of . Define by .  Prove that is a bijection.  On the basis of part a, describe a set of bijections on the set of integers.     is injective: .  is surjective: For all , has the solution .  Functions of the form , where is any integer, are bijections    Rephrase and write out a clear proof.  Prove by induction on that if , are elements of a group , , then . Interpret this result in terms of and .  Basis: ( ) by .  Induction: Assume that for some , We must show that  This can be accomplished as follows:   True or false? If , , are elements of a group , and , then . Explain your answer.  Prove . In this answer, we will refer to simply as the lemma.   Let be , where is any element of group . First we will prove that is true for all .  Basis: If , Using the definition of the zero exponent, , while . Therefore, is true.  Induction: Assume that for some , ) is true.   If is negative, then is positive and   For , let be for all . The basis for this proof follows directly from the basis for the definition of exponentiation.  Induction: Assume that for some , is true. Then To complete the proof, you need to consider the cases where and\/or are negative.  Let be for all integers .  Basis: and therefore, is true.  Induction; Assume that is true for some 0, Finally, if is negative, we can verify that using many of the same steps as the positive case.      Each of the following facts can be derived by identifying a certain group and then applying one of the theorems of this section to it. For each fact, list the group and the theorem that are used.  is the only solution of .   .  If are matrices over the real numbers, with , then .  There is only one subset of the natural numbers for which for every subset of the natural numbers.      "
},
{
  "id": "theorem-11-3-1",
  "level": "2",
  "url": "s-general-properties-of-groups.html#theorem-11-3-1",
  "type": "Theorem",
  "number": "11.3.1",
  "title": "Identities are Unique.",
  "body": "Identities are Unique The identity of a group is unique. "
},
{
  "id": "theorem-11-3-1-a",
  "level": "2",
  "url": "s-general-properties-of-groups.html#theorem-11-3-1-a",
  "type": "Theorem",
  "number": "11.3.2",
  "title": "Identities are Unique - Rephrased.",
  "body": "Identities are Unique - Rephrased  If is a group and is an identity of , then no other element of is an identity of .  (Indirect): Suppose that , , and is an identity of . We will show that , which is a contradiction, completing the proof.  "
},
{
  "id": "theorem-11-3-2",
  "level": "2",
  "url": "s-general-properties-of-groups.html#theorem-11-3-2",
  "type": "Theorem",
  "number": "11.3.3",
  "title": "Inverses are Unique.",
  "body": "Inverses are Unique  The inverse of any element of a group is unique. "
},
{
  "id": "ex-some-inverses",
  "level": "2",
  "url": "s-general-properties-of-groups.html#ex-some-inverses",
  "type": "Example",
  "number": "11.3.4",
  "title": "Some Inverses.",
  "body": "Some Inverses  In any group, is the inverse of the identity , which always is .   is the inverse of , which is always equal to (see below).   is the inverse of .  In a concrete group with an operation that is based on addition, the inverse of is usually written . For example, the inverse of in the group is written . In the group of matrices over the real numbers under matrix addition, the inverse of is written , which equals .   "
},
{
  "id": "theorem-11-3-3",
  "level": "2",
  "url": "s-general-properties-of-groups.html#theorem-11-3-3",
  "type": "Theorem",
  "number": "11.3.5",
  "title": "Inverse of Inverse Theorem.",
  "body": "Inverse of Inverse Theorem  If a is an element of group , then . "
},
{
  "id": "theorem-11-3-3-rephrased",
  "level": "2",
  "url": "s-general-properties-of-groups.html#theorem-11-3-3-rephrased",
  "type": "Theorem",
  "number": "11.3.6",
  "title": "Inverse of Inverse Theorem (Rephrased).",
  "body": "Inverse of Inverse Theorem (Rephrased)  If has inverse and has inverse , then .      "
},
{
  "id": "theorem-11-3-4",
  "level": "2",
  "url": "s-general-properties-of-groups.html#theorem-11-3-4",
  "type": "Theorem",
  "number": "11.3.7",
  "title": "Inverse of a Product.",
  "body": "Inverse of a Product  If and are elements of group , then .  Let . We will prove that inverts . Since we know that the inverse is unique, we will have proved the theorem. Similarly, ; therefore,   "
},
{
  "id": "theorem-11-3-cancellation",
  "level": "2",
  "url": "s-general-properties-of-groups.html#theorem-11-3-cancellation",
  "type": "Theorem",
  "number": "11.3.8",
  "title": "Cancellation Laws.",
  "body": "Cancellation Laws  Cancellation in Groups  If , , and are elements of group , then   We will prove the left cancellation law. The right law can be proved in exactly the same way. Starting with , we can operate on both and on the left with : Applying the associative property to both sides we get   "
},
{
  "id": "theorem-11-3-linear-in-groups",
  "level": "2",
  "url": "s-general-properties-of-groups.html#theorem-11-3-linear-in-groups",
  "type": "Theorem",
  "number": "11.3.9",
  "title": "Linear Equations in a Group.",
  "body": "Linear Equations in a Group  Linear Equations in a Group  If is a group and , the equation has a unique solution, . In addition, the equation has a unique solution, .  We prove the theorem only for , since the second statement is proven identically. By the cancellation law, we can conclude that .  If and are two solutions of the equation , then and, by the cancellation law, . This verifies that is the only solution of .  "
},
{
  "id": "s-general-properties-of-groups-4-17",
  "level": "2",
  "url": "s-general-properties-of-groups.html#s-general-properties-of-groups-4-17",
  "type": "Note",
  "number": "11.3.10",
  "title": "",
  "body": "Our proof of was analogous to solving the concrete equation in the following way: Therefore, by cancelling 4, "
},
{
  "id": "def-exponents-in-groups",
  "level": "2",
  "url": "s-general-properties-of-groups.html#def-exponents-in-groups",
  "type": "Definition",
  "number": "11.3.11",
  "title": "Exponentiation in Groups.",
  "body": "Exponentiation in Groups  Exponentiation in Groups  For , define recursively by and if . Also, if , . "
},
{
  "id": "ex-concrete-exponents",
  "level": "2",
  "url": "s-general-properties-of-groups.html#ex-concrete-exponents",
  "type": "Example",
  "number": "11.3.12",
  "title": "Some concrete exponentiations.",
  "body": "Some concrete exponentiations   In the group of positive real numbers with multiplication, and  In a group with addition, we use a different form of notation, reflecting the fact that in addition repeated terms are multiples, not powers. For example, in , is written as , is written as , etc. The inverse of a multiple of such as is written as .   "
},
{
  "id": "lemma-11-3-1",
  "level": "2",
  "url": "s-general-properties-of-groups.html#lemma-11-3-1",
  "type": "Lemma",
  "number": "11.3.13",
  "title": "",
  "body": " Let be a group. If and , then , and hence .  (By induction): If ,   Now assume the formula of the lemma is true for some .   "
},
{
  "id": "theorem-11-3-expo-properties",
  "level": "2",
  "url": "s-general-properties-of-groups.html#theorem-11-3-expo-properties",
  "type": "Theorem",
  "number": "11.3.14",
  "title": "Properties of Exponentiation.",
  "body": "Properties of Exponentiation  If a is an element of a group , and and are integers,   and hence        We will leave the proofs of these properties to the reader. All three parts can be done by induction. For example the proof of the second part would start by defining the proposition , , to be . The basis is .   "
},
{
  "id": "theorem-11-3-finite",
  "level": "2",
  "url": "s-general-properties-of-groups.html#theorem-11-3-finite",
  "type": "Theorem",
  "number": "11.3.15",
  "title": "",
  "body": " If is a finite group, , and is an element of , then there exists a positive integer such that and .  Consider the list . Since there are elements of in this list, there must be some duplication. Suppose that , with . Let . Then Furthermore, since , . "
},
{
  "id": "exercises-11-3-2",
  "level": "2",
  "url": "s-general-properties-of-groups.html#exercises-11-3-2",
  "type": "Exercise",
  "number": "11.3.3.1",
  "title": "",
  "body": "Let be a group and be an element of . Define by .  Prove that is a bijection.  On the basis of part a, describe a set of bijections on the set of integers.     is injective: .  is surjective: For all , has the solution .  Functions of the form , where is any integer, are bijections   "
},
{
  "id": "exercises-11-3-3",
  "level": "2",
  "url": "s-general-properties-of-groups.html#exercises-11-3-3",
  "type": "Exercise",
  "number": "11.3.3.2",
  "title": "",
  "body": "Rephrase and write out a clear proof. "
},
{
  "id": "exercises-11-3-4",
  "level": "2",
  "url": "s-general-properties-of-groups.html#exercises-11-3-4",
  "type": "Exercise",
  "number": "11.3.3.3",
  "title": "",
  "body": "Prove by induction on that if , are elements of a group , , then . Interpret this result in terms of and .  Basis: ( ) by .  Induction: Assume that for some , We must show that  This can be accomplished as follows:  "
},
{
  "id": "exercises-11-3-5",
  "level": "2",
  "url": "s-general-properties-of-groups.html#exercises-11-3-5",
  "type": "Exercise",
  "number": "11.3.3.4",
  "title": "",
  "body": "True or false? If , , are elements of a group , and , then . Explain your answer. "
},
{
  "id": "exercises-11-3-6",
  "level": "2",
  "url": "s-general-properties-of-groups.html#exercises-11-3-6",
  "type": "Exercise",
  "number": "11.3.3.5",
  "title": "",
  "body": "Prove . In this answer, we will refer to simply as the lemma.   Let be , where is any element of group . First we will prove that is true for all .  Basis: If , Using the definition of the zero exponent, , while . Therefore, is true.  Induction: Assume that for some , ) is true.   If is negative, then is positive and   For , let be for all . The basis for this proof follows directly from the basis for the definition of exponentiation.  Induction: Assume that for some , is true. Then To complete the proof, you need to consider the cases where and\/or are negative.  Let be for all integers .  Basis: and therefore, is true.  Induction; Assume that is true for some 0, Finally, if is negative, we can verify that using many of the same steps as the positive case.    "
},
{
  "id": "exercises-11-3-7",
  "level": "2",
  "url": "s-general-properties-of-groups.html#exercises-11-3-7",
  "type": "Exercise",
  "number": "11.3.3.6",
  "title": "",
  "body": " Each of the following facts can be derived by identifying a certain group and then applying one of the theorems of this section to it. For each fact, list the group and the theorem that are used.  is the only solution of .   .  If are matrices over the real numbers, with , then .  There is only one subset of the natural numbers for which for every subset of the natural numbers.    "
},
{
  "id": "s-gcds-and-zsubn",
  "level": "1",
  "url": "s-gcds-and-zsubn.html",
  "type": "Section",
  "number": "11.4",
  "title": "Greatest Common Divisors  and the Integers Modulo <span xmlns:pf=\"https:\/\/prefigure.org\" class=\"process-math\">\\(n\\)<\/span>",
  "body": " Greatest Common Divisors and the Integers Modulo  In this section introduce the greatest common divisor operation, and introduce an important family of concrete groups, the integers modulo .  Greatest Common Divisors  We start with a theorem about integer division that is intuitively clear. We leave the proof as an exercise.  The Division Property for Integers  Division Property for Integers  If , , then there exist two unique integers, (the quotient) and (the remainder), such that and .  The division property says that if is divided by , you will obtain a quotient and a remainder, where the remainder is less than . This is a fact that most elementary school students learn when they are introduced to long division. In doing the division problem , you obtain a quotient of 20 and a remainder of 46. This result could either be written or . The latter form is how the division property is normally expressed in higher mathematics.   We now remind the reader of some interchangeable terminology that is used when , i. e., . All of the following say the same thing, just from slightly different points of view.   divides divides  multiple is a multiple of  factor is a factor of  divisor is a divisor of   We use the notation if divides .   For example and , but .  Caution: Don't confuse the divides symbol with the divided by symbol. The former is vertical while the latter is slanted. Notice however that the statement is related to the fact that is a whole number.  Greatest Common Divisor  Greatest Common Divisor ( )  the greatest common divisor of and Given two integers, and , not both zero, the greatest common divisor of and is the positive integer such that , , and   A little simpler way to think of is as the largest positive integer that is a divisor of both and . However, our definition is easier to apply in proving properties of greatest common divisors.  For small numbers, a simple way to determine the greatest common divisor is to use factorization. For example if we want the greatest common divisor of 660 and 350, you can factor the two integers: and . Single factors of 2 and 5 are the only ones that appear in both factorizations, so the greatest common divisor is .  Some pairs of integers have no common divisors other than 1. Such pairs are called relatively prime pairs .  Relatively Prime  Relatively Prime  A pair of integers, and , are relatively prime if  For example, and are relatively prime. Notice that neither 128 nor 135 are primes. In general, and need not be prime in order to be relatively prime. However, if you start with a prime, like 23, for example, it will be relatively prime to everything but its multiples. This theorem, which we prove later, generalizes this observation.   If is a prime and is any integer such that then   The Euclidean Algorithm Euclidean Algorithm, The  As early as Euclid's time it was known that factorization wasn't the best way to compute greatest common divisors.  The Euclidean Algorithm is based on the following properties of the greatest common divisor.  To compute , we divide into and get a remainder such that . By the property above, . We repeat the process until we get zero for a remainder. The last nonzero number that is the second entry in our pairs is the greatest common divisor. This is inevitable because the second number in each pair is smaller than the previous one. shows an example of how this calculation can be systematically performed.   A Table to Compute    - 99 53  1 53 46  1 46 7  6 7 4  1 4 3  1 3 1  3 1 0    Here is a Sage computation to verify that . At each line, the value of is divided by the value of . The quotient is placed on the next line along with the new value of , which is the previous ; and the remainder, which is the new value of . Recall that in Sage, a%b is the remainder when dividing b into a .   If you were allowed to pick two integers less than 100, which would you pick in order to force Euclid to work hardest? Here's a hint: The size of the quotient at each step determines how quickly the numbers decrease.  If quotient in division is 1, then we get the slowest possible completion. If , then working backwards, each remainder would be the sum of the two previous remainders. This described a sequence like the Fibonacci sequence and indeed, the greatest common divisor of two consecutive Fibonacci numbers will take the most steps to reach a final value of 1.   For fixed values of and , consider integers of the form where and can be any two integers. For example if = 36 and = 27, some of these results are tabulated below with values along the left column and the values on top.   Linear combinations of 36 and 27   Linear combinations of 36 and 27    Do you notice any patterns? What is the smallest positive value that you see in this table? How is it connected to 36 and 27?  Bézout's lemma Bézout's lemma  If and are positive integers, the smallest positive value of is the greatest common divisor of and , .  If , since and , we know that for any integers and , so can't be less than . To show that is exactly the least positive value, we show that can be attained by extending the Euclidean Algorithm. Performing the extended algorithm involves building a table of numbers. The way in which it is built maintains an invariant, and by , we can be sure that the desired values of and are produced.   To illustrate the algorithm, displays how to compute . In the column, you will find 152 and 53, and then the successive remainders from division. So each number in after the first two is the remainder after dividing the number immediately above it into the next number up. To the left of each remainder is the quotient from the division. In this case the third row of the table tells us that . The last nonzero value in is the greatest common divisor.   The extended Euclidean algorithm to compute    152 1 0  53 0 1  2 46 1  1 7 3  6 4 7  1 3 23  1 1 15  3 0 152    The and columns are new. The values of and in each row are maintained so that is equal to the number in the column. Notice that   Invariant in computing           The next-to-last equation is what we're looking for in the end! The main problem is to identify how to determine these values after the first two rows. The first two rows in these columns will always be the same. Let's look at the general case of computing . If the and values in rows and are correct, we have In addition, we know that If you substitute the expressions for and from (A) into this last equation and then collect the and terms separately you get or Look closely at the equations for . Their forms are all the same. With a little bit of practice you should be able to compute and values quickly.   Modular Arithmetic Modular Arithmetic  We remind you of the relation on the integers that we call . If two integers, and , differ by a multiple of , we say that they are congruent modulo , denoted . For example, because , which is a multiple of 5.  The Integers Modulo  The Integers Modulo  the integers modulo  If is a positive integer greater than one, we define the integers modulo to be the set .   Modular Addition  Modular Addition  the mod sum of and  If is a positive integer, we define addition modulo  ) as follows. If ,   Modular Multiplication  Modular Multiplication  the mod product of and  If is a positive integer, we define multiplication modulo  ) as follows. If ,    The result of doing arithmetic modulo is always an integer between 0 and , by the Division Property. This observation implies that is closed under modulo arithmetic.  It is always true that and . For example, and .  One interpretation of is that each element is a representative of its equivalence class with respect to congruence modulo . For example, if , the number in really represents all numbers in . In doing modular arithmetic, we can temporarily replace elements of with other elements in their equivalence class modulo .     Some Examples  We are all somewhat familiar with since the hours of the day are counted using this group, except for the fact that 12 is used in place of 0. Military time uses the mod 24 system and does begin at 0. If someone started a four-hour trip at hour 21, the time at which she would arrive is . If a satellite orbits the earth every four hours and starts its first orbit at hour 5, it would end its first orbit at time . Its tenth orbit would end at hours on the clock  Virtually all computers represent unsigned integers in binary form with a fixed number of digits. A very small computer might reserve seven bits to store the value of an integer. There are only different values that can be stored in seven bits. Since the smallest value is 0, represented as 0000000, the maximum value will be , represented as 1111111. When a command is given to add two integer values, and the two values have a sum of 128 or more, overflow occurs. For example, if we try to add 56 and 95, the sum is an eight-digit binary integer 10010111. One common procedure is to retain the seven lowest-ordered digits. The result of adding 56 and 95 would be . Integer arithmetic with this computer would actually be modulo 128 arithmetic.     Properties of Modular Arithmetic Modular Arithmetic Properties  Additive Inverses in  If , , then the additive inverse of a is . , since . Therefore, .  Addition modulo is always commutative and associative; 0 is the identity for and every element of has an additive inverse. These properties can be summarized by noting that for each , is a group.   The Additive Group of Integers Modulo  Integers Modulo Additive Group  The Additive Group of Integer Modulo   The Additive Group of Integers Modulo is the group with domain and with the operation of mod addition. It is denoted as .    Multiplication modulo is always commutative and associative, and 1 is the identity for .  Notice that the algebraic properties of and on are identical to the properties of addition and multiplication on .  Notice that a group cannot be formed from the whole set with mod multiplication since zero never has a multiplicative inverse. Depending on the value of there may be other numbers that will be excluded.   The Multiplicative Group of Integers Modulo  Integers Modulo Multiplicative Group  The Multiplicative Group of Integer Modulo   The Multiplicative Group of Integers Modulo is the group with domain and with the operation of mod multiplication. It is denoted as .    Some operation tables     Here are examples of operation tables for modular groups. Notice that although 8 is greater than 5, the two groups and both have order 4. In the case of , since 5 is prime all of the nonzero elements of are included. Since 8 isn't prime we don't include integers that share a common factor with 8, the even integers in this case.   Operation Table for the group            0  0  1  2  3  4    1  1  2  3  4  0    2  2  3  4  0  1    3  3  4  0  1  2    4  4  0  1  2  3        Operation table for the group           1  1  2  3  4    2  2  4  1  3    3  3  1  4  2    4  4  3  2  1      Operation table for the group           1  1  3  5  7    3  3  1  7  5    5  5  7  1  3    7  7  5  3  1       Computing Modular Multiplicative Inverses. Unlike the nice neat formula for additive inverses mod , multiplicative inverses can most easily computed by applying . If is an element of the group , then by definition , and so there exist integers and such that . They can be computed with the Extended Euclidean Algorithm. Since might not be in you might need take the remainder after dividing it by . Normally, that involves simply adding to .  For example, in , if we want the muliplicative inverse of , we run the Extended Euclidean Algorithm and find that Thus, the multiplicative inverse of 1001 is . See the SageMath Note below to see how to run the Extended Euclidean Algorithm.   SageMath Note - Modular Arithmetic SageMath Note Modular Arithmetic  Sage inherits the basic integer division functions from Python that compute a quotient and remainder in integer division. For example, here is how to divide 561 into 2017 and get the quotient and remainder.   In Sage, is the greatest common divisor function. It can be used in two ways. For the gcd of 2343 and 4319 we can evaluate the expression . If we are working with a fixed modulus that has a value established in your Sage session, the expression to compute the greatest common divisor of and any integer value . The extended Euclidean algorithm can also be called upon with :   Sage has some extremely powerful tool for working with groups. The integers modulo are represented by the expression and the addition and multiplications tables can be generated as follows.   Once we have assigned a value of , we can do calculations by wrapping around the integers 0 through 5. Here is a list containing the mod 6 sum and product, respectively, of 5 and 4:   Generating the multiplication table for the family of groups takes a bit more code. Here we restrict the allowed inputs to be integers from 2 to 64.     Exercises  Determine the greatest common divisors of the following pairs of integers without using any computational assistance.   and   and   and  12112 and 0            12112     Find all possible values of the following, assuming that is a positive integer.          Calculate:                       2  5  0  0  2  2  1  3  0    List the additive inverses of the following elements:  4, 6, 9 in  16, 25, 40 in    In the group , what are:  3(4)?  36(4)?  How could you efficiently compute , ?    1  1  , where ,    When defining notice that we could define this operation on all integers, but we restricted the set to to satisfy the properties of a group. Can you explain what goes wrong if we define on all of the integers?  The first thing that goes wrong is that will not be an identity for addition modulo on the whole set of integers. Notice that .   A student is asked to solve the following equations under the requirement that all arithmetic should be done in . List all solutions.   .   .   Since the solutions, if they exist, must come from , substitution is the easiest approach.  1 is the only solution, since and  No solutions, since , and    Determine the solutions of the same equations as in Exercise 5 in .    Write out the operation table for on , and convince your self that this is a group.  Let be the elements of that have inverses with respect to . Convince yourself that is a group under .  Prove that the elements of are those elements such that . You may use in this proof.     Prove the division property, .  Prove by induction on that you can divide any positive integer into . That is, let be For all greater than zero, there exist unique integers and such that . In the induction step, divide into .   Suppose such where and are integer constants. Furthermore, assume that and . Find a formula for and also find a formula for the inverse of . The given conditions can be converted to a system of linear equations:  If we subtract the first equation from the second, we get . This implies that , and . To get a formula for the inverse of we solve for , using the fact that the multiplicative inverse of 10 (mod 17) is 12. Therefore .   Write out the operation table for mod 10 multiplication on . Is a monoid? Is it a group?  This system is a monoid with identity 6 (surprise!). However it is not a group since 0 has no inverse.  Given that , explain why is an element of the group and determine its inverse in that group.  By , is an element of . It's inverse in the group is because   Let . Solve for in the group   Let be an odd prime. Find all solutions to the equation in the group .   It was observed above that in doing modular arithmetic, one can replace an element of with any other element of its equivalence class modulo . For example, if one is computing , the alternative to multiplying time and then dividing by to get the remainder in , we can replace with and get a product of which is conguent of . Use this trick to compute the following without the use of a calculator.        The solution to      We associate the set with the addition modulo , , because the pair form a group. Why must we use a matching modulus? Explain why by considering the following two examples.        "
},
{
  "id": "th-division-property",
  "level": "2",
  "url": "s-gcds-and-zsubn.html#th-division-property",
  "type": "Theorem",
  "number": "11.4.1",
  "title": "The Division Property for Integers.",
  "body": "The Division Property for Integers  Division Property for Integers  If , , then there exist two unique integers, (the quotient) and (the remainder), such that and . "
},
{
  "id": "ss-greatest-common-divisors-4",
  "level": "2",
  "url": "s-gcds-and-zsubn.html#ss-greatest-common-divisors-4",
  "type": "Note",
  "number": "11.4.2",
  "title": "",
  "body": "The division property says that if is divided by , you will obtain a quotient and a remainder, where the remainder is less than . This is a fact that most elementary school students learn when they are introduced to long division. In doing the division problem , you obtain a quotient of 20 and a remainder of 46. This result could either be written or . The latter form is how the division property is normally expressed in higher mathematics. "
},
{
  "id": "divisability-terms",
  "level": "2",
  "url": "s-gcds-and-zsubn.html#divisability-terms",
  "type": "List",
  "number": "11.4.3",
  "title": "",
  "body": " We now remind the reader of some interchangeable terminology that is used when , i. e., . All of the following say the same thing, just from slightly different points of view.   divides divides  multiple is a multiple of  factor is a factor of  divisor is a divisor of   We use the notation if divides .  "
},
{
  "id": "def-gcd",
  "level": "2",
  "url": "s-gcds-and-zsubn.html#def-gcd",
  "type": "Definition",
  "number": "11.4.4",
  "title": "Greatest Common Divisor.",
  "body": "Greatest Common Divisor  Greatest Common Divisor ( )  the greatest common divisor of and Given two integers, and , not both zero, the greatest common divisor of and is the positive integer such that , , and  "
},
{
  "id": "def-def-rel-prime",
  "level": "2",
  "url": "s-gcds-and-zsubn.html#def-def-rel-prime",
  "type": "Definition",
  "number": "11.4.5",
  "title": "Relatively Prime.",
  "body": "Relatively Prime  Relatively Prime  A pair of integers, and , are relatively prime if "
},
{
  "id": "th-primes-are-mostly-relprime",
  "level": "2",
  "url": "s-gcds-and-zsubn.html#th-primes-are-mostly-relprime",
  "type": "Theorem",
  "number": "11.4.6",
  "title": "",
  "body": " If is a prime and is any integer such that then "
},
{
  "id": "table-euclidean-example",
  "level": "2",
  "url": "s-gcds-and-zsubn.html#table-euclidean-example",
  "type": "Table",
  "number": "11.4.7",
  "title": "A Table to Compute <span xmlns:pf=\"https:\/\/prefigure.org\" class=\"process-math\">\\(\\gcd(99,53)\\)<\/span>",
  "body": " A Table to Compute    - 99 53  1 53 46  1 46 7  6 7 4  1 4 3  1 3 1  3 1 0   "
},
{
  "id": "sss-euclidean-algorithm-9",
  "level": "2",
  "url": "s-gcds-and-zsubn.html#sss-euclidean-algorithm-9",
  "type": "Investigation",
  "number": "11.4.1",
  "title": "",
  "body": "If you were allowed to pick two integers less than 100, which would you pick in order to force Euclid to work hardest? Here's a hint: The size of the quotient at each step determines how quickly the numbers decrease.  If quotient in division is 1, then we get the slowest possible completion. If , then working backwards, each remainder would be the sum of the two previous remainders. This described a sequence like the Fibonacci sequence and indeed, the greatest common divisor of two consecutive Fibonacci numbers will take the most steps to reach a final value of 1.  "
},
{
  "id": "fig-integer-linear-comb",
  "level": "2",
  "url": "s-gcds-and-zsubn.html#fig-integer-linear-comb",
  "type": "Figure",
  "number": "11.4.8",
  "title": "",
  "body": " Linear combinations of 36 and 27   Linear combinations of 36 and 27   "
},
{
  "id": "theorem-11-4-1",
  "level": "2",
  "url": "s-gcds-and-zsubn.html#theorem-11-4-1",
  "type": "Theorem",
  "number": "11.4.9",
  "title": "Bézout’s lemma.",
  "body": "Bézout's lemma Bézout's lemma  If and are positive integers, the smallest positive value of is the greatest common divisor of and , .  If , since and , we know that for any integers and , so can't be less than . To show that is exactly the least positive value, we show that can be attained by extending the Euclidean Algorithm. Performing the extended algorithm involves building a table of numbers. The way in which it is built maintains an invariant, and by , we can be sure that the desired values of and are produced.  "
},
{
  "id": "table-x-euclidean-example",
  "level": "2",
  "url": "s-gcds-and-zsubn.html#table-x-euclidean-example",
  "type": "Table",
  "number": "11.4.10",
  "title": "The extended Euclidean algorithm to compute <span xmlns:pf=\"https:\/\/prefigure.org\" class=\"process-math\">\\(\\gcd(152,53)\\)<\/span>",
  "body": " The extended Euclidean algorithm to compute    152 1 0  53 0 1  2 46 1  1 7 3  6 4 7  1 3 23  1 1 15  3 0 152   "
},
{
  "id": "table-invariants-11",
  "level": "2",
  "url": "s-gcds-and-zsubn.html#table-invariants-11",
  "type": "Table",
  "number": "11.4.11",
  "title": "Invariant in computing <span xmlns:pf=\"https:\/\/prefigure.org\" class=\"process-math\">\\(\\gcd(152,53)\\)<\/span>",
  "body": " Invariant in computing          "
},
{
  "id": "def-integers-modulo-n",
  "level": "2",
  "url": "s-gcds-and-zsubn.html#def-integers-modulo-n",
  "type": "Definition",
  "number": "11.4.12",
  "title": "The Integers Modulo <span xmlns:pf=\"https:\/\/prefigure.org\" class=\"process-math\">\\(n\\)<\/span>.",
  "body": "The Integers Modulo  The Integers Modulo  the integers modulo  If is a positive integer greater than one, we define the integers modulo to be the set .  "
},
{
  "id": "def-def-modular-addition",
  "level": "2",
  "url": "s-gcds-and-zsubn.html#def-def-modular-addition",
  "type": "Definition",
  "number": "11.4.13",
  "title": "Modular Addition.",
  "body": "Modular Addition  Modular Addition  the mod sum of and  If is a positive integer, we define addition modulo  ) as follows. If ,  "
},
{
  "id": "def-def-modular-multiplication",
  "level": "2",
  "url": "s-gcds-and-zsubn.html#def-def-modular-multiplication",
  "type": "Definition",
  "number": "11.4.14",
  "title": "Modular Multiplication.",
  "body": "Modular Multiplication  Modular Multiplication  the mod product of and  If is a positive integer, we define multiplication modulo  ) as follows. If ,  "
},
{
  "id": "ss-modular-arithmetic-7",
  "level": "2",
  "url": "s-gcds-and-zsubn.html#ss-modular-arithmetic-7",
  "type": "Note",
  "number": "11.4.15",
  "title": "",
  "body": " The result of doing arithmetic modulo is always an integer between 0 and , by the Division Property. This observation implies that is closed under modulo arithmetic.  It is always true that and . For example, and .  One interpretation of is that each element is a representative of its equivalence class with respect to congruence modulo . For example, if , the number in really represents all numbers in . In doing modular arithmetic, we can temporarily replace elements of with other elements in their equivalence class modulo .    "
},
{
  "id": "ex-11-4-modular",
  "level": "2",
  "url": "s-gcds-and-zsubn.html#ex-11-4-modular",
  "type": "Example",
  "number": "11.4.16",
  "title": "Some Examples.",
  "body": "Some Examples  We are all somewhat familiar with since the hours of the day are counted using this group, except for the fact that 12 is used in place of 0. Military time uses the mod 24 system and does begin at 0. If someone started a four-hour trip at hour 21, the time at which she would arrive is . If a satellite orbits the earth every four hours and starts its first orbit at hour 5, it would end its first orbit at time . Its tenth orbit would end at hours on the clock  Virtually all computers represent unsigned integers in binary form with a fixed number of digits. A very small computer might reserve seven bits to store the value of an integer. There are only different values that can be stored in seven bits. Since the smallest value is 0, represented as 0000000, the maximum value will be , represented as 1111111. When a command is given to add two integer values, and the two values have a sum of 128 or more, overflow occurs. For example, if we try to add 56 and 95, the sum is an eight-digit binary integer 10010111. One common procedure is to retain the seven lowest-ordered digits. The result of adding 56 and 95 would be . Integer arithmetic with this computer would actually be modulo 128 arithmetic.   "
},
{
  "id": "theorem-modular-add-inverse",
  "level": "2",
  "url": "s-gcds-and-zsubn.html#theorem-modular-add-inverse",
  "type": "Theorem",
  "number": "11.4.17",
  "title": "Additive Inverses in <span xmlns:pf=\"https:\/\/prefigure.org\" class=\"process-math\">\\(\\mathbb{Z}_n\\)<\/span>.",
  "body": "Additive Inverses in  If , , then the additive inverse of a is . , since . Therefore, . "
},
{
  "id": "ss-properties-modular-arithmetic-5",
  "level": "2",
  "url": "s-gcds-and-zsubn.html#ss-properties-modular-arithmetic-5",
  "type": "Definition",
  "number": "11.4.18",
  "title": "The Additive Group of Integers Modulo <span xmlns:pf=\"https:\/\/prefigure.org\" class=\"process-math\">\\(n\\)<\/span>.",
  "body": " The Additive Group of Integers Modulo  Integers Modulo Additive Group  The Additive Group of Integer Modulo   The Additive Group of Integers Modulo is the group with domain and with the operation of mod addition. It is denoted as .   "
},
{
  "id": "ss-properties-modular-arithmetic-9",
  "level": "2",
  "url": "s-gcds-and-zsubn.html#ss-properties-modular-arithmetic-9",
  "type": "Definition",
  "number": "11.4.19",
  "title": "The Multiplicative Group of Integers Modulo <span xmlns:pf=\"https:\/\/prefigure.org\" class=\"process-math\">\\(n\\)<\/span>.",
  "body": " The Multiplicative Group of Integers Modulo  Integers Modulo Multiplicative Group  The Multiplicative Group of Integer Modulo   The Multiplicative Group of Integers Modulo is the group with domain and with the operation of mod multiplication. It is denoted as .   "
},
{
  "id": "ex-11-4-op-tables",
  "level": "2",
  "url": "s-gcds-and-zsubn.html#ex-11-4-op-tables",
  "type": "Example",
  "number": "11.4.20",
  "title": "Some operation tables.",
  "body": "Some operation tables     Here are examples of operation tables for modular groups. Notice that although 8 is greater than 5, the two groups and both have order 4. In the case of , since 5 is prime all of the nonzero elements of are included. Since 8 isn't prime we don't include integers that share a common factor with 8, the even integers in this case.   Operation Table for the group            0  0  1  2  3  4    1  1  2  3  4  0    2  2  3  4  0  1    3  3  4  0  1  2    4  4  0  1  2  3        Operation table for the group           1  1  2  3  4    2  2  4  1  3    3  3  1  4  2    4  4  3  2  1      Operation table for the group           1  1  3  5  7    3  3  1  7  5    5  5  7  1  3    7  7  5  3  1      "
},
{
  "id": "exercises-11-4-2",
  "level": "2",
  "url": "s-gcds-and-zsubn.html#exercises-11-4-2",
  "type": "Exercise",
  "number": "11.4.6.1",
  "title": "",
  "body": "Determine the greatest common divisors of the following pairs of integers without using any computational assistance.   and   and   and  12112 and 0            12112    "
},
{
  "id": "exercises-11-4-3",
  "level": "2",
  "url": "s-gcds-and-zsubn.html#exercises-11-4-3",
  "type": "Exercise",
  "number": "11.4.6.2",
  "title": "",
  "body": "Find all possible values of the following, assuming that is a positive integer.         "
},
{
  "id": "exercises-11-4-4",
  "level": "2",
  "url": "s-gcds-and-zsubn.html#exercises-11-4-4",
  "type": "Exercise",
  "number": "11.4.6.3",
  "title": "",
  "body": "Calculate:                       2  5  0  0  2  2  1  3  0   "
},
{
  "id": "exercises-11-4-5",
  "level": "2",
  "url": "s-gcds-and-zsubn.html#exercises-11-4-5",
  "type": "Exercise",
  "number": "11.4.6.4",
  "title": "",
  "body": "List the additive inverses of the following elements:  4, 6, 9 in  16, 25, 40 in   "
},
{
  "id": "exercises-11-4-6",
  "level": "2",
  "url": "s-gcds-and-zsubn.html#exercises-11-4-6",
  "type": "Exercise",
  "number": "11.4.6.5",
  "title": "",
  "body": "In the group , what are:  3(4)?  36(4)?  How could you efficiently compute , ?    1  1  , where ,   "
},
{
  "id": "exercises-11-4-7",
  "level": "2",
  "url": "s-gcds-and-zsubn.html#exercises-11-4-7",
  "type": "Exercise",
  "number": "11.4.6.6",
  "title": "",
  "body": "When defining notice that we could define this operation on all integers, but we restricted the set to to satisfy the properties of a group. Can you explain what goes wrong if we define on all of the integers?  The first thing that goes wrong is that will not be an identity for addition modulo on the whole set of integers. Notice that .  "
},
{
  "id": "exercises-11-4-8",
  "level": "2",
  "url": "s-gcds-and-zsubn.html#exercises-11-4-8",
  "type": "Exercise",
  "number": "11.4.6.7",
  "title": "",
  "body": "A student is asked to solve the following equations under the requirement that all arithmetic should be done in . List all solutions.   .   .   Since the solutions, if they exist, must come from , substitution is the easiest approach.  1 is the only solution, since and  No solutions, since , and   "
},
{
  "id": "exercises-11-4-9",
  "level": "2",
  "url": "s-gcds-and-zsubn.html#exercises-11-4-9",
  "type": "Exercise",
  "number": "11.4.6.8",
  "title": "",
  "body": "Determine the solutions of the same equations as in Exercise 5 in . "
},
{
  "id": "exercise_u_n",
  "level": "2",
  "url": "s-gcds-and-zsubn.html#exercise_u_n",
  "type": "Exercise",
  "number": "11.4.6.9",
  "title": "",
  "body": "  Write out the operation table for on , and convince your self that this is a group.  Let be the elements of that have inverses with respect to . Convince yourself that is a group under .  Prove that the elements of are those elements such that . You may use in this proof.    "
},
{
  "id": "exercises-11-4-11",
  "level": "2",
  "url": "s-gcds-and-zsubn.html#exercises-11-4-11",
  "type": "Exercise",
  "number": "11.4.6.10",
  "title": "",
  "body": "Prove the division property, .  Prove by induction on that you can divide any positive integer into . That is, let be For all greater than zero, there exist unique integers and such that . In the induction step, divide into .  "
},
{
  "id": "exercises-11-4-12",
  "level": "2",
  "url": "s-gcds-and-zsubn.html#exercises-11-4-12",
  "type": "Exercise",
  "number": "11.4.6.11",
  "title": "",
  "body": "Suppose such where and are integer constants. Furthermore, assume that and . Find a formula for and also find a formula for the inverse of . The given conditions can be converted to a system of linear equations:  If we subtract the first equation from the second, we get . This implies that , and . To get a formula for the inverse of we solve for , using the fact that the multiplicative inverse of 10 (mod 17) is 12. Therefore .  "
},
{
  "id": "exercises-11-4-13",
  "level": "2",
  "url": "s-gcds-and-zsubn.html#exercises-11-4-13",
  "type": "Exercise",
  "number": "11.4.6.12",
  "title": "",
  "body": "Write out the operation table for mod 10 multiplication on . Is a monoid? Is it a group?  This system is a monoid with identity 6 (surprise!). However it is not a group since 0 has no inverse. "
},
{
  "id": "exercises-11-4-14",
  "level": "2",
  "url": "s-gcds-and-zsubn.html#exercises-11-4-14",
  "type": "Exercise",
  "number": "11.4.6.13",
  "title": "",
  "body": "Given that , explain why is an element of the group and determine its inverse in that group.  By , is an element of . It's inverse in the group is because  "
},
{
  "id": "exercises-11-4-15",
  "level": "2",
  "url": "s-gcds-and-zsubn.html#exercises-11-4-15",
  "type": "Exercise",
  "number": "11.4.6.14",
  "title": "",
  "body": "Let . Solve for in the group  "
},
{
  "id": "exercises-11-4-16",
  "level": "2",
  "url": "s-gcds-and-zsubn.html#exercises-11-4-16",
  "type": "Exercise",
  "number": "11.4.6.15",
  "title": "",
  "body": "Let be an odd prime. Find all solutions to the equation in the group .  "
},
{
  "id": "exercises-11-4-17",
  "level": "2",
  "url": "s-gcds-and-zsubn.html#exercises-11-4-17",
  "type": "Exercise",
  "number": "11.4.6.16",
  "title": "",
  "body": "It was observed above that in doing modular arithmetic, one can replace an element of with any other element of its equivalence class modulo . For example, if one is computing , the alternative to multiplying time and then dividing by to get the remainder in , we can replace with and get a product of which is conguent of . Use this trick to compute the following without the use of a calculator.        The solution to     "
},
{
  "id": "exercises-11-4-18",
  "level": "2",
  "url": "s-gcds-and-zsubn.html#exercises-11-4-18",
  "type": "Exercise",
  "number": "11.4.6.17",
  "title": "",
  "body": "We associate the set with the addition modulo , , because the pair form a group. Why must we use a matching modulus? Explain why by considering the following two examples.      "
},
{
  "id": "s-Subsystems",
  "level": "1",
  "url": "s-Subsystems.html",
  "type": "Section",
  "number": "11.5",
  "title": "Subsystems",
  "body": " Subsystems  Subsystems  Definition  The subsystem is a fundamental concept of algebra at the universal level.  Subsystem  Subsystem  is a subsystem of  If is an algebraic system of a certain kind and is a subset of , then is a subsystem of if is an algebraic system of the same kind as . The usual notation for is a subsystem of is .   Since the definition of a subsystem is at the universal level, we can cite examples of the concept of subsystems at both the axiomatic and concrete level.  Examples of Subsystems   Subgroup (Axiomatic) If is a group, and is a subset of , then is a subgroup of if is a group.  (Concrete) is a subgroup of . Take the time now to write out the multiplication table of and convince yourself that is a group.  (Concrete) The even integers, is a subgroup of . Convince yourself of this fact.  (Concrete) The set of nonnegative integers is not a subgroup of . All of the group axioms are true for this subset except one: no positive integer has a positive additive inverse. Therefore, the inverse property is not true. Note that every group axiom must be true for a subset to be a subgroup.  (Axiomatic) If is a monoid and is a subset of , then is a submonoid of if is a monoid.  (Concrete) If is the set of strings of 0's and 1's of length zero or more with the operation of concatenation, then two examples of submonoids of are: (i) the set of strings of even length, and (ii) the set of strings that contain no 0's. The set of strings of length less than 50 is not a submonoid because it isn't closed under concatenation. Why isn't the set of strings of length 50 or more a submonoid of ?     Subgroups  For the remainder of this section, we will concentrate on the properties of subgroups. The first order of business is to establish a systematic way of determining whether a subset of a group is a subgroup.  Subgroup Conditions  Subgroup Conditions   To determine whether H, a subset of group , is a subgroup, it is sufficient to prove:  H is closed under ; that is, ;  H contains the identity element for ; and  H contains the inverse of each of its elements; that is, .     Our proof consists of verifying that if the three properties above are true, then all the axioms of a group are true for . By Condition (a), can be considered an operation on . The associative, identity, and inverse properties are the axioms that are needed. The identity and inverse properties are true by conditions (b) and (c), respectively, leaving only the associative property. Since, is a group, for all . Certainly, if this equation is true for all choices of three elements from , it will be true for all choices of three elements from , since is a subset of .  For every group with at least two elements, there are at least two subgroups: they are the whole group and . Since these two are automatic, they are not considered very interesting and are called the improper subgroups of the group; is sometimes referred to as the trivial subgroup. All other subgroups, if there are any, are called proper subgroups.  We can apply at both the concrete and axiomatic levels.  Applying Conditions for a Subgroup   (Concrete) We can verify that , as stated in . Whenever you want to discuss a subset, you must find some convenient way of describing its elements. An element of can be described as 2 times an integer; that is, is equivalent to . Now we can verify that the three conditions of are true for 2 . First, if , then there exist such that and . A common error is to write something like and . This would mean that , which is not necessarily true. That is why two different variables are needed to describe and . Returning to our proof, we can add and : . Since is an integer, is an element of . Second, the identity, , belongs to 2 ( ). Finally, if and , and , therefore, . By , . How would this argument change if you were asked to prove that ? or ?  (Concrete) We can prove that is a subgroup of . First, for each ordered pair , is in . This can be checked without too much trouble since . Thus we can conclude that is closed under . Second, . Third, , , , and . Therefore, the inverse of each element of is in .  (Axiomatic) If and are both subgroups of a group , then is a subgroup of . To justify this statement, we have no concrete information to work with, only the facts that and . Our proof that reflects this and is an exercise in applying the definitions of intersection and subgroup, (i) If and are elements of , then and both belong to , and since , must be an element of . Similarly, ; therefore, . (ii) The identity of must belong to both and ; hence it belongs to . (iii) If , then , and since , . Similarly, . Hence, by the theorem, . Now that this fact has been established, we can apply it to any pair of subgroups of any group. For example, since and are both subgroups of , is also a subgroup of . Note that if , must have a factor of 3; that is, there exists such that . In addition, must be even, therefore must be even. There exists such that , therefore . This shows that . The opposite containment can easily be established; therefore, .    Given a finite group, we can apply to obtain a simpler condition for a subset to be a subgroup.   Condition for a Subgroup of Finite Group  Given that is a finite group and is a nonempty subset of , if is closed under , then is a subgroup of .  In this proof, we demonstrate that Conditions (b) and (c) of follow from the closure of under , which is condition (a) of the theorem. First, select any element of ; call it . The powers of : , , are all in by the closure property. By , there exists , , such that ; hence . To prove that (c) is true, we let be any element of . If , then is in since . If , for some between 2 and and Therefore, , which belongs to since .    Sage Note - Applying the condition for a subgroup of a finite group  To determine whether and are subgroups of , we need only write out the addition tables (modulo 15) for these sets. This is easy to do with a bit of Sage code that we include below and then for any modulus and subset, we can generate the body of an addition table. The code is set up for but can be easily adjusted for .   Note that is a subgroup of . Since the interior of the addition table for contains elements that are outside of , is not a subgroup of .   Cyclic Subgroups  One kind of subgroup that merits special mention due to its simplicity is the cyclic subgroup.  Cyclic Subgroup  Cyclic Subgroup  Generator  the cyclic subgroup generated by  If is a group and , the cyclic subgroup generated by , , is the set of all powers of : . We refer to as a generator of subgroup .  A subgroup of a group is cyclic if there exists such that .   Cyclic Group  Cyclic Group  A group is cyclic if there exists such that .  If the operation on is additive, then .  Order of a Group Element  Order of a Group Element  Order of a  The order of an element of group is the number of elements in the cyclic subgroup of generated by . The order of is denoted .   In , .  In , . Here is why: If is finite, you need list only the positive powers (or multiples) of up to the first occurrence of the identity to obtain all of . In , the multiples of 6 are 6, , , , and . Note that is also , , and . This shows that a cyclic subgroup can have different generators.    If you want to list the cyclic subgroups of a group, the following theorem can save you some time.   If is an element of group , then .  This is an easy way of seeing, for example, that in equals , since .    Exercises  Which of the following subsets of the real numbers is a subgroup of ?  the rational numbers  the positive real numbers          Only a and c are subgroups.  Describe in simpler terms the following subgroups of :     (be careful)  the only finite subgroup of    Find at least two proper subgroups of , the set of rook matrices (see ).   , , , and are all the proper subgroups of .  Let and be subgroups of with elements located in the following Venn diagram. Where should you place the following elements in ?          Figure for exercise 4   Figure for exercise 4 of Section 11.5 consisting of a Venn diagram with element and in , and in .       List the cyclic subgroups of and draw an ordering diagram for the relation is a subset of on these subgroups.  Do the same for .  Do the same for .  On the basis of your results in parts a, b, and c, what would you expect if you did the same with ?       , , ,   , , , , ,   , , ,  Based on the ordering diagrams for parts a through c in , we would expect to see an ordering diagram similar to the one for divides on (the divisors of 24) if we were to examine the subgroups of . This is indeed the case.     Figure for exercise 5   Figure for exercise 5 of Section 11.5    Subgroups generated by subsets of a group The concept of a cyclic subgroup is a special case of the concept that we will discuss here. Let be a group and a nonempty subset of . Define the set recursively by:  If , then .  If , then , and  If , then .     By its definition, has all of the properties needed to be a subgroup of . The only thing that isn't obvious is that the identity of is in . Prove that the identity of is in .  What is in ?  Prove that if and , then . This proves that is contained in every subgroup of that contains ; that is, .  Describe in and in .  If , is a cyclic subgroup of . In terms of and , what is a generator of ?    Prove that if , and , then or .  Use an indirect argument. Assume that and are subgroups of group , and that, as in , there are elements and . Consider the product . Where could it be placed in the Venn diagram? If we can prove that it must lie in the outer region, , then we have proven that is not closed under and cannot be a subgroup of , Assume that . Since is in , is in and so by closure which is a contradiction. Similarly, .  One way to interpret this theorem is that no group is the union of two groups.   Prove that the order of an element, of a group is the least positive integer, , such that is the identity of the group.    "
},
{
  "id": "def-subsystem",
  "level": "2",
  "url": "s-Subsystems.html#def-subsystem",
  "type": "Definition",
  "number": "11.5.1",
  "title": "Subsystem.",
  "body": "Subsystem  Subsystem  is a subsystem of  If is an algebraic system of a certain kind and is a subset of , then is a subsystem of if is an algebraic system of the same kind as . The usual notation for is a subsystem of is .  "
},
{
  "id": "ex-subsystems",
  "level": "2",
  "url": "s-Subsystems.html#ex-subsystems",
  "type": "Example",
  "number": "11.5.2",
  "title": "Examples of Subsystems.",
  "body": "Examples of Subsystems   Subgroup (Axiomatic) If is a group, and is a subset of , then is a subgroup of if is a group.  (Concrete) is a subgroup of . Take the time now to write out the multiplication table of and convince yourself that is a group.  (Concrete) The even integers, is a subgroup of . Convince yourself of this fact.  (Concrete) The set of nonnegative integers is not a subgroup of . All of the group axioms are true for this subset except one: no positive integer has a positive additive inverse. Therefore, the inverse property is not true. Note that every group axiom must be true for a subset to be a subgroup.  (Axiomatic) If is a monoid and is a subset of , then is a submonoid of if is a monoid.  (Concrete) If is the set of strings of 0's and 1's of length zero or more with the operation of concatenation, then two examples of submonoids of are: (i) the set of strings of even length, and (ii) the set of strings that contain no 0's. The set of strings of length less than 50 is not a submonoid because it isn't closed under concatenation. Why isn't the set of strings of length 50 or more a submonoid of ?   "
},
{
  "id": "th-subgroup-conditions",
  "level": "2",
  "url": "s-Subsystems.html#th-subgroup-conditions",
  "type": "Theorem",
  "number": "11.5.3",
  "title": "Subgroup Conditions.",
  "body": "Subgroup Conditions  Subgroup Conditions   To determine whether H, a subset of group , is a subgroup, it is sufficient to prove:  H is closed under ; that is, ;  H contains the identity element for ; and  H contains the inverse of each of its elements; that is, .     Our proof consists of verifying that if the three properties above are true, then all the axioms of a group are true for . By Condition (a), can be considered an operation on . The associative, identity, and inverse properties are the axioms that are needed. The identity and inverse properties are true by conditions (b) and (c), respectively, leaving only the associative property. Since, is a group, for all . Certainly, if this equation is true for all choices of three elements from , it will be true for all choices of three elements from , since is a subset of . "
},
{
  "id": "ex-subgroup-proofs",
  "level": "2",
  "url": "s-Subsystems.html#ex-subgroup-proofs",
  "type": "Example",
  "number": "11.5.4",
  "title": "Applying Conditions for a Subgroup.",
  "body": "Applying Conditions for a Subgroup   (Concrete) We can verify that , as stated in . Whenever you want to discuss a subset, you must find some convenient way of describing its elements. An element of can be described as 2 times an integer; that is, is equivalent to . Now we can verify that the three conditions of are true for 2 . First, if , then there exist such that and . A common error is to write something like and . This would mean that , which is not necessarily true. That is why two different variables are needed to describe and . Returning to our proof, we can add and : . Since is an integer, is an element of . Second, the identity, , belongs to 2 ( ). Finally, if and , and , therefore, . By , . How would this argument change if you were asked to prove that ? or ?  (Concrete) We can prove that is a subgroup of . First, for each ordered pair , is in . This can be checked without too much trouble since . Thus we can conclude that is closed under . Second, . Third, , , , and . Therefore, the inverse of each element of is in .  (Axiomatic) If and are both subgroups of a group , then is a subgroup of . To justify this statement, we have no concrete information to work with, only the facts that and . Our proof that reflects this and is an exercise in applying the definitions of intersection and subgroup, (i) If and are elements of , then and both belong to , and since , must be an element of . Similarly, ; therefore, . (ii) The identity of must belong to both and ; hence it belongs to . (iii) If , then , and since , . Similarly, . Hence, by the theorem, . Now that this fact has been established, we can apply it to any pair of subgroups of any group. For example, since and are both subgroups of , is also a subgroup of . Note that if , must have a factor of 3; that is, there exists such that . In addition, must be even, therefore must be even. There exists such that , therefore . This shows that . The opposite containment can easily be established; therefore, .   "
},
{
  "id": "th-subgroup-of-finite-group",
  "level": "2",
  "url": "s-Subsystems.html#th-subgroup-of-finite-group",
  "type": "Theorem",
  "number": "11.5.5",
  "title": "Condition for a Subgroup of Finite Group.",
  "body": " Condition for a Subgroup of Finite Group  Given that is a finite group and is a nonempty subset of , if is closed under , then is a subgroup of .  In this proof, we demonstrate that Conditions (b) and (c) of follow from the closure of under , which is condition (a) of the theorem. First, select any element of ; call it . The powers of : , , are all in by the closure property. By , there exists , , such that ; hence . To prove that (c) is true, we let be any element of . If , then is in since . If , for some between 2 and and Therefore, , which belongs to since . "
},
{
  "id": "def-cyclic-subgroup",
  "level": "2",
  "url": "s-Subsystems.html#def-cyclic-subgroup",
  "type": "Definition",
  "number": "11.5.6",
  "title": "Cyclic Subgroup.",
  "body": "Cyclic Subgroup  Cyclic Subgroup  Generator  the cyclic subgroup generated by  If is a group and , the cyclic subgroup generated by , , is the set of all powers of : . We refer to as a generator of subgroup .  A subgroup of a group is cyclic if there exists such that .  "
},
{
  "id": "def-cyclic-group",
  "level": "2",
  "url": "s-Subsystems.html#def-cyclic-group",
  "type": "Definition",
  "number": "11.5.7",
  "title": "Cyclic Group.",
  "body": "Cyclic Group  Cyclic Group  A group is cyclic if there exists such that . "
},
{
  "id": "s-Subsystems-6-5",
  "level": "2",
  "url": "s-Subsystems.html#s-Subsystems-6-5",
  "type": "Note",
  "number": "11.5.8",
  "title": "",
  "body": "If the operation on is additive, then . "
},
{
  "id": "def-order-of-element",
  "level": "2",
  "url": "s-Subsystems.html#def-order-of-element",
  "type": "Definition",
  "number": "11.5.9",
  "title": "Order of a Group Element.",
  "body": "Order of a Group Element  Order of a Group Element  Order of a  The order of an element of group is the number of elements in the cyclic subgroup of generated by . The order of is denoted . "
},
{
  "id": "ex-11-5-4",
  "level": "2",
  "url": "s-Subsystems.html#ex-11-5-4",
  "type": "Example",
  "number": "11.5.10",
  "title": "",
  "body": " In , .  In , . Here is why: If is finite, you need list only the positive powers (or multiples) of up to the first occurrence of the identity to obtain all of . In , the multiples of 6 are 6, , , , and . Note that is also , , and . This shows that a cyclic subgroup can have different generators.   "
},
{
  "id": "th-cyclic-pairs",
  "level": "2",
  "url": "s-Subsystems.html#th-cyclic-pairs",
  "type": "Theorem",
  "number": "11.5.11",
  "title": "",
  "body": " If is an element of group , then . "
},
{
  "id": "exercises-11-5-2",
  "level": "2",
  "url": "s-Subsystems.html#exercises-11-5-2",
  "type": "Exercise",
  "number": "11.5.5.1",
  "title": "",
  "body": "Which of the following subsets of the real numbers is a subgroup of ?  the rational numbers  the positive real numbers          Only a and c are subgroups. "
},
{
  "id": "exercises-11-5-3",
  "level": "2",
  "url": "s-Subsystems.html#exercises-11-5-3",
  "type": "Exercise",
  "number": "11.5.5.2",
  "title": "",
  "body": "Describe in simpler terms the following subgroups of :     (be careful)  the only finite subgroup of   "
},
{
  "id": "exercises-11-5-4",
  "level": "2",
  "url": "s-Subsystems.html#exercises-11-5-4",
  "type": "Exercise",
  "number": "11.5.5.3",
  "title": "",
  "body": "Find at least two proper subgroups of , the set of rook matrices (see ).   , , , and are all the proper subgroups of . "
},
{
  "id": "exercises-11-5-5",
  "level": "2",
  "url": "s-Subsystems.html#exercises-11-5-5",
  "type": "Exercise",
  "number": "11.5.5.4",
  "title": "",
  "body": "Let and be subgroups of with elements located in the following Venn diagram. Where should you place the following elements in ?          Figure for exercise 4   Figure for exercise 4 of Section 11.5 consisting of a Venn diagram with element and in , and in .    "
},
{
  "id": "exercises-11-5-6",
  "level": "2",
  "url": "s-Subsystems.html#exercises-11-5-6",
  "type": "Exercise",
  "number": "11.5.5.5",
  "title": "",
  "body": "  List the cyclic subgroups of and draw an ordering diagram for the relation is a subset of on these subgroups.  Do the same for .  Do the same for .  On the basis of your results in parts a, b, and c, what would you expect if you did the same with ?       , , ,   , , , , ,   , , ,  Based on the ordering diagrams for parts a through c in , we would expect to see an ordering diagram similar to the one for divides on (the divisors of 24) if we were to examine the subgroups of . This is indeed the case.     Figure for exercise 5   Figure for exercise 5 of Section 11.5   "
},
{
  "id": "exercises-11-5-7",
  "level": "2",
  "url": "s-Subsystems.html#exercises-11-5-7",
  "type": "Exercise",
  "number": "11.5.5.6",
  "title": "Subgroups generated by subsets of a group.",
  "body": "Subgroups generated by subsets of a group The concept of a cyclic subgroup is a special case of the concept that we will discuss here. Let be a group and a nonempty subset of . Define the set recursively by:  If , then .  If , then , and  If , then .     By its definition, has all of the properties needed to be a subgroup of . The only thing that isn't obvious is that the identity of is in . Prove that the identity of is in .  What is in ?  Prove that if and , then . This proves that is contained in every subgroup of that contains ; that is, .  Describe in and in .  If , is a cyclic subgroup of . In terms of and , what is a generator of ?   "
},
{
  "id": "exercises-11-5-8",
  "level": "2",
  "url": "s-Subsystems.html#exercises-11-5-8",
  "type": "Exercise",
  "number": "11.5.5.7",
  "title": "",
  "body": "Prove that if , and , then or .  Use an indirect argument. Assume that and are subgroups of group , and that, as in , there are elements and . Consider the product . Where could it be placed in the Venn diagram? If we can prove that it must lie in the outer region, , then we have proven that is not closed under and cannot be a subgroup of , Assume that . Since is in , is in and so by closure which is a contradiction. Similarly, .  One way to interpret this theorem is that no group is the union of two groups.  "
},
{
  "id": "exercises-11-5-9",
  "level": "2",
  "url": "s-Subsystems.html#exercises-11-5-9",
  "type": "Exercise",
  "number": "11.5.5.8",
  "title": "",
  "body": "Prove that the order of an element, of a group is the least positive integer, , such that is the identity of the group.  "
},
{
  "id": "s-direct-products",
  "level": "1",
  "url": "s-direct-products.html",
  "type": "Section",
  "number": "11.6",
  "title": "Direct Products",
  "body": " Direct Products  Direct Products  Definition  Our second universal algebraic concept lets us look in the opposite direction from subsystems. Direct products allow us to create larger systems. In the following definition, we avoid complicating the notation by not specifying how many operations the systems have.  Direct Product  Direct Product   The direct product of algebraic structures  If , are algebraic systems of the same kind, then the direct product of these systems is , with operations defined below. The elements of are -tuples of the form , where , . The systems , are called the factors of . There are as many operations on as there are in the factors. Each of these operations is defined componentwise:  If ,   A Direct Product of Monoids  Consider the monoids (the set of natural numbers with addition) and (the set of finite strings of 0's and 1's with concatenation). The direct product of with is a monoid. We illustrate its operation, which we will denote by , with examples: Note that our new monoid is not commutative. What is the identity for ?  The definiton of a is quite general and may be confusing to some. Here is the definiton of the direct product of two groups. The definition extends easily to the direct product of three or more groups.   Direct Product of Two Groups  Direct Product of Two Groups    The direct product of groups and    Let and be two groups. Their direct product is the system with domain equal to the Cartesian product of the domains of the two groups and with the coordinatewise operation defined by for .     Concurrent calculation in a direct product   Concurrent calculation in a direct product       On notation. If two or more consecutive factors in a direct product are identical, it is common to combine them using exponential notation. For example, can be written , and can be written . This is purely a notational convenience; no exponentiation is really taking place.  We call the operations in a direct product componentwise operations , and they are indeed operations on . If two -tuples, and , are selected from , the first components of and , and , are operated on with to obtain , the first component of . Note that since is an operation on , is an element of . Similarly, all other components of , as they are defined, belong to their proper sets.  One significant fact about componentwise operations is that the components of the result can all be computed at the same time (concurrently). The time required to compute in a direct product can be reduced to a length of time that is not much longer than the maximum amount of time needed to compute in the factors.  A direct product of algebraic systems is not always an algebraic system of the same type as its factors. This is due to the fact that certain axioms that are true for the factors may not be true for the set of -tuples. This situation does not occur with groups however. You will find that whenever a new type of algebraic system is introduced, call it type , one of the first theorems that is usually proven, if possible, is that the direct product of two or more systems of type is a system of type .      Direct Products of Groups  We will explore properties of direct products of groups and examine some concrete examples  The Direct Product of Groups is a Group  The direct product of two or more groups is a group; that is, the algebraic properties of a system obtained by taking the direct product of two or more groups includes the group axioms.   We will only present the proof of this theorem for the direct product of two groups. Some slight revisions can be made to produce a proof for any number of factors.  Stating that the direct product of two groups is a group is a short way of saying that if and are groups, then is also a group, where is the componentwise operation on . Associativity of : If , Notice how the associativity property hinges on the associativity in each factor. An identity for : As you might expect, if and are identities for and , respectively, then is the identity for . If , Similarly, .  Inverses in : The inverse of an element is determined componentwise . To verify, we compute : Similarly, .   Some New Groups   If , , the direct product of factors of , is a group with elements. We will take a closer look at . The elements of this group are triples of zeros and ones. Since the operation on is , we will use the symbol + for the operation on . Two of the eight triples in the group are and . Their sum is . One interesting fact about this group is that each element is its own inverse. For example ; therefore . We use the additive notation for the inverse of because we are using a form of addition. Note that is a subgroup of . Write out the addition table for this set and apply . The same can be said for any set consisting of (0, 0, 0) and another element of .   The direct product of the positive real numbers with the integers modulo 4, is an infinite group since one of its factors is infinite. The operations on the factors are multiplication and modular addition, so we will select the neutral symbol for the operation on . If and , then   It would be incorrect to say that is a subgroup of , but there is a subgroup of the direct product that closely resembles . It is . Its table is   Imagine erasing throughout the table and writing in place of . What would you get? We will explore this phenomenon in detail in the next section.  The whole direct product could be visualized as four parallel half-lines labeled 0, 1, 2, and 3 as in . On the th line, the point that lies units to the right of the zero mark would be . The set , which is depicted on the figure is a subgroup of . What cyclic subgroup is it?  The answer: or . There are two different generators.     Visualization of the group    Visualization of the group     A more conventional direct product is , the direct product of two factors of . The operation on is componentwise addition; hence we will use + as the operation symbol for this group. You should be familiar with this operation, since it is identical to addition of matrices. The Cartesian coordinate system can be used to visualize geometrically. We plot the pair on the plane in the usual way: units along the axis and units along the axis. There is a variety of different subgroups of , a few of which are:  , all of the points on the axis;   , all of the points that are on the line 2x - y = 0;  If , . The first two subgroups are special cases of this one, which represents any line that passes through the origin.   , a union of a set of lines that are parallel to .   , which is the only countable subgroup that we have listed.   We will leave it to the reader to verify that these sets are subgroups. We will only point out how the fourth example, call it , is closed under addition. If and and both belong to , then and , where both and are integers. We can determine whether belongs to by deciding whether or not is an integer: Since and are integers, so is . This completes a proof that is closed under the operation of .  Several useful facts can be stated in regards to the direct product of two or more groups. We will combine them into one theorem, which we will present with no proof. Parts a and c were derived for in the proof of .  Properties of Direct Products of Groups  If is a direct product of groups and , then:  The identity of is , where is the identity of .   .   for all .   is abelian if and only if each of the factors is abelian.  lf are subgroups of the corresponding factors, then is a subgroup of .      Not all subgroups of a direct product can be created using part e of . For example, is a subgroup of , but is not a direct product of two subgroups of .  Linked Lists using a Direct Product - XOR Linked Lists  XOR linked list  Using the identity , in , we can devise a scheme for representing a symmetrically linked list using only one link field. A symmetrically linked list is a list in which each node contains a pointer to its immediate successor and its immediate predecessor (see ). If the pointers are -digit binary addresses, then each pointer can be taken as an element of . Lists of this type can be accomplished using cells with only one link. In place of a left and a right pointer, the only link is the value of the sum (left link) + (right link). All standard list operations (merge, insert, delete, traverse, and so on) are possible with this structure, provided that you know the value of the nil pointer and the address, , of the first (i. e., leftmost) cell. Since first is nil, we can recover by adding the value of nil: , which is the address of the second item. Now if we temporarily retain the address, , of the second cell, we can recover the address of the third item. The link field of the second item contains the sum . Therefore   We no longer need the address of the first cell, only the second and third, to recover the fourth address, and so forth.   Symmetric Linked Lists   Symmetric Linked Lists    The following more formal algorithm uses names that reflects the timing of the visits.  Given a symmetric list, a traversal of the list is accomplished as follows, where is the address of the first cell. We presume that each item has some information that is represented by and a field called item.link that is the sum of the left and right links.    (1) yesterday =nil  (2) today =first  (3) while :  (3.1)Write(today.info)  (3.2)tomorrow = today.link + yesterday  (3.3)yesterday = today  (3.4)today = tomorrow.    At any point in this algorithm it would be quite easy to insert a cell between today and tomorrow. Can you describe how this would be accomplished?  This implementation of doubly linked lists is often referred to as an XOR linked list. For more information see the Wikipedia page .     Exercises  Write out the group table of and find the two proper subgroups of this group. Table of : The only two proper subgroups are and   List more examples of proper subgroups of that are different from the ones listed in this section.  Algebraic properties of the -cube   The four elements of can be visualized geometrically as the four corners of the 2-cube. Algebraically describe the statements:  Corners and are adjacent.  Corners and are diagonally opposite one another.  The eight elements of can be visualized as the eight corners of the 3-cube. One face contains and the opposite face contains the remaining four elements so that is behind . As in part a, describe statements i and ii algebraically.  If you could imagine a geometric figure similar to the square or cube in dimensions, and its corners were labeled by elements of as in parts a and b, how would statements i and ii be expressed algebraically?     (i) . (ii) .  (i) . (ii) .  (i) has exactly one 1. (ii) has all .      Suppose that you were to be given a group and asked to solve the equation . Without knowing the group, can you anticipate how many solutions there will be?  Answer the same question as part a for the equation .     Which of the following sets are subgroups of ? Give a reason for any negative answers.               No, 0 is not an element of .  Yes.  No, (0, 0) is not an element of this set.  No, the set is not closed: and is not in the set.  Yes.   Determine the following values in the group :    the identity element       "
},
{
  "id": "def-direct-product",
  "level": "2",
  "url": "s-direct-products.html#def-direct-product",
  "type": "Definition",
  "number": "11.6.1",
  "title": "Direct Product.",
  "body": "Direct Product  Direct Product   The direct product of algebraic structures  If , are algebraic systems of the same kind, then the direct product of these systems is , with operations defined below. The elements of are -tuples of the form , where , . The systems , are called the factors of . There are as many operations on as there are in the factors. Each of these operations is defined componentwise:  If ,  "
},
{
  "id": "ex-product-monoids",
  "level": "2",
  "url": "s-direct-products.html#ex-product-monoids",
  "type": "Example",
  "number": "11.6.2",
  "title": "A Direct Product of Monoids.",
  "body": "A Direct Product of Monoids  Consider the monoids (the set of natural numbers with addition) and (the set of finite strings of 0's and 1's with concatenation). The direct product of with is a monoid. We illustrate its operation, which we will denote by , with examples: Note that our new monoid is not commutative. What is the identity for ? "
},
{
  "id": "def-direct-product-of-groups",
  "level": "2",
  "url": "s-direct-products.html#def-direct-product-of-groups",
  "type": "Definition",
  "number": "11.6.3",
  "title": "Direct Product of Two Groups.",
  "body": " Direct Product of Two Groups  Direct Product of Two Groups    The direct product of groups and    Let and be two groups. Their direct product is the system with domain equal to the Cartesian product of the domains of the two groups and with the coordinatewise operation defined by for .   "
},
{
  "id": "fig-concurrent",
  "level": "2",
  "url": "s-direct-products.html#fig-concurrent",
  "type": "Figure",
  "number": "11.6.4",
  "title": "",
  "body": " Concurrent calculation in a direct product   Concurrent calculation in a direct product   "
},
{
  "id": "s-direct-products-3-8",
  "level": "2",
  "url": "s-direct-products.html#s-direct-products-3-8",
  "type": "Note",
  "number": "11.6.5",
  "title": "",
  "body": "   On notation. If two or more consecutive factors in a direct product are identical, it is common to combine them using exponential notation. For example, can be written , and can be written . This is purely a notational convenience; no exponentiation is really taking place.  We call the operations in a direct product componentwise operations , and they are indeed operations on . If two -tuples, and , are selected from , the first components of and , and , are operated on with to obtain , the first component of . Note that since is an operation on , is an element of . Similarly, all other components of , as they are defined, belong to their proper sets.  One significant fact about componentwise operations is that the components of the result can all be computed at the same time (concurrently). The time required to compute in a direct product can be reduced to a length of time that is not much longer than the maximum amount of time needed to compute in the factors.  A direct product of algebraic systems is not always an algebraic system of the same type as its factors. This is due to the fact that certain axioms that are true for the factors may not be true for the set of -tuples. This situation does not occur with groups however. You will find that whenever a new type of algebraic system is introduced, call it type , one of the first theorems that is usually proven, if possible, is that the direct product of two or more systems of type is a system of type .    "
},
{
  "id": "theorem-product-of-groups",
  "level": "2",
  "url": "s-direct-products.html#theorem-product-of-groups",
  "type": "Theorem",
  "number": "11.6.6",
  "title": "The Direct Product of Groups is a Group.",
  "body": "The Direct Product of Groups is a Group  The direct product of two or more groups is a group; that is, the algebraic properties of a system obtained by taking the direct product of two or more groups includes the group axioms.   We will only present the proof of this theorem for the direct product of two groups. Some slight revisions can be made to produce a proof for any number of factors.  Stating that the direct product of two groups is a group is a short way of saying that if and are groups, then is also a group, where is the componentwise operation on . Associativity of : If , Notice how the associativity property hinges on the associativity in each factor. An identity for : As you might expect, if and are identities for and , respectively, then is the identity for . If , Similarly, .  Inverses in : The inverse of an element is determined componentwise . To verify, we compute : Similarly, .  "
},
{
  "id": "ex-new-groups-products",
  "level": "2",
  "url": "s-direct-products.html#ex-new-groups-products",
  "type": "Example",
  "number": "11.6.7",
  "title": "Some New Groups.",
  "body": "Some New Groups   If , , the direct product of factors of , is a group with elements. We will take a closer look at . The elements of this group are triples of zeros and ones. Since the operation on is , we will use the symbol + for the operation on . Two of the eight triples in the group are and . Their sum is . One interesting fact about this group is that each element is its own inverse. For example ; therefore . We use the additive notation for the inverse of because we are using a form of addition. Note that is a subgroup of . Write out the addition table for this set and apply . The same can be said for any set consisting of (0, 0, 0) and another element of .   The direct product of the positive real numbers with the integers modulo 4, is an infinite group since one of its factors is infinite. The operations on the factors are multiplication and modular addition, so we will select the neutral symbol for the operation on . If and , then   It would be incorrect to say that is a subgroup of , but there is a subgroup of the direct product that closely resembles . It is . Its table is   Imagine erasing throughout the table and writing in place of . What would you get? We will explore this phenomenon in detail in the next section.  The whole direct product could be visualized as four parallel half-lines labeled 0, 1, 2, and 3 as in . On the th line, the point that lies units to the right of the zero mark would be . The set , which is depicted on the figure is a subgroup of . What cyclic subgroup is it?  The answer: or . There are two different generators.     Visualization of the group    Visualization of the group    "
},
{
  "id": "theorem-direct-product-properties",
  "level": "2",
  "url": "s-direct-products.html#theorem-direct-product-properties",
  "type": "Theorem",
  "number": "11.6.9",
  "title": "Properties of Direct Products of Groups.",
  "body": "Properties of Direct Products of Groups  If is a direct product of groups and , then:  The identity of is , where is the identity of .   .   for all .   is abelian if and only if each of the factors is abelian.  lf are subgroups of the corresponding factors, then is a subgroup of .     "
},
{
  "id": "ex-linked-lists",
  "level": "2",
  "url": "s-direct-products.html#ex-linked-lists",
  "type": "Example",
  "number": "11.6.10",
  "title": "Linked Lists using a Direct Product - XOR Linked Lists.",
  "body": "Linked Lists using a Direct Product - XOR Linked Lists  XOR linked list  Using the identity , in , we can devise a scheme for representing a symmetrically linked list using only one link field. A symmetrically linked list is a list in which each node contains a pointer to its immediate successor and its immediate predecessor (see ). If the pointers are -digit binary addresses, then each pointer can be taken as an element of . Lists of this type can be accomplished using cells with only one link. In place of a left and a right pointer, the only link is the value of the sum (left link) + (right link). All standard list operations (merge, insert, delete, traverse, and so on) are possible with this structure, provided that you know the value of the nil pointer and the address, , of the first (i. e., leftmost) cell. Since first is nil, we can recover by adding the value of nil: , which is the address of the second item. Now if we temporarily retain the address, , of the second cell, we can recover the address of the third item. The link field of the second item contains the sum . Therefore   We no longer need the address of the first cell, only the second and third, to recover the fourth address, and so forth.   Symmetric Linked Lists   Symmetric Linked Lists    The following more formal algorithm uses names that reflects the timing of the visits.  Given a symmetric list, a traversal of the list is accomplished as follows, where is the address of the first cell. We presume that each item has some information that is represented by and a field called item.link that is the sum of the left and right links.    (1) yesterday =nil  (2) today =first  (3) while :  (3.1)Write(today.info)  (3.2)tomorrow = today.link + yesterday  (3.3)yesterday = today  (3.4)today = tomorrow.    At any point in this algorithm it would be quite easy to insert a cell between today and tomorrow. Can you describe how this would be accomplished?  This implementation of doubly linked lists is often referred to as an XOR linked list. For more information see the Wikipedia page .  "
},
{
  "id": "exercises-11-6-2",
  "level": "2",
  "url": "s-direct-products.html#exercises-11-6-2",
  "type": "Exercise",
  "number": "11.6.3.1",
  "title": "",
  "body": "Write out the group table of and find the two proper subgroups of this group. Table of : The only two proper subgroups are and  "
},
{
  "id": "exercises-11-6-3",
  "level": "2",
  "url": "s-direct-products.html#exercises-11-6-3",
  "type": "Exercise",
  "number": "11.6.3.2",
  "title": "",
  "body": "List more examples of proper subgroups of that are different from the ones listed in this section. "
},
{
  "id": "exercise-n-cube-algebra",
  "level": "2",
  "url": "s-direct-products.html#exercise-n-cube-algebra",
  "type": "Exercise",
  "number": "11.6.3.3",
  "title": "Algebraic properties of the <span xmlns:pf=\"https:\/\/prefigure.org\" class=\"process-math\">\\(n\\)<\/span>-cube.",
  "body": "Algebraic properties of the -cube   The four elements of can be visualized geometrically as the four corners of the 2-cube. Algebraically describe the statements:  Corners and are adjacent.  Corners and are diagonally opposite one another.  The eight elements of can be visualized as the eight corners of the 3-cube. One face contains and the opposite face contains the remaining four elements so that is behind . As in part a, describe statements i and ii algebraically.  If you could imagine a geometric figure similar to the square or cube in dimensions, and its corners were labeled by elements of as in parts a and b, how would statements i and ii be expressed algebraically?     (i) . (ii) .  (i) . (ii) .  (i) has exactly one 1. (ii) has all .   "
},
{
  "id": "exercises-11-6-5",
  "level": "2",
  "url": "s-direct-products.html#exercises-11-6-5",
  "type": "Exercise",
  "number": "11.6.3.4",
  "title": "",
  "body": "  Suppose that you were to be given a group and asked to solve the equation . Without knowing the group, can you anticipate how many solutions there will be?  Answer the same question as part a for the equation .   "
},
{
  "id": "exercises-11-6-6",
  "level": "2",
  "url": "s-direct-products.html#exercises-11-6-6",
  "type": "Exercise",
  "number": "11.6.3.5",
  "title": "",
  "body": " Which of the following sets are subgroups of ? Give a reason for any negative answers.               No, 0 is not an element of .  Yes.  No, (0, 0) is not an element of this set.  No, the set is not closed: and is not in the set.  Yes.  "
},
{
  "id": "exercises-11-6-7",
  "level": "2",
  "url": "s-direct-products.html#exercises-11-6-7",
  "type": "Exercise",
  "number": "11.6.3.6",
  "title": "",
  "body": "Determine the following values in the group :    the identity element     "
},
{
  "id": "s-isomorphisms",
  "level": "1",
  "url": "s-isomorphisms.html",
  "type": "Section",
  "number": "11.7",
  "title": "Isomorphisms",
  "body": " Isomorphisms  Isomorphisms  The following informal definition of isomorphic systems should be memorized. No matter how technical a discussion about isomorphic systems becomes, keep in mind that this is the essence of the concept.  Isomorphic Systems\/Isomorphism - Informal Version  Two algebraic systems are isomorphic if there exists a translation rule between them so that any true statement in one system can be translated to a true statement in the other.  How to Do Greek Arithmetic Imagine that you are a six-year-old child who has been reared in an English-speaking family, has moved to Greece, and has been enrolled in a Greek school. Suppose that your new teacher asks the class to do the following addition problem that has been written out in Greek. The natural thing for you to do is to take out your Greek-English\/English-Greek dictionary and translate the Greek words to English, as outlined in After you've solved the problem, you can consult the same dictionary to find the proper Greek word that the teacher wants. Although this is not the recommended method of learning a foreign language, it will surely yield the correct answer to the problem. Mathematically, we may say that the system of Greek integers with addition ( ) is isomorphic to English integers with addition (plus). The problem of translation between natural languages is more difficult than this though, because two complete natural languages are not isomorphic, or at least the isomorphism between them is not contained in a simple dictionary.   Solution of a Greek arithmetic problem   Solution of a Greek arithmetic problem     Software Implementation of Sets  In this example, we will describe how set variables can be implemented on a computer. We will describe the two systems first and then describe the isomorphism between them.  System 1: The power set of with the operation union, . For simplicity, we will only discuss union. However, the other operations are implemented in a similar way.  System 2: Strings of five bits of computer memory with an OR gate. Individual bit values are either zero or one, so the elements of this system can be visualized as sequences of five 0's and 1's. An OR gate, , is a small piece of computer hardware that accepts two bit values at any one time and outputs either a zero or one, depending on the inputs. The output of an OR gate is one, except when the two bit values that it accepts are both zero, in which case the output is zero. The operation on this system actually consists of sequentially inputting the values of two bit strings into the OR gate. The result will be a new string of five 0's and 1's. An alternate method of operating in this system is to use five OR gates and to input corresponding pairs of bits from the input strings into the gates concurrently.   Translation between sets and strings of bits   Translation between sets and strings of bits    The Isomorphism: Since each system has only one operation, it is clear that union and the OR gate translate into one another. The translation between sets and bit strings is easiest to describe by showing how to construct a set from a bit string. If , is a bit string in System 2, the set that it translates to contains the number if and only if equals 1. For example, is translated to the set , while the set is translated to Now imagine that your computer is like the child who knows English and must do a Greek problem. To execute a program that has code that includes the set expression , it will follow the same procedure as the child to obtain the result, as shown in .   Translation of a problem in set theory   Translation of a problem in set theory      Group Isomorphisms  Multiplying without doing multiplication  This isomorphism is between and . Until the 1970s, when the price of calculators dropped, multiplication and exponentiation were performed with an isomorphism between these systems. The isomorphism to ) between the two groups is that is translated into and any positive real number is translated to the logarithm of . To translate back from to , you invert the logarithm function. If base ten logarithms are used, an element of , , will be translated to . In pre-calculator days, the translation was done with a table of logarithms or with a slide rule. An example of how the isomorphism is used appears in .   Multiplication using logarithms   Multiplication using logarithms     The following definition of an isomorphism between two groups is a more formal one that appears in most abstract algebra texts. At first glance, it appears different, it is really a slight variation on the informal definition. It is the common definition because it is easy to apply; that is, given a function, this definition tells you what to do to determine whether that function is an isomorphism.  Group Isomorphism  Isomorphism Group  is isomorphic to  If and are groups, is an isomorphism from into if:   is a bijection, and   for all    If such a function exists, then we say is isomorphic to , denoted .   We should note that is isomorphic to is an equivalence relation on the set of all groups. We leave it to the reader to verify the following.  The identity function on a group is an isomorphism.  Bijections have inverses, the inverse of an isomorphism is an isomorphism.  The composition of any two isomorphisms that can be composed is an isomorphism.     Steps in proving that and are isomorphic   Steps in proving that and are isomorphic      There could be several different isomorphisms between the same pair of groups. Thus, if you are asked to demonstrate that two groups are isomorphic, your answer need not be unique.  Any application of this definition requires a procedure outlined in . The first condition, that an isomorphism be a bijection, reflects the fact that every true statement in the first group should have exactly one corresponding true statement in the second group. This is exactly why we run into difficulty in translating between two natural languages. To see how Condition (b) of the formal definition is consistent with the informal definition, consider the function defined by . The translation diagram between and for the multiplication problem appears in . We arrive at the same result by computing as we do by computing . If we apply the function to the two results, we get the same image: since . Note that is exactly Condition b of the formal definition applied to the two groups and .     General Multiplication using logarithms   General Multiplication using logarithms     Consider with matrix multiplication. The group is isomorphic to . Our translation rule is the function defined by . Since groups have only one operation, there is no need to state explicitly that addition is translated to matrix multiplication. That is a bijection is clear from its definition.  If and are any real numbers,   We can apply this translation rule to determine the inverse of a matrix in . We know that is a true statement in . Using to translate this statement, we get or therefore,   The next theorem summarizes some of the general facts about group isomorphisms that are used most often in applications. We leave the proof to the reader.  Properties of Isomorphisms  If and are groups with identities and , respectively, and is an isomorphism from into , then:     for all , and  If is a subgroup of , then is a subgroup of and is isomorphic to .    Is isomorphic to is an equivalence relation on the set of all groups. Therefore, the set of all groups is partitioned into equivalence classes, each equivalence class containing groups that are isomorphic to one another.   The order sequence of a finite group  Propp, Jim This topic is somewhat obscure. It doesn't appear in most texts, but is a nice companion to degree sequences in graph theory. Recall that every undirected graph has a degree sequence, and graphs with different degree sequences are not isomorphic. This is a convenient way to identify non-isomorphic graphs. We see below that order sequences play exactly the same role in identifying whether two finite groups are isomorphic. Furthermore, identical order sequences of two finite groups give an excellent set of hints for constructing an isomorphism, if one such exists. My collegue, Jim Propp, has been using this idea for a while in his classes and I discovered it later. Neither of us can claim originality. Much of the following discussion is paraphrased from Jim's notes.  Order Sequence Order Sequence  The order sequence of a finite group is the sequence whose terms are the respective orders of all the elements of the group, arranged in increasing order.   In the element 0 has order 1, the element 1 has order 3, and the element 2 has order 3, so the order sequence of this group is 1,3,3.  In the element 0 has order 1, the element 1 has order 4, the element 2 has order 2, and the element 3 has order 4, so the order sequence of this group is 1,2,4,4. (Note that we have arranged the numbers 1,4,2,4 in increasing order.)    If and are finite groups and is an isomorphism between them, with and , the order of in equals the order of in .   Consequently:  If two groups are isomorphic, they have the same order sequence.  The theorem is a handy tools for proving that two particular groups are not isomorphic. Consider the group ; the element has order 1 while the other elements , , and each have order 2, implying that the order sequence is 1,2,2,2. Since this is different from the sequence 1,2,4,4, the group is not isomorphic to the group .  Order sequences are also useful in helping one find isomorphisms. Consider the group (the set with mod-5 multiplication). Its order sequence is , which suggests that it might be isomorphic to . In fact, any isomorphism from to must map (the only element of order 1 in ) to (the only element of order 1 in ) and must map (the only element of order 2 in ) to (the only element of order 2 in ). There are only two bijections from to satisfying and , so these are the only two candidate isomorphisms (and both candidates turn out to be true isomorphisms).  The following code will compute the order sequence for the group of integers mod . The default value of is 12 and you can change it in the last line of input.    Conditions for groups to not be isomorphic  How do you decide that two groups are not isomorphic to one another? The negation of and are isomorphic is that no translation rule between and exists. If and have different cardinalities, then no bijection from into can exist. Hence they are not isomorphic. Given that , it is usually impractical to list all bijections from into and show that none of them satisfy Condition b of the formal definition. The best way to prove that two groups are not isomorphic is to find a true statement about one group that is not true about the other group. We illustrate this method in the following checklist that you can apply to most pairs of non-isomorphic groups in this book.  Assume that and are groups. The following are reasons for and to be not isomorphic.   and do not have the same cardinality. For example, can't be isomorphic to and can't be isomorphic to .   is abelian and is not abelian since is always true in , but would not always be true. We have seen two groups with six elements that apply here. They are and the group of rook matrices (see ). The second group is non-abelian, therefore it can't be isomorphic to .   has a certain kind of subgroup that doesn't have. Part (c) of states that this cannot happen if is isomorphic to . and are not isomorphic since has a subgroup with two elements, , while the proper subgroups of are all infinite (convince yourself of this fact!).  The number of solutions of in is not equal to the number of solutions of in . is not isomorphic to since has two solutions, 0 and 4, while is true for all . If the operation in is defined by a table, then the number of solutions of will be the number of occurrences of in the main diagonal of the table. The equations , can also be used in the same way to identify pairs of non-isomorphic groups.  One of the cyclic subgroups of equals (i. e., is cyclic), while none of 's cyclic subgroups equals (i. e., is noncyclic). This is a special case of Condition c. and are not isomorphic since and is not cyclic.     Exercises  State whether each pair of groups below is isomorphic. For each pair that is, give an isomorphism; for those that are not, give your reason.   and   and   and   with symmetric difference and   and   and with matrix addition   and    and the rook matrices   and       Yes, for is an isomorphism.  No, has a two element subgroup while does not.  No. is countable and is not. Therefore, no bijection can exist between them.  Yes.  No.  Yes, one isomorphism is defined by .  Yes, one isomorphism is defined by .  Yes.  Yes .    If you know two natural languages, show that they are not isomorphic.  Prove that the relation is isomorphic to on groups is transitive. Consider three groups , , and with operations , respectively. We want to show that if is isomorphic to , and if is isomorphic to , then is isomorphic to .  If we compose with , we get the function , By and , is a bijection, and if , Therefore, is an isomorphism from into , proving that is isomorphic to is transitive.   Write out the operation table for where is the complex number for which . Show that is isomorphic to .  Solve in by first translating the equation to , solving the equation in , and then translating back to .     The two groups and are isomorphic. One isomorphism is partially defined by . Determine the values of , , and . By (a), must be 1. . Since is a bijection, .  Prove .  Prove that all infinite cyclic groups are isomorphic to . Let be an infinite cyclic group generated by . Then, using multiplicative notation, . The map defined by is an isomorphism. This is indeed a function, since implies . Otherwise, would have a finite order and would not generate .    is one-to-one, since implies , so .   is onto, since for any , .       Prove that is isomorphic to .  Describe how multiplication of nonzero real numbers can be accomplished doing only additions and translations.     Automorphism Inner Prove that if is any group and is some fixed element of , then the function defined by is an isomorphism from into itself. An isomorphism of this type is called an inner automorphism.  Prove that is isomorphic to is an equivalence relation on the set of all groups by expanding on the observations made immediately after the definiton of an isomorphism.  It can be shown that there are five non-isomorphic groups of order eight. You should be able to describe at least three of them. Do so without use of tables. Be sure to explain why they are not isomorphic.  , , and . One other is the fourth dihedral group, introduced in Section 15.3.  In we posed the question of whether the two monoids and , both monoids on the power set of some nonempty universal set , are different or really the same. At the time we didn't have the notion of isomorphism to draw upon. Now that we do, determine whether they are isomorphic monoids.   Prove that the number of 3's in an order sequence is even.  Each 3 is the order of an element whose inverse is it's square; i. e., if has order 3, is distinct from and also has order 3 and contributes a second matching 3.   Prove that the number of 5's an order sequence is a multiple of four.    "
},
{
  "id": "def-isomorphism-informal",
  "level": "2",
  "url": "s-isomorphisms.html#def-isomorphism-informal",
  "type": "Definition",
  "number": "11.7.1",
  "title": "Isomorphic Systems\/Isomorphism - Informal Version.",
  "body": "Isomorphic Systems\/Isomorphism - Informal Version  Two algebraic systems are isomorphic if there exists a translation rule between them so that any true statement in one system can be translated to a true statement in the other. "
},
{
  "id": "ex-greek-arithmetic",
  "level": "2",
  "url": "s-isomorphisms.html#ex-greek-arithmetic",
  "type": "Example",
  "number": "11.7.2",
  "title": "How to Do Greek Arithmetic.",
  "body": "How to Do Greek Arithmetic Imagine that you are a six-year-old child who has been reared in an English-speaking family, has moved to Greece, and has been enrolled in a Greek school. Suppose that your new teacher asks the class to do the following addition problem that has been written out in Greek. The natural thing for you to do is to take out your Greek-English\/English-Greek dictionary and translate the Greek words to English, as outlined in After you've solved the problem, you can consult the same dictionary to find the proper Greek word that the teacher wants. Although this is not the recommended method of learning a foreign language, it will surely yield the correct answer to the problem. Mathematically, we may say that the system of Greek integers with addition ( ) is isomorphic to English integers with addition (plus). The problem of translation between natural languages is more difficult than this though, because two complete natural languages are not isomorphic, or at least the isomorphism between them is not contained in a simple dictionary.   Solution of a Greek arithmetic problem   Solution of a Greek arithmetic problem    "
},
{
  "id": "ex-set-isomorphism",
  "level": "2",
  "url": "s-isomorphisms.html#ex-set-isomorphism",
  "type": "Example",
  "number": "11.7.4",
  "title": "Software Implementation of Sets.",
  "body": "Software Implementation of Sets  In this example, we will describe how set variables can be implemented on a computer. We will describe the two systems first and then describe the isomorphism between them.  System 1: The power set of with the operation union, . For simplicity, we will only discuss union. However, the other operations are implemented in a similar way.  System 2: Strings of five bits of computer memory with an OR gate. Individual bit values are either zero or one, so the elements of this system can be visualized as sequences of five 0's and 1's. An OR gate, , is a small piece of computer hardware that accepts two bit values at any one time and outputs either a zero or one, depending on the inputs. The output of an OR gate is one, except when the two bit values that it accepts are both zero, in which case the output is zero. The operation on this system actually consists of sequentially inputting the values of two bit strings into the OR gate. The result will be a new string of five 0's and 1's. An alternate method of operating in this system is to use five OR gates and to input corresponding pairs of bits from the input strings into the gates concurrently.   Translation between sets and strings of bits   Translation between sets and strings of bits    The Isomorphism: Since each system has only one operation, it is clear that union and the OR gate translate into one another. The translation between sets and bit strings is easiest to describe by showing how to construct a set from a bit string. If , is a bit string in System 2, the set that it translates to contains the number if and only if equals 1. For example, is translated to the set , while the set is translated to Now imagine that your computer is like the child who knows English and must do a Greek problem. To execute a program that has code that includes the set expression , it will follow the same procedure as the child to obtain the result, as shown in .   Translation of a problem in set theory   Translation of a problem in set theory    "
},
{
  "id": "ex-log-multiplication",
  "level": "2",
  "url": "s-isomorphisms.html#ex-log-multiplication",
  "type": "Example",
  "number": "11.7.7",
  "title": "Multiplying without doing multiplication.",
  "body": "Multiplying without doing multiplication  This isomorphism is between and . Until the 1970s, when the price of calculators dropped, multiplication and exponentiation were performed with an isomorphism between these systems. The isomorphism to ) between the two groups is that is translated into and any positive real number is translated to the logarithm of . To translate back from to , you invert the logarithm function. If base ten logarithms are used, an element of , , will be translated to . In pre-calculator days, the translation was done with a table of logarithms or with a slide rule. An example of how the isomorphism is used appears in .   Multiplication using logarithms   Multiplication using logarithms    "
},
{
  "id": "def-group-isomorphism",
  "level": "2",
  "url": "s-isomorphisms.html#def-group-isomorphism",
  "type": "Definition",
  "number": "11.7.9",
  "title": "Group Isomorphism.",
  "body": "Group Isomorphism  Isomorphism Group  is isomorphic to  If and are groups, is an isomorphism from into if:   is a bijection, and   for all    If such a function exists, then we say is isomorphic to , denoted .  "
},
{
  "id": "fig-steps-iso",
  "level": "2",
  "url": "s-isomorphisms.html#fig-steps-iso",
  "type": "Figure",
  "number": "11.7.10",
  "title": "",
  "body": " Steps in proving that and are isomorphic   Steps in proving that and are isomorphic   "
},
{
  "id": "ss-group-isomorphisms-7",
  "level": "2",
  "url": "s-isomorphisms.html#ss-group-isomorphisms-7",
  "type": "Note",
  "number": "11.7.11",
  "title": "",
  "body": "  There could be several different isomorphisms between the same pair of groups. Thus, if you are asked to demonstrate that two groups are isomorphic, your answer need not be unique.  Any application of this definition requires a procedure outlined in . The first condition, that an isomorphism be a bijection, reflects the fact that every true statement in the first group should have exactly one corresponding true statement in the second group. This is exactly why we run into difficulty in translating between two natural languages. To see how Condition (b) of the formal definition is consistent with the informal definition, consider the function defined by . The translation diagram between and for the multiplication problem appears in . We arrive at the same result by computing as we do by computing . If we apply the function to the two results, we get the same image: since . Note that is exactly Condition b of the formal definition applied to the two groups and .   "
},
{
  "id": "fig-log-mult-general",
  "level": "2",
  "url": "s-isomorphisms.html#fig-log-mult-general",
  "type": "Figure",
  "number": "11.7.12",
  "title": "",
  "body": " General Multiplication using logarithms   General Multiplication using logarithms   "
},
{
  "id": "ex-another-iso-pair",
  "level": "2",
  "url": "s-isomorphisms.html#ex-another-iso-pair",
  "type": "Example",
  "number": "11.7.13",
  "title": "",
  "body": " Consider with matrix multiplication. The group is isomorphic to . Our translation rule is the function defined by . Since groups have only one operation, there is no need to state explicitly that addition is translated to matrix multiplication. That is a bijection is clear from its definition.  If and are any real numbers,   We can apply this translation rule to determine the inverse of a matrix in . We know that is a true statement in . Using to translate this statement, we get or therefore,  "
},
{
  "id": "theorem-isomorphism-properties",
  "level": "2",
  "url": "s-isomorphisms.html#theorem-isomorphism-properties",
  "type": "Theorem",
  "number": "11.7.14",
  "title": "Properties of Isomorphisms.",
  "body": "Properties of Isomorphisms  If and are groups with identities and , respectively, and is an isomorphism from into , then:     for all , and  If is a subgroup of , then is a subgroup of and is isomorphic to .   "
},
{
  "id": "s-isomorphisms-5-3",
  "level": "2",
  "url": "s-isomorphisms.html#s-isomorphisms-5-3",
  "type": "Definition",
  "number": "11.7.15",
  "title": "Order Sequence.",
  "body": "Order Sequence Order Sequence  The order sequence of a finite group is the sequence whose terms are the respective orders of all the elements of the group, arranged in increasing order.  "
},
{
  "id": "s-isomorphisms-5-6",
  "level": "2",
  "url": "s-isomorphisms.html#s-isomorphisms-5-6",
  "type": "Theorem",
  "number": "11.7.16",
  "title": "",
  "body": "  If and are finite groups and is an isomorphism between them, with and , the order of in equals the order of in .  "
},
{
  "id": "s-isomorphisms-5-8",
  "level": "2",
  "url": "s-isomorphisms.html#s-isomorphisms-5-8",
  "type": "Corollary",
  "number": "11.7.17",
  "title": "",
  "body": "If two groups are isomorphic, they have the same order sequence. "
},
{
  "id": "sss-exercises-11-7-2",
  "level": "2",
  "url": "s-isomorphisms.html#sss-exercises-11-7-2",
  "type": "Exercise",
  "number": "11.7.4.1",
  "title": "",
  "body": "State whether each pair of groups below is isomorphic. For each pair that is, give an isomorphism; for those that are not, give your reason.   and   and   and   with symmetric difference and   and   and with matrix addition   and    and the rook matrices   and       Yes, for is an isomorphism.  No, has a two element subgroup while does not.  No. is countable and is not. Therefore, no bijection can exist between them.  Yes.  No.  Yes, one isomorphism is defined by .  Yes, one isomorphism is defined by .  Yes.  Yes .   "
},
{
  "id": "sss-exercises-11-7-3",
  "level": "2",
  "url": "s-isomorphisms.html#sss-exercises-11-7-3",
  "type": "Exercise",
  "number": "11.7.4.2",
  "title": "",
  "body": "If you know two natural languages, show that they are not isomorphic. "
},
{
  "id": "sss-exercises-11-7-4",
  "level": "2",
  "url": "s-isomorphisms.html#sss-exercises-11-7-4",
  "type": "Exercise",
  "number": "11.7.4.3",
  "title": "",
  "body": "Prove that the relation is isomorphic to on groups is transitive. Consider three groups , , and with operations , respectively. We want to show that if is isomorphic to , and if is isomorphic to , then is isomorphic to .  If we compose with , we get the function , By and , is a bijection, and if , Therefore, is an isomorphism from into , proving that is isomorphic to is transitive. "
},
{
  "id": "sss-exercises-11-7-5",
  "level": "2",
  "url": "s-isomorphisms.html#sss-exercises-11-7-5",
  "type": "Exercise",
  "number": "11.7.4.4",
  "title": "",
  "body": " Write out the operation table for where is the complex number for which . Show that is isomorphic to .  Solve in by first translating the equation to , solving the equation in , and then translating back to .    "
},
{
  "id": "sss-exercises-11-7-6",
  "level": "2",
  "url": "s-isomorphisms.html#sss-exercises-11-7-6",
  "type": "Exercise",
  "number": "11.7.4.5",
  "title": "",
  "body": "The two groups and are isomorphic. One isomorphism is partially defined by . Determine the values of , , and . By (a), must be 1. . Since is a bijection, . "
},
{
  "id": "sss-exercises-11-7-7",
  "level": "2",
  "url": "s-isomorphisms.html#sss-exercises-11-7-7",
  "type": "Exercise",
  "number": "11.7.4.6",
  "title": "",
  "body": "Prove . "
},
{
  "id": "sss-exercises-11-7-8",
  "level": "2",
  "url": "s-isomorphisms.html#sss-exercises-11-7-8",
  "type": "Exercise",
  "number": "11.7.4.7",
  "title": "",
  "body": "Prove that all infinite cyclic groups are isomorphic to . Let be an infinite cyclic group generated by . Then, using multiplicative notation, . The map defined by is an isomorphism. This is indeed a function, since implies . Otherwise, would have a finite order and would not generate .    is one-to-one, since implies , so .   is onto, since for any , .     "
},
{
  "id": "sss-exercises-11-7-9",
  "level": "2",
  "url": "s-isomorphisms.html#sss-exercises-11-7-9",
  "type": "Exercise",
  "number": "11.7.4.8",
  "title": "",
  "body": " Prove that is isomorphic to .  Describe how multiplication of nonzero real numbers can be accomplished doing only additions and translations.    "
},
{
  "id": "sss-exercises-11-7-10",
  "level": "2",
  "url": "s-isomorphisms.html#sss-exercises-11-7-10",
  "type": "Exercise",
  "number": "11.7.4.9",
  "title": "",
  "body": "Automorphism Inner Prove that if is any group and is some fixed element of , then the function defined by is an isomorphism from into itself. An isomorphism of this type is called an inner automorphism. "
},
{
  "id": "sss-exercises-11-7-11",
  "level": "2",
  "url": "s-isomorphisms.html#sss-exercises-11-7-11",
  "type": "Exercise",
  "number": "11.7.4.10",
  "title": "",
  "body": "Prove that is isomorphic to is an equivalence relation on the set of all groups by expanding on the observations made immediately after the definiton of an isomorphism. "
},
{
  "id": "sss-exercises-11-7-12",
  "level": "2",
  "url": "s-isomorphisms.html#sss-exercises-11-7-12",
  "type": "Exercise",
  "number": "11.7.4.11",
  "title": "",
  "body": "It can be shown that there are five non-isomorphic groups of order eight. You should be able to describe at least three of them. Do so without use of tables. Be sure to explain why they are not isomorphic.  , , and . One other is the fourth dihedral group, introduced in Section 15.3. "
},
{
  "id": "sss-exercises-11-7-13",
  "level": "2",
  "url": "s-isomorphisms.html#sss-exercises-11-7-13",
  "type": "Exercise",
  "number": "11.7.4.12",
  "title": "",
  "body": "In we posed the question of whether the two monoids and , both monoids on the power set of some nonempty universal set , are different or really the same. At the time we didn't have the notion of isomorphism to draw upon. Now that we do, determine whether they are isomorphic monoids.  "
},
{
  "id": "sss-exercises-11-7-14",
  "level": "2",
  "url": "s-isomorphisms.html#sss-exercises-11-7-14",
  "type": "Exercise",
  "number": "11.7.4.13",
  "title": "",
  "body": "Prove that the number of 3's in an order sequence is even.  Each 3 is the order of an element whose inverse is it's square; i. e., if has order 3, is distinct from and also has order 3 and contributes a second matching 3.  "
},
{
  "id": "sss-exercises-11-7-15",
  "level": "2",
  "url": "s-isomorphisms.html#sss-exercises-11-7-15",
  "type": "Exercise",
  "number": "11.7.4.14",
  "title": "",
  "body": "Prove that the number of 5's an order sequence is a multiple of four.  "
},
{
  "id": "s-systems-linear-equations",
  "level": "1",
  "url": "s-systems-linear-equations.html",
  "type": "Section",
  "number": "12.1",
  "title": "Systems of Linear Equations",
  "body": " Systems of Linear Equations  Systems of Linear Equations  Solutions  The method of solving systems of equations by matrices that we will look at is based on procedures involving equations that we are familiar with from previous mathematics courses. The main idea is to reduce a given system of equations to another simpler system that has the same solutions.  Solution Set  Solution Set  Given a system of equations involving real variables , , , the solution set of the system is the set of -tuples in , such that the substitutions , , make all the equations true.  In terms of logic, a solution set is a truth set of a system of equations, which is a proposition over -tuples of real numbers.  In general, if the variables are from a set , then the solution set will be a subset of . For example, in number theory mathematicians study Diophantine equations, where the variables can only take on integer values instead of real values.  Equivalent Systems of Equations  Two systems of linear equations are called equivalent if they have the same set of solutions.  Two equivalent systems The previous definition tells us that if we know that the system is equivalent to the system then both systems have the solution set . In other words, the simultaneous values , , and are the only values of the variables that make all three equations in either system true.   Elementary Operations on Equations  Elementary Operations on Equations  Elementary Operations on Equations   If any sequence of the following operations is performed on a system of equations, the resulting system is equivalent to the original system:  Interchange any two equations in the system.  Multiply both sides of any equation by a nonzero constant.  Multiply both sides of any equation by a nonzero constant and add the result to a second equation in the system, with the sum replacing the latter equation.     Let us now use the above theorem to work out the details of and see how we can arrive at the simpler system.  The original system:   Step 1. We will first change the coefficient of in the first equation to one and then use it as a pivot to obtain 0's for the coefficients of in Equations 2 and 3.  Multiply Equation 1 by to obtain   Multiply Equation 1 by and add the result to Equation 2 to obtain   Multiply Equation 1 by and add the result to Equation 3 to obtain    We've explicitly written terms with zero coefficients such as to make a point that all variables can be thought of as being involved in all equations. After this example is complete, we will discontinue this practice in favor of the normal practice of making these terms disappear.  Step 2. We would now like to proceed in a fashion analogous to Step 1; namely, multiply the coefficient of in the second equation by a suitable number so that the result is 1. Then use it as a pivot to obtain 0's as coefficients for in the first and third equations. This is clearly impossible (Why?), so we will first interchange Equations 2 and 3 and proceed as outlined above.  Exchange Equations 2 and 3 to obtain   Multiply Equation 2 by and subtract the result from Equation 1 to obtain    Step 3. Next, we will change the coefficient of in the third equation to one and then use it as a pivot to obtain 0's for the coefficients of in Equations 1 and 2. Notice that the coefficient of is already zero in Equation 1, so we have been saved some work!  Multiply Equation 3 by to obtain   Multiply Equation 3 by and add the result to Equation 2 to obtain    From the system of equations at the end of Step 3, we see that the solution to the original system is , , and .   Transition to Matrices  In the above sequence of steps, we note that the variables serve the sole purpose of keeping the coefficients in the appropriate location. This we can effect by using matrices. The matrix of the original system in our example is where the matrix of the first three columns is called the coefficient matrix and the complete matrix is referred to as the augmented matrix. Since we are now using matrices to solve the system, we will translate into matrix language.   Elementary Row Operations   Elementary Row Operations  Elementary Row Operations  If any sequence of the following operations is performed on the augmented matrix of a system of equations, the resulting matrix is a system that is equivalent to the original system. The following operations on a matrix are called elementary row operations:  Exchange any two rows of the matrix.  Multiply any row of the matrix by a nonzero constant.  Multiply any row of the matrix by a nonzero constant and add the result to a second row, with the sum replacing that second row.     Row Equivalent Matrices  Row Equivalent Matrices  Two matrices, and , are said to be row-equivalent if one can be obtained from the other by any sequence of zero or more elementary row operations.    If we use the notation to stand for Row of a matrix and to stand for row equivalence, then means that the matrix is obtained from the matrix by multiplying the Row of by and adding the result to Row . The operation of multiplying row by is indicated by while exchanging rows and is denoted by .  The matrix notation for the system given in our first example, with the subsequent steps, is: This again gives us the solution. This procedure is called the Gauss-Jordan elimination method .  It is important to remember when solving any system of equations via this or any similar approach that at any step in the procedure we can rewrite the matrix in equation format to help us to interpret the meaning of the augmented matrix.  In our first example we found a unique solution, only one triple, namely , which satisfies all three equations. For a system involving three unknowns, are there any other possible results? To answer this question, let's review some basic facts from analytic geometry.  The graph of a linear equation in three-dimensional space is a plane. So geometrically we can visualize the three linear equations as three planes in three-space. Certainly the three planes can intersect in a unique point, as in the first example, or two of the planes could be parallel. If two planes are parallel, there are no common points of intersection; that is, there are no triple of real numbers that will satisfy all three equations. Another possibility is that the three planes could intersect along a common axis or line. In this case, there would be an infinite number of real number triples in . Yet another possibility would be if the first two planes intersect in a line, but the line is parallel to, but not on, the third plane, giving us no solution. Finally if all three equations describe the same plane, the solution set would be that plane.  We can generalize these observations. In a system of linear equations, unknowns, there can be  a unique solution,  no solution, or  an infinite number of solutions.   To illustrate these points, consider the following examples:  A system with no solutions  Find all solutions to the system   The reader can verify that the augmented matrix of this system, , reduces to .  We can attempt to row-reduce this matrix further if we wish. However, any further row-reduction will not substantially change the last row, which, in equation form, is , or simply . It is clear that we cannot find real numbers , , and that will satisfy this equation. Hence we cannot find real numbers that will satisfy all three original equations simultaneously. When this occurs, we say that the system has no solution, or the solution set is empty.  A system with an infinite number of solutions Next, let's attempt to find all of the solutions to:  The augmented matrix for the system is which reduces to   If we apply additional elementary row operations to this matrix, it will only become more complicated. In particular, we cannot get a one in the third row, third column. Since the matrix is in simplest form, we will express it in equation format to help us determine the solution set. Any real numbers will satisfy the last equation. However, the first equation can be rewritten as , which describes the coordinate in terms of . Similarly, the second equation gives in terms of . A convenient way of listing the solutions of this system is to use set notation. If we call the solution set of the system , then .  What this means is that if we wanted to list all solutions, we would replace by all possible numbers. Clearly, there is an infinite number of solutions, two of which are and , when takes on the values 0 and 11, respectively.  A Word Of Caution: Frequently we may can get different-looking answers to the same problem when a system has an infinite number of solutions. Assume the solutions set in this example is reported to be . Certainly the result described by looks different from that described by . To see whether they indeed describe the same set, we wish to determine whether every solution produced in can be generated in . For example, the solution generated by when is . The same triple can be produced by by taking . We must prove that every solution described in is described in and, conversely, that every solution described in is described in . (See Exercise 6 of this section.)   To summarize the procedure in the Gauss-Jordan technique for solving systems of equations, we attempt to obtain 1's along the main diagonal of the coefficient matrix with 0's above and below the diagonal. We may find in attempting this that this objective cannot be completed, as in the last two examples we have seen. Depending on the way we interpret the results in equation form, we either recognize that no solution exists, or we identify free variables on which an infinite number of solutions are based. The final matrix forms that we have produced in our examples are referred to as echelon forms .  In practice, larger systems of linear equations are solved using computers. Generally, the Gauss-Jordan algorithm is the most useful; however, slight variations of this algorithm are also used. The different approaches share many of the same advantages and disadvantages. The two major concerns of all methods are:  minimizing inaccuracies due to round-off errors, and  minimizing computer time.     The Gauss-Jordan Algorithm  The accuracy of the Gauss-Jordan method can be improved by always choosing the element with the largest absolute value as the pivot element, as in the following algorithm.  The Gauss-Jordan Algorithm  Gauss-Jordan Algorithm  Given a matrix equation , where A is , let be the augmented matrix . The process of row-reducing to echelon form involves performing the following algorithm where is the row of .  i = 1  j = 1   while i <= n and j <= m) :  maxi=i  for k = i+1 to n:  if abs(C[k,j])>abs(C[maxi,j]) : then maxi=k  if C[maxi,j] != 0 then:  exchange rows i and maxi  divide each entry in row i by C[i,j]  for u = i+1 to n :  subtract C[u,j]*C[i] from C[u]  i = i+1    j=j+1        At the end of this algorithm, with the final form of you can revert back to the equation form of the system and a solution should be clear. In general,  If any row of is all zeros, it can be ignored.  If any row of has all zero entries except for the entry in the position, the system has no solution. Otherwise, if a column has no pivot, the variable corresponding to it is a free variable. Variables corresponding to pivots are basic variables and can be expressed in terms of the free variables.    If we apply to the system the augmented matrix is is reduced to Therefore, is a free variable in the solution and general solution of the system is   This conclusion is easy to see if you revert back to the equations that the final value the reduced matrix represents.     SageMath Note - Matrix Reduction  SageMath Note Matrix Reduction  Given an augmented matrix, , there is a matrix method called echelon_form that can be used to row reduce . Here is the result for the system in . In the assignment of a matrix value to , notice that the first argument is QQ, which indicates that the entries should be rational numbers. As long as all the entries are rational, which is the case here since integers are rational, the row-reduced matrix will be all rational.   If we don't specify the set from which entries are taken, it would assumed to be the integers and we do not get a fully row-reduced matrix. This is because the next step in working with the next output would involve multiplying row 2 by and row 3 by , but these multipliers are not integers.   If we specifying real entries, the result isn't as nice and clean as the rational output.   The default number of decimal places may vary from what you see here, but it can be controlled. The single small number in row three column four isn't exactly zero because of round-off but we could just set it to zero.    Exercises  Solve the following systems by describing the solution sets completely:                          Solve the following systems by describing the solution sets completely:               Given the final augmented matrices below from the Gauss-Jordan Algorithm, identify the solutions sets. Identify the basic and free variables, and describe the solution set of the original system.              Basic variables: , and . Free variable: . Solution set:  Basic variables: and . Free variable: . The solution set is empty because the last row of the matrix converts to the inconsistent equation .  Basic variables: and . Free variable: . Solution set:  Basic variables: , and . Free variable: . Solution set:     Write out the details of .  Write out the details of .  Write out the details of .     Solve the following systems using only mod 5 arithmetic. Your solutions should be from .   (compare your solution to the system in 5(a))              Use the solution set of to list three different solutions to the given system. Then show that each of these solutions can be described by the set in the same example.  Prove that .     Given a system of linear equations in unknowns in matrix form , prove that if is a matrix of all zeros, then the solution set of is a subgroup of . Proof: Since is the matrix of 0{'}s, let's call it . Let S be the set of solutions to . If and be in . Then so ; or in other words, is closed under addition in .  The identity of is , which is in . Finally, let be in . Then and so is also in .    "
},
{
  "id": "def-solution-set",
  "level": "2",
  "url": "s-systems-linear-equations.html#def-solution-set",
  "type": "Definition",
  "number": "12.1.1",
  "title": "Solution Set.",
  "body": "Solution Set  Solution Set  Given a system of equations involving real variables , , , the solution set of the system is the set of -tuples in , such that the substitutions , , make all the equations true. "
},
{
  "id": "def-equivalent-systems",
  "level": "2",
  "url": "s-systems-linear-equations.html#def-equivalent-systems",
  "type": "Definition",
  "number": "12.1.2",
  "title": "Equivalent Systems of Equations.",
  "body": "Equivalent Systems of Equations  Two systems of linear equations are called equivalent if they have the same set of solutions. "
},
{
  "id": "ex-equivalent-systems",
  "level": "2",
  "url": "s-systems-linear-equations.html#ex-equivalent-systems",
  "type": "Example",
  "number": "12.1.3",
  "title": "Two equivalent systems.",
  "body": "Two equivalent systems The previous definition tells us that if we know that the system is equivalent to the system then both systems have the solution set . In other words, the simultaneous values , , and are the only values of the variables that make all three equations in either system true. "
},
{
  "id": "theorem-elem-equation-operations",
  "level": "2",
  "url": "s-systems-linear-equations.html#theorem-elem-equation-operations",
  "type": "Theorem",
  "number": "12.1.4",
  "title": "Elementary Operations on Equations.",
  "body": "Elementary Operations on Equations  Elementary Operations on Equations   If any sequence of the following operations is performed on a system of equations, the resulting system is equivalent to the original system:  Interchange any two equations in the system.  Multiply both sides of any equation by a nonzero constant.  Multiply both sides of any equation by a nonzero constant and add the result to a second equation in the system, with the sum replacing the latter equation.    "
},
{
  "id": "theorem-elementary-row-operations",
  "level": "2",
  "url": "s-systems-linear-equations.html#theorem-elementary-row-operations",
  "type": "Theorem",
  "number": "12.1.5",
  "title": "Elementary Row Operations.",
  "body": " Elementary Row Operations  Elementary Row Operations  If any sequence of the following operations is performed on the augmented matrix of a system of equations, the resulting matrix is a system that is equivalent to the original system. The following operations on a matrix are called elementary row operations:  Exchange any two rows of the matrix.  Multiply any row of the matrix by a nonzero constant.  Multiply any row of the matrix by a nonzero constant and add the result to a second row, with the sum replacing that second row.    "
},
{
  "id": "def-row-equivalent",
  "level": "2",
  "url": "s-systems-linear-equations.html#def-row-equivalent",
  "type": "Definition",
  "number": "12.1.6",
  "title": "Row Equivalent Matrices.",
  "body": "Row Equivalent Matrices  Row Equivalent Matrices  Two matrices, and , are said to be row-equivalent if one can be obtained from the other by any sequence of zero or more elementary row operations.   "
},
{
  "id": "s-systems-linear-equations-6-5",
  "level": "2",
  "url": "s-systems-linear-equations.html#s-systems-linear-equations-6-5",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Gauss-Jordan elimination method "
},
{
  "id": "ex-system-no-solution",
  "level": "2",
  "url": "s-systems-linear-equations.html#ex-system-no-solution",
  "type": "Example",
  "number": "12.1.7",
  "title": "A system with no solutions.",
  "body": "A system with no solutions  Find all solutions to the system   The reader can verify that the augmented matrix of this system, , reduces to .  We can attempt to row-reduce this matrix further if we wish. However, any further row-reduction will not substantially change the last row, which, in equation form, is , or simply . It is clear that we cannot find real numbers , , and that will satisfy this equation. Hence we cannot find real numbers that will satisfy all three original equations simultaneously. When this occurs, we say that the system has no solution, or the solution set is empty. "
},
{
  "id": "ex-system-infinite-solutions",
  "level": "2",
  "url": "s-systems-linear-equations.html#ex-system-infinite-solutions",
  "type": "Example",
  "number": "12.1.8",
  "title": "A system with an infinite number of solutions.",
  "body": "A system with an infinite number of solutions Next, let's attempt to find all of the solutions to:  The augmented matrix for the system is which reduces to   If we apply additional elementary row operations to this matrix, it will only become more complicated. In particular, we cannot get a one in the third row, third column. Since the matrix is in simplest form, we will express it in equation format to help us determine the solution set. Any real numbers will satisfy the last equation. However, the first equation can be rewritten as , which describes the coordinate in terms of . Similarly, the second equation gives in terms of . A convenient way of listing the solutions of this system is to use set notation. If we call the solution set of the system , then .  What this means is that if we wanted to list all solutions, we would replace by all possible numbers. Clearly, there is an infinite number of solutions, two of which are and , when takes on the values 0 and 11, respectively.  A Word Of Caution: Frequently we may can get different-looking answers to the same problem when a system has an infinite number of solutions. Assume the solutions set in this example is reported to be . Certainly the result described by looks different from that described by . To see whether they indeed describe the same set, we wish to determine whether every solution produced in can be generated in . For example, the solution generated by when is . The same triple can be produced by by taking . We must prove that every solution described in is described in and, conversely, that every solution described in is described in . (See Exercise 6 of this section.)  "
},
{
  "id": "s-systems-linear-equations-6-13",
  "level": "2",
  "url": "s-systems-linear-equations.html#s-systems-linear-equations-6-13",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "echelon forms "
},
{
  "id": "algorithm-gauss-jordan",
  "level": "2",
  "url": "s-systems-linear-equations.html#algorithm-gauss-jordan",
  "type": "Algorithm",
  "number": "12.1.9",
  "title": "The Gauss-Jordan Algorithm.",
  "body": "The Gauss-Jordan Algorithm  Gauss-Jordan Algorithm  Given a matrix equation , where A is , let be the augmented matrix . The process of row-reducing to echelon form involves performing the following algorithm where is the row of .  i = 1  j = 1   while i <= n and j <= m) :  maxi=i  for k = i+1 to n:  if abs(C[k,j])>abs(C[maxi,j]) : then maxi=k  if C[maxi,j] != 0 then:  exchange rows i and maxi  divide each entry in row i by C[i,j]  for u = i+1 to n :  subtract C[u,j]*C[i] from C[u]  i = i+1    j=j+1       "
},
{
  "id": "s-systems-linear-equations-7-4",
  "level": "2",
  "url": "s-systems-linear-equations.html#s-systems-linear-equations-7-4",
  "type": "Note",
  "number": "12.1.10",
  "title": "",
  "body": "At the end of this algorithm, with the final form of you can revert back to the equation form of the system and a solution should be clear. In general,  If any row of is all zeros, it can be ignored.  If any row of has all zero entries except for the entry in the position, the system has no solution. Otherwise, if a column has no pivot, the variable corresponding to it is a free variable. Variables corresponding to pivots are basic variables and can be expressed in terms of the free variables.   "
},
{
  "id": "ex-12-1-system4",
  "level": "2",
  "url": "s-systems-linear-equations.html#ex-12-1-system4",
  "type": "Example",
  "number": "12.1.11",
  "title": "",
  "body": "If we apply to the system the augmented matrix is is reduced to Therefore, is a free variable in the solution and general solution of the system is   This conclusion is easy to see if you revert back to the equations that the final value the reduced matrix represents.  "
},
{
  "id": "exercise-12-1-1",
  "level": "2",
  "url": "s-systems-linear-equations.html#exercise-12-1-1",
  "type": "Exercise",
  "number": "12.1.7.1",
  "title": "",
  "body": "Solve the following systems by describing the solution sets completely:                         "
},
{
  "id": "exercises-12-1-3",
  "level": "2",
  "url": "s-systems-linear-equations.html#exercises-12-1-3",
  "type": "Exercise",
  "number": "12.1.7.2",
  "title": "",
  "body": "Solve the following systems by describing the solution sets completely:              "
},
{
  "id": "exercises-12-1-4",
  "level": "2",
  "url": "s-systems-linear-equations.html#exercises-12-1-4",
  "type": "Exercise",
  "number": "12.1.7.3",
  "title": "",
  "body": "Given the final augmented matrices below from the Gauss-Jordan Algorithm, identify the solutions sets. Identify the basic and free variables, and describe the solution set of the original system.              Basic variables: , and . Free variable: . Solution set:  Basic variables: and . Free variable: . The solution set is empty because the last row of the matrix converts to the inconsistent equation .  Basic variables: and . Free variable: . Solution set:  Basic variables: , and . Free variable: . Solution set:   "
},
{
  "id": "exercises-12-1-5",
  "level": "2",
  "url": "s-systems-linear-equations.html#exercises-12-1-5",
  "type": "Exercise",
  "number": "12.1.7.4",
  "title": "",
  "body": " Write out the details of .  Write out the details of .  Write out the details of .    "
},
{
  "id": "exercises-12-1-6",
  "level": "2",
  "url": "s-systems-linear-equations.html#exercises-12-1-6",
  "type": "Exercise",
  "number": "12.1.7.5",
  "title": "",
  "body": "Solve the following systems using only mod 5 arithmetic. Your solutions should be from .   (compare your solution to the system in 5(a))            "
},
{
  "id": "exercises-12-1-7",
  "level": "2",
  "url": "s-systems-linear-equations.html#exercises-12-1-7",
  "type": "Exercise",
  "number": "12.1.7.6",
  "title": "",
  "body": " Use the solution set of to list three different solutions to the given system. Then show that each of these solutions can be described by the set in the same example.  Prove that .    "
},
{
  "id": "exercises-12-1-8",
  "level": "2",
  "url": "s-systems-linear-equations.html#exercises-12-1-8",
  "type": "Exercise",
  "number": "12.1.7.7",
  "title": "",
  "body": "Given a system of linear equations in unknowns in matrix form , prove that if is a matrix of all zeros, then the solution set of is a subgroup of . Proof: Since is the matrix of 0{'}s, let's call it . Let S be the set of solutions to . If and be in . Then so ; or in other words, is closed under addition in .  The identity of is , which is in . Finally, let be in . Then and so is also in .  "
},
{
  "id": "s-matrix-inversion",
  "level": "1",
  "url": "s-matrix-inversion.html",
  "type": "Section",
  "number": "12.2",
  "title": "Matrix Inversion",
  "body": " Matrix Inversion  Matrix Inversion  Developing the Process  In Chapter 5 we defined the inverse of an matrix. We noted that not all matrices have inverses, but when the inverse of a matrix exists, it is unique. This enables us to define the inverse of an matrix as the unique matrix such that , where is the identity matrix. In order to get some practical experience, we developed a formula that allowed us to determine the inverse of invertible matrices. We will now use the Gauss-Jordan procedure for solving systems of linear equations to compute the inverses, when they exist, of matrices, . The following procedure for a matrix can be generalized for matrices, .  Given the matrix , we want to find its inverse, the matrix , if it exists, such that and . We will concentrate on finding a matrix that satisfies the first equation and then verify that B also satisfies the second equation.  The equation is equivalent to  By definition of equality of matrices, this gives us three systems of equations to solve. The augmented matrix of one of the systems, the one equating the first columns of the two matrices is:  Using the Gauss-Jordan algorithm, we have: So and , which gives us the first column of .  The matrix form of the system to obtain , , and , the second column of B, is: which reduces to The critical thing to note here is that the coefficient matrix in is the same as the matrix in , hence the sequence of row operations that we used in row reduction are the same in both cases.  To determine the third column of , we reduce to obtain and . Here again it is important to note that the sequence of row operations used to solve this system is exactly the same as those we used in the first system. Why not save ourselves a considerable amount of time and effort and solve all three systems simultaneously? This we can do this by augmenting the coefficient matrix by the identity matrix . We then have, by applying the same sequence of row operations as above, So that The reader should verify that so that .   The General Method for Computing Inverses  As the following theorem indicates, the verification that is not necessary. The proof of the theorem is beyond the scope of this text. The interested reader can find it in most linear algebra texts.   Let be an matrix. If a matrix can be found such that , then , so that . In fact, to find , we need only find a matrix that satisfies one of the two conditions or .  It is clear from Chapter 5 and our discussions in this chapter that not all matrices have inverses. How do we determine whether a matrix has an inverse using this method? The answer is quite simple: the technique we developed to compute inverses is a matrix approach to solving several systems of equations simultaneously.  Recognition of a non-invertible matrix  The reader can verify that if then the augmented matrix reduces to   Although this matrix can be row-reduced further, it is not necessary to do so since, in equation form, we have:        Clearly, there are no solutions to the first two systems, therefore does not exist. From this discussion it should be obvious to the reader that the zero row of the coefficient matrix together with the nonzero entry in the fourth column of that row in matrix tells us that does not exist.    Exercises  In order to develop an understanding of the technique of this section, work out all the details of .   Use the method of this section to find the inverses of the following matrices whenever possible. If an inverse does not exist, explain why.                 Use the method of this section to find the inverses of the following matrices whenever possible. If an inverse does not exist, explain why.                     The inverse does not exist. When the augmented matrix is row-reduced (see below), the last row of the first half cannot be manipulated to match the identity matrix.    The inverse does not exist.        Find the inverses of the following matrices.         If is a diagonal matrix whose diagonal entries are nonzero, what is ?    Express each system of equations in in the form . When possible, solve each system by first finding the inverse of the matrix of coefficients.  The solutions are in the solution section of Section 12.1, exercise 1, We illustrate with the outline of the solution to part (c). The matrix version of the system is We compute the inverse of the matrix of coefficients and get and    "
},
{
  "id": "theorem-12-2-1",
  "level": "2",
  "url": "s-matrix-inversion.html#theorem-12-2-1",
  "type": "Theorem",
  "number": "12.2.1",
  "title": "",
  "body": " Let be an matrix. If a matrix can be found such that , then , so that . In fact, to find , we need only find a matrix that satisfies one of the two conditions or . "
},
{
  "id": "ex-no-inverse-reduction",
  "level": "2",
  "url": "s-matrix-inversion.html#ex-no-inverse-reduction",
  "type": "Example",
  "number": "12.2.2",
  "title": "Recognition of a non-invertible matrix.",
  "body": "Recognition of a non-invertible matrix  The reader can verify that if then the augmented matrix reduces to   Although this matrix can be row-reduced further, it is not necessary to do so since, in equation form, we have:        Clearly, there are no solutions to the first two systems, therefore does not exist. From this discussion it should be obvious to the reader that the zero row of the coefficient matrix together with the nonzero entry in the fourth column of that row in matrix tells us that does not exist. "
},
{
  "id": "exercises-12-2-2",
  "level": "2",
  "url": "s-matrix-inversion.html#exercises-12-2-2",
  "type": "Exercise",
  "number": "12.2.3.1",
  "title": "",
  "body": "In order to develop an understanding of the technique of this section, work out all the details of .  "
},
{
  "id": "exercises-12-2-3",
  "level": "2",
  "url": "s-matrix-inversion.html#exercises-12-2-3",
  "type": "Exercise",
  "number": "12.2.3.2",
  "title": "",
  "body": "Use the method of this section to find the inverses of the following matrices whenever possible. If an inverse does not exist, explain why.                "
},
{
  "id": "exercises-12-2-4",
  "level": "2",
  "url": "s-matrix-inversion.html#exercises-12-2-4",
  "type": "Exercise",
  "number": "12.2.3.3",
  "title": "",
  "body": "Use the method of this section to find the inverses of the following matrices whenever possible. If an inverse does not exist, explain why.                     The inverse does not exist. When the augmented matrix is row-reduced (see below), the last row of the first half cannot be manipulated to match the identity matrix.    The inverse does not exist.     "
},
{
  "id": "exercises-12-2-5",
  "level": "2",
  "url": "s-matrix-inversion.html#exercises-12-2-5",
  "type": "Exercise",
  "number": "12.2.3.4",
  "title": "",
  "body": "  Find the inverses of the following matrices.         If is a diagonal matrix whose diagonal entries are nonzero, what is ?   "
},
{
  "id": "exercises-12-2-6",
  "level": "2",
  "url": "s-matrix-inversion.html#exercises-12-2-6",
  "type": "Exercise",
  "number": "12.2.3.5",
  "title": "",
  "body": "Express each system of equations in in the form . When possible, solve each system by first finding the inverse of the matrix of coefficients.  The solutions are in the solution section of Section 12.1, exercise 1, We illustrate with the outline of the solution to part (c). The matrix version of the system is We compute the inverse of the matrix of coefficients and get and  "
},
{
  "id": "s-intro-to-vector-spaces",
  "level": "1",
  "url": "s-intro-to-vector-spaces.html",
  "type": "Section",
  "number": "12.3",
  "title": "An Introduction to Vector Spaces",
  "body": " An Introduction to Vector Spaces  Vector Spaces  Motivation for the study of vector spaces   When we encountered various types of matrices in Chapter 5, it became apparent that a particular kind of matrix, the diagonal matrix, was much easier to use in computations. For example, if , then can be found, but its computation is tedious. If then Even when presented with a non-diagonal matrix, we will see that it is sometimes possible to do a bit of work to be able to work with a diagonal matrix. This process is called diagonalization .  In a variety of applications it is beneficial to be able to diagonalize a matrix. In this section we will investigate what this means and consider a few applications. In order to understand when the diagonalization process can be performed, it is necessary to develop several of the underlying concepts of linear algebra.   Vector Spaces  By now, you realize that mathematicians tend to generalize. Once we have found a good thing, something that is useful, we apply it to as many different concepts as possible. In doing so, we frequently find that the different concepts are not really different but only look different. Four sentences in four different languages might look dissimilar, but when they are translated into a common language, they might very well express the exact same idea.  Early in the development of mathematics, the concept of a vector led to a variety of applications in physics and engineering. We can certainly picture vectors, or arrows, in the and even in the three-dimensional space. Does it make sense to talk about vectors in four-dimensional space, in ten-dimensional space, or in any other mathematical situation? If so, what is the essence of a vector? Is it its shape or the rules it follows? The shape in two- or three-space is just a picture, or geometric interpretation, of a vector. The essence is the rules, or properties, we wish vectors to follow so we can manipulate them algebraically. What follows is a definition of what is called a vector space. It is a list of all the essential properties of vectors, and it is the basic definition of the branch of mathematics called linear algebra.  Vector Space Vector Space  Let be any nonempty set of objects. Define on an operation, called addition, for any two elements , and denote this operation by . Let scalar multiplication be defined for a real number and any element and denote this operation by . The set together with operations of addition and scalar multiplication is called a vector space over if the following hold for all , and :      There exists a vector , such that for all .  For each vector , there exists a unique vector , such that .  These are the main properties associated with the operation of addition. They can be summarized by saying that is an abelian group.  The next four properties are associated with the operation of scalar multiplication and how it relates to vector addition.           .    In a vector space it is common to call the elements of vectors and those from scalars. Vector spaces over the real numbers are also called real vector spaces.  A Vector Space of Matrices Let and let the operations of addition and scalar multiplication be the usual operations of addition and scalar multiplication on matrices. Then together with these operations is a real vector space. The reader is strongly encouraged to verify the definition for this example before proceeding further (see Exercise 3 of this section). Note we can call the elements of vectors even though they are not arrows.  The Vector Space Let . If we define addition and scalar multiplication the natural way, that is, as we would on matrices, then is a vector space over . See of this section.  In this example, we have the bonus that we can illustrate the algebraic concept geometrically. In mathematics, a geometric bonus does not always occur and is not necessary for the development or application of the concept. However, geometric illustrations are quite useful in helping us understand concepts and should be utilized whenever available.   Sum of two vectors in    Sum of two vectors in    Let's consider some illustrations of the vector space . Let and . We illustrate the vector as a directed line segment, or arrow, from the point to the point . The vectors and are as shown in together with . The vector is a vector in the same direction as , but with twice its length.    The common convention is to use that boldface letters toward the end of the alphabet for vectors, while letters early in the alphabet are scalars.  A common alternate notation for vectors is to place an arrow about a variable to indicate that it is a vector such as this: .  The vector is referred to as an -tuple.  For those familiar with vector calculus, we are expressing the vector as . This allows us to discuss vectors in in much simpler notation.     In many situations a vector space is given and we would like to describe the whole vector space by the smallest number of essential reference vectors. An example of this is the description of , the -plane, via the and axes. Again our concepts must be algebraic in nature so we are not restricted solely to geometric considerations.  Linear Combination Linear Combination. A vector in vector space (over ) is a linear combination of the vectors , , if there exist scalars in such that   A Basic Example The vector in is a linear combination of the vectors and since .  A little less obvious example  Prove that the vector is a linear combination of the vectors (3, 1) and (1, 4).  By the definition we must show that there exist scalars and such that: This system has the solution , .  Hence, if we replace and both by 1, then the two vectors (3, 1) and (1, 4) produce, or generate, the vector (4,5). Of course, if we replace and by different scalars, we can generate more vectors from . If, for example, and , then   Will the vectors and generate any vector we choose in ? To see if this is so, we let be an arbitrary vector in and see if we can always find scalars and such that . This is equivalent to solving the following system of equations: which always has solutions for and , regardless of the values of the real numbers and . Why? We formalize this situation in a definition:  Generation of a Vector Space  Generate Span  Let be a set of vectors in a vector space over . This set is said to generate , or span, if, for any given vector , we can always find scalars , , such that . A set that generates a vector space is called a generating set .  We now give a geometric interpretation of the previous examples.  We know that the standard coordinate system, axis and axis, were introduced in basic algebra in order to describe all points in the -plane algebraically. It is also quite clear that to describe any point in the plane we need exactly two axes.  We can set up a new coordinate system in the following way. Draw the vector and an axis from the origin through (3, 1) and label it the axis. Also draw the vector and an axis from the origin through to be labeled the axis. Draw the coordinate grid for the axis, that is, lines parallel, and let the unit lengths of this new plane be the lengths of the respective vectors, and , so that we obtain .  From and , we see that any vector on the plane can be described using the standard -axes or our new -axes. Hence the position which had the name in reference to the standard axes has the name with respect to the axes, or, in the phraseology of linear algebra, the coordinates of the point with respect to the axes are .   Two sets of axes for the plane   Two sets of axes for the plane    One point, Two position descriptions From we found that if we choose and , then the two vectors and generate the vector . Another geometric interpretation of this problem is that the coordinates of the position with respect to the axes of is . In other words, a position in the plane has the name in reference to the -axes and the same position has the name in reference to the axes.  From the above, it is clear that we can use different axes to describe points or vectors in the plane. No matter what choice we use, we want to be able to describe each position in a unique manner. This is not the case in . Any point in the plane could be described via the axes, the axes or the axes. Therefore, in this case, a single point would have three different names, a very confusing situation.   Three axes on a plane   Three axes on a plane     We formalize the our observations in the previous examples in two definitions and a theorem.   Linear Independence\/Linear Dependence  Linear Independence  Linear Dependence  A set of vectors from a real vector space is linearly independent if the only solution to the equation is . Otherwise the set is called a linearly dependent set.   Basis  Basis  A set of vectors is a basis for a vector space if:  generates , and  is linearly independent.    The fundamental property of a basis  If is a basis for a vector space V over , then any vector can be uniquely expressed as a linear combination of the .   Assume that is a basis for over . We must prove two facts:  each vector can be expressed as a linear combination of the , and  each such expression is unique.    Part 1 is trivial since a basis, by its definition, must generate all of .  The proof of part 2 is a bit more difficult. We follow the standard approach for any uniqueness facts. Let be any vector in and assume that there are two different ways of expressing , namely and where at least one is different from the corresponding . Then equating these two linear combinations we get so that  Now a crucial observation: since the form a linearly independent set, the only solution to the previous equation is that each of the coefficients must equal zero, so for . Hence , for all . This contradicts our assumption that at least one is different from the corresponding , so each vector can be expressed in one and only one way.   This theorem, together with the previous examples, gives us a clear insight into the significance of linear independence, namely uniqueness in representing any vector.  Another basis for Prove that is a basis for over and explain what this means geometrically.  First we show that the vectors and generate all of . We can do this by imitating and leave it to the reader (see of this section). Secondly, we must prove that the set is linearly independent.  Let and be scalars such that . We must prove that the only solution to the equation is that and must both equal zero. The above equation becomes which gives us the system The augmented matrix of this system reduces in such way that the only solution is the trivial one of all zeros: Therefore, the set is linearly independent.   To explain the results geometrically, note through Exercise 12, part a, that the coordinates of each vector can be determined uniquely using the vectors (1,1) and (-1, 1). The concept of dimension is quite obvious for those vector spaces that have an immediate geometric interpretation. For example, the dimension of is two and that of is three. How can we define the concept of dimension algebraically so that the resulting definition correlates with that of and ? First we need a theorem, which we will state without proof.  Basis Size is Constant   If is a vector space with a basis containing elements, then all bases of contain elements.  Dimension of a Vector Space  Dimension of a Vector Space  The dimension of vector space  Let be a vector space over with basis . Then the dimension of is . We use the notation to indicate that is -dimensional.    Exercises   If , , , , and verify that all properties of the definition of a vector space are true for with these values.   Let , , , ,and . Verify that all properties of the definition of a vector space are true for for these values.    Verify that is a vector space over . What is its dimension?  Is a vector space over ? If so, what is its dimension?    The dimension of is 6 and yes, is also a vector space of dimension . One basis for is where is the matrix with entries all equal to zero except for in row , column where the entry is 1.    Verify that is a vector space over .  Is a vector space over for every positive integer ?      Let ; that is, is the set of all polynomials in having real coefficients with degree less than or equal to three. Verify that is a vector space over . What is its dimension?   For each of the following, express the vector as a linear combination of the vectors and .   , , and   , , and   , , and     Express the vector , as a linear combination of , , and  If the matrices are named , , , , and , then   Express the vector as a linear combination of the vectors 1, , , and .     Show that the set generates for each of the parts in Exercise 6 of this section.  Show that generates where , , and .  Create a set of four or more vectors that generates .  What is the smallest number of vectors needed to generate ? ?  Show that the set generates  Show that generates .      If , , and , then . If , , and , then .  If is any vector in , then  One solution is to add any vector(s) to , , and of part b.  2,     .    Complete by showing that generates .     Prove that is a basis for over .  Prove that is a basis for over .  Prove that is a basis for over .  Prove that the sets in Exercise 9, parts e and f, form bases of the respective vector spaces.      The set is linearly independent: let and be scalars such that , then which has as its only solutions. The set generates all of : let be an arbitrary vector in . We want to show that we can always find scalars and such that . This is equivalent to finding scalars such that and . This system has a unique solution , and . Therefore, the set generates .    Determine the coordinates of the points or vectors , , and with respect to the basis of . Interpret your results geometrically.  Determine the coordinates of the points or vector with respect to the basis . Explain why this basis is called the standard basis for .       Let , , and . Find and .  Let , and . Find and .  Let , , and . Find and .  Are the vector spaces , and isomorphic to each other? Discuss with reference to previous parts of this exercise.    The answer to the last part is that the three vector spaces are all isomorphic to one another. Once you have completed part (a) of this exercise, the following translation rules will give you the answer to parts (b) and (c),    "
},
{
  "id": "s-intro-to-vector-spaces-3-3",
  "level": "2",
  "url": "s-intro-to-vector-spaces.html#s-intro-to-vector-spaces-3-3",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "diagonalization "
},
{
  "id": "def-vector-space",
  "level": "2",
  "url": "s-intro-to-vector-spaces.html#def-vector-space",
  "type": "Definition",
  "number": "12.3.1",
  "title": "Vector Space.",
  "body": "Vector Space Vector Space  Let be any nonempty set of objects. Define on an operation, called addition, for any two elements , and denote this operation by . Let scalar multiplication be defined for a real number and any element and denote this operation by . The set together with operations of addition and scalar multiplication is called a vector space over if the following hold for all , and :      There exists a vector , such that for all .  For each vector , there exists a unique vector , such that .  These are the main properties associated with the operation of addition. They can be summarized by saying that is an abelian group.  The next four properties are associated with the operation of scalar multiplication and how it relates to vector addition.           .   "
},
{
  "id": "ex-matrices-as-vectors",
  "level": "2",
  "url": "s-intro-to-vector-spaces.html#ex-matrices-as-vectors",
  "type": "Example",
  "number": "12.3.2",
  "title": "A Vector Space of Matrices.",
  "body": "A Vector Space of Matrices Let and let the operations of addition and scalar multiplication be the usual operations of addition and scalar multiplication on matrices. Then together with these operations is a real vector space. The reader is strongly encouraged to verify the definition for this example before proceeding further (see Exercise 3 of this section). Note we can call the elements of vectors even though they are not arrows. "
},
{
  "id": "ex-vector-space-r2",
  "level": "2",
  "url": "s-intro-to-vector-spaces.html#ex-vector-space-r2",
  "type": "Example",
  "number": "12.3.3",
  "title": "The Vector Space <span xmlns:pf=\"https:\/\/prefigure.org\" class=\"process-math\">\\(\\mathbb{R}^2\\)<\/span>.",
  "body": "The Vector Space Let . If we define addition and scalar multiplication the natural way, that is, as we would on matrices, then is a vector space over . See of this section.  In this example, we have the bonus that we can illustrate the algebraic concept geometrically. In mathematics, a geometric bonus does not always occur and is not necessary for the development or application of the concept. However, geometric illustrations are quite useful in helping us understand concepts and should be utilized whenever available.   Sum of two vectors in    Sum of two vectors in    Let's consider some illustrations of the vector space . Let and . We illustrate the vector as a directed line segment, or arrow, from the point to the point . The vectors and are as shown in together with . The vector is a vector in the same direction as , but with twice its length.  "
},
{
  "id": "s-intro-to-vector-spaces-4-8",
  "level": "2",
  "url": "s-intro-to-vector-spaces.html#s-intro-to-vector-spaces-4-8",
  "type": "Note",
  "number": "12.3.5",
  "title": "",
  "body": " The common convention is to use that boldface letters toward the end of the alphabet for vectors, while letters early in the alphabet are scalars.  A common alternate notation for vectors is to place an arrow about a variable to indicate that it is a vector such as this: .  The vector is referred to as an -tuple.  For those familiar with vector calculus, we are expressing the vector as . This allows us to discuss vectors in in much simpler notation.    "
},
{
  "id": "def-linear-combination",
  "level": "2",
  "url": "s-intro-to-vector-spaces.html#def-linear-combination",
  "type": "Definition",
  "number": "12.3.6",
  "title": "Linear Combination.",
  "body": "Linear Combination Linear Combination. A vector in vector space (over ) is a linear combination of the vectors , , if there exist scalars in such that  "
},
{
  "id": "ex-basic-linear-combination",
  "level": "2",
  "url": "s-intro-to-vector-spaces.html#ex-basic-linear-combination",
  "type": "Example",
  "number": "12.3.7",
  "title": "A Basic Example.",
  "body": "A Basic Example The vector in is a linear combination of the vectors and since . "
},
{
  "id": "ex-lc-less-obvious",
  "level": "2",
  "url": "s-intro-to-vector-spaces.html#ex-lc-less-obvious",
  "type": "Example",
  "number": "12.3.8",
  "title": "A little less obvious example.",
  "body": "A little less obvious example  Prove that the vector is a linear combination of the vectors (3, 1) and (1, 4).  By the definition we must show that there exist scalars and such that: This system has the solution , .  Hence, if we replace and both by 1, then the two vectors (3, 1) and (1, 4) produce, or generate, the vector (4,5). Of course, if we replace and by different scalars, we can generate more vectors from . If, for example, and , then  "
},
{
  "id": "def-generate",
  "level": "2",
  "url": "s-intro-to-vector-spaces.html#def-generate",
  "type": "Definition",
  "number": "12.3.9",
  "title": "Generation of a Vector Space.",
  "body": "Generation of a Vector Space  Generate Span  Let be a set of vectors in a vector space over . This set is said to generate , or span, if, for any given vector , we can always find scalars , , such that . A set that generates a vector space is called a generating set . "
},
{
  "id": "fig-two-sets-of-axes",
  "level": "2",
  "url": "s-intro-to-vector-spaces.html#fig-two-sets-of-axes",
  "type": "Figure",
  "number": "12.3.10",
  "title": "",
  "body": " Two sets of axes for the plane   Two sets of axes for the plane   "
},
{
  "id": "ex-two-positions",
  "level": "2",
  "url": "s-intro-to-vector-spaces.html#ex-two-positions",
  "type": "Example",
  "number": "12.3.11",
  "title": "One point, Two position descriptions.",
  "body": "One point, Two position descriptions From we found that if we choose and , then the two vectors and generate the vector . Another geometric interpretation of this problem is that the coordinates of the position with respect to the axes of is . In other words, a position in the plane has the name in reference to the -axes and the same position has the name in reference to the axes.  From the above, it is clear that we can use different axes to describe points or vectors in the plane. No matter what choice we use, we want to be able to describe each position in a unique manner. This is not the case in . Any point in the plane could be described via the axes, the axes or the axes. Therefore, in this case, a single point would have three different names, a very confusing situation.   Three axes on a plane   Three axes on a plane    "
},
{
  "id": "def-linear-independence",
  "level": "2",
  "url": "s-intro-to-vector-spaces.html#def-linear-independence",
  "type": "Definition",
  "number": "12.3.13",
  "title": "Linear Independence\/Linear Dependence.",
  "body": " Linear Independence\/Linear Dependence  Linear Independence  Linear Dependence  A set of vectors from a real vector space is linearly independent if the only solution to the equation is . Otherwise the set is called a linearly dependent set.  "
},
{
  "id": "def-basis",
  "level": "2",
  "url": "s-intro-to-vector-spaces.html#def-basis",
  "type": "Definition",
  "number": "12.3.14",
  "title": "Basis.",
  "body": "Basis  Basis  A set of vectors is a basis for a vector space if:  generates , and  is linearly independent.   "
},
{
  "id": "theorem-basis-property",
  "level": "2",
  "url": "s-intro-to-vector-spaces.html#theorem-basis-property",
  "type": "Theorem",
  "number": "12.3.15",
  "title": "The fundamental property of a basis.",
  "body": "The fundamental property of a basis  If is a basis for a vector space V over , then any vector can be uniquely expressed as a linear combination of the .   Assume that is a basis for over . We must prove two facts:  each vector can be expressed as a linear combination of the , and  each such expression is unique.    Part 1 is trivial since a basis, by its definition, must generate all of .  The proof of part 2 is a bit more difficult. We follow the standard approach for any uniqueness facts. Let be any vector in and assume that there are two different ways of expressing , namely and where at least one is different from the corresponding . Then equating these two linear combinations we get so that  Now a crucial observation: since the form a linearly independent set, the only solution to the previous equation is that each of the coefficients must equal zero, so for . Hence , for all . This contradicts our assumption that at least one is different from the corresponding , so each vector can be expressed in one and only one way.  "
},
{
  "id": "ex-another-basis-for-r2",
  "level": "2",
  "url": "s-intro-to-vector-spaces.html#ex-another-basis-for-r2",
  "type": "Example",
  "number": "12.3.16",
  "title": "Another basis for <span xmlns:pf=\"https:\/\/prefigure.org\" class=\"process-math\">\\(\\mathbb{R}^2\\)<\/span>.",
  "body": "Another basis for Prove that is a basis for over and explain what this means geometrically.  First we show that the vectors and generate all of . We can do this by imitating and leave it to the reader (see of this section). Secondly, we must prove that the set is linearly independent.  Let and be scalars such that . We must prove that the only solution to the equation is that and must both equal zero. The above equation becomes which gives us the system The augmented matrix of this system reduces in such way that the only solution is the trivial one of all zeros: Therefore, the set is linearly independent.  "
},
{
  "id": "theorem-basis-size",
  "level": "2",
  "url": "s-intro-to-vector-spaces.html#theorem-basis-size",
  "type": "Theorem",
  "number": "12.3.17",
  "title": "Basis Size is Constant.",
  "body": "Basis Size is Constant   If is a vector space with a basis containing elements, then all bases of contain elements. "
},
{
  "id": "def-def-dimension",
  "level": "2",
  "url": "s-intro-to-vector-spaces.html#def-def-dimension",
  "type": "Definition",
  "number": "12.3.18",
  "title": "Dimension of a Vector Space.",
  "body": "Dimension of a Vector Space  Dimension of a Vector Space  The dimension of vector space  Let be a vector space over with basis . Then the dimension of is . We use the notation to indicate that is -dimensional. "
},
{
  "id": "exercises-12-3-2",
  "level": "2",
  "url": "s-intro-to-vector-spaces.html#exercises-12-3-2",
  "type": "Exercise",
  "number": "12.3.3.1",
  "title": "",
  "body": " If , , , , and verify that all properties of the definition of a vector space are true for with these values.  "
},
{
  "id": "exercises-12-3-3",
  "level": "2",
  "url": "s-intro-to-vector-spaces.html#exercises-12-3-3",
  "type": "Exercise",
  "number": "12.3.3.2",
  "title": "",
  "body": "Let , , , ,and . Verify that all properties of the definition of a vector space are true for for these values.  "
},
{
  "id": "exercises-12-3-4",
  "level": "2",
  "url": "s-intro-to-vector-spaces.html#exercises-12-3-4",
  "type": "Exercise",
  "number": "12.3.3.3",
  "title": "",
  "body": " Verify that is a vector space over . What is its dimension?  Is a vector space over ? If so, what is its dimension?    The dimension of is 6 and yes, is also a vector space of dimension . One basis for is where is the matrix with entries all equal to zero except for in row , column where the entry is 1.  "
},
{
  "id": "exercise-12-3-4",
  "level": "2",
  "url": "s-intro-to-vector-spaces.html#exercise-12-3-4",
  "type": "Exercise",
  "number": "12.3.3.4",
  "title": "",
  "body": " Verify that is a vector space over .  Is a vector space over for every positive integer ?    "
},
{
  "id": "exercises-12-3-6",
  "level": "2",
  "url": "s-intro-to-vector-spaces.html#exercises-12-3-6",
  "type": "Exercise",
  "number": "12.3.3.5",
  "title": "",
  "body": " Let ; that is, is the set of all polynomials in having real coefficients with degree less than or equal to three. Verify that is a vector space over . What is its dimension?  "
},
{
  "id": "exercises-12-3-7",
  "level": "2",
  "url": "s-intro-to-vector-spaces.html#exercises-12-3-7",
  "type": "Exercise",
  "number": "12.3.3.6",
  "title": "",
  "body": "For each of the following, express the vector as a linear combination of the vectors and .   , , and   , , and   , , and    "
},
{
  "id": "exercises-12-3-8",
  "level": "2",
  "url": "s-intro-to-vector-spaces.html#exercises-12-3-8",
  "type": "Exercise",
  "number": "12.3.3.7",
  "title": "",
  "body": "Express the vector , as a linear combination of , , and  If the matrices are named , , , , and , then  "
},
{
  "id": "exercises-12-3-9",
  "level": "2",
  "url": "s-intro-to-vector-spaces.html#exercises-12-3-9",
  "type": "Exercise",
  "number": "12.3.3.8",
  "title": "",
  "body": "Express the vector as a linear combination of the vectors 1, , , and .  "
},
{
  "id": "exercises-12-3-10",
  "level": "2",
  "url": "s-intro-to-vector-spaces.html#exercises-12-3-10",
  "type": "Exercise",
  "number": "12.3.3.9",
  "title": "",
  "body": "  Show that the set generates for each of the parts in Exercise 6 of this section.  Show that generates where , , and .  Create a set of four or more vectors that generates .  What is the smallest number of vectors needed to generate ? ?  Show that the set generates  Show that generates .      If , , and , then . If , , and , then .  If is any vector in , then  One solution is to add any vector(s) to , , and of part b.  2,     .   "
},
{
  "id": "exercise-12-3-10",
  "level": "2",
  "url": "s-intro-to-vector-spaces.html#exercise-12-3-10",
  "type": "Exercise",
  "number": "12.3.3.10",
  "title": "",
  "body": "Complete by showing that generates .  "
},
{
  "id": "exercises-12-3-12",
  "level": "2",
  "url": "s-intro-to-vector-spaces.html#exercises-12-3-12",
  "type": "Exercise",
  "number": "12.3.3.11",
  "title": "",
  "body": "  Prove that is a basis for over .  Prove that is a basis for over .  Prove that is a basis for over .  Prove that the sets in Exercise 9, parts e and f, form bases of the respective vector spaces.      The set is linearly independent: let and be scalars such that , then which has as its only solutions. The set generates all of : let be an arbitrary vector in . We want to show that we can always find scalars and such that . This is equivalent to finding scalars such that and . This system has a unique solution , and . Therefore, the set generates . "
},
{
  "id": "exercises-12-3-13",
  "level": "2",
  "url": "s-intro-to-vector-spaces.html#exercises-12-3-13",
  "type": "Exercise",
  "number": "12.3.3.12",
  "title": "",
  "body": "  Determine the coordinates of the points or vectors , , and with respect to the basis of . Interpret your results geometrically.  Determine the coordinates of the points or vector with respect to the basis . Explain why this basis is called the standard basis for .    "
},
{
  "id": "exercises-12-3-14",
  "level": "2",
  "url": "s-intro-to-vector-spaces.html#exercises-12-3-14",
  "type": "Exercise",
  "number": "12.3.3.13",
  "title": "",
  "body": "  Let , , and . Find and .  Let , and . Find and .  Let , , and . Find and .  Are the vector spaces , and isomorphic to each other? Discuss with reference to previous parts of this exercise.    The answer to the last part is that the three vector spaces are all isomorphic to one another. Once you have completed part (a) of this exercise, the following translation rules will give you the answer to parts (b) and (c),  "
},
{
  "id": "s-the-diagonalization-process",
  "level": "1",
  "url": "s-the-diagonalization-process.html",
  "type": "Section",
  "number": "12.4",
  "title": "The Diagonalization Process",
  "body": " The Diagonalization Process  Diagonalization Process, The  Eigenvalues and Eigenvectors  We now have the background to understand the main ideas behind the diagonalization process.  Eigenvalue, Eigenvector  Eigenvalue  Eigenvector  Let be an matrix over . is an eigenvalue of if for some nonzero column vector we have . is called an eigenvector corresponding to the eigenvalue  .  Examples of eigenvalues and eigenvectors Find the eigenvalues and corresponding eigenvectors of the matrix .  We want to find nonzero vectors and real numbers such that   The last matrix equation will have nonzero solutions if and only if or , which simplifies to . Therefore, the solutions to this quadratic equation, and , are the eigenvalues of . We now have to find eigenvectors associated with each eigenvalue.  Case 1. For , becomes: which reduces to the single equation, . From this, . This means the solution set of this equation is (in column notation) So any column vector of the form where is any nonzero real number is an eigenvector associated with . The reader should verify that, for example, so that is an eigenvector associated with eigenvalue 1.  Case 2. For  becomes: which reduces to the single equation , so that . The solution set of the equation is Therefore, all eigenvectors of associated with the eigenvalue are of the form , where can be any nonzero number.  The following theorems summarize the most important aspects of the previous example.  Characterization of Eigenvalues of a Square Matrix  Let be any matrix over . Then is an eigenvalue of if and only if .   The equation is called the characteristic equation , and the left side of this equation is called the characteristic polynomial of .  Linear Independence of Eigenvectors  Nonzero eigenvectors corresponding to distinct eigenvalues are linearly independent.  The solution space of is called the eigenspace of corresponding to . This terminology is justified by Exercise 2 of this section.   Diagonalization  We now consider the main aim of this section. Given an (square) matrix , we would like to transform into a diagonal matrix , perform our tasks with the simpler matrix , and then describe the results in terms of the given matrix .   Diagonalizable Matrix  Diagonalizable Matrix  An matrix is called diagonalizable if there exists an invertible matrix such that is a diagonal matrix . The matrix is said to diagonalize the matrix .   Diagonalization of a Matrix  We will now diagonalize the matrix of . We form the matrix as follows: Let be the first column of . Choose for any eigenvector from . We may as well choose a simple vector in so is our candidate.  Similarly, let be the second column of , and choose for any eigenvector from . The vector is a reasonable choice, thus so that Notice that the elements on the main diagonal of are the eigenvalues of , where is the eigenvalue corresponding to the eigenvector .   The first step in the diagonalization process is the determination of the eigenvalues. The ordering of the eigenvalues is purely arbitrary. If we designate and , the columns of would be interchanged and would be (see Exercise 3b of this section). Nonetheless, the final outcome of the application to which we are applying the diagonalization process would be the same.  If is an matrix with distinct eigenvalues, then is also an matrix whose columns , , are linearly independent vectors.     Diagonalization of a 3 by 3 matrix Diagonalize the matrix .  First, we find the eigenvalues of . Hence, the equation becomes Therefore, our eigenvalues for are and . We note that we do not have three distinct eigenvalues, but we proceed as in the previous example.  Case 1. For the equation becomes We can row reduce the matrix of coefficients to .  The matrix equation is then equivalent to the equations . Therefore, the solution set, or eigenspace, corresponding to consists of vectors of the form   Therefore is an eigenvector corresponding to the eigenvalue , and can be used for our first column of :   Before we continue we make the observation: is a subspace of with basis and .  Case 2. If , then the equation becomes   Without the aid of any computer technology, it should be clear that all three equations that correspond to this matrix equation are equivalent to , or . Notice that can take on any value, so any vector of the form will solve the matrix equation.  We note that the solution set contains two independent variables, and . Further, note that we cannot express the eigenspace as a linear combination of a single vector as in Case 1. However, it can be written as   We can replace any vector in a basis is with a nonzero multiple of that vector. Simply for aesthetic reasons, we will multiply the second vector that generates by 2. Therefore, the eigenspace is a subspace of with basis and so .  What this means with respect to the diagonalization process is that gives us both Column 2 and Column 3 the diagonalizing matrix. The order is not important so we have  The reader can verify (see Exercise 5 of this section) that and   In doing , the given matrix produced only two, not three, distinct eigenvalues, yet we were still able to diagonalize . The reason we were able to do so was because we were able to find three linearly independent eigenvectors. Again, the main idea is to produce a matrix that does the diagonalizing. If is an matrix, will be an matrix, and its columns must be linearly independent eigenvectors. The main question in the study of diagonalizability is When can it be done? This is summarized in the following theorem.  A condition for diagonalizability Let be an matrix. Then is diagonalizable if and only if has linearly independent eigenvectors.  Outline of a proof: ( ) Assume that has linearly independent eigenvectors, , with corresponding eigenvalues , , . We want to prove that is diagonalizable. Column of the matrix is (see Exercise 7 of this section). Then, since the is an eigenvector of associated with the eigenvalue we have for . But this means that , where is the diagonal matrix with diagonal entries , , . If we multiply both sides of the equation by we get the desired .  ( ) The proof in this direction involves a concept that is not covered in this text (rank of a matrix); so we refer the interested reader to virtually any linear algebra text for a proof.  We now give an example of a matrix that is not diagonalizable.   A Matrix that is Not Diagonalizable  Let us attempt to diagonalize the matrix  First, we determine the eigenvalues. Therefore there are two eigenvalues, and . Since is an eigenvalue of degree one, it will have an eigenspace of dimension 1. Since is a double root of the characteristic equation, the dimension of its eigenspace must be 2 in order to be able to diagonalize.  Case 1. For , the equation becomes   Row reduction of this system reveals one free variable and eigenspace Hence, is a basis for the eigenspace of .  Case 2. For , the equation becomes   Once again there is only one free variable in the row reduction and so the dimension of the eigenspace will be one: Hence, is a basis for the eigenspace of . This means that produces only one column for . Since we began with only two eigenvalues, we had hoped that would produce a vector space of dimension two, or, in matrix terms, two linearly independent columns for . Since does not have three linearly independent eigenvectors cannot be diagonalized.   SageMath Note - Diagonalization  SageMath Note Matrix Diagonalization  We demonstrate how diagonalization can be done in Sage. We start by defining the matrix to be diagonalized, and also declare and to be variables.   We have been working with right eigenvectors since the in is a column vector. It's not so common but still desirable in some situations to consider left eigenvectors, so SageMath allows either one. The right_eigenmatrix method returns a pair of matrices. The diagonal matrix, , with eigenvalues and the diagonalizing matrix, , which is made up of columns that are eigenvectors corresponding to the eigenvectors of .   We should note here that is not unique because even if an eigenspace has dimension one, any nonzero vector in that space will serve as an eigenvector. For that reason, the generated by Sage isn't necessarily the same as the one computed by any other computer algebra system such as Mathematica. Here we verify the result for our Sage calculation. Recall that an asterisk is used for matrix multiplication in Sage.   Here is a second matrix to diagonalize.   Here we've already specified that the underlying system is the rational numbers. Since the eigenvalues are not rational, Sage will revert to approximate number by default. We'll just pull out the matrix of eigenvectors this time and display rounded entries.   Finally, we examine how Sage reacts to the matrix from that couldn't be diagonalized. Notice that the last column is a zero column, indicating the absence of one needed eigenvector.     Exercises  List three different eigenvectors of , the matrix of , associated with each of the two eigenvalues 1 and 4. Verify your results.  Choose one of the three eigenvectors corresponding to 1 and one of the three eigenvectors corresponding to 4, and show that the two chosen vectors are linearly independent.     Any nonzero multiple of is an eigenvector associated with .  Any nonzero multiple of is an eigenvector associated with .  Let and . You can verify that if and only if Therefore, is linearly independent.    Verify that and in are vector spaces over . Since they are also subsets of , they are called subvector-spaces, or subspaces for short, of . Since these are subspaces consisting of eigenvectors, they are called eigenspaces.  Use the definition of dimension in the previous section to find and . Note that . This is not a coincidence.    Verify that is indeed equal to , as indicated in .  Choose and and verify that the new value of satisfies .  Take two different (from the previous part) linearly independent eigenvectors of the matrix of and verify that is a diagonal matrix.    Part c: You should obtain or , depending on how you order the eigenvalues.  Let be the matrix in and . Without doing any actual matrix multiplications, determine the value of  If you choose the columns of in the reverse order, what is ?     Diagonalize the following, if possible:                   If , then .  If , then .  If , then .  If , then .   is not diagonalizable. Five is a double root of the characteristic equation, but has an eigenspace with dimension only 1.  If , then .    Diagonalize the following, if possible:                   Let and be as in . Show that the columns of the matrix can be found by computing ,  .  This is a direct application of the definition of matrix multiplication. Let be the row of , and let be the column of . Then the column of the product is   Hence, for . Thus, each column of depends on and the column of .  Prove that if is an matrix and is a diagonal matrix with diagonal entries ,  , then is the matrix obtained from , by multiplying column of by , .    "
},
{
  "id": "def-eigenvalue-eigenvector",
  "level": "2",
  "url": "s-the-diagonalization-process.html#def-eigenvalue-eigenvector",
  "type": "Definition",
  "number": "12.4.1",
  "title": "Eigenvalue, Eigenvector.",
  "body": "Eigenvalue, Eigenvector  Eigenvalue  Eigenvector  Let be an matrix over . is an eigenvalue of if for some nonzero column vector we have . is called an eigenvector corresponding to the eigenvalue  . "
},
{
  "id": "ex-some-evalues",
  "level": "2",
  "url": "s-the-diagonalization-process.html#ex-some-evalues",
  "type": "Example",
  "number": "12.4.2",
  "title": "Examples of eigenvalues and eigenvectors.",
  "body": "Examples of eigenvalues and eigenvectors Find the eigenvalues and corresponding eigenvectors of the matrix .  We want to find nonzero vectors and real numbers such that   The last matrix equation will have nonzero solutions if and only if or , which simplifies to . Therefore, the solutions to this quadratic equation, and , are the eigenvalues of . We now have to find eigenvectors associated with each eigenvalue.  Case 1. For , becomes: which reduces to the single equation, . From this, . This means the solution set of this equation is (in column notation) So any column vector of the form where is any nonzero real number is an eigenvector associated with . The reader should verify that, for example, so that is an eigenvector associated with eigenvalue 1.  Case 2. For  becomes: which reduces to the single equation , so that . The solution set of the equation is Therefore, all eigenvectors of associated with the eigenvalue are of the form , where can be any nonzero number. "
},
{
  "id": "theorem-evalue-det-theorem",
  "level": "2",
  "url": "s-the-diagonalization-process.html#theorem-evalue-det-theorem",
  "type": "Theorem",
  "number": "12.4.3",
  "title": "Characterization of Eigenvalues of a Square Matrix.",
  "body": "Characterization of Eigenvalues of a Square Matrix  Let be any matrix over . Then is an eigenvalue of if and only if .  "
},
{
  "id": "s-the-diagonalization-process-3-7",
  "level": "2",
  "url": "s-the-diagonalization-process.html#s-the-diagonalization-process-3-7",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "characteristic equation characteristic polynomial "
},
{
  "id": "theorem-evector-independence",
  "level": "2",
  "url": "s-the-diagonalization-process.html#theorem-evector-independence",
  "type": "Theorem",
  "number": "12.4.4",
  "title": "Linear Independence of Eigenvectors.",
  "body": "Linear Independence of Eigenvectors  Nonzero eigenvectors corresponding to distinct eigenvalues are linearly independent. "
},
{
  "id": "def-diagonalizable-matrix",
  "level": "2",
  "url": "s-the-diagonalization-process.html#def-diagonalizable-matrix",
  "type": "Definition",
  "number": "12.4.5",
  "title": "Diagonalizable Matrix.",
  "body": " Diagonalizable Matrix  Diagonalizable Matrix  An matrix is called diagonalizable if there exists an invertible matrix such that is a diagonal matrix . The matrix is said to diagonalize the matrix . "
},
{
  "id": "ex-diagonalization-of-matrix",
  "level": "2",
  "url": "s-the-diagonalization-process.html#ex-diagonalization-of-matrix",
  "type": "Example",
  "number": "12.4.6",
  "title": "Diagonalization of a Matrix.",
  "body": " Diagonalization of a Matrix  We will now diagonalize the matrix of . We form the matrix as follows: Let be the first column of . Choose for any eigenvector from . We may as well choose a simple vector in so is our candidate.  Similarly, let be the second column of , and choose for any eigenvector from . The vector is a reasonable choice, thus so that Notice that the elements on the main diagonal of are the eigenvalues of , where is the eigenvalue corresponding to the eigenvector . "
},
{
  "id": "s-the-diagonalization-process-4-5",
  "level": "2",
  "url": "s-the-diagonalization-process.html#s-the-diagonalization-process-4-5",
  "type": "Note",
  "number": "12.4.7",
  "title": "",
  "body": " The first step in the diagonalization process is the determination of the eigenvalues. The ordering of the eigenvalues is purely arbitrary. If we designate and , the columns of would be interchanged and would be (see Exercise 3b of this section). Nonetheless, the final outcome of the application to which we are applying the diagonalization process would be the same.  If is an matrix with distinct eigenvalues, then is also an matrix whose columns , , are linearly independent vectors.    "
},
{
  "id": "ex-another-diagonalization",
  "level": "2",
  "url": "s-the-diagonalization-process.html#ex-another-diagonalization",
  "type": "Example",
  "number": "12.4.8",
  "title": "Diagonalization of a 3 by 3 matrix.",
  "body": "Diagonalization of a 3 by 3 matrix Diagonalize the matrix .  First, we find the eigenvalues of . Hence, the equation becomes Therefore, our eigenvalues for are and . We note that we do not have three distinct eigenvalues, but we proceed as in the previous example.  Case 1. For the equation becomes We can row reduce the matrix of coefficients to .  The matrix equation is then equivalent to the equations . Therefore, the solution set, or eigenspace, corresponding to consists of vectors of the form   Therefore is an eigenvector corresponding to the eigenvalue , and can be used for our first column of :   Before we continue we make the observation: is a subspace of with basis and .  Case 2. If , then the equation becomes   Without the aid of any computer technology, it should be clear that all three equations that correspond to this matrix equation are equivalent to , or . Notice that can take on any value, so any vector of the form will solve the matrix equation.  We note that the solution set contains two independent variables, and . Further, note that we cannot express the eigenspace as a linear combination of a single vector as in Case 1. However, it can be written as   We can replace any vector in a basis is with a nonzero multiple of that vector. Simply for aesthetic reasons, we will multiply the second vector that generates by 2. Therefore, the eigenspace is a subspace of with basis and so .  What this means with respect to the diagonalization process is that gives us both Column 2 and Column 3 the diagonalizing matrix. The order is not important so we have  The reader can verify (see Exercise 5 of this section) that and  "
},
{
  "id": "theorem-diagon-condition",
  "level": "2",
  "url": "s-the-diagonalization-process.html#theorem-diagon-condition",
  "type": "Theorem",
  "number": "12.4.9",
  "title": "A condition for diagonalizability.",
  "body": "A condition for diagonalizability Let be an matrix. Then is diagonalizable if and only if has linearly independent eigenvectors.  Outline of a proof: ( ) Assume that has linearly independent eigenvectors, , with corresponding eigenvalues , , . We want to prove that is diagonalizable. Column of the matrix is (see Exercise 7 of this section). Then, since the is an eigenvector of associated with the eigenvalue we have for . But this means that , where is the diagonal matrix with diagonal entries , , . If we multiply both sides of the equation by we get the desired .  ( ) The proof in this direction involves a concept that is not covered in this text (rank of a matrix); so we refer the interested reader to virtually any linear algebra text for a proof. "
},
{
  "id": "ex-not-diagonalizable",
  "level": "2",
  "url": "s-the-diagonalization-process.html#ex-not-diagonalizable",
  "type": "Example",
  "number": "12.4.10",
  "title": "A Matrix that is Not Diagonalizable.",
  "body": " A Matrix that is Not Diagonalizable  Let us attempt to diagonalize the matrix  First, we determine the eigenvalues. Therefore there are two eigenvalues, and . Since is an eigenvalue of degree one, it will have an eigenspace of dimension 1. Since is a double root of the characteristic equation, the dimension of its eigenspace must be 2 in order to be able to diagonalize.  Case 1. For , the equation becomes   Row reduction of this system reveals one free variable and eigenspace Hence, is a basis for the eigenspace of .  Case 2. For , the equation becomes   Once again there is only one free variable in the row reduction and so the dimension of the eigenspace will be one: Hence, is a basis for the eigenspace of . This means that produces only one column for . Since we began with only two eigenvalues, we had hoped that would produce a vector space of dimension two, or, in matrix terms, two linearly independent columns for . Since does not have three linearly independent eigenvectors cannot be diagonalized. "
},
{
  "id": "exercises-12-4-2",
  "level": "2",
  "url": "s-the-diagonalization-process.html#exercises-12-4-2",
  "type": "Exercise",
  "number": "12.4.4.1",
  "title": "",
  "body": "List three different eigenvectors of , the matrix of , associated with each of the two eigenvalues 1 and 4. Verify your results.  Choose one of the three eigenvectors corresponding to 1 and one of the three eigenvectors corresponding to 4, and show that the two chosen vectors are linearly independent.     Any nonzero multiple of is an eigenvector associated with .  Any nonzero multiple of is an eigenvector associated with .  Let and . You can verify that if and only if Therefore, is linearly independent.   "
},
{
  "id": "exercises-12-4-3",
  "level": "2",
  "url": "s-the-diagonalization-process.html#exercises-12-4-3",
  "type": "Exercise",
  "number": "12.4.4.2",
  "title": "",
  "body": "Verify that and in are vector spaces over . Since they are also subsets of , they are called subvector-spaces, or subspaces for short, of . Since these are subspaces consisting of eigenvectors, they are called eigenspaces.  Use the definition of dimension in the previous section to find and . Note that . This is not a coincidence.   "
},
{
  "id": "exercises-12-4-4",
  "level": "2",
  "url": "s-the-diagonalization-process.html#exercises-12-4-4",
  "type": "Exercise",
  "number": "12.4.4.3",
  "title": "",
  "body": "Verify that is indeed equal to , as indicated in .  Choose and and verify that the new value of satisfies .  Take two different (from the previous part) linearly independent eigenvectors of the matrix of and verify that is a diagonal matrix.    Part c: You should obtain or , depending on how you order the eigenvalues. "
},
{
  "id": "exercises-12-4-5",
  "level": "2",
  "url": "s-the-diagonalization-process.html#exercises-12-4-5",
  "type": "Exercise",
  "number": "12.4.4.4",
  "title": "",
  "body": "Let be the matrix in and . Without doing any actual matrix multiplications, determine the value of  If you choose the columns of in the reverse order, what is ?   "
},
{
  "id": "exercises-12-4-6",
  "level": "2",
  "url": "s-the-diagonalization-process.html#exercises-12-4-6",
  "type": "Exercise",
  "number": "12.4.4.5",
  "title": "",
  "body": " Diagonalize the following, if possible:                   If , then .  If , then .  If , then .  If , then .   is not diagonalizable. Five is a double root of the characteristic equation, but has an eigenspace with dimension only 1.  If , then .   "
},
{
  "id": "exercises-12-4-7",
  "level": "2",
  "url": "s-the-diagonalization-process.html#exercises-12-4-7",
  "type": "Exercise",
  "number": "12.4.4.6",
  "title": "",
  "body": "Diagonalize the following, if possible:                 "
},
{
  "id": "exercises-12-4-8",
  "level": "2",
  "url": "s-the-diagonalization-process.html#exercises-12-4-8",
  "type": "Exercise",
  "number": "12.4.4.7",
  "title": "",
  "body": " Let and be as in . Show that the columns of the matrix can be found by computing ,  .  This is a direct application of the definition of matrix multiplication. Let be the row of , and let be the column of . Then the column of the product is   Hence, for . Thus, each column of depends on and the column of . "
},
{
  "id": "exercises-12-4-9",
  "level": "2",
  "url": "s-the-diagonalization-process.html#exercises-12-4-9",
  "type": "Exercise",
  "number": "12.4.4.8",
  "title": "",
  "body": "Prove that if is an matrix and is a diagonal matrix with diagonal entries ,  , then is the matrix obtained from , by multiplying column of by , .  "
},
{
  "id": "s-some-applications",
  "level": "1",
  "url": "s-some-applications.html",
  "type": "Section",
  "number": "12.5",
  "title": "Some Applications",
  "body": " Some Applications   A large and varied number of applications involve computations of powers of matrices. These applications can be found in science, the social sciences, economics, engineering, and, many other areas where mathematics is used. We will consider a few diverse examples the mathematics behind these applications here.   Diagonalization  We begin by developing a helpful technique for computing , . If can be diagonalized, then there is a matrix such that , where is a diagonal matrix and   The proof of this identity was an exercise in Section 5.4. The condition that be a diagonal matrix is not necessary but when it is, the calculation on the right side is particularly easy to perform. Although the formal proof is done by induction, the reason why it is true is easily seen by writing out an example such as :   Application to Recursion: Matrix Computation of the Fibonnaci Sequence  Fibonacci Sequence Matrix Representation  Consider the computation of terms of the Fibonnaci Sequence. Recall that and for .  In order to formulate the calculation in matrix form, we introduced the dummy equation  so that now we have two equations If , these two equations can be expressed in matrix form as We can use induction to prove that if ,   Next, by diagonalizing and using the fact that . we can show that   Some comments on this example:  An equation of the form , where and are given constants, is referred to as a linear homogeneous second-order difference equation. The conditions and , where and are constants, are called initial conditions. Those of you who are familiar with differential equations may recognize that this language parallels what is used in differential equations. Difference (aka recurrence) equations move forward discretely; that is, in a finite number of positive steps. On the other hand, a differential equation moves continuously; that is, takes an infinite number of infinitesimal steps.  A recurrence relationship of the form , where and are constants, is called a first-order difference equation. In order to write out the sequence, we need to know one initial condition. Equations of this type can be solved similarly to the method outlined in the example by introducing the superfluous equation to obtain in matrix equation:      Path Counting  In the next example, we apply the following theorem, which can be proven by induction.  Path Counting Theorem If is the adjacency matrix of a graph with vertices , then the entry is the number of paths of length from node to node .   Counting Paths with Diagonalization  Consider the graph in .   Counting Numbers of Paths   Counting Numbers of Paths    As we saw in Section 6.4, the adjacency matrix of this graph is .  Recall that is the adjacency matrix of the relation , where is the relation of the above graph. Also recall that in computing , we used Boolean arithmetic. What happens if we use regular arithmetic? If we square we get   How can we interpret this? We note that and that there are two paths of length two from (the third node) to . Also, , and there is one path of length 2 from to . The reader should verify these claims by examining the graph.  How do we compute for possibly large values of ? From the discussion at the beginning of this section, we know that if is diagonalizable. We leave to the reader to show that are eigenvalues of with eigenvectors   Then where and   See of this section for the completion of this example.    Matrix Calculus  Matrix Calculus - Exponentials  Those who have studied calculus recall that the Maclaurin series is a useful way of expressing many common functions. For example, . Indeed, calculators and computers use these series for calculations. Given a polynomial , we defined the matrix-polynomial for square matrices in Chapter 5. Hence, we are in a position to describe for an matrix as a limit of polynomials, the partial sums of the series. Formally, we write   Again we encounter the need to compute high powers of a matrix. Let be an diagonalizable matrix. Then there exists an invertible matrix such that , a diagonal matrix, so that   The infinite sum in the middle of this final expression can be easily evaluated if is diagonal. All entries of powers off the diagonal are zero and the entry of the diagonal is  For example, if , the first matrix we diagonalized in Section 12.3, we found that and .  Therefore,    Many of the ideas of calculus can be developed using matrices. For example, if then  Many of the basic formulas in calculus are true in matrix calculus. For example, and if is a constant matrix, Matrix calculus can be used to solve systems of differential equations in a similar manner to the procedure used in ordinary differential equations.    SageMath Note - Matrix Exponential  SageMath Note Matrix Exponential  Sage's matrix exponential method is exp .     Exercises  Write out all the details of to show that the formula for given in the text is correct.  Use induction to prove the assertion made in the example that     Do using the method outlined in . Note that the terminology characteristic equation, characteristic polynomial, and so on, introduced in Chapter 8, comes from the language of matrix algebra,  What is the significance of , part c, with respect to this section?     Solve , with , using the method of this section.    How many paths are there of length 6 from vertex 1 to vertex 3 in ? How many paths from vertex 2 to vertex 2 of length 6 are there?   Graph for exercise 4   Graph for exercise 4    The characteristic polynomial of the adjacency matrix is .  Regarding ,  Use matrices to determine the number of paths of length 1 that exist from vertex to each of the vertices in the given graph. Verify using the graph. Do the same for vertices and .  Verify all the details provided in the example.  Use matrices to determine the number of paths of length 4 there between each pair of nodes in the graph. Verify your results using the graph.     Since , there are 0 paths of length 1 from: node c to node a, node b to node b, and node a to node c; and there is 1 path of length 1 for every other pair of nodes.  The characteristic polynomial is  Solving the characteristic equation we find solutions 1, 2, and -1.  If , we find the associated eigenvector by finding a nonzero solution to One of these, which will be the first column of , is   If , the system yields eigenvectors, including , which will be the second column of .  If , then the system determining the eigenvectors is and we can select , although any nonzero multiple of this vector could be the third column of .  Assembling the results of part (b) we have .     Hence there are five different paths of length 4 between distinct vertices, and six different paths that start and end at the same vertex. The reader can verify these facts from    Let  Find  Recall that and compute .  Formulate a reasonable definition of the natural logarithm of a matrix and compute .     We noted in Chapter 5 that since matrix algebra is not commutative under multiplication, certain difficulties arise. Let and .  Compute , , and . Compare , and .  Show that if is the zero matrix, then .  Prove that if and are two matrices that do commute, then , thereby proving that and commute.  Prove that for any matrix , .       , , and  Let be the zero matrix, .  Assume that and commute. We will examine the first few terms in the product . The pattern that is established does continue in general. In what follows, it is important that . For example, in the last step, expands to , not , if we can't assume commutativity.   Since and commute, we can apply part d;       Another observation for adjacency matrices: For the matrix in , note that the sum of the elements in the row corresponding to the node (that is, the first row) gives the outdegree of . Similarly, the sum of the elements in any given column gives the indegree of the node corresponding to that column.   Graph for exercise 8   Graph for exercise 8     Using the matrix of , find the outdegree and the indegree of each node. Verify by the graph.  Repeat part (a) for the directed graphs in .      "
},
{
  "id": "ex-fibonnaci-matrix",
  "level": "2",
  "url": "s-some-applications.html#ex-fibonnaci-matrix",
  "type": "Example",
  "number": "12.5.1",
  "title": "Application to Recursion: Matrix Computation of the Fibonnaci Sequence.",
  "body": "Application to Recursion: Matrix Computation of the Fibonnaci Sequence  Fibonacci Sequence Matrix Representation  Consider the computation of terms of the Fibonnaci Sequence. Recall that and for .  In order to formulate the calculation in matrix form, we introduced the dummy equation  so that now we have two equations If , these two equations can be expressed in matrix form as We can use induction to prove that if ,   Next, by diagonalizing and using the fact that . we can show that   Some comments on this example:  An equation of the form , where and are given constants, is referred to as a linear homogeneous second-order difference equation. The conditions and , where and are constants, are called initial conditions. Those of you who are familiar with differential equations may recognize that this language parallels what is used in differential equations. Difference (aka recurrence) equations move forward discretely; that is, in a finite number of positive steps. On the other hand, a differential equation moves continuously; that is, takes an infinite number of infinitesimal steps.  A recurrence relationship of the form , where and are constants, is called a first-order difference equation. In order to write out the sequence, we need to know one initial condition. Equations of this type can be solved similarly to the method outlined in the example by introducing the superfluous equation to obtain in matrix equation:    "
},
{
  "id": "theorem-12-5-1",
  "level": "2",
  "url": "s-some-applications.html#theorem-12-5-1",
  "type": "Theorem",
  "number": "12.5.2",
  "title": "Path Counting Theorem.",
  "body": "Path Counting Theorem If is the adjacency matrix of a graph with vertices , then the entry is the number of paths of length from node to node . "
},
{
  "id": "ex-diagonalization-in-graph-theory",
  "level": "2",
  "url": "s-some-applications.html#ex-diagonalization-in-graph-theory",
  "type": "Example",
  "number": "12.5.3",
  "title": "Counting Paths with Diagonalization.",
  "body": " Counting Paths with Diagonalization  Consider the graph in .   Counting Numbers of Paths   Counting Numbers of Paths    As we saw in Section 6.4, the adjacency matrix of this graph is .  Recall that is the adjacency matrix of the relation , where is the relation of the above graph. Also recall that in computing , we used Boolean arithmetic. What happens if we use regular arithmetic? If we square we get   How can we interpret this? We note that and that there are two paths of length two from (the third node) to . Also, , and there is one path of length 2 from to . The reader should verify these claims by examining the graph.  How do we compute for possibly large values of ? From the discussion at the beginning of this section, we know that if is diagonalizable. We leave to the reader to show that are eigenvalues of with eigenvectors   Then where and   See of this section for the completion of this example.  "
},
{
  "id": "ex-matrix-calculus",
  "level": "2",
  "url": "s-some-applications.html#ex-matrix-calculus",
  "type": "Example",
  "number": "12.5.5",
  "title": "Matrix Calculus - Exponentials.",
  "body": "Matrix Calculus - Exponentials  Those who have studied calculus recall that the Maclaurin series is a useful way of expressing many common functions. For example, . Indeed, calculators and computers use these series for calculations. Given a polynomial , we defined the matrix-polynomial for square matrices in Chapter 5. Hence, we are in a position to describe for an matrix as a limit of polynomials, the partial sums of the series. Formally, we write   Again we encounter the need to compute high powers of a matrix. Let be an diagonalizable matrix. Then there exists an invertible matrix such that , a diagonal matrix, so that   The infinite sum in the middle of this final expression can be easily evaluated if is diagonal. All entries of powers off the diagonal are zero and the entry of the diagonal is  For example, if , the first matrix we diagonalized in Section 12.3, we found that and .  Therefore,   "
},
{
  "id": "s-some-applications-5-3",
  "level": "2",
  "url": "s-some-applications.html#s-some-applications-5-3",
  "type": "Remark",
  "number": "12.5.6",
  "title": "",
  "body": "Many of the ideas of calculus can be developed using matrices. For example, if then  Many of the basic formulas in calculus are true in matrix calculus. For example, and if is a constant matrix, Matrix calculus can be used to solve systems of differential equations in a similar manner to the procedure used in ordinary differential equations.  "
},
{
  "id": "exercise-12-5-1",
  "level": "2",
  "url": "s-some-applications.html#exercise-12-5-1",
  "type": "Exercise",
  "number": "12.5.5.1",
  "title": "",
  "body": "Write out all the details of to show that the formula for given in the text is correct.  Use induction to prove the assertion made in the example that    "
},
{
  "id": "exercises-12-5-3",
  "level": "2",
  "url": "s-some-applications.html#exercises-12-5-3",
  "type": "Exercise",
  "number": "12.5.5.2",
  "title": "",
  "body": "Do using the method outlined in . Note that the terminology characteristic equation, characteristic polynomial, and so on, introduced in Chapter 8, comes from the language of matrix algebra,  What is the significance of , part c, with respect to this section?    "
},
{
  "id": "exercises-12-5-4",
  "level": "2",
  "url": "s-some-applications.html#exercises-12-5-4",
  "type": "Exercise",
  "number": "12.5.5.3",
  "title": "",
  "body": "Solve , with , using the method of this section.  "
},
{
  "id": "exercises-12-5-5",
  "level": "2",
  "url": "s-some-applications.html#exercises-12-5-5",
  "type": "Exercise",
  "number": "12.5.5.4",
  "title": "",
  "body": " How many paths are there of length 6 from vertex 1 to vertex 3 in ? How many paths from vertex 2 to vertex 2 of length 6 are there?   Graph for exercise 4   Graph for exercise 4    The characteristic polynomial of the adjacency matrix is . "
},
{
  "id": "exercise-12-5-5",
  "level": "2",
  "url": "s-some-applications.html#exercise-12-5-5",
  "type": "Exercise",
  "number": "12.5.5.5",
  "title": "",
  "body": "Regarding ,  Use matrices to determine the number of paths of length 1 that exist from vertex to each of the vertices in the given graph. Verify using the graph. Do the same for vertices and .  Verify all the details provided in the example.  Use matrices to determine the number of paths of length 4 there between each pair of nodes in the graph. Verify your results using the graph.     Since , there are 0 paths of length 1 from: node c to node a, node b to node b, and node a to node c; and there is 1 path of length 1 for every other pair of nodes.  The characteristic polynomial is  Solving the characteristic equation we find solutions 1, 2, and -1.  If , we find the associated eigenvector by finding a nonzero solution to One of these, which will be the first column of , is   If , the system yields eigenvectors, including , which will be the second column of .  If , then the system determining the eigenvectors is and we can select , although any nonzero multiple of this vector could be the third column of .  Assembling the results of part (b) we have .     Hence there are five different paths of length 4 between distinct vertices, and six different paths that start and end at the same vertex. The reader can verify these facts from   "
},
{
  "id": "exercises-12-5-7",
  "level": "2",
  "url": "s-some-applications.html#exercises-12-5-7",
  "type": "Exercise",
  "number": "12.5.5.6",
  "title": "",
  "body": "Let  Find  Recall that and compute .  Formulate a reasonable definition of the natural logarithm of a matrix and compute .    "
},
{
  "id": "exercises-12-5-8",
  "level": "2",
  "url": "s-some-applications.html#exercises-12-5-8",
  "type": "Exercise",
  "number": "12.5.5.7",
  "title": "",
  "body": "We noted in Chapter 5 that since matrix algebra is not commutative under multiplication, certain difficulties arise. Let and .  Compute , , and . Compare , and .  Show that if is the zero matrix, then .  Prove that if and are two matrices that do commute, then , thereby proving that and commute.  Prove that for any matrix , .       , , and  Let be the zero matrix, .  Assume that and commute. We will examine the first few terms in the product . The pattern that is established does continue in general. In what follows, it is important that . For example, in the last step, expands to , not , if we can't assume commutativity.   Since and commute, we can apply part d;     "
},
{
  "id": "exercises-12-5-9",
  "level": "2",
  "url": "s-some-applications.html#exercises-12-5-9",
  "type": "Exercise",
  "number": "12.5.5.8",
  "title": "",
  "body": " Another observation for adjacency matrices: For the matrix in , note that the sum of the elements in the row corresponding to the node (that is, the first row) gives the outdegree of . Similarly, the sum of the elements in any given column gives the indegree of the node corresponding to that column.   Graph for exercise 8   Graph for exercise 8     Using the matrix of , find the outdegree and the indegree of each node. Verify by the graph.  Repeat part (a) for the directed graphs in .    "
},
{
  "id": "s-systems-linear-equations-mod-2",
  "level": "1",
  "url": "s-systems-linear-equations-mod-2.html",
  "type": "Section",
  "number": "12.6",
  "title": "Linear Equations over the Integers Mod 2",
  "body": " Linear Equations over the Integers Mod 2  Linear Equations over the Integers Mod 2   Row reduction mod 2  The methods we have studied for solving systems of equations up to this point can be applied to systems in which all arithmetic is done over other algebraic systems, including the integers modulo 2. The mod 2 case will become particularly useful in our later study of coding theory.  When solving systems of equations with mod 2 arithmetic, the elementary row operations are still fundamental. However, since there is only one nonzero element, 1, you never need to multiply a row by a nonzero constant. One other big difference is that the number of possible solutions is always finite. If you have linear equations in unknowns, each unknown can only take on one of two values, 0 or 1. Therefore there are only possible -tuples to from which to draw a solution set. Assuming , you typically (but not always) will have basic variables after row-reduction and free variable. If this is the case, and any solution exists, there will be different solutions.  Let's look at an example, which is coverted to matrix form immediately.   The augmented matrix of the system is The steps in row-reducing this matrix follow. Entries on which we pivot are displayed in bold face to more easily identify the basic variables.   Notice that at this point, we cannot pivot on the third row, third column since that entry is zero. Therefore we move over to the next column, making the basic.   This completes the row reduction and we can now identify the solution set. Keep in mind that since addition is subtraction, terms can be moved to either side of an equals sign without any change in sign. The basic variables are , , and , while the other three variables are free. The general solution of the system is   With three free variables, there are solutions to this system. For example, one of them is obtained by setting , , and , which produces .  We can check our row reduction with SageMath:     Exercises  In all of the exercises that follow, the systems of equations are over , and so mod 2 arithmetic should be used in solving them.  Solve the following systems, describing the solution sets completely:               This exercise provides an example in which the number of basic variables is less than the number of equations. The only difference between the two systems below is the right hand sides. You can start with an augmented matrix having two right side columns and do row reduction for both systems at the same time.          As suggested here is the augmented matrix with both right sides, and its row reduction: There are only two basic variables here because the left side of the last equation is the sum of the left sides of the first two equations.  Ignoring the last column of both matrices, we see that the last equation of the first system reduces to , which is always true, and the first two equations yield two free variables, and . The general solution is the set of quadruples . The cardinality of the solution set is 4.  If we replace the fifth column with the sixth one, the last row indicates that , which means that the solution set is empty.     This exercise motivates the concept of a coset in Chapter 15.  Solve the following system and prove that the solution set is a linear combination of vectors in and also a subgroup of the group under coordinatewise mod 2 addition.   Describe the solution set to the following system as it relates to the solution set to the system in the previous part of this exercise.       Row reduction produces a solution with one free variable, .  The solution set has only two elements. It is . Since is a finite group, the solution set is a subgroup because it is closed with respect to coordinatewise mod 2 addition.   The row-reduced augmented matrix of coefficients provides the solution  Therefore, the solution to this system is a shift of the solution set to the homogeneous system by the vector , which is      "
},
{
  "id": "exercise-12-6-1",
  "level": "2",
  "url": "s-systems-linear-equations-mod-2.html#exercise-12-6-1",
  "type": "Exercise",
  "number": "12.6.2.1",
  "title": "",
  "body": "Solve the following systems, describing the solution sets completely:              "
},
{
  "id": "exercise-12-6-3",
  "level": "2",
  "url": "s-systems-linear-equations-mod-2.html#exercise-12-6-3",
  "type": "Exercise",
  "number": "12.6.2.2",
  "title": "",
  "body": "This exercise provides an example in which the number of basic variables is less than the number of equations. The only difference between the two systems below is the right hand sides. You can start with an augmented matrix having two right side columns and do row reduction for both systems at the same time.          As suggested here is the augmented matrix with both right sides, and its row reduction: There are only two basic variables here because the left side of the last equation is the sum of the left sides of the first two equations.  Ignoring the last column of both matrices, we see that the last equation of the first system reduces to , which is always true, and the first two equations yield two free variables, and . The general solution is the set of quadruples . The cardinality of the solution set is 4.  If we replace the fifth column with the sixth one, the last row indicates that , which means that the solution set is empty.    "
},
{
  "id": "exercise-12-6-2",
  "level": "2",
  "url": "s-systems-linear-equations-mod-2.html#exercise-12-6-2",
  "type": "Exercise",
  "number": "12.6.2.3",
  "title": "",
  "body": "This exercise motivates the concept of a coset in Chapter 15.  Solve the following system and prove that the solution set is a linear combination of vectors in and also a subgroup of the group under coordinatewise mod 2 addition.   Describe the solution set to the following system as it relates to the solution set to the system in the previous part of this exercise.       Row reduction produces a solution with one free variable, .  The solution set has only two elements. It is . Since is a finite group, the solution set is a subgroup because it is closed with respect to coordinatewise mod 2 addition.   The row-reduced augmented matrix of coefficients provides the solution  Therefore, the solution to this system is a shift of the solution set to the homogeneous system by the vector , which is    "
},
{
  "id": "s-posets-revisited",
  "level": "1",
  "url": "s-posets-revisited.html",
  "type": "Section",
  "number": "13.1",
  "title": "Posets Revisited",
  "body": " Posets Revisited  Posets Revisited  We recall the definition a partially ordering:  Partial Ordering Partial Ordering  Let be a relation on a set . We say that is a partial ordering on if it is reflexive, antisymmetric, and transitive. That is:  is reflexive:  is antisymmetric:   is transitive:   The set together with the relation is called a poset.    Some posets  We recall a few examples of posets:   is a poset. Notice that our generic symbol for the partial ordering, , is selected to remind us that a partial ordering is similar to less than or equal to.  Let . Then is a poset.  Let . Then is a poset.    The posets we will concentrate on in this chapter will be those which have upper and lower bounds in relation to any pair of elements. Next, we define this concept precisely.  Lower Bound, Upper Bound  Lower Bound  Upper Bound  Let be a poset, and . Then is a lower bound of and if and . Also, is an upper bound of and if and .  In most of the posets that will interest us, every pair of elements have both upper and lower bounds, though there are posets for which this is not true.  Greatest Lower Bound  Greatest Lower Bound  Let be a poset. If , then is a greatest lower bound of and if and only if    If such that and , then .    The last condition in the definition of Greatest Lower Bound says that if is also a lower bound, then is greater in relation to than . The definition of a least upper bound is a mirror image of a greatest lower bound:  Least Upper Bound  Least Upper Bound  Let be a poset. If , then is a least upper bound of and if and only if    If such that if and , then .    Notice that the two definitions above refer to ...a greatest lower bound and a least upper bound. Any time you define an object like these you need to have an open mind as to whether more than one such object can exist. In fact, we now can prove that there can't be two greatest lower bounds or two least upper bounds.   Uniqueness of Least Upper and Greatest Lower Bounds  Let be a poset, and . If a greatest lower bound of and exists, then it is unique. The same is true of a least upper bound, if it exists.  Let and be greatest lower bounds of and . We will prove that .   a greatest lower bound of and   is a lower bound of and .   a greatest lower bound of and and a lower bound of and  , by the definition of greatest lower bound.   a greatest lower bound of and  is a lower bound of and .   a greatest lower bound of and and a lower bound of and . by the definition of greatest lower bound.   and by the antisymmetry property of a partial ordering.    The proof of the second statement in the theorem is almost identical to the first and is left to the reader.  Greatest Element, Least Element  Greatest Element  Least Element  least element in a poset  greatest element in a poset  Let be a poset. is called the greatest (maximum) element of if, for all , . In addition, is called the least (minimum) element of if for all , . The greatest and least elements, when they exist, are frequently denoted by and respectively.  Bounds on the divisors of 105 Consider the partial ordering divides on . Then is a poset. To determine the least upper bound of 3 and 7, we look for all , such that and . Certainly, both and satisfy these conditions and no other element of does. Next, since , is the least upper bound of 3 and 7. Similarly, the least upper bound of 3 and 5 is 15. The greatest element of is 105 since for all . To find the greatest lower bound of 15 and 35, we first consider all elements of such that . They are 1, 3, 5, and 15. The elements for which are 1, 5, 7, and 35. From these two lists, we see that and satisfy the required conditions. But since , the greatest lower bound is 5. The least element of is 1 since for all .  The Set of Divisors of an Integer  Divisors of an Integer  the set of divisors of integer  For any positive integer , the divisors of is the set of integers that divide evenly into . We denote this set .  For example, the set of is .  The power set of a three element set Consider the poset , where . The greatest lower bound of and is . For any other element which is a subset of and (there is only one; what is it?), . The least element of is and the greatest element is . The Hasse diagram of this poset is shown in .   Power Set of    Power Set of     The previous examples and definitions indicate that the least upper bound and greatest lower bound are defined in terms of the partial ordering of the given poset. It is not yet clear whether all posets have the property such that every pair of elements always has both a least upper bound and greatest lower bound. Indeed, this is not the case (see ).   Exercises  Consider the poset , where .  Find all lower bounds of 10 and 15.  Find the greatest lower bound of 10 and 15.  Find all upper bounds of 10 and 15.  Determine the least upper bound of 10 and 15.  Draw the Hasse diagram for with respect to . Compare this Hasse diagram with that of . Note that the two diagrams are structurally the same.     1, 5  5  30  30  See the Sage cell below with the default input displaying a Hasse diagram for .     List the elements of the sets , , and . For each set, draw the Hasse diagram for divides.   contains Hasse diagrams of posets.  Determine the least upper bound and greatest lower bound of all pairs of elements when they exist. Indicate those pairs that do not have a least upper bound (or a greatest lower bound ).  Find the least and greatest elements when they exist.     Figure for Exercise 3   Section 13.1, Exercise 3      Solution for Hasse diagram (b):      is the least element and is the greatest element.   Partial solution for Hasse diagram (f):   and do not exist.  No greatest element exists, but is the least element.       For the poset , what are the greatest lower bound and least upper bound of two elements and ? Are there least and\/or greatest elements?   Prove the second part of , the least upper bound of two elements in a poset is unique, if one exists.  Prove that if a poset has a least element, then that element is unique.   If and are distinct least elements, then    We naturally order the numbers in with less than or equal to, which is a partial ordering. We define an ordering, on the elements of by   Prove that is a partial ordering on .  Draw the ordering diagrams for on , , and .  In general, how does one determine the least upper bound and greatest lower bound of two elements of , and ?  Are there least and\/or greatest elements in ?    Let be the set of all subsets of such that the sum of the elements in is even. (Note that the empty set will be included as an element of .) For instance, is in because is even, but is not in because is odd. Consider the poset . Let and be elements of .  Explain why is not element of the poset.  Use the definitions of the italicized terms and the given partial ordering to complete the following statements:   is an upper bound of and if   is the least element of if   Find three different upper bounds of and .  Find the least upper bound of and . If it doesn't exist, explain why not.       The sum of elements in is odd and disqualifies the set from being an element of the poset.  The following correctly complete the statements in this part.   and    for all ,    Any set that contains the union of but also contains 3 or 5, but not both will be an upper bound. You can create several by including on not including 4 or 8.  The least upper bound doesn't exist. Notice that the union of and isn't in . One of the two sets and is contained within every upper bound of and but neither is contained within the other.      "
},
{
  "id": "def-partial-ordering-13",
  "level": "2",
  "url": "s-posets-revisited.html#def-partial-ordering-13",
  "type": "Definition",
  "number": "13.1.1",
  "title": "Partial Ordering.",
  "body": "Partial Ordering Partial Ordering  Let be a relation on a set . We say that is a partial ordering on if it is reflexive, antisymmetric, and transitive. That is:  is reflexive:  is antisymmetric:   is transitive:   The set together with the relation is called a poset.   "
},
{
  "id": "ex-some-posets",
  "level": "2",
  "url": "s-posets-revisited.html#ex-some-posets",
  "type": "Example",
  "number": "13.1.2",
  "title": "Some posets.",
  "body": "Some posets  We recall a few examples of posets:   is a poset. Notice that our generic symbol for the partial ordering, , is selected to remind us that a partial ordering is similar to less than or equal to.  Let . Then is a poset.  Let . Then is a poset.   "
},
{
  "id": "def-bounds",
  "level": "2",
  "url": "s-posets-revisited.html#def-bounds",
  "type": "Definition",
  "number": "13.1.3",
  "title": "Lower Bound, Upper Bound.",
  "body": "Lower Bound, Upper Bound  Lower Bound  Upper Bound  Let be a poset, and . Then is a lower bound of and if and . Also, is an upper bound of and if and . "
},
{
  "id": "def-glb",
  "level": "2",
  "url": "s-posets-revisited.html#def-glb",
  "type": "Definition",
  "number": "13.1.4",
  "title": "Greatest Lower Bound.",
  "body": "Greatest Lower Bound  Greatest Lower Bound  Let be a poset. If , then is a greatest lower bound of and if and only if    If such that and , then .   "
},
{
  "id": "def-lub",
  "level": "2",
  "url": "s-posets-revisited.html#def-lub",
  "type": "Definition",
  "number": "13.1.5",
  "title": "Least Upper Bound.",
  "body": "Least Upper Bound  Least Upper Bound  Let be a poset. If , then is a least upper bound of and if and only if    If such that if and , then .   "
},
{
  "id": "theorem-unique-lub-glb",
  "level": "2",
  "url": "s-posets-revisited.html#theorem-unique-lub-glb",
  "type": "Theorem",
  "number": "13.1.6",
  "title": "Uniqueness of Least Upper and Greatest Lower Bounds.",
  "body": " Uniqueness of Least Upper and Greatest Lower Bounds  Let be a poset, and . If a greatest lower bound of and exists, then it is unique. The same is true of a least upper bound, if it exists.  Let and be greatest lower bounds of and . We will prove that .   a greatest lower bound of and   is a lower bound of and .   a greatest lower bound of and and a lower bound of and  , by the definition of greatest lower bound.   a greatest lower bound of and  is a lower bound of and .   a greatest lower bound of and and a lower bound of and . by the definition of greatest lower bound.   and by the antisymmetry property of a partial ordering.    The proof of the second statement in the theorem is almost identical to the first and is left to the reader. "
},
{
  "id": "def-greatest-least",
  "level": "2",
  "url": "s-posets-revisited.html#def-greatest-least",
  "type": "Definition",
  "number": "13.1.7",
  "title": "Greatest Element, Least Element.",
  "body": "Greatest Element, Least Element  Greatest Element  Least Element  least element in a poset  greatest element in a poset  Let be a poset. is called the greatest (maximum) element of if, for all , . In addition, is called the least (minimum) element of if for all , . The greatest and least elements, when they exist, are frequently denoted by and respectively. "
},
{
  "id": "ex-bounds-105",
  "level": "2",
  "url": "s-posets-revisited.html#ex-bounds-105",
  "type": "Example",
  "number": "13.1.8",
  "title": "Bounds on the divisors of 105.",
  "body": "Bounds on the divisors of 105 Consider the partial ordering divides on . Then is a poset. To determine the least upper bound of 3 and 7, we look for all , such that and . Certainly, both and satisfy these conditions and no other element of does. Next, since , is the least upper bound of 3 and 7. Similarly, the least upper bound of 3 and 5 is 15. The greatest element of is 105 since for all . To find the greatest lower bound of 15 and 35, we first consider all elements of such that . They are 1, 3, 5, and 15. The elements for which are 1, 5, 7, and 35. From these two lists, we see that and satisfy the required conditions. But since , the greatest lower bound is 5. The least element of is 1 since for all . "
},
{
  "id": "def-set-of-divisors",
  "level": "2",
  "url": "s-posets-revisited.html#def-set-of-divisors",
  "type": "Definition",
  "number": "13.1.9",
  "title": "The Set of Divisors of an Integer.",
  "body": "The Set of Divisors of an Integer  Divisors of an Integer  the set of divisors of integer  For any positive integer , the divisors of is the set of integers that divide evenly into . We denote this set . "
},
{
  "id": "ex-power-3",
  "level": "2",
  "url": "s-posets-revisited.html#ex-power-3",
  "type": "Example",
  "number": "13.1.10",
  "title": "The power set of a three element set.",
  "body": "The power set of a three element set Consider the poset , where . The greatest lower bound of and is . For any other element which is a subset of and (there is only one; what is it?), . The least element of is and the greatest element is . The Hasse diagram of this poset is shown in .   Power Set of    Power Set of    "
},
{
  "id": "exercise-13-1-1",
  "level": "2",
  "url": "s-posets-revisited.html#exercise-13-1-1",
  "type": "Exercise",
  "number": "13.1.1",
  "title": "",
  "body": "Consider the poset , where .  Find all lower bounds of 10 and 15.  Find the greatest lower bound of 10 and 15.  Find all upper bounds of 10 and 15.  Determine the least upper bound of 10 and 15.  Draw the Hasse diagram for with respect to . Compare this Hasse diagram with that of . Note that the two diagrams are structurally the same.     1, 5  5  30  30  See the Sage cell below with the default input displaying a Hasse diagram for .   "
},
{
  "id": "exercises-13-1-3",
  "level": "2",
  "url": "s-posets-revisited.html#exercises-13-1-3",
  "type": "Exercise",
  "number": "13.1.2",
  "title": "",
  "body": " List the elements of the sets , , and . For each set, draw the Hasse diagram for divides.  "
},
{
  "id": "exercise-13-1-3",
  "level": "2",
  "url": "s-posets-revisited.html#exercise-13-1-3",
  "type": "Exercise",
  "number": "13.1.3",
  "title": "",
  "body": "contains Hasse diagrams of posets.  Determine the least upper bound and greatest lower bound of all pairs of elements when they exist. Indicate those pairs that do not have a least upper bound (or a greatest lower bound ).  Find the least and greatest elements when they exist.     Figure for Exercise 3   Section 13.1, Exercise 3      Solution for Hasse diagram (b):      is the least element and is the greatest element.   Partial solution for Hasse diagram (f):   and do not exist.  No greatest element exists, but is the least element.     "
},
{
  "id": "exercises-13-1-5",
  "level": "2",
  "url": "s-posets-revisited.html#exercises-13-1-5",
  "type": "Exercise",
  "number": "13.1.4",
  "title": "",
  "body": " For the poset , what are the greatest lower bound and least upper bound of two elements and ? Are there least and\/or greatest elements? "
},
{
  "id": "exercises-13-1-6",
  "level": "2",
  "url": "s-posets-revisited.html#exercises-13-1-6",
  "type": "Exercise",
  "number": "13.1.5",
  "title": "",
  "body": " Prove the second part of , the least upper bound of two elements in a poset is unique, if one exists.  Prove that if a poset has a least element, then that element is unique.   If and are distinct least elements, then   "
},
{
  "id": "exercises-13-1-7",
  "level": "2",
  "url": "s-posets-revisited.html#exercises-13-1-7",
  "type": "Exercise",
  "number": "13.1.6",
  "title": "",
  "body": "We naturally order the numbers in with less than or equal to, which is a partial ordering. We define an ordering, on the elements of by   Prove that is a partial ordering on .  Draw the ordering diagrams for on , , and .  In general, how does one determine the least upper bound and greatest lower bound of two elements of , and ?  Are there least and\/or greatest elements in ?   "
},
{
  "id": "exercises-13-1-8",
  "level": "2",
  "url": "s-posets-revisited.html#exercises-13-1-8",
  "type": "Exercise",
  "number": "13.1.7",
  "title": "",
  "body": "Let be the set of all subsets of such that the sum of the elements in is even. (Note that the empty set will be included as an element of .) For instance, is in because is even, but is not in because is odd. Consider the poset . Let and be elements of .  Explain why is not element of the poset.  Use the definitions of the italicized terms and the given partial ordering to complete the following statements:   is an upper bound of and if   is the least element of if   Find three different upper bounds of and .  Find the least upper bound of and . If it doesn't exist, explain why not.       The sum of elements in is odd and disqualifies the set from being an element of the poset.  The following correctly complete the statements in this part.   and    for all ,    Any set that contains the union of but also contains 3 or 5, but not both will be an upper bound. You can create several by including on not including 4 or 8.  The least upper bound doesn't exist. Notice that the union of and isn't in . One of the two sets and is contained within every upper bound of and but neither is contained within the other.    "
},
{
  "id": "s-lattices",
  "level": "1",
  "url": "s-lattices.html",
  "type": "Section",
  "number": "13.2",
  "title": "Lattices",
  "body": " Lattices  Lattices  In this section, we restrict our discussion to lattices, those posets for which every pair of elements has both a greatest lower bound and least upper bound. We first introduce some notation.  Join, Meet  Join  Meet  the join, or least upper bound of and  the meet, or greatest lower bound of and  Let be a poset, and . We define:   , read join , as the least upper bound of and , if it exists. and   , read meet , as the greatest lower bound of and , if it exists.    Since the join and meet produce a unique result in all cases where they exist, by , we can consider them as binary operations on a set if they always exist. Thus the following definition:  Lattice  Lattice  A lattice with domain having meet and join operations  A lattice is a poset for which every pair of elements has a greatest lower bound and least upper bound. Since a lattice is an algebraic system with binary operations and , it is denoted by . If we want to make it clear what partial ordering the lattice is based on, we say it is a lattice under .   The power set of a three element set Consider the poset we examined in . It isn't too surprising that every pair of sets had a greatest lower bound and least upper bound. Thus, we have a lattice in this case; and and . The reader is encouraged to write out the operation tables .  Our first concrete lattice can be generalized to the case of any set , producing the lattice , where the join operation is the set operation of union and the meet operation is the operation intersection; that is, and .  It can be shown (see the exercises) that the commutative laws, associative laws, idempotent laws, and absorption laws are all true for any lattice. A concrete example of this is clearly , since these laws hold in the algebra of sets. This lattice also has distributive property in that join is distributive over meet and meet is distributive over join. However, this is not always the case for lattices in general.  Distributive Lattice  Distributive Lattice  Let be a lattice under . is called a distributive lattice if and only if the distributive laws hold; that is, for all we have   A Nondistributive Lattice  We now give an example of a lattice where the distributive laws do not hold. Let . We define the partial ordering on by the set The operation tables for and on are:   Since every pair of elements in has both a join and a meet, is a lattice (under divides). Is this lattice distributive? We note that: and . Therefore, for some values of . Thus, this lattice is not distributive.  Our next observation uses the term sublattice , which we have not defined at this point, but we would hope that you could anticipate a definition, and we will leave it as an exercise to do so.  It can be shown that a lattice is nondistributive if and only if it contains a sublattice isomorphic to one of the lattices in . The ordering diagram on the right of this figure, produces the diamond lattice , which is precisely the one that is defined in . The lattice based on the left hand poset is called the pentagon lattice .   Nondistributive lattices, the pentagon and diamond lattices   Nondistributive Lattices     Exercises  Let be the set of all propositions generated by and . What are the meet and join operations in this lattice under implication? What are the maximum and minimum elements?  Which of the posets in are lattices? Which of the lattices are distributive?   State the commutative laws, associative laws, idempotent laws, and absorption laws for lattices.  Prove laws you stated.     Demonstrate that the pentagon lattice is nondistributive.  What is a reasonable definition of the term sublattice ? One reasonable definition would be this: Let be a lattice and let be a nonempty subset of . Then is a sublattice of if and only if is closed under both and  Let be a lattice based on a partial ordering . Prove that if ,   .   .   and .      "
},
{
  "id": "def-id",
  "level": "2",
  "url": "s-lattices.html#def-id",
  "type": "Definition",
  "number": "13.2.1",
  "title": "Join, Meet.",
  "body": "Join, Meet  Join  Meet  the join, or least upper bound of and  the meet, or greatest lower bound of and  Let be a poset, and . We define:   , read join , as the least upper bound of and , if it exists. and   , read meet , as the greatest lower bound of and , if it exists.   "
},
{
  "id": "def-lattice",
  "level": "2",
  "url": "s-lattices.html#def-lattice",
  "type": "Definition",
  "number": "13.2.2",
  "title": "Lattice.",
  "body": "Lattice  Lattice  A lattice with domain having meet and join operations  A lattice is a poset for which every pair of elements has a greatest lower bound and least upper bound. Since a lattice is an algebraic system with binary operations and , it is denoted by . If we want to make it clear what partial ordering the lattice is based on, we say it is a lattice under .  "
},
{
  "id": "ex-power-set-3-lattice",
  "level": "2",
  "url": "s-lattices.html#ex-power-set-3-lattice",
  "type": "Example",
  "number": "13.2.3",
  "title": "The power set of a three element set.",
  "body": "The power set of a three element set Consider the poset we examined in . It isn't too surprising that every pair of sets had a greatest lower bound and least upper bound. Thus, we have a lattice in this case; and and . The reader is encouraged to write out the operation tables . "
},
{
  "id": "def-distributive-lattice",
  "level": "2",
  "url": "s-lattices.html#def-distributive-lattice",
  "type": "Definition",
  "number": "13.2.4",
  "title": "Distributive Lattice.",
  "body": "Distributive Lattice  Distributive Lattice  Let be a lattice under . is called a distributive lattice if and only if the distributive laws hold; that is, for all we have  "
},
{
  "id": "ex-a-nondistributive-lattice",
  "level": "2",
  "url": "s-lattices.html#ex-a-nondistributive-lattice",
  "type": "Example",
  "number": "13.2.5",
  "title": "A Nondistributive Lattice.",
  "body": "A Nondistributive Lattice  We now give an example of a lattice where the distributive laws do not hold. Let . We define the partial ordering on by the set The operation tables for and on are:   Since every pair of elements in has both a join and a meet, is a lattice (under divides). Is this lattice distributive? We note that: and . Therefore, for some values of . Thus, this lattice is not distributive. "
},
{
  "id": "s-lattices-13",
  "level": "2",
  "url": "s-lattices.html#s-lattices-13",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "diamond lattice pentagon lattice "
},
{
  "id": "fig-nondistributive-lattices",
  "level": "2",
  "url": "s-lattices.html#fig-nondistributive-lattices",
  "type": "Figure",
  "number": "13.2.6",
  "title": "",
  "body": " Nondistributive lattices, the pentagon and diamond lattices   Nondistributive Lattices   "
},
{
  "id": "exercises-13-2-2",
  "level": "2",
  "url": "s-lattices.html#exercises-13-2-2",
  "type": "Exercise",
  "number": "13.2.1",
  "title": "",
  "body": "Let be the set of all propositions generated by and . What are the meet and join operations in this lattice under implication? What are the maximum and minimum elements? "
},
{
  "id": "exercises-13-2-3",
  "level": "2",
  "url": "s-lattices.html#exercises-13-2-3",
  "type": "Exercise",
  "number": "13.2.2",
  "title": "",
  "body": "Which of the posets in are lattices? Which of the lattices are distributive? "
},
{
  "id": "exercises-13-2-4",
  "level": "2",
  "url": "s-lattices.html#exercises-13-2-4",
  "type": "Exercise",
  "number": "13.2.3",
  "title": "",
  "body": " State the commutative laws, associative laws, idempotent laws, and absorption laws for lattices.  Prove laws you stated.    "
},
{
  "id": "exercises-13-2-5",
  "level": "2",
  "url": "s-lattices.html#exercises-13-2-5",
  "type": "Exercise",
  "number": "13.2.4",
  "title": "",
  "body": "Demonstrate that the pentagon lattice is nondistributive. "
},
{
  "id": "exercises-13-2-6",
  "level": "2",
  "url": "s-lattices.html#exercises-13-2-6",
  "type": "Exercise",
  "number": "13.2.5",
  "title": "",
  "body": "What is a reasonable definition of the term sublattice ? One reasonable definition would be this: Let be a lattice and let be a nonempty subset of . Then is a sublattice of if and only if is closed under both and "
},
{
  "id": "exercises-13-2-7",
  "level": "2",
  "url": "s-lattices.html#exercises-13-2-7",
  "type": "Exercise",
  "number": "13.2.6",
  "title": "",
  "body": "Let be a lattice based on a partial ordering . Prove that if ,   .   .   and .    "
},
{
  "id": "s-boolean-algebras",
  "level": "1",
  "url": "s-boolean-algebras.html",
  "type": "Section",
  "number": "13.3",
  "title": "Boolean Algebras",
  "body": " Boolean Algebras  Boolean Algebras  In order to define a Boolean algebra, we need the additional concept of complementation. A lattice must have both a greatest element and a least element in order for complementation to take place. The following definition will save us some words in the rest of this section.  Bounded Lattice  Bounded Lattice  A bounded lattice is a lattice that contains both a least element and a greatest element.  We use the symbols and for the least and greatest elements of a bounded lattice in the remainder of this section.  The Complement of a Lattice Element  Complement of a Lattice Element  Let be a bounded lattice. If , then has a complement if there exists such that   Notice that by the commutative laws for lattices, if complements , then complements .  Complemented Lattice  Complemented Lattice  Let be a bounded lattice. is a complemented lattice if every element of has a complement in .  Set Complement is a Complement In Chapter 1, we defined the complement of a subset of any universe. This turns out to be a concrete example of the general concept we have just defined, but we will reason through why this is the case here. Let , where . Then is a bounded lattice with and . To find the complement, if it exists, of , for example, we want such that It's not too difficult to see that , since we need to include to make the first condition true and can't include or if the second condition is to be true. Of course this is precisely how we defined in Chapter 1. Since it can be shown that each element of has a complement (see Exercise 1), is a complemented lattice. Note that if is any set and , then is a complemented lattice where the complement of is .  In , we observed that the complement of each element of is unique. Is this always true in a complemented lattice? The answer is no. Consider the following.  A Lattice for which complements are not unique Let and consider the lattice (under divides ). The least element of is 1 and the greatest element is 30. Let us compute the complement of the element . We want to determine such that and . Certainly, works, but so does , so the complement of in this lattice is not unique. However, is still a complemented lattice since each element does have at least one complement.   Complementation as an operation  Complement of a Lattice Element as an operation  The complement of lattice element  If a complemented lattice has the property that the complement of every element is unique, then we consider complementation to be a unary operation. The usual notation for the complement of is .    The following theorem gives us an insight into when uniqueness of complements occurs.   One condition for unique complements  If is a complemented, distributive lattice, then the complement of each element is unique.  Let and assume to the contrary that has two complements, namely and . Then by the definition of complement, .  Then On the other hand, Hence , which contradicts the assumption that has two different complements.  Boolean Algebra  Boolean Algebra  a boolean algebra with operations join, meet and complementation  A Boolean algebra is a lattice that contains a least element and a greatest element and that is both complemented and distributive. The notation is used to denote the boolean algebra with operations join, meet and complementation.  Since the complement of each element in a Boolean algebra is unique (by ), complementation is a valid unary operation over the set under discussion, which is why we will list it together with the other two operations to emphasize that we are discussing a set together with three operations. Also, to help emphasize the distinction between lattices and lattices that are Boolean algebras, we will use the letter as the generic symbol for the set of a Boolean algebra; that is, will stand for a general Boolean algebra.  Boolean Algebra of Sets Let be any set, and let . Then is a Boolean algebra. Here, stands for the complement of an element of with respect to , .  This is a key example for us since all finite Boolean algebras and many infinite Boolean algebras look like this example for some . In fact, a glance at the basic Boolean algebra laws in , in comparison with the set laws of Chapter 4 and the basic laws of logic of Chapter 3, indicates that all three systems behave the same; that is, they are isomorphic.  Divisors of 30 A somewhat less standard example of a boolean algebra is derived from the lattice of divisors of 30 under the relation divides . If you examine the ordering diagram for this lattice, you see that it is structurally the same as the boolean algebra of subsets of a three element set. Therefore, the join, meet and complementation operations act the same as union, intersection and set complementation. We might conjecture that the lattice of divisors of any integer will produce a boolean algebra, but it is only the case of certain integers. Try out a few integers to see if you can identify what is necessary to produce a boolean algebra.   Basic Boolean Algebra Laws   Commutative Laws     Associative Laws     Distributive Laws     Identity Laws     Complement Laws     Idempotent Laws     Null Laws     Absorption Laws     DeMorgan's Laws     Involution Law      The pairings of the boolean algebra laws reminds us of the principle of duality, which we state for a Boolean algebra.  Principle of Duality for Boolean Algebras  Duality for Boolean Algebras   Let be a Boolean algebra under , and let be a true statement for . If is obtained from by replacing with (this is equivalent to turning the graph upside down), with , with , with , and with , then is also a true statement in .   Exercises   Determine the complement of each element in . Is this lattice a Boolean algebra? Why?   This lattice is a Boolean algebra since it is a distributive complemented lattice.     Determine the complement of each element of in .  Repeat part a using the lattice in .  Repeat part a using the lattice in .  Are the lattices in parts a, b, and c Boolean algebras? Why?     Determine which of the lattices of of Section 13.1 are Boolean algebras.  a and g.  Let and .  Prove that is a Boolean algebra.  Write out the operation tables for the Boolean algebra.    It can be shown that the following statement, , holds for any Boolean algebra : if and only if .  Write the dual, , of the statement .  Write the statement and its dual, , in the language of sets.  Are the statements in part b true for all sets?  Write the statement and its dual, , in the language of logic.  Are the statements in part d true for all propositions?       The dual of is  Yes  The dual of is  Yes    State the dual of:   .   .   .     Formulate a definition for isomorphic Boolean algebras.   is isomorphic to if and only if there exists a function such that   is a bijection;       .     For what positive integers, , does the lattice produce a boolean algebra?    "
},
{
  "id": "def-boundeded-lattice",
  "level": "2",
  "url": "s-boolean-algebras.html#def-boundeded-lattice",
  "type": "Definition",
  "number": "13.3.1",
  "title": "Bounded Lattice.",
  "body": "Bounded Lattice  Bounded Lattice  A bounded lattice is a lattice that contains both a least element and a greatest element. "
},
{
  "id": "def-lattice-complement",
  "level": "2",
  "url": "s-boolean-algebras.html#def-lattice-complement",
  "type": "Definition",
  "number": "13.3.2",
  "title": "The Complement of a Lattice Element.",
  "body": "The Complement of a Lattice Element  Complement of a Lattice Element  Let be a bounded lattice. If , then has a complement if there exists such that  "
},
{
  "id": "def-complemented-lattice",
  "level": "2",
  "url": "s-boolean-algebras.html#def-complemented-lattice",
  "type": "Definition",
  "number": "13.3.3",
  "title": "Complemented Lattice.",
  "body": "Complemented Lattice  Complemented Lattice  Let be a bounded lattice. is a complemented lattice if every element of has a complement in . "
},
{
  "id": "ex-set-complement-is-complementation",
  "level": "2",
  "url": "s-boolean-algebras.html#ex-set-complement-is-complementation",
  "type": "Example",
  "number": "13.3.4",
  "title": "Set Complement is a Complement.",
  "body": "Set Complement is a Complement In Chapter 1, we defined the complement of a subset of any universe. This turns out to be a concrete example of the general concept we have just defined, but we will reason through why this is the case here. Let , where . Then is a bounded lattice with and . To find the complement, if it exists, of , for example, we want such that It's not too difficult to see that , since we need to include to make the first condition true and can't include or if the second condition is to be true. Of course this is precisely how we defined in Chapter 1. Since it can be shown that each element of has a complement (see Exercise 1), is a complemented lattice. Note that if is any set and , then is a complemented lattice where the complement of is . "
},
{
  "id": "ex-nonunique-complements",
  "level": "2",
  "url": "s-boolean-algebras.html#ex-nonunique-complements",
  "type": "Example",
  "number": "13.3.5",
  "title": "A Lattice for which complements are not unique.",
  "body": "A Lattice for which complements are not unique Let and consider the lattice (under divides ). The least element of is 1 and the greatest element is 30. Let us compute the complement of the element . We want to determine such that and . Certainly, works, but so does , so the complement of in this lattice is not unique. However, is still a complemented lattice since each element does have at least one complement. "
},
{
  "id": "def-complement-operation",
  "level": "2",
  "url": "s-boolean-algebras.html#def-complement-operation",
  "type": "Definition",
  "number": "13.3.6",
  "title": "Complementation as an operation.",
  "body": " Complementation as an operation  Complement of a Lattice Element as an operation  The complement of lattice element  If a complemented lattice has the property that the complement of every element is unique, then we consider complementation to be a unary operation. The usual notation for the complement of is .   "
},
{
  "id": "theorem-unique-complements",
  "level": "2",
  "url": "s-boolean-algebras.html#theorem-unique-complements",
  "type": "Theorem",
  "number": "13.3.7",
  "title": "One condition for unique complements.",
  "body": " One condition for unique complements  If is a complemented, distributive lattice, then the complement of each element is unique.  Let and assume to the contrary that has two complements, namely and . Then by the definition of complement, .  Then On the other hand, Hence , which contradicts the assumption that has two different complements. "
},
{
  "id": "def-boolean-algebra",
  "level": "2",
  "url": "s-boolean-algebras.html#def-boolean-algebra",
  "type": "Definition",
  "number": "13.3.8",
  "title": "Boolean Algebra.",
  "body": "Boolean Algebra  Boolean Algebra  a boolean algebra with operations join, meet and complementation  A Boolean algebra is a lattice that contains a least element and a greatest element and that is both complemented and distributive. The notation is used to denote the boolean algebra with operations join, meet and complementation. "
},
{
  "id": "ex-set-boolean-algebra",
  "level": "2",
  "url": "s-boolean-algebras.html#ex-set-boolean-algebra",
  "type": "Example",
  "number": "13.3.9",
  "title": "Boolean Algebra of Sets.",
  "body": "Boolean Algebra of Sets Let be any set, and let . Then is a Boolean algebra. Here, stands for the complement of an element of with respect to , .  This is a key example for us since all finite Boolean algebras and many infinite Boolean algebras look like this example for some . In fact, a glance at the basic Boolean algebra laws in , in comparison with the set laws of Chapter 4 and the basic laws of logic of Chapter 3, indicates that all three systems behave the same; that is, they are isomorphic. "
},
{
  "id": "ex-boolean-algebra-d30",
  "level": "2",
  "url": "s-boolean-algebras.html#ex-boolean-algebra-d30",
  "type": "Example",
  "number": "13.3.10",
  "title": "Divisors of 30.",
  "body": "Divisors of 30 A somewhat less standard example of a boolean algebra is derived from the lattice of divisors of 30 under the relation divides . If you examine the ordering diagram for this lattice, you see that it is structurally the same as the boolean algebra of subsets of a three element set. Therefore, the join, meet and complementation operations act the same as union, intersection and set complementation. We might conjecture that the lattice of divisors of any integer will produce a boolean algebra, but it is only the case of certain integers. Try out a few integers to see if you can identify what is necessary to produce a boolean algebra. "
},
{
  "id": "table-boolean-laws",
  "level": "2",
  "url": "s-boolean-algebras.html#table-boolean-laws",
  "type": "Table",
  "number": "13.3.11",
  "title": "Basic Boolean Algebra Laws",
  "body": " Basic Boolean Algebra Laws   Commutative Laws     Associative Laws     Distributive Laws     Identity Laws     Complement Laws     Idempotent Laws     Null Laws     Absorption Laws     DeMorgan's Laws     Involution Law     "
},
{
  "id": "def-boolean-duality",
  "level": "2",
  "url": "s-boolean-algebras.html#def-boolean-duality",
  "type": "Definition",
  "number": "13.3.12",
  "title": "Principle of Duality for Boolean Algebras.",
  "body": "Principle of Duality for Boolean Algebras  Duality for Boolean Algebras   Let be a Boolean algebra under , and let be a true statement for . If is obtained from by replacing with (this is equivalent to turning the graph upside down), with , with , with , and with , then is also a true statement in . "
},
{
  "id": "exercises-13-3-2",
  "level": "2",
  "url": "s-boolean-algebras.html#exercises-13-3-2",
  "type": "Exercise",
  "number": "13.3.1",
  "title": "",
  "body": " Determine the complement of each element in . Is this lattice a Boolean algebra? Why?   This lattice is a Boolean algebra since it is a distributive complemented lattice. "
},
{
  "id": "exercises-13-3-3",
  "level": "2",
  "url": "s-boolean-algebras.html#exercises-13-3-3",
  "type": "Exercise",
  "number": "13.3.2",
  "title": "",
  "body": "   Determine the complement of each element of in .  Repeat part a using the lattice in .  Repeat part a using the lattice in .  Are the lattices in parts a, b, and c Boolean algebras? Why?    "
},
{
  "id": "exercises-13-3-4",
  "level": "2",
  "url": "s-boolean-algebras.html#exercises-13-3-4",
  "type": "Exercise",
  "number": "13.3.3",
  "title": "",
  "body": "Determine which of the lattices of of Section 13.1 are Boolean algebras.  a and g. "
},
{
  "id": "exercises-13-3-5",
  "level": "2",
  "url": "s-boolean-algebras.html#exercises-13-3-5",
  "type": "Exercise",
  "number": "13.3.4",
  "title": "",
  "body": "Let and .  Prove that is a Boolean algebra.  Write out the operation tables for the Boolean algebra.   "
},
{
  "id": "exercises-13-3-6",
  "level": "2",
  "url": "s-boolean-algebras.html#exercises-13-3-6",
  "type": "Exercise",
  "number": "13.3.5",
  "title": "",
  "body": "It can be shown that the following statement, , holds for any Boolean algebra : if and only if .  Write the dual, , of the statement .  Write the statement and its dual, , in the language of sets.  Are the statements in part b true for all sets?  Write the statement and its dual, , in the language of logic.  Are the statements in part d true for all propositions?       The dual of is  Yes  The dual of is  Yes   "
},
{
  "id": "exercises-13-3-7",
  "level": "2",
  "url": "s-boolean-algebras.html#exercises-13-3-7",
  "type": "Exercise",
  "number": "13.3.6",
  "title": "",
  "body": "State the dual of:   .   .   .    "
},
{
  "id": "exercises-13-3-8",
  "level": "2",
  "url": "s-boolean-algebras.html#exercises-13-3-8",
  "type": "Exercise",
  "number": "13.3.7",
  "title": "",
  "body": "Formulate a definition for isomorphic Boolean algebras.   is isomorphic to if and only if there exists a function such that   is a bijection;       .    "
},
{
  "id": "exercises-13-3-9",
  "level": "2",
  "url": "s-boolean-algebras.html#exercises-13-3-9",
  "type": "Exercise",
  "number": "13.3.8",
  "title": "",
  "body": "For what positive integers, , does the lattice produce a boolean algebra?  "
},
{
  "id": "s-atoms-of-a-boolean-algebra",
  "level": "1",
  "url": "s-atoms-of-a-boolean-algebra.html",
  "type": "Section",
  "number": "13.4",
  "title": "Atoms of a Boolean Algebra",
  "body": " Atoms of a Boolean Algebra  In this section we will look more closely at something we've hinted at, which is that every finite Boolean algebra is isomorphic to an algebra of sets. We will show that every finite Boolean algebra has elements for some with precisely generators, called atoms.  Consider the Boolean algebra , whose ordering diagram is depicted in   Illustration of the atom concept   Illustration of the atom concept    We note that , , , and ; that is, each of the elements above level one can be described completely and uniquely in terms of the elements on level one. The 's have uniquely generated the non-least elements of much like a basis in linear algebra generates the elements in a vector space. We also note that the 's are the immediate successors of the minimum element, 0. In any Boolean algebra, the immediate successors of the minimum element are called atoms . For example, let be any nonempty set. In the Boolean algebra (over ), the singleton sets are the generators, or atoms, of the algebraic structure since each element can be described completely and uniquely as the join, or union, of singleton sets.  Atom  Atom of a Boolean Algebra  A non-least element in a Boolean algebra is called an atom if for every , or .  The condition that tells us that is a successor of ; that is, , as depicted in (a)  The condition is true only when and are not connected. This occurs when is another atom or if is a successor of atoms different from , as depicted in (b).   Conditions for an atom   Conditions for an atom    An alternate definition of an atom is based on the concept of covering.  The Covering Relation  covering relation  Given a Boolean algebra , let . We say that  covers  iff and there does not exist with .  It can be proven that the atoms of Boolean algebra are precisely those elements that cover the zero element.  The set of atoms of the Boolean algebra is . To see that is an atom, let be any non-least element of and note that one of the two conditions or holds. Of course, to apply the definition to this Boolean algebra, we must remind ourselves that in this case the 0-element is 1, the operation is greatest common divisor, and the poset relation is divides. So if , we have (or ), so Condition 1 holds. If , the first condition is not true. (Why?) However, Condition 2, , is true. The reader is encouraged to show that 3 and 5 also satisfy the definition of an atom. Next, if we should compute the join (the least common multiple in this case) of all possible combinations of the atoms 2, 3, and 5 to generate all nonzero (non-1 in this case) elements of . For example, and . We state this concept formally in the following theorem, which we give without proof.   Let be any finite Boolean algebra. Let be the set of all atoms of . Then every element in can be expressed uniquely as the join of a subset of .  The least element in relation to this theorem bears noting. If we consider the empty set of atoms, we would consider the join of elements in the empty set to be the least element. This makes the statement of the theorem above a bit more tidy since we don't need to qualify what elements can be generated from atoms.  We now ask ourselves if we can be more definitive about the structure of different Boolean algebras of a given order. Certainly, the Boolean algebras and have the same graph (that of ), the same number of atoms, and, in all respects, look the same except for the names of the elements and the operations. In fact, when we apply corresponding operations to corresponding elements, we obtain corresponding results. We know from Chapter 11 that this means that the two structures are isomorphic as Boolean algebras. Furthermore, the graphs of these examples are exactly the same as that of , which is an arbitrary Boolean algebra of order .  In these examples of a Boolean algebra of order 8, we note that each had 3 atoms and number of elements, and all were isomorphic to , where . This leads us to the following questions:  Are there any different (nonisomorphic) Boolean algebras of order 8?  What is the relationship, if any, between finite Boolean algebras and their atoms?  How many different (nonisomorphic) Boolean algebras are there of order 2? Order 3? Order 4? etc.    The answers to these questions are given in the following theorem and corollaries.   Let be any finite Boolean algebra, and let A be the set of all atoms of . Then is isomorphic to    An isomorphism that serves to prove this theorem is defined by , where is interpreted as the zero of . We leave it to the reader to prove that this is indeed an isomorphism.   Every finite Boolean algebra has elements for some positive integer .  Let be the set of all atoms of and let . Then there are exactly elements (subsets) in ,and by , is isomorphic to and must also have elements.   All Boolean algebras of order are isomorphic to one another.    Isomorphisms to be combined   Isomorphism to be combined    Every Boolean algebra of order is isomorphic to when . Hence, if and each have elements, they each have atoms. Suppose their sets of atoms are and , respectively. We know there are isomorphisms and , where , . In addition we have an isomorphism, from into , which we ask you to prove in . We can combine these isomorphisms to produce the isomorphism , which proves the corollary.  The above theorem and corollaries tell us that we can only have finite Boolean algebras of orders , and that all finite Boolean algebras of any given order are isomorphic. These are powerful tools in determining the structure of finite Boolean algebras. In the next section, we will discuss one of the easiest ways of describing a Boolean algebra of any given order.  Exercises   Show that is an atom of the Boolean algebra .  Repeat part a for the elements 3 and 5 of .  Verify for the Boolean algebra .      For we must show that for each one of the following is true: or . We do this through the following table: For , a similar verification can be performed.  , , , and .    Let .  Rewrite the definition of atom for . What does mean in this example?  Find all atoms of .  Verify for .    Verify and its corollaries for the Boolean algebras in Exercises 1 and 2 of this section.  If 30 then and is isomorphic to , where and   Give an example of a Boolean algebra of order 16 whose elements are certain subsets of the set    implies that there do not exist Boolean algebras of orders 3, 5, 6, 7, 9, etc. (orders different from ). Without this corollary, directly show that we cannot have a Boolean algebra of order 3.  Assume that is a Boolean algebra of order 3 where and show that this cannot happen by investigating the possibilities for its operation tables. Assume that is the third element of a Boolean algebra. Then there is only one possible set of tables for join and meet, all following from required properties of the Boolean algebra. Next, to find the complement of we want such that and . No element satisfies both conditions; hence the lattice is not complemented and cannot be a Boolean algebra. The lack of a complement can also be seen from the ordering diagram from which and must be derived.   There are many different, yet isomorphic, Boolean algebras with two elements. Describe one such Boolean algebra that is derived from a power set, , under . Describe a second that is described from , for some , under divides.  Since the elements of a two-element Boolean algebra must be the greatest and least elements, 1 and 0, the tables for the operations on are determined by the Boolean algebra laws. Write out the operation tables for .      Find a Boolean algebra with a countably infinite number of elements.  Let be any countably infinite set, such as the integers. A subset of is cofinite if it is finite or its complement is finite. The set of all cofinite subsets of is:  Countably infinite - this might not be obvious, but here is a hint. Assume . For each finite subset of , map that set to the integer You can do a similar thing to sets that have a finite complement, but map them to negative integers. Only one minor adjustment needs to be made to accommodate both the empty set and .  Closed under union  Closed under intersection, and  Closed under complementation.  Therefore, if , then is a countable Boolean algebra under the usual set operations.  Prove that the direct product of two Boolean algebras is a Boolean algebra.  Copy the corresponding proof for groups in Section 11.6.  Prove if two finite sets and both have elements then is isomorphic to  Prove an element of a Boolean algebra is an atom if and only if it covers the zero element.   "
},
{
  "id": "fig-atoms",
  "level": "2",
  "url": "s-atoms-of-a-boolean-algebra.html#fig-atoms",
  "type": "Figure",
  "number": "13.4.1",
  "title": "",
  "body": " Illustration of the atom concept   Illustration of the atom concept   "
},
{
  "id": "s-atoms-of-a-boolean-algebra-5",
  "level": "2",
  "url": "s-atoms-of-a-boolean-algebra.html#s-atoms-of-a-boolean-algebra-5",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "atoms "
},
{
  "id": "def-atom",
  "level": "2",
  "url": "s-atoms-of-a-boolean-algebra.html#def-atom",
  "type": "Definition",
  "number": "13.4.2",
  "title": "Atom.",
  "body": "Atom  Atom of a Boolean Algebra  A non-least element in a Boolean algebra is called an atom if for every , or . "
},
{
  "id": "fig-atom-conditions",
  "level": "2",
  "url": "s-atoms-of-a-boolean-algebra.html#fig-atom-conditions",
  "type": "Figure",
  "number": "13.4.3",
  "title": "",
  "body": " Conditions for an atom   Conditions for an atom   "
},
{
  "id": "def-cover",
  "level": "2",
  "url": "s-atoms-of-a-boolean-algebra.html#def-cover",
  "type": "Definition",
  "number": "13.4.4",
  "title": "The Covering Relation.",
  "body": "The Covering Relation  covering relation  Given a Boolean algebra , let . We say that  covers  iff and there does not exist with . "
},
{
  "id": "theorem-atom-join",
  "level": "2",
  "url": "s-atoms-of-a-boolean-algebra.html#theorem-atom-join",
  "type": "Theorem",
  "number": "13.4.5",
  "title": "",
  "body": " Let be any finite Boolean algebra. Let be the set of all atoms of . Then every element in can be expressed uniquely as the join of a subset of . "
},
{
  "id": "theorem-boolean-set-isomorphism",
  "level": "2",
  "url": "s-atoms-of-a-boolean-algebra.html#theorem-boolean-set-isomorphism",
  "type": "Theorem",
  "number": "13.4.6",
  "title": "",
  "body": " Let be any finite Boolean algebra, and let A be the set of all atoms of . Then is isomorphic to    An isomorphism that serves to prove this theorem is defined by , where is interpreted as the zero of . We leave it to the reader to prove that this is indeed an isomorphism. "
},
{
  "id": "corollary-power-of-2",
  "level": "2",
  "url": "s-atoms-of-a-boolean-algebra.html#corollary-power-of-2",
  "type": "Corollary",
  "number": "13.4.7",
  "title": "",
  "body": " Every finite Boolean algebra has elements for some positive integer .  Let be the set of all atoms of and let . Then there are exactly elements (subsets) in ,and by , is isomorphic to and must also have elements. "
},
{
  "id": "corollary-one-power-2",
  "level": "2",
  "url": "s-atoms-of-a-boolean-algebra.html#corollary-one-power-2",
  "type": "Corollary",
  "number": "13.4.8",
  "title": "",
  "body": " All Boolean algebras of order are isomorphic to one another.    Isomorphisms to be combined   Isomorphism to be combined    Every Boolean algebra of order is isomorphic to when . Hence, if and each have elements, they each have atoms. Suppose their sets of atoms are and , respectively. We know there are isomorphisms and , where , . In addition we have an isomorphism, from into , which we ask you to prove in . We can combine these isomorphisms to produce the isomorphism , which proves the corollary. "
},
{
  "id": "exercises-13-4-2",
  "level": "2",
  "url": "s-atoms-of-a-boolean-algebra.html#exercises-13-4-2",
  "type": "Exercise",
  "number": "13.4.1",
  "title": "",
  "body": " Show that is an atom of the Boolean algebra .  Repeat part a for the elements 3 and 5 of .  Verify for the Boolean algebra .      For we must show that for each one of the following is true: or . We do this through the following table: For , a similar verification can be performed.  , , , and .   "
},
{
  "id": "exercises-13-4-3",
  "level": "2",
  "url": "s-atoms-of-a-boolean-algebra.html#exercises-13-4-3",
  "type": "Exercise",
  "number": "13.4.2",
  "title": "",
  "body": "Let .  Rewrite the definition of atom for . What does mean in this example?  Find all atoms of .  Verify for .   "
},
{
  "id": "exercises-13-4-4",
  "level": "2",
  "url": "s-atoms-of-a-boolean-algebra.html#exercises-13-4-4",
  "type": "Exercise",
  "number": "13.4.3",
  "title": "",
  "body": "Verify and its corollaries for the Boolean algebras in Exercises 1 and 2 of this section.  If 30 then and is isomorphic to , where and  "
},
{
  "id": "exercises-13-4-5",
  "level": "2",
  "url": "s-atoms-of-a-boolean-algebra.html#exercises-13-4-5",
  "type": "Exercise",
  "number": "13.4.4",
  "title": "",
  "body": "Give an example of a Boolean algebra of order 16 whose elements are certain subsets of the set  "
},
{
  "id": "exercises-13-4-6",
  "level": "2",
  "url": "s-atoms-of-a-boolean-algebra.html#exercises-13-4-6",
  "type": "Exercise",
  "number": "13.4.5",
  "title": "",
  "body": " implies that there do not exist Boolean algebras of orders 3, 5, 6, 7, 9, etc. (orders different from ). Without this corollary, directly show that we cannot have a Boolean algebra of order 3.  Assume that is a Boolean algebra of order 3 where and show that this cannot happen by investigating the possibilities for its operation tables. Assume that is the third element of a Boolean algebra. Then there is only one possible set of tables for join and meet, all following from required properties of the Boolean algebra. Next, to find the complement of we want such that and . No element satisfies both conditions; hence the lattice is not complemented and cannot be a Boolean algebra. The lack of a complement can also be seen from the ordering diagram from which and must be derived. "
},
{
  "id": "exercises-13-4-7",
  "level": "2",
  "url": "s-atoms-of-a-boolean-algebra.html#exercises-13-4-7",
  "type": "Exercise",
  "number": "13.4.6",
  "title": "",
  "body": " There are many different, yet isomorphic, Boolean algebras with two elements. Describe one such Boolean algebra that is derived from a power set, , under . Describe a second that is described from , for some , under divides.  Since the elements of a two-element Boolean algebra must be the greatest and least elements, 1 and 0, the tables for the operations on are determined by the Boolean algebra laws. Write out the operation tables for .    "
},
{
  "id": "exercises-13-4-8",
  "level": "2",
  "url": "s-atoms-of-a-boolean-algebra.html#exercises-13-4-8",
  "type": "Exercise",
  "number": "13.4.7",
  "title": "",
  "body": " Find a Boolean algebra with a countably infinite number of elements.  Let be any countably infinite set, such as the integers. A subset of is cofinite if it is finite or its complement is finite. The set of all cofinite subsets of is:  Countably infinite - this might not be obvious, but here is a hint. Assume . For each finite subset of , map that set to the integer You can do a similar thing to sets that have a finite complement, but map them to negative integers. Only one minor adjustment needs to be made to accommodate both the empty set and .  Closed under union  Closed under intersection, and  Closed under complementation.  Therefore, if , then is a countable Boolean algebra under the usual set operations. "
},
{
  "id": "exercises-13-4-9",
  "level": "2",
  "url": "s-atoms-of-a-boolean-algebra.html#exercises-13-4-9",
  "type": "Exercise",
  "number": "13.4.8",
  "title": "",
  "body": "Prove that the direct product of two Boolean algebras is a Boolean algebra.  Copy the corresponding proof for groups in Section 11.6. "
},
{
  "id": "exercise-set-boolean-isomorphism",
  "level": "2",
  "url": "s-atoms-of-a-boolean-algebra.html#exercise-set-boolean-isomorphism",
  "type": "Exercise",
  "number": "13.4.9",
  "title": "",
  "body": "Prove if two finite sets and both have elements then is isomorphic to "
},
{
  "id": "exercise-covering-definition",
  "level": "2",
  "url": "s-atoms-of-a-boolean-algebra.html#exercise-covering-definition",
  "type": "Exercise",
  "number": "13.4.10",
  "title": "",
  "body": "Prove an element of a Boolean algebra is an atom if and only if it covers the zero element. "
},
{
  "id": "s-finite-boolean-algebras-ntuples",
  "level": "1",
  "url": "s-finite-boolean-algebras-ntuples.html",
  "type": "Section",
  "number": "13.5",
  "title": "Finite Boolean Algebras as <span xmlns:pf=\"https:\/\/prefigure.org\" class=\"process-math\">\\(n\\)<\/span>-tuples of 0’s and 1’s",
  "body": " Finite Boolean Algebras as -tuples of 0's and 1's  From the previous section we know that all finite Boolean algebras are of order , where is the number of atoms in the algebra. We can therefore completely describe every finite Boolean algebra by the algebra of power sets. Is there a more convenient, or at least an alternate way, of defining finite Boolean algebras? In Chapter 11 we found that we could produce new groups by taking Cartesian products of previously known groups. We imitate this process for Boolean algebras.  The simplest nontrivial Boolean algebra is the Boolean algebra on the set . The ordering on is the natural one, . If we treat 0 and 1 as the truth values false and true, respectively, we see that the Boolean operations and are nothing more than the logical operation with the same symbols. The Boolean operation, , (complementation) is the logical (negation). In fact, this is why these symbols were chosen as the names of the Boolean operations. The operation tables for are simply those of or,  and, and not, which we repeat here.   By and its corollaries, all Boolean algebras of order 2 are isomorphic to this one.  We know that if we form we obtain the set , a set of order 4. We define operations on the natural way, namely componentwise, so that , and . We claim that is a Boolean algebra under the componentwise operations. Hence, is a Boolean algebra of order 4. Since all Boolean algebras of order 4 are isomorphic to one other, we have found a simple way of describing all Boolean algebras of order 4.  It is quite clear that we can describe any Boolean algebra of order 8 by considering and, more generally, any Boolean algebra of order with ( factors).    Exercises   Write out the operation tables for  Draw the Hasse diagram for and compare your results with .  Find the atoms of this Boolean algebra.       The graphs are isomorphic.  (0, 1) and (1,0)     Write out the operation tables for  Draw the Hasse diagram for     List all atoms of .  Describe the atoms of .      , , , and are the atoms.  The -tuples of bits with exactly one 1.    tells us we can think of any finite Boolean algebra in terms of sets. In Chapter 4, we defined minsets and minset normal form . Rephrase these definitions in the language of Boolean algebra. The generalization of minsets are called minterms .    "
},
{
  "id": "exercises-13-5-2",
  "level": "2",
  "url": "s-finite-boolean-algebras-ntuples.html#exercises-13-5-2",
  "type": "Exercise",
  "number": "13.5.1",
  "title": "",
  "body": " Write out the operation tables for  Draw the Hasse diagram for and compare your results with .  Find the atoms of this Boolean algebra.       The graphs are isomorphic.  (0, 1) and (1,0)   "
},
{
  "id": "exercises-13-5-3",
  "level": "2",
  "url": "s-finite-boolean-algebras-ntuples.html#exercises-13-5-3",
  "type": "Exercise",
  "number": "13.5.2",
  "title": "",
  "body": " Write out the operation tables for  Draw the Hasse diagram for   "
},
{
  "id": "exercises-13-5-4",
  "level": "2",
  "url": "s-finite-boolean-algebras-ntuples.html#exercises-13-5-4",
  "type": "Exercise",
  "number": "13.5.3",
  "title": "",
  "body": " List all atoms of .  Describe the atoms of .      , , , and are the atoms.  The -tuples of bits with exactly one 1.   "
},
{
  "id": "exercises-13-5-5",
  "level": "2",
  "url": "s-finite-boolean-algebras-ntuples.html#exercises-13-5-5",
  "type": "Exercise",
  "number": "13.5.4",
  "title": "",
  "body": "tells us we can think of any finite Boolean algebra in terms of sets. In Chapter 4, we defined minsets and minset normal form . Rephrase these definitions in the language of Boolean algebra. The generalization of minsets are called minterms .  "
},
{
  "id": "s-boolean-expressions",
  "level": "1",
  "url": "s-boolean-expressions.html",
  "type": "Section",
  "number": "13.6",
  "title": "Boolean Expressions",
  "body": " Boolean Expressions  Boolean Expressions  In this section, we will use our background from the previous sections and set theory to develop a procedure for simplifying Boolean expressions. This procedure has considerable application to the simplification of circuits in switching theory or logical design.   Boolean Expression  Boolean Expression  Let be any Boolean algebra, and let be variables in ; that is, variables that can assume values from . A Boolean expression generated by is any valid combination of the and the elements of with the operations of meet, join, and complementation.    This definition is the analog of the definition of a proposition generated by a set of propositions, presented in Section 3.2.  Each Boolean expression generated by variables, , defines a function where . If is a finite Boolean algebra, then there are a finite number of functions from into . Those functions that are defined in terms of Boolean expressions are called Boolean functions. As we will see, there is an infinite number of Boolean expressions that define each Boolean function. Naturally, the shortest of these expressions will be preferred. Since electronic circuits can be described as Boolean functions with , this economization is quite useful.  In what follows, we make use of in Section 7.1 for counting number of functions.  Two variables over  Consider any Boolean algebra of order 2, . How many functions are there? First, all Boolean algebras of order 2 are isomorphic to so we want to determine the number of functions . If we consider a Boolean function of two variables, and , we note that each variable has two possible values 0 and 1, so there are ways of assigning these two values to the variables. Hence, the table below has rows. So far we have a table such as this one:   How many possible different functions can there be? To list a few: , , , , , etc. Each of these will fill in the question marks in the table above. The tables for and are   Two functions are different if and only if their tables are different for at least one row. Of course by using the basic laws of Boolean algebra we can see that . Why? So if we simply list by brute force all combinations of we will obtain unnecessary duplication. However, we note that for any combination of the variables , and there are only two possible values for , namely 0 or 1. Thus, we could write different functions on 2 variables.   Now, let's count the number of different Boolean functions in a more general setting. We will consider two cases: first, when , and second, when is any finite Boolean algebra with elements.  Let . Each function is defined in terms of a table having rows. Therefore, since there are two possible images for each element of , there are 2 raised to the , or different functions. We will show that every one of these functions is a Boolean function.  Now suppose that . A function from into can still be defined in terms of a table. There are rows to each table and possible images for each row. Therefore, there are raised to the power different functions. We will show that if , not every one of these functions is a Boolean function.  Since all Boolean algebras are isomorphic to a Boolean algebra of sets, the analogues of statements in sets are useful in Boolean algebras.  Minterm  Minterm  the minterm generated by , where if and if  A Boolean expression generated by that has the form where each may be either or is called a minterm generated by . We use the notation for the minterm generated by , where if and if   An example of the notation is that .  By a direct application of the Rule of Products we see that there are different minterms generated by .  Minterm Normal Form  Minterm Normal Form  A Boolean expression generated by is in minterm normal form if it is the join of expressions of the form , where and is a minterm generated by . That is, it is of the form where , and are the minterms generated by .    We seem to require every minterm generated by , in , and we really do. However, some of the values of can be , which effectively makes the corresponding minterm disappear.  If , then each in a minterm normal form is either 0 or 1. Therefore, is either 0 or .    Uniqueness of Minterm Normal Form  Let be a Boolean expression over B. There exists a unique minterm normal form that is equivalent to in the sense that e and M define the same function from into .  The uniqueness in this theorem does not include the possible ordering of the minterms in (commonly referred to as uniqueness up to the order of minterms ). The proof of this theorem would be quite lengthy, and not very instructive, so we will leave it to the interested reader to attempt. The implications of the theorem are very interesting, however.  If , then there are raised to the different minterm normal forms. Since each different minterm normal form defines a different function, there are a like number of Boolean functions from into . If , there are as many Boolean functions (2 raised to the ) as there are functions from into , since there are raised to the functions from into . The significance of this result is that any desired function can be realized using electronic circuits having 0 or 1 (off or on, positive or negative) values.  More complex, multivalued circuits corresponding to boolean algebras with more than two values would not have this flexibility because of the number of minterm normal forms, and hence the number of boolean functions, is strictly less than the number of functions.  We will close this section by examining minterm normal forms for expressions over , since they are a starting point for circuit economization.   Consider the Boolean expression . One method of determining the minterm normal form of is to think in terms of sets. Consider the diagram with the usual translation of notation in . Then    Visualization of minterms for    Visualization of minterms      Definition of the boolean function              Consider the function defined by .  The minterm normal form for can be obtained by taking the join of minterms that correspond to rows that have an image value of 1. If , then include the minterm where Or, to use alternate notation, include in the expression if and only if  Therefore,    Karnaugh map The minterm normal form is a first step in obtaining an economical way of expressing a given Boolean function. For functions of more than three variables, the above set theory approach tends to be awkward. Other procedures are used to write the normal form. The most convenient is the Karnaugh map, a discussion of which can be found in any logical design\/switching theory text (see, for example, ), on .   Exercises    Write the 16 possible functions of .  Write out the tables of several of the above Boolean functions to show that they are indeed different.  Determine the minterm normal forms of              The truth table for the functions in part (a) are                Consider the Boolean expression on   Simplify this expression using basic Boolean algebra laws.  Write this expression in minterm normal form.  Write out the table for the given function defined by and compare it to the tables of the functions in parts a and b.  How many possible different functions in three variables on are there?    Let be a Boolean algebra of order 4, and let be a Boolean function of two variables on .  How many elements are there in the domain of f?  How many different Boolean functions are there of two, variables? Three variables?  Determine the minterm normal form of .  If , define a function from into that is not a Boolean function.     The number of elements in the domain of is  With two variables, there are different Boolean functions. With three variables, there are different Boolean functions.    Consider , defined by , , , , and , with the images of all other pairs in defined arbitrarily. This function is not a Boolean function. If we assume that it is Boolean function then can be computed with a Boolean expression . This expression can be put into minterm normal form:  Therefore, and so, using this formula, This contradicts , and so is not a Boolean function.     "
},
{
  "id": "def-boolean-expression",
  "level": "2",
  "url": "s-boolean-expressions.html#def-boolean-expression",
  "type": "Definition",
  "number": "13.6.1",
  "title": "Boolean Expression.",
  "body": " Boolean Expression  Boolean Expression  Let be any Boolean algebra, and let be variables in ; that is, variables that can assume values from . A Boolean expression generated by is any valid combination of the and the elements of with the operations of meet, join, and complementation.   "
},
{
  "id": "ex-two-variable-b2",
  "level": "2",
  "url": "s-boolean-expressions.html#ex-two-variable-b2",
  "type": "Example",
  "number": "13.6.2",
  "title": "Two variables over <span xmlns:pf=\"https:\/\/prefigure.org\" class=\"process-math\">\\(B_2\\)<\/span>.",
  "body": "Two variables over  Consider any Boolean algebra of order 2, . How many functions are there? First, all Boolean algebras of order 2 are isomorphic to so we want to determine the number of functions . If we consider a Boolean function of two variables, and , we note that each variable has two possible values 0 and 1, so there are ways of assigning these two values to the variables. Hence, the table below has rows. So far we have a table such as this one:   How many possible different functions can there be? To list a few: , , , , , etc. Each of these will fill in the question marks in the table above. The tables for and are   Two functions are different if and only if their tables are different for at least one row. Of course by using the basic laws of Boolean algebra we can see that . Why? So if we simply list by brute force all combinations of we will obtain unnecessary duplication. However, we note that for any combination of the variables , and there are only two possible values for , namely 0 or 1. Thus, we could write different functions on 2 variables.  "
},
{
  "id": "def-minterm",
  "level": "2",
  "url": "s-boolean-expressions.html#def-minterm",
  "type": "Definition",
  "number": "13.6.3",
  "title": "Minterm.",
  "body": "Minterm  Minterm  the minterm generated by , where if and if  A Boolean expression generated by that has the form where each may be either or is called a minterm generated by . We use the notation for the minterm generated by , where if and if  "
},
{
  "id": "def-minterm-normal-form",
  "level": "2",
  "url": "s-boolean-expressions.html#def-minterm-normal-form",
  "type": "Definition",
  "number": "13.6.4",
  "title": "Minterm Normal Form.",
  "body": "Minterm Normal Form  Minterm Normal Form  A Boolean expression generated by is in minterm normal form if it is the join of expressions of the form , where and is a minterm generated by . That is, it is of the form where , and are the minterms generated by . "
},
{
  "id": "s-boolean-expressions-17",
  "level": "2",
  "url": "s-boolean-expressions.html#s-boolean-expressions-17",
  "type": "Note",
  "number": "13.6.5",
  "title": "",
  "body": "  We seem to require every minterm generated by , in , and we really do. However, some of the values of can be , which effectively makes the corresponding minterm disappear.  If , then each in a minterm normal form is either 0 or 1. Therefore, is either 0 or .   "
},
{
  "id": "th-minterm-normal-form",
  "level": "2",
  "url": "s-boolean-expressions.html#th-minterm-normal-form",
  "type": "Theorem",
  "number": "13.6.6",
  "title": "Uniqueness of Minterm Normal Form.",
  "body": "Uniqueness of Minterm Normal Form  Let be a Boolean expression over B. There exists a unique minterm normal form that is equivalent to in the sense that e and M define the same function from into . "
},
{
  "id": "ex-mnf-example-1",
  "level": "2",
  "url": "s-boolean-expressions.html#ex-mnf-example-1",
  "type": "Example",
  "number": "13.6.7",
  "title": "",
  "body": " Consider the Boolean expression . One method of determining the minterm normal form of is to think in terms of sets. Consider the diagram with the usual translation of notation in . Then    Visualization of minterms for    Visualization of minterms    "
},
{
  "id": "table-boolean-function",
  "level": "2",
  "url": "s-boolean-expressions.html#table-boolean-function",
  "type": "Table",
  "number": "13.6.9",
  "title": "Definition of the boolean function <span xmlns:pf=\"https:\/\/prefigure.org\" class=\"process-math\">\\(g\\)<\/span>",
  "body": " Definition of the boolean function             "
},
{
  "id": "ex-mnf-example-2",
  "level": "2",
  "url": "s-boolean-expressions.html#ex-mnf-example-2",
  "type": "Example",
  "number": "13.6.10",
  "title": "",
  "body": "Consider the function defined by .  The minterm normal form for can be obtained by taking the join of minterms that correspond to rows that have an image value of 1. If , then include the minterm where Or, to use alternate notation, include in the expression if and only if  Therefore,   "
},
{
  "id": "exercises-13-6-2",
  "level": "2",
  "url": "s-boolean-expressions.html#exercises-13-6-2",
  "type": "Exercise",
  "number": "13.6.1",
  "title": "",
  "body": "  Write the 16 possible functions of .  Write out the tables of several of the above Boolean functions to show that they are indeed different.  Determine the minterm normal forms of              The truth table for the functions in part (a) are               "
},
{
  "id": "exercises-13-6-3",
  "level": "2",
  "url": "s-boolean-expressions.html#exercises-13-6-3",
  "type": "Exercise",
  "number": "13.6.2",
  "title": "",
  "body": "Consider the Boolean expression on   Simplify this expression using basic Boolean algebra laws.  Write this expression in minterm normal form.  Write out the table for the given function defined by and compare it to the tables of the functions in parts a and b.  How many possible different functions in three variables on are there?   "
},
{
  "id": "exercises-13-6-4",
  "level": "2",
  "url": "s-boolean-expressions.html#exercises-13-6-4",
  "type": "Exercise",
  "number": "13.6.3",
  "title": "",
  "body": "Let be a Boolean algebra of order 4, and let be a Boolean function of two variables on .  How many elements are there in the domain of f?  How many different Boolean functions are there of two, variables? Three variables?  Determine the minterm normal form of .  If , define a function from into that is not a Boolean function.     The number of elements in the domain of is  With two variables, there are different Boolean functions. With three variables, there are different Boolean functions.    Consider , defined by , , , , and , with the images of all other pairs in defined arbitrarily. This function is not a Boolean function. If we assume that it is Boolean function then can be computed with a Boolean expression . This expression can be put into minterm normal form:  Therefore, and so, using this formula, This contradicts , and so is not a Boolean function.   "
},
{
  "id": "s-logic-design",
  "level": "1",
  "url": "s-logic-design.html",
  "type": "Section",
  "number": "13.7",
  "title": "A Brief Introduction to Switching Theory and Logic Design",
  "body": " A Brief Introduction to Switching Theory and Logic Design  Switching Theory  Logic Design  Disclaimer : I'm still looking for a good application for drawing logic gates. The figures here are quite rough.  Early computers relied on many switches to perform the logical operations needed for computation. This was true as late as the 1970's when early personal computers such as the Altair ( ) started to appear. Pioneering computer scientists such as Claude Shannon realized that the operation of these computers could be simplified by making use of an isomorphism between computer circuits and boolean algebra. The term Switching Theory was used at the time. Logical gates realized through increasingly smaller and smaller integrated circuits still perform the same functions as in early computers, but using purely electronic means. In this section, we give examples of some switching circuits. Soon afterward, we will transition to the more modern form of circuits that are studied in Logic Design , where gates replace switches. Our main goal is to give you an overview of how boolean functions corresponds to any such circuit. We will introduce the common system notation used in logic design and show how it corresponds with the mathematical notation of Boolean algebras. Any computer scientist should be familiar with both systems.   The Altair Computer, an early PC, by Todd Dailey, Creative Commons   Image of an Altair computer from https:\/\/en.wikipedia.org\/wiki\/Altair_8800#\/media\/File:Altair_8800_at_the_Computer_History_Museum,_cropped.jpg at the Computer History Museum    The simplest switching device is the on-off switch. If the switch is closed\/ON, current will pass through it; if it is open\/OFF, current will not pass through it. If we designate ON by 1, and OFF by 0, we can describe electrical circuits containing switches by Boolean expressions with the variables representing the variable states of switches or the variable bits passing through gates.  The electronics involved in these switches take into account whether we are negating a switch or not. For electromagnetic switches, a magnet is used to control whether the switch is open or closed. The magnets themselves may be controlled by simple ON\/OFF switches. There are two types of electromagnetic switches. One is normally open (OFF) when the magnet is not activated, but activating the magnet will close the circuit and the switch is then ON. A separate type of switch corresponds with a negated switch. For that type, the switch is closed when the magnet is not activated, and when the magnet is activated, the switch opens. We won't be overly concerned with the details of these switches or the electronics corresponding to logical gates. We will simply assume they are available to plug into a circuit. For simplicity, we use the inversion symbol on a variable that labels a switch to indicate that it is a switch of the second type, as in .  Standby power generators that many people have in their homes use a transfer switch to connect the generator to the home power system. This switch is open (OFF) if there is power coming from the normal municipal power supply. It stays OFF because a magnet is keeping it open. When power is lost, the magnet is no longer activated, and the switch closes and is ON. So the transfer switch is a normally ON switch.    Representation of a normally OFF switch controlled by variable    Representation of a normally OFF switch     Representation of a normally ON switch controlled by variable    Representation of a normally ON switch     The standard notation used for Boolean algebra operations in switching theory and logic design is for join, instead of ; and for meet, instead of . Complementation is the same in both notational systems, denoted with an overline.  The expression represents the situation in which a series of two switches appears in sequence as in . In order for current to flow through the circuit, both switches must be ON; that is, they must both have the value 1. Similarly, a pair of parallel switches, as in , is described algebraically by . Here, current flows through this part of the circuit as long as at least on of the switches is ON.    Two switches in AND configuration realizing    Two switches in AND configuration      Two switches in OR configuration realizing    Two switches in OR configuration     All laws and concepts developed previously for Boolean algebras hold. The only change is purely notational. We make the change in this section solely to introduce the reader to another frequently used system of notation.  Many of the laws of Boolean algebra can be visualized thought switching theory. For example, the distributive law of meet over join is expressed as The switching circuit analogue of the above statement is that the circuits in the two images below are equivalent. In circuit (b), the presence of two 's might represent two electromagnetic switches controlled by the same magnet.    (a)   The first of equivalent switching circuits     (b)   The second of equivalent switching circuits     The circuits in a computer are now composed of large quantities of gates, which serve the same purpose as switches, but can be miniaturized to a great degree. For example, the OR gate, usually drawn as in implements the logical OR function. This happens electronically, but is equivalent to . The AND gate, which is equivalent to two sequential switches is shown in .    An OR gate   An OR gate     An AND gate   An AND gate     The complementation process is represented in a gate diagram by an inverter, as pictured in .   Inverter, or NOT gate   Inverter     When drawing more complex circuits, multiple AND's or OR's are sometimes depicted using a more general gate drawing. For example if we want to depict an OR gate with three inputs that is ON as long as at least one input is ON, we would draw it as in , although this would really be two binary gates, as in . Both diagrams are realizing the boolean expression . Strictly speaking, the gates in represent , but the associative law for join tells us that the grouping doesn't matter.    Simple version of a ternary OR gate   Simple version of a ternary OR gate     A ternary OR gate created with binary OR gates   A ternary OR gate created with binary OR gates     In , we show a few other commonly used gates, XOR, NAND, and NOR, which correspond to the boolean exressions , , and , respectively.   Other common gates   Other common gates that represent XOR, NAND, and NOR     Let's start with a logic circuit and see how the laws of boolean algebra can help us simplify it.  Simplification of a circuit  Consider the circuit in . As usual, we assume that three inputs enter on the left and the output exits on the right.   Initial gate diagram   Initial gate diagram    If we trace the inputs through the gates we see that this circuit realizes the boolean function   We simplify the boolean expression that defines , simplifying the circuit in so doing. You should be able to identify the laws of Boolean algebra that are used in each of the steps. See . Therefore, , which can be realized with the much simpler circuit in , without using the input .   Simplified gate diagram   Simplified gate diagram     Next, we start with a table of desired outputs based on three bits of input and design an efficient circuit to realize this output.   Consider the following table of desired outputs for the three input bits .   Desired output table    0 0 0 0  0 0 1 1  0 1 0 0  0 1 1 0  1 0 0 1  1 0 1 1  1 1 0 0  1 1 1 0    The first step is to write the of . Since we are working with the two value Boolean algebra, , the constants in each minterm are either 0 or 1, and we simply list the minterms that have a 1. These correspond with the rows of the table above that have an output of 1. We will then attempt to simplify the expression as much as possible.     Therefore we can realize our table with the boolean function . A circuit diagram for this function is . But is this the simplest circuit that realizes the table? See .   A realization of the table of desired outputs.   Simplified realization of the desired table of outputs       List the laws of boolean algebra that justify the steps in the simplification of the boolean function in . Some steps use more than one law.   Associative, commutative, and idempotent laws.  Distributive law.  Idempotent and complement laws.  Null and identity laws  Distributive law.  Null and identity laws.    Write the following Boolean expression in the notation of logic design.    Find a further simplification of the boolean function in , and draw the corresponding gate diagram for the circuit that it realizes. A simpler boolean expression for the function is .   An even simpler circuit   Figure for exercise 3      Consider the switching circuit in .   Can this circuit be simplifed?   Figure for exercise 3      Draw the corresponding gate diagram for this circuit.  Construct a table of outputs for each of the eight inputs to this circuit.  Determine the minterm normal of the Boolean function based on the table.  Simplify the circuit as much as possible.      Consider the circuit in .   Can this circuit be simplifed?   Figure for exercise 3      Trace the inputs though this circuit and determine the Boolean function that it realizes.  Construct a table of outputs for each of the eight inputs to this circuit.  Find the minterm normal form of .  Draw the circuit based on the minterm normal form.  Simplify the circuit algebraically and draw the resulting circuit.     Consider the Boolean function    Simplify algebraically.  Draw the gate diagram based on the simplified version of .    Draw a logic circuit using only AND, OR and NOT gates that realizes an XOR gate.   Draw a logic circuit using only AND, OR and NOT gates that realizes the Boolean function on three variables that returns 1 if the majority of inputs are 1 and 0 otherwise.    "
},
{
  "id": "s-logic-design-5",
  "level": "2",
  "url": "s-logic-design.html#s-logic-design-5",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Switching Theory Logic Design "
},
{
  "id": "fig-altair",
  "level": "2",
  "url": "s-logic-design.html#fig-altair",
  "type": "Figure",
  "number": "13.7.1",
  "title": "",
  "body": " The Altair Computer, an early PC, by Todd Dailey, Creative Commons   Image of an Altair computer from https:\/\/en.wikipedia.org\/wiki\/Altair_8800#\/media\/File:Altair_8800_at_the_Computer_History_Museum,_cropped.jpg at the Computer History Museum   "
},
{
  "id": "fig-switch-normal-off",
  "level": "2",
  "url": "s-logic-design.html#fig-switch-normal-off",
  "type": "Figure",
  "number": "13.7.2",
  "title": "",
  "body": " Representation of a normally OFF switch controlled by variable    Representation of a normally OFF switch   "
},
{
  "id": "fig-switch-normal-on",
  "level": "2",
  "url": "s-logic-design.html#fig-switch-normal-on",
  "type": "Figure",
  "number": "13.7.3",
  "title": "",
  "body": " Representation of a normally ON switch controlled by variable    Representation of a normally ON switch   "
},
{
  "id": "fig-and-switch",
  "level": "2",
  "url": "s-logic-design.html#fig-and-switch",
  "type": "Figure",
  "number": "13.7.4",
  "title": "",
  "body": " Two switches in AND configuration realizing    Two switches in AND configuration   "
},
{
  "id": "fig-or-switch",
  "level": "2",
  "url": "s-logic-design.html#fig-or-switch",
  "type": "Figure",
  "number": "13.7.5",
  "title": "",
  "body": " Two switches in OR configuration realizing    Two switches in OR configuration   "
},
{
  "id": "fig-switches-distributive1",
  "level": "2",
  "url": "s-logic-design.html#fig-switches-distributive1",
  "type": "Figure",
  "number": "13.7.6",
  "title": "",
  "body": " (a)   The first of equivalent switching circuits   "
},
{
  "id": "fig-switches-distributive2",
  "level": "2",
  "url": "s-logic-design.html#fig-switches-distributive2",
  "type": "Figure",
  "number": "13.7.7",
  "title": "",
  "body": " (b)   The second of equivalent switching circuits   "
},
{
  "id": "fig-or-gate",
  "level": "2",
  "url": "s-logic-design.html#fig-or-gate",
  "type": "Figure",
  "number": "13.7.8",
  "title": "",
  "body": " An OR gate   An OR gate   "
},
{
  "id": "fig-and-gate",
  "level": "2",
  "url": "s-logic-design.html#fig-and-gate",
  "type": "Figure",
  "number": "13.7.9",
  "title": "",
  "body": " An AND gate   An AND gate   "
},
{
  "id": "fig-inverter",
  "level": "2",
  "url": "s-logic-design.html#fig-inverter",
  "type": "Figure",
  "number": "13.7.10",
  "title": "",
  "body": " Inverter, or NOT gate   Inverter   "
},
{
  "id": "fig-triple-or",
  "level": "2",
  "url": "s-logic-design.html#fig-triple-or",
  "type": "Figure",
  "number": "13.7.11",
  "title": "",
  "body": " Simple version of a ternary OR gate   Simple version of a ternary OR gate   "
},
{
  "id": "fig-triple-or-real",
  "level": "2",
  "url": "s-logic-design.html#fig-triple-or-real",
  "type": "Figure",
  "number": "13.7.12",
  "title": "",
  "body": " A ternary OR gate created with binary OR gates   A ternary OR gate created with binary OR gates   "
},
{
  "id": "fig-gates",
  "level": "2",
  "url": "s-logic-design.html#fig-gates",
  "type": "Figure",
  "number": "13.7.13",
  "title": "",
  "body": " Other common gates   Other common gates that represent XOR, NAND, and NOR   "
},
{
  "id": "example-logic-design-1",
  "level": "2",
  "url": "s-logic-design.html#example-logic-design-1",
  "type": "Example",
  "number": "13.7.14",
  "title": "Simplification of a circuit.",
  "body": "Simplification of a circuit  Consider the circuit in . As usual, we assume that three inputs enter on the left and the output exits on the right.   Initial gate diagram   Initial gate diagram    If we trace the inputs through the gates we see that this circuit realizes the boolean function   We simplify the boolean expression that defines , simplifying the circuit in so doing. You should be able to identify the laws of Boolean algebra that are used in each of the steps. See . Therefore, , which can be realized with the much simpler circuit in , without using the input .   Simplified gate diagram   Simplified gate diagram    "
},
{
  "id": "example-logic-design-2",
  "level": "2",
  "url": "s-logic-design.html#example-logic-design-2",
  "type": "Example",
  "number": "13.7.17",
  "title": "",
  "body": " Consider the following table of desired outputs for the three input bits .   Desired output table    0 0 0 0  0 0 1 1  0 1 0 0  0 1 1 0  1 0 0 1  1 0 1 1  1 1 0 0  1 1 1 0    The first step is to write the of . Since we are working with the two value Boolean algebra, , the constants in each minterm are either 0 or 1, and we simply list the minterms that have a 1. These correspond with the rows of the table above that have an output of 1. We will then attempt to simplify the expression as much as possible.     Therefore we can realize our table with the boolean function . A circuit diagram for this function is . But is this the simplest circuit that realizes the table? See .   A realization of the table of desired outputs.   Simplified realization of the desired table of outputs    "
},
{
  "id": "exercise-137-1",
  "level": "2",
  "url": "s-logic-design.html#exercise-137-1",
  "type": "Exercise",
  "number": "13.7.1",
  "title": "",
  "body": "List the laws of boolean algebra that justify the steps in the simplification of the boolean function in . Some steps use more than one law.   Associative, commutative, and idempotent laws.  Distributive law.  Idempotent and complement laws.  Null and identity laws  Distributive law.  Null and identity laws.   "
},
{
  "id": "s-logic-design-27-2",
  "level": "2",
  "url": "s-logic-design.html#s-logic-design-27-2",
  "type": "Exercise",
  "number": "13.7.2",
  "title": "",
  "body": "Write the following Boolean expression in the notation of logic design.   "
},
{
  "id": "exerc-simpler-circuit",
  "level": "2",
  "url": "s-logic-design.html#exerc-simpler-circuit",
  "type": "Exercise",
  "number": "13.7.3",
  "title": "",
  "body": "Find a further simplification of the boolean function in , and draw the corresponding gate diagram for the circuit that it realizes. A simpler boolean expression for the function is .   An even simpler circuit   Figure for exercise 3    "
},
{
  "id": "s-logic-design-27-4",
  "level": "2",
  "url": "s-logic-design.html#s-logic-design-27-4",
  "type": "Exercise",
  "number": "13.7.4",
  "title": "",
  "body": " Consider the switching circuit in .   Can this circuit be simplifed?   Figure for exercise 3      Draw the corresponding gate diagram for this circuit.  Construct a table of outputs for each of the eight inputs to this circuit.  Determine the minterm normal of the Boolean function based on the table.  Simplify the circuit as much as possible.    "
},
{
  "id": "s-logic-design-27-5",
  "level": "2",
  "url": "s-logic-design.html#s-logic-design-27-5",
  "type": "Exercise",
  "number": "13.7.5",
  "title": "",
  "body": " Consider the circuit in .   Can this circuit be simplifed?   Figure for exercise 3      Trace the inputs though this circuit and determine the Boolean function that it realizes.  Construct a table of outputs for each of the eight inputs to this circuit.  Find the minterm normal form of .  Draw the circuit based on the minterm normal form.  Simplify the circuit algebraically and draw the resulting circuit.    "
},
{
  "id": "s-logic-design-27-6",
  "level": "2",
  "url": "s-logic-design.html#s-logic-design-27-6",
  "type": "Exercise",
  "number": "13.7.6",
  "title": "",
  "body": "Consider the Boolean function    Simplify algebraically.  Draw the gate diagram based on the simplified version of .   "
},
{
  "id": "s-logic-design-27-7",
  "level": "2",
  "url": "s-logic-design.html#s-logic-design-27-7",
  "type": "Exercise",
  "number": "13.7.7",
  "title": "",
  "body": "Draw a logic circuit using only AND, OR and NOT gates that realizes an XOR gate.  "
},
{
  "id": "s-logic-design-27-8",
  "level": "2",
  "url": "s-logic-design.html#s-logic-design-27-8",
  "type": "Exercise",
  "number": "13.7.8",
  "title": "",
  "body": "Draw a logic circuit using only AND, OR and NOT gates that realizes the Boolean function on three variables that returns 1 if the majority of inputs are 1 and 0 otherwise.  "
},
{
  "id": "s-Monoids",
  "level": "1",
  "url": "s-Monoids.html",
  "type": "Section",
  "number": "14.1",
  "title": "Monoids",
  "body": " Monoids  Monoids  Recall that in we introduced systems called monoids. Here is the formal definition.  Monoid Monoid  A monoid is a set together with a binary operation with the properties   is associative: , and   has an identity in : such that ,   Since the requirements for a group contain the requirements for a monoid, every group is a monoid.  Some Monoids   The power set of any set together with any one of the operations intersection, union, or symmetric difference is a monoid.  The set of integers, , with multiplication, is a monoid. With addition, is also a monoid.  The set of matrices over the integers, , , with matrix multiplication, is a monoid. This follows from the fact that matrix multiplication is associative and has an identity, . This is an example of a noncommutative monoid since there are matrices, and , for which .   , is a monoid with identity 1.  Let be a nonempty set. The set of all functions from into , often denoted , is a monoid over function composition. In Chapter 7, we saw that function composition is associative. The function defined by is the identity element for this system. If is greater than 1 then it is a noncommutative monoid. If is finite, . For example, if . The functions defined by the graphs in , are the elements of . This monoid is not a group. Do you know why?  One reason why is noncommutative is that because while .     The functions on    The functions on    Virtually all of the group concepts that were discussed in Chapter 11 are applicable to monoids. When we introduced subsystems, we saw that a submonoid of monoid is a subset of ; that is, it is a monoid with the operation of . To prove that a subset is a submonoid, you can apply the following theorem.  Submonoid Test  Assume is a monoid and is a nonempty subset of . Then is a submonoid of if and only if the following two conditions are met.  If , then. ; i. e., is closed with under .  The identity of belongs to .    Often we will want to discuss the smallest submonoid that includes a certain subset of a monoid . This submonoid can be defined recursively by the following definition.  Submonoid Generated by a Set Submonoid Generated by a Set  If is a subset of monoid , the submonoid generated by , , is defined by:.  (Basis) The identity of belongs to ; and .  (Recursion) .  If , we write in place of .  Some Submonoids  One example of a submonoid of is .  The power set of , , over union is a monoid with identity . If , then is the power set of . If then is the set of finite subsets of the integers.    As you might expect, two monoids are isomorphic if and only if there exists a translation rule between them so that any true proposition in one monoid is translated to a true proposition in the other.   is isomorphic to , where the operation in is componentwise mod 2 multiplication. A translation rule is that if , then it is translated to where Two cases of how this translation rule works are: .  A more precise definition of a monoid isomorphism is identical to the definition of a group isomorphism, .   Exercises  For each of the subsets of the indicated monoid, determine whether the subset is a submonoid.   and in   and in the monoid .   in      is not a submonoid since the identity of , which is 1, is not in . is a submonoid since and is closed under multiplication; that is, for all , is in .  The identity of is the identity function defined by , . If , , thus the identity of is in . However, the image of 1 under any function in is 2, and thus the identity of is not in , so is not a submonoid. The composition of any two functions in , and , will be a function in : and the two conditions of a submonoid are satisfied and is a submonoid of .  The first set is a submonoid, but the second is not since the null set has a non-finite complement.    For each subset, describe the submonoid that it generates.   in    the set of prime numbers in      An matrix of real numbers is called stochastic if and only if each entry is nonnegative and the sum of entries in each column is 1. Prove that the set of stochastic matrices is a monoid over matrix multiplication. The set of real matrices is a monoid under matrix multiplication. This follows from the laws of matrix algebra in Chapter 5. To prove that the set of stochastic matrices is a monoid over matrix multiplication, we need only show that the identity matrix is stochastic (this is obvious) and that the set of stochastic matrices is closed under matrix multiplication. Let and be stochastic matrices.   The sum of the column is   A semigroup is an algebraic system with the only axiom that be associative on . Prove that if is a finite set, then there must exist an idempotent element, that is, an such that .  Let be a Boolean algebra and the set of all Boolean functions on . Let be defined on by . Prove that is a monoid. Construct the operation table of for the case of .  Let , and . Therefore and is associative.  The identity for is the function where = the one of . If , . Therefore . Similarly, .  There are functions in for . These four functions are named in the text. See . The table for is    "
},
{
  "id": "def-monoid",
  "level": "2",
  "url": "s-Monoids.html#def-monoid",
  "type": "Definition",
  "number": "14.1.1",
  "title": "Monoid.",
  "body": "Monoid Monoid  A monoid is a set together with a binary operation with the properties   is associative: , and   has an identity in : such that ,  "
},
{
  "id": "s-Monoids-5",
  "level": "2",
  "url": "s-Monoids.html#s-Monoids-5",
  "type": "Note",
  "number": "14.1.2",
  "title": "",
  "body": "Since the requirements for a group contain the requirements for a monoid, every group is a monoid. "
},
{
  "id": "ex-some-monoids",
  "level": "2",
  "url": "s-Monoids.html#ex-some-monoids",
  "type": "Example",
  "number": "14.1.3",
  "title": "Some Monoids.",
  "body": "Some Monoids   The power set of any set together with any one of the operations intersection, union, or symmetric difference is a monoid.  The set of integers, , with multiplication, is a monoid. With addition, is also a monoid.  The set of matrices over the integers, , , with matrix multiplication, is a monoid. This follows from the fact that matrix multiplication is associative and has an identity, . This is an example of a noncommutative monoid since there are matrices, and , for which .   , is a monoid with identity 1.  Let be a nonempty set. The set of all functions from into , often denoted , is a monoid over function composition. In Chapter 7, we saw that function composition is associative. The function defined by is the identity element for this system. If is greater than 1 then it is a noncommutative monoid. If is finite, . For example, if . The functions defined by the graphs in , are the elements of . This monoid is not a group. Do you know why?  One reason why is noncommutative is that because while .   "
},
{
  "id": "fig-functions-on-B2",
  "level": "2",
  "url": "s-Monoids.html#fig-functions-on-B2",
  "type": "Figure",
  "number": "14.1.4",
  "title": "",
  "body": " The functions on    The functions on   "
},
{
  "id": "th-submonoid",
  "level": "2",
  "url": "s-Monoids.html#th-submonoid",
  "type": "Theorem",
  "number": "14.1.5",
  "title": "Submonoid Test.",
  "body": "Submonoid Test  Assume is a monoid and is a nonempty subset of . Then is a submonoid of if and only if the following two conditions are met.  If , then. ; i. e., is closed with under .  The identity of belongs to .   "
},
{
  "id": "def-submonoid-generated-by-a-set",
  "level": "2",
  "url": "s-Monoids.html#def-submonoid-generated-by-a-set",
  "type": "Definition",
  "number": "14.1.6",
  "title": "Submonoid Generated by a Set.",
  "body": "Submonoid Generated by a Set Submonoid Generated by a Set  If is a subset of monoid , the submonoid generated by , , is defined by:.  (Basis) The identity of belongs to ; and .  (Recursion) . "
},
{
  "id": "s-Monoids-12",
  "level": "2",
  "url": "s-Monoids.html#s-Monoids-12",
  "type": "Note",
  "number": "14.1.7",
  "title": "",
  "body": "If , we write in place of . "
},
{
  "id": "ex-some-submonoids",
  "level": "2",
  "url": "s-Monoids.html#ex-some-submonoids",
  "type": "Example",
  "number": "14.1.8",
  "title": "Some Submonoids.",
  "body": "Some Submonoids  One example of a submonoid of is .  The power set of , , over union is a monoid with identity . If , then is the power set of . If then is the set of finite subsets of the integers.   "
},
{
  "id": "ex-14-1-iso",
  "level": "2",
  "url": "s-Monoids.html#ex-14-1-iso",
  "type": "Example",
  "number": "14.1.9",
  "title": "",
  "body": " is isomorphic to , where the operation in is componentwise mod 2 multiplication. A translation rule is that if , then it is translated to where Two cases of how this translation rule works are: . "
},
{
  "id": "exercises-14-1-2",
  "level": "2",
  "url": "s-Monoids.html#exercises-14-1-2",
  "type": "Exercise",
  "number": "14.1.1",
  "title": "",
  "body": "For each of the subsets of the indicated monoid, determine whether the subset is a submonoid.   and in   and in the monoid .   in      is not a submonoid since the identity of , which is 1, is not in . is a submonoid since and is closed under multiplication; that is, for all , is in .  The identity of is the identity function defined by , . If , , thus the identity of is in . However, the image of 1 under any function in is 2, and thus the identity of is not in , so is not a submonoid. The composition of any two functions in , and , will be a function in : and the two conditions of a submonoid are satisfied and is a submonoid of .  The first set is a submonoid, but the second is not since the null set has a non-finite complement.   "
},
{
  "id": "exercises-14-1-3",
  "level": "2",
  "url": "s-Monoids.html#exercises-14-1-3",
  "type": "Exercise",
  "number": "14.1.2",
  "title": "",
  "body": "For each subset, describe the submonoid that it generates.   in    the set of prime numbers in     "
},
{
  "id": "exercises-14-1-4",
  "level": "2",
  "url": "s-Monoids.html#exercises-14-1-4",
  "type": "Exercise",
  "number": "14.1.3",
  "title": "",
  "body": "An matrix of real numbers is called stochastic if and only if each entry is nonnegative and the sum of entries in each column is 1. Prove that the set of stochastic matrices is a monoid over matrix multiplication. The set of real matrices is a monoid under matrix multiplication. This follows from the laws of matrix algebra in Chapter 5. To prove that the set of stochastic matrices is a monoid over matrix multiplication, we need only show that the identity matrix is stochastic (this is obvious) and that the set of stochastic matrices is closed under matrix multiplication. Let and be stochastic matrices.   The sum of the column is  "
},
{
  "id": "exercises-14-1-5",
  "level": "2",
  "url": "s-Monoids.html#exercises-14-1-5",
  "type": "Exercise",
  "number": "14.1.4",
  "title": "",
  "body": "A semigroup is an algebraic system with the only axiom that be associative on . Prove that if is a finite set, then there must exist an idempotent element, that is, an such that . "
},
{
  "id": "exercises-14-1-6",
  "level": "2",
  "url": "s-Monoids.html#exercises-14-1-6",
  "type": "Exercise",
  "number": "14.1.5",
  "title": "",
  "body": "Let be a Boolean algebra and the set of all Boolean functions on . Let be defined on by . Prove that is a monoid. Construct the operation table of for the case of .  Let , and . Therefore and is associative.  The identity for is the function where = the one of . If , . Therefore . Similarly, .  There are functions in for . These four functions are named in the text. See . The table for is  "
},
{
  "id": "s-free-monoids-and-languages",
  "level": "1",
  "url": "s-free-monoids-and-languages.html",
  "type": "Section",
  "number": "14.2",
  "title": "Free Monoids and Languages",
  "body": " Free Monoids and Languages  Free Monoids and Languages  In this section, we will introduce the concept of a language. Languages are subsets of a certain type of monoid, the free monoid over an alphabet. After defining a free monoid, we will discuss languages and some of the basic problems relating to them. We will also discuss the common ways in which languages are defined.  Let be a nonempty set, which we will call an alphabet. Our primary interest will be in the case where is finite; however, could be infinite for most of the situations that we will describe. The elements of are called letters or symbols. Among the alphabets that we will use are , and the set of ASCII (American Standard Code for Information Interchange) characters, which we symbolize as .  Strings over an Alphabet  Strings over an Alphabet  The set of all strings over an alphabet  The set of all strings of length over an alphabet  The empty string  A string of length , over alphabet is a sequence of letters from : . The null string, , is defined as the string of length zero containing no letters. The set of strings of length over is denoted by . The set of all strings over is denoted .    If the length of string is , we write .  The null string is not the same as the empty set, although they are similar in many ways. .  ; that is, is a partition of .  An element of can appear any number of times in a string.      If is countable, then is countable.  Case 1. Given the alphabet , we can define a bijection from the positive integers into . Each positive integer has a binary expansion , where each is 0 or 1 and . If has such a binary expansion, then . We define by where . Every one of the strings of length are the images of exactly one of the integers between From its definition, is clearly a bijection; therefore, is countable.  Case 2: is Finite. We will describe how this case is handled with an example first and then give the general proof. If , then we can code the letters in into strings from . One of the coding schemes (there are many) is . Now every string in corresponds to a different string in ; for example, . would correspond with . The cardinality of is equal to the cardinality of the set of strings that can be obtained from this encoding system. The possible coded strings must be countable, since they are a subset of a countable set, . Therefore, is countable.  If , then the letters in can be coded using a set of fixed-length strings from . If , then there are at least as many strings of length in as there are letters in . Now we can associate each letter in with with a different element of . Then any string in . corresponds to a string in . By the same reasoning as in the example above, is countable.  Case 3: is Countably Infinite. We will leave this case as an exercise.  Concatenation  Concatenation  The concatenation of strings and  Let and be strings of length and , respectively. The concatenation of with , , is the string of length .  There are several symbols that are used for concatenation. We chose to use the one that is also used in Python and SageMath.   The set of strings over any alphabet is a monoid under concatenation.     The null string is the identity element of . Henceforth, we will denote the monoid of strings over by .  Concatenation is noncommutative, provided .  If , then the monoids and are isomorphic. An isomorphism can be defined using any bijection . If , defines a bijection from into . We will leave it to the reader to prove that for all .     The languages of the world, English, German, Russian, Chinese, and so forth, are called natural languages. In order to communicate in writing in any one of them, you must first know the letters of the alphabet and then know how to combine the letters in meaningful ways. A formal language is an abstraction of this situation.   Formal Language  Formal Language  If is an alphabet, a formal language over is a subset of .  Some Formal Languages   English can be thought of as a language over of letters , both upper and lower case, and other special symbols, such as punctuation marks and the blank. Exactly what subset of the strings over this alphabet defines the English language is difficult to pin down exactly. This is a characteristic of natural languages that we try to avoid with formal languages.  The set of all ASCII stream files can be defined in terms of a language over ASCII. An ASCII stream file is a sequence of zero or more lines followed by an end-of-file symbol. A line is defined as a sequence of ASCII characters that ends with the a new line character. The end-of-file symbol is system-dependent.  The set of all syntactically correct expressions in any computer language is a language over the set of ASCII strings.  A few languages over are      = the submonoid of generated by .     Two Fundamental Problems: Recognition and Generation  The generation and recognition problems are basic to computer programming. Given a language, , the programmer must know how to write (or generate) a syntactically correct program that solves a problem. On the other hand, the compiler must be written to recognize whether a program contains any syntax errors.   The Recognition Problem  Recognition Problem  Given a formal language over alphabet , the Recognition Problem is to design an algorithm that determines the truth of in a finite number of steps for all . Any such algorithm is called a recognition algorithm.   Recursive Language  Recursive Language  A language is recursive if there exists a recognition algorithm for it.  Some Recursive Languages   The language of syntactically correct propositions over set of propositional variables expressions is recursive.  The three languages in (d) are all recursive. Recognition algorithms for and should be easy for you to imagine. The reason a recognition algorithm for might not be obvious is that the definition of is more cryptic. It doesn't tell us what belongs to , just what can be used to create strings in . This is how many languages are defined. With a second description of , we can easily design a recognition algorithm. You can prove that .     The Generation Problem  Generation Problem  Design an algorithm that generates or produces any string in . Here we presume that is either finite or countably infinite; hence, is countable by , and must be countable. Therefore, the generation of amounts to creating a list of strings in . The list may be either finite or infinite, and you must be able to show that every string in appears somewhere in the list.  Recursive implies Generating   If is countable, then there exists a generating algorithm for .  If is a recursive language over a countable alphabet, then there exists a generating algorithm for .      Part (a) follows from the fact that is countable; therefore, there exists a complete list of strings in .  To generate all strings of , start with a list of all strings in and an empty list, , of strings in . For each string , use a recognition algorithm (one exists since is recursive) to determine whether . If , add it to ; otherwise throw it out. Then go to the next string in the list of .  Since all of the languages in (d) are recursive, they must have generating algorithms. The one given in the proof of is not usually the most efficient. You could probably design more efficient generating algorithms for and ; however, a better generating algorithm for is not quite so obvious.  The recognition and generation problems can vary in difficulty depending on how a language is defined and what sort of algorithms we allow ourselves to use. This is not to say that the means by which a language is defined determines whether it is recursive. It just means that the truth of the statement is recursive may be more difficult to determine with one definition than with another. We will close this section with a discussion of grammars, which are standard forms of definition for a language. When we restrict ourselves to only certain types of algorithms, we can affect our ability to determine whether is true. In defining a recursive language, we do not restrict ourselves in any way in regard to the type of algorithm that will be used. In the next section, we will consider machines called finite automata, which can only perform simple algorithms.  One common way of defining a language is by means of a phrase structure grammar (or grammar, for short). The set of strings that can be produced using set of grammar rules is called a phrase structure language.  Zeros before Ones We can define the set of all strings over for which all 0's precede all 1's as follows. Define the starting symbol and establish rules that can be replaced with any of the following: , , or . These replacement rules are usually called production rules. They are usually written in the format , , and . Now define to be the set of all strings that can be produced by starting with and applying the production rules until no longer appears. The strings in are exactly the ones that are described above.  Phrase Structure Grammar  Phrase Structure Grammar  Language created by phrase structure grammar  A phrase structure grammar consists of four components:  A nonempty finite set of terminal characters, . If the grammar is defining a language over , is a subset of .  A finite set of nonterminal characters, .  A starting symbol, .  A finite set of production rules, each of the form , where and are strings over such that and contains at least one nonterminal symbol.    If is a phrase structure grammar, is the set of strings that can be produced by starting with and applying the production rules a finite number of times until no nonterminal characters remain. If a language can be defined by a phrase structure grammar, then it is called a phrase structure language.    Alternating bits language The language over consisting of strings of alternating 0's and 1's is a phrase structure language. It can be defined by the following grammar:  Terminal characters: , , and  Nonterminal characters: , , and  Starting symbol:  Production rules:     These rules can be visualized with a graph:   Production rules for the language of alternating 0's and 1's   Production rules for the language of alternating 0's and 1's    We can verify that a string such as 10101 belongs to the language by starting with and producing 10101 using the production rules a finite number of times: .  Valid SageMath Variables Let be the grammar with components:  Terminal symbols = all letters of the alphabet (both upper and lower case), digits 0 through 9, and underscore  Nonterminal symbols: ,  Starting symbol:  Production rules: , where is any letter, for any letter , for any letter, digit or underscore, , and for any letter, digit or underscore, . There are a total of production rules for this grammar. The language consists of all valid SageMath variable names.   Backus-Naur Form  Backus-Naur form (BNF) is a popular alternate form of defining the production rules in a grammar. If the production rules are part of a grammar, they would be written in BNF as . The symbol in BNF is read as or while the is read as is defined as. Additional notations of BNF are that , represents zero or more repetitions of and means that is optional.  A BNF version of the production rules for a SageMath variable, , is    The language of simple arithmetic expressions An arithmetic expression can be defined in BNF. For simplicity, we will consider only expressions obtained using addition and multiplication of integers. The terminal symbols are ( , ) , + , * , - , and the digits 0 through 9. The nonterminal symbols are (for expression), (term), (factor), and (number). The starting symbol is . Production rules are .  One particularly simple type of phrase structure grammar is the regular grammar.   Regular Grammar  Regular Grammar  A regular (right-hand form) grammar is a grammar whose production rules are all of the form and , where and are nonterminal and is terminal. A left-hand form grammar allows only and . A language that has a regular phrase structure language is called a regular language.   The set of Sage variable names is a regular language since the grammar by which we defined the set is a regular grammar.  The language of all strings for which all 0's precede all 1's ( ) is regular; however, the grammar by which we defined this set is not regular. Can you define these strings with a regular grammar?  The language of arithmetic expressions is not regular.    Exercises   If a computer is being designed to operate with a character set of 350 symbols, how many bits must be reserved for each character? Assume each character will use the same number of bits.  Do the same for 3,500 symbols.      For a character set of 350 symbols, the number of bits needed for each character is the smallest such that is greater than or equal to 350. Since , 9 bits are needed,   ; therefore, 12 bits are needed.    It was pointed out in the text that the null string and the null set are different. The former is a string and the latter is a set, two different kinds of objects. Discuss how the two are similar.   What sets of strings are defined by the following grammar?  Terminal symbols: , 0 and 1  Nonterminal symbols: and  Starting symbol:   Production rules:    This grammar defines the set of all strings over for which each string is a palindrome (same string if read forward or backward).  What sets of strings are defined by the following grammar?  Terminal symbols: , , , and  Nonterminal symbols:  Starting symbol:  Production rules:     Define the following languages over with phrase structure grammars. Which of these languages are regular?  The strings with an odd number of characters.  The strings of length 4 or less.  The palindromes, strings that are the same backwards as forwards.      Terminal symbols: The null string, 0, and 1. Nonterminal symbols: , . Starting symbol: . Production rules: , , , , , , This is a regular grammar.  Terminal symbols: The null string, 0, and 1. Nonterminal symbols: , , , Starting symbol: Production rules: , , , , , , , , , , , This is a regular grammar.  See Exercise 3. This language is not regular.    Define the following languages over with phrase structure grammars. Which of these languages are regular?  The strings with more 0's than 1's.  The strings with an even number of 1's.  The strings for which all 0's precede all 1's.     Prove that if a language over is recursive, then its complement is also recursive.  If is in and is recursive, we can answer the question Is s in ? by negating the answer to Is in ?   Use BNF to define the grammars in Exercises 3 and 4.    Prove that if is a countable sequence of countable sets, the union of these sets, is countable.  Using the fact that the countable union of countable sets is countable, prove that if is countable, then is countable.    List the elements of each set in a sequence , , . Then draw arrows as shown below and list the elements of the union in order established by this pattern: , , , , , , , , , , ,  Each of the sets , , , are countable and is the union of these sets; hence is countable.      Exercise 9   Exercise 9      "
},
{
  "id": "def-strings-over-alphabet",
  "level": "2",
  "url": "s-free-monoids-and-languages.html#def-strings-over-alphabet",
  "type": "Definition",
  "number": "14.2.1",
  "title": "Strings over an Alphabet.",
  "body": "Strings over an Alphabet  Strings over an Alphabet  The set of all strings over an alphabet  The set of all strings of length over an alphabet  The empty string  A string of length , over alphabet is a sequence of letters from : . The null string, , is defined as the string of length zero containing no letters. The set of strings of length over is denoted by . The set of all strings over is denoted . "
},
{
  "id": "s-free-monoids-and-languages-6",
  "level": "2",
  "url": "s-free-monoids-and-languages.html#s-free-monoids-and-languages-6",
  "type": "Note",
  "number": "14.2.2",
  "title": "",
  "body": "  If the length of string is , we write .  The null string is not the same as the empty set, although they are similar in many ways. .  ; that is, is a partition of .  An element of can appear any number of times in a string.    "
},
{
  "id": "theorem-14-2-1",
  "level": "2",
  "url": "s-free-monoids-and-languages.html#theorem-14-2-1",
  "type": "Theorem",
  "number": "14.2.3",
  "title": "",
  "body": " If is countable, then is countable.  Case 1. Given the alphabet , we can define a bijection from the positive integers into . Each positive integer has a binary expansion , where each is 0 or 1 and . If has such a binary expansion, then . We define by where . Every one of the strings of length are the images of exactly one of the integers between From its definition, is clearly a bijection; therefore, is countable.  Case 2: is Finite. We will describe how this case is handled with an example first and then give the general proof. If , then we can code the letters in into strings from . One of the coding schemes (there are many) is . Now every string in corresponds to a different string in ; for example, . would correspond with . The cardinality of is equal to the cardinality of the set of strings that can be obtained from this encoding system. The possible coded strings must be countable, since they are a subset of a countable set, . Therefore, is countable.  If , then the letters in can be coded using a set of fixed-length strings from . If , then there are at least as many strings of length in as there are letters in . Now we can associate each letter in with with a different element of . Then any string in . corresponds to a string in . By the same reasoning as in the example above, is countable.  Case 3: is Countably Infinite. We will leave this case as an exercise. "
},
{
  "id": "def-concatenation",
  "level": "2",
  "url": "s-free-monoids-and-languages.html#def-concatenation",
  "type": "Definition",
  "number": "14.2.4",
  "title": "Concatenation.",
  "body": "Concatenation  Concatenation  The concatenation of strings and  Let and be strings of length and , respectively. The concatenation of with , , is the string of length . "
},
{
  "id": "s-free-monoids-and-languages-12",
  "level": "2",
  "url": "s-free-monoids-and-languages.html#s-free-monoids-and-languages-12",
  "type": "Note",
  "number": "14.2.5",
  "title": "",
  "body": "   The null string is the identity element of . Henceforth, we will denote the monoid of strings over by .  Concatenation is noncommutative, provided .  If , then the monoids and are isomorphic. An isomorphism can be defined using any bijection . If , defines a bijection from into . We will leave it to the reader to prove that for all .    "
},
{
  "id": "def-formal-language",
  "level": "2",
  "url": "s-free-monoids-and-languages.html#def-formal-language",
  "type": "Definition",
  "number": "14.2.6",
  "title": "Formal Language.",
  "body": " Formal Language  Formal Language  If is an alphabet, a formal language over is a subset of . "
},
{
  "id": "ex-some-formal-languages",
  "level": "2",
  "url": "s-free-monoids-and-languages.html#ex-some-formal-languages",
  "type": "Example",
  "number": "14.2.7",
  "title": "Some Formal Languages.",
  "body": "Some Formal Languages   English can be thought of as a language over of letters , both upper and lower case, and other special symbols, such as punctuation marks and the blank. Exactly what subset of the strings over this alphabet defines the English language is difficult to pin down exactly. This is a characteristic of natural languages that we try to avoid with formal languages.  The set of all ASCII stream files can be defined in terms of a language over ASCII. An ASCII stream file is a sequence of zero or more lines followed by an end-of-file symbol. A line is defined as a sequence of ASCII characters that ends with the a new line character. The end-of-file symbol is system-dependent.  The set of all syntactically correct expressions in any computer language is a language over the set of ASCII strings.  A few languages over are      = the submonoid of generated by .    "
},
{
  "id": "invest-language-problems",
  "level": "2",
  "url": "s-free-monoids-and-languages.html#invest-language-problems",
  "type": "Investigation",
  "number": "14.2.1",
  "title": "Two Fundamental Problems: Recognition and Generation.",
  "body": "Two Fundamental Problems: Recognition and Generation  The generation and recognition problems are basic to computer programming. Given a language, , the programmer must know how to write (or generate) a syntactically correct program that solves a problem. On the other hand, the compiler must be written to recognize whether a program contains any syntax errors. "
},
{
  "id": "prob-recognition",
  "level": "2",
  "url": "s-free-monoids-and-languages.html#prob-recognition",
  "type": "Problem",
  "number": "14.2.8",
  "title": "The Recognition Problem.",
  "body": " The Recognition Problem  Recognition Problem  Given a formal language over alphabet , the Recognition Problem is to design an algorithm that determines the truth of in a finite number of steps for all . Any such algorithm is called a recognition algorithm.  "
},
{
  "id": "def-recursive-language",
  "level": "2",
  "url": "s-free-monoids-and-languages.html#def-recursive-language",
  "type": "Definition",
  "number": "14.2.9",
  "title": "Recursive Language.",
  "body": "Recursive Language  Recursive Language  A language is recursive if there exists a recognition algorithm for it. "
},
{
  "id": "ex-some-recursive-languages",
  "level": "2",
  "url": "s-free-monoids-and-languages.html#ex-some-recursive-languages",
  "type": "Example",
  "number": "14.2.10",
  "title": "Some Recursive Languages.",
  "body": "Some Recursive Languages   The language of syntactically correct propositions over set of propositional variables expressions is recursive.  The three languages in (d) are all recursive. Recognition algorithms for and should be easy for you to imagine. The reason a recognition algorithm for might not be obvious is that the definition of is more cryptic. It doesn't tell us what belongs to , just what can be used to create strings in . This is how many languages are defined. With a second description of , we can easily design a recognition algorithm. You can prove that .   "
},
{
  "id": "prob-generation",
  "level": "2",
  "url": "s-free-monoids-and-languages.html#prob-generation",
  "type": "Problem",
  "number": "14.2.11",
  "title": "The Generation Problem.",
  "body": "The Generation Problem  Generation Problem  Design an algorithm that generates or produces any string in . Here we presume that is either finite or countably infinite; hence, is countable by , and must be countable. Therefore, the generation of amounts to creating a list of strings in . The list may be either finite or infinite, and you must be able to show that every string in appears somewhere in the list. "
},
{
  "id": "theorem-14-2-2",
  "level": "2",
  "url": "s-free-monoids-and-languages.html#theorem-14-2-2",
  "type": "Theorem",
  "number": "14.2.12",
  "title": "Recursive implies Generating.",
  "body": "Recursive implies Generating   If is countable, then there exists a generating algorithm for .  If is a recursive language over a countable alphabet, then there exists a generating algorithm for .      Part (a) follows from the fact that is countable; therefore, there exists a complete list of strings in .  To generate all strings of , start with a list of all strings in and an empty list, , of strings in . For each string , use a recognition algorithm (one exists since is recursive) to determine whether . If , add it to ; otherwise throw it out. Then go to the next string in the list of . "
},
{
  "id": "ex-examples-better-algorithms-q",
  "level": "2",
  "url": "s-free-monoids-and-languages.html#ex-examples-better-algorithms-q",
  "type": "Example",
  "number": "14.2.13",
  "title": "",
  "body": "Since all of the languages in (d) are recursive, they must have generating algorithms. The one given in the proof of is not usually the most efficient. You could probably design more efficient generating algorithms for and ; however, a better generating algorithm for is not quite so obvious. "
},
{
  "id": "s-free-monoids-and-languages-24",
  "level": "2",
  "url": "s-free-monoids-and-languages.html#s-free-monoids-and-languages-24",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "phrase structure grammar "
},
{
  "id": "ex-psl-0s-before-1s",
  "level": "2",
  "url": "s-free-monoids-and-languages.html#ex-psl-0s-before-1s",
  "type": "Example",
  "number": "14.2.14",
  "title": "Zeros before Ones.",
  "body": "Zeros before Ones We can define the set of all strings over for which all 0's precede all 1's as follows. Define the starting symbol and establish rules that can be replaced with any of the following: , , or . These replacement rules are usually called production rules. They are usually written in the format , , and . Now define to be the set of all strings that can be produced by starting with and applying the production rules until no longer appears. The strings in are exactly the ones that are described above. "
},
{
  "id": "def-phrase-structure-language",
  "level": "2",
  "url": "s-free-monoids-and-languages.html#def-phrase-structure-language",
  "type": "Definition",
  "number": "14.2.15",
  "title": "Phrase Structure Grammar.",
  "body": "Phrase Structure Grammar  Phrase Structure Grammar  Language created by phrase structure grammar  A phrase structure grammar consists of four components:  A nonempty finite set of terminal characters, . If the grammar is defining a language over , is a subset of .  A finite set of nonterminal characters, .  A starting symbol, .  A finite set of production rules, each of the form , where and are strings over such that and contains at least one nonterminal symbol.    If is a phrase structure grammar, is the set of strings that can be produced by starting with and applying the production rules a finite number of times until no nonterminal characters remain. If a language can be defined by a phrase structure grammar, then it is called a phrase structure language.   "
},
{
  "id": "ex-alternating-bits",
  "level": "2",
  "url": "s-free-monoids-and-languages.html#ex-alternating-bits",
  "type": "Example",
  "number": "14.2.16",
  "title": "Alternating bits language.",
  "body": "Alternating bits language The language over consisting of strings of alternating 0's and 1's is a phrase structure language. It can be defined by the following grammar:  Terminal characters: , , and  Nonterminal characters: , , and  Starting symbol:  Production rules:     These rules can be visualized with a graph:   Production rules for the language of alternating 0's and 1's   Production rules for the language of alternating 0's and 1's    We can verify that a string such as 10101 belongs to the language by starting with and producing 10101 using the production rules a finite number of times: . "
},
{
  "id": "ex-valid-names-names",
  "level": "2",
  "url": "s-free-monoids-and-languages.html#ex-valid-names-names",
  "type": "Example",
  "number": "14.2.18",
  "title": "Valid SageMath Variables.",
  "body": "Valid SageMath Variables Let be the grammar with components:  Terminal symbols = all letters of the alphabet (both upper and lower case), digits 0 through 9, and underscore  Nonterminal symbols: ,  Starting symbol:  Production rules: , where is any letter, for any letter , for any letter, digit or underscore, , and for any letter, digit or underscore, . There are a total of production rules for this grammar. The language consists of all valid SageMath variable names.  "
},
{
  "id": "ex-bnf",
  "level": "2",
  "url": "s-free-monoids-and-languages.html#ex-bnf",
  "type": "Example",
  "number": "14.2.19",
  "title": "Backus-Naur Form.",
  "body": "Backus-Naur Form  Backus-Naur form (BNF) is a popular alternate form of defining the production rules in a grammar. If the production rules are part of a grammar, they would be written in BNF as . The symbol in BNF is read as or while the is read as is defined as. Additional notations of BNF are that , represents zero or more repetitions of and means that is optional.  A BNF version of the production rules for a SageMath variable, , is   "
},
{
  "id": "ex-arithmetic-expressions",
  "level": "2",
  "url": "s-free-monoids-and-languages.html#ex-arithmetic-expressions",
  "type": "Example",
  "number": "14.2.20",
  "title": "The language of simple arithmetic expressions.",
  "body": "The language of simple arithmetic expressions An arithmetic expression can be defined in BNF. For simplicity, we will consider only expressions obtained using addition and multiplication of integers. The terminal symbols are ( , ) , + , * , - , and the digits 0 through 9. The nonterminal symbols are (for expression), (term), (factor), and (number). The starting symbol is . Production rules are . "
},
{
  "id": "def-regular-grammar",
  "level": "2",
  "url": "s-free-monoids-and-languages.html#def-regular-grammar",
  "type": "Definition",
  "number": "14.2.21",
  "title": "Regular Grammar.",
  "body": " Regular Grammar  Regular Grammar  A regular (right-hand form) grammar is a grammar whose production rules are all of the form and , where and are nonterminal and is terminal. A left-hand form grammar allows only and . A language that has a regular phrase structure language is called a regular language. "
},
{
  "id": "ex-some-regular-grammars",
  "level": "2",
  "url": "s-free-monoids-and-languages.html#ex-some-regular-grammars",
  "type": "Example",
  "number": "14.2.22",
  "title": "",
  "body": " The set of Sage variable names is a regular language since the grammar by which we defined the set is a regular grammar.  The language of all strings for which all 0's precede all 1's ( ) is regular; however, the grammar by which we defined this set is not regular. Can you define these strings with a regular grammar?  The language of arithmetic expressions is not regular.  "
},
{
  "id": "exercises-14-2-2",
  "level": "2",
  "url": "s-free-monoids-and-languages.html#exercises-14-2-2",
  "type": "Exercise",
  "number": "14.2.1",
  "title": "",
  "body": " If a computer is being designed to operate with a character set of 350 symbols, how many bits must be reserved for each character? Assume each character will use the same number of bits.  Do the same for 3,500 symbols.      For a character set of 350 symbols, the number of bits needed for each character is the smallest such that is greater than or equal to 350. Since , 9 bits are needed,   ; therefore, 12 bits are needed.   "
},
{
  "id": "exercises-14-2-3",
  "level": "2",
  "url": "s-free-monoids-and-languages.html#exercises-14-2-3",
  "type": "Exercise",
  "number": "14.2.2",
  "title": "",
  "body": "It was pointed out in the text that the null string and the null set are different. The former is a string and the latter is a set, two different kinds of objects. Discuss how the two are similar.  "
},
{
  "id": "exercises-14-2-4",
  "level": "2",
  "url": "s-free-monoids-and-languages.html#exercises-14-2-4",
  "type": "Exercise",
  "number": "14.2.3",
  "title": "",
  "body": "What sets of strings are defined by the following grammar?  Terminal symbols: , 0 and 1  Nonterminal symbols: and  Starting symbol:   Production rules:    This grammar defines the set of all strings over for which each string is a palindrome (same string if read forward or backward). "
},
{
  "id": "exercises-14-2-5",
  "level": "2",
  "url": "s-free-monoids-and-languages.html#exercises-14-2-5",
  "type": "Exercise",
  "number": "14.2.4",
  "title": "",
  "body": "What sets of strings are defined by the following grammar?  Terminal symbols: , , , and  Nonterminal symbols:  Starting symbol:  Production rules:    "
},
{
  "id": "exercises-14-2-6",
  "level": "2",
  "url": "s-free-monoids-and-languages.html#exercises-14-2-6",
  "type": "Exercise",
  "number": "14.2.5",
  "title": "",
  "body": "Define the following languages over with phrase structure grammars. Which of these languages are regular?  The strings with an odd number of characters.  The strings of length 4 or less.  The palindromes, strings that are the same backwards as forwards.      Terminal symbols: The null string, 0, and 1. Nonterminal symbols: , . Starting symbol: . Production rules: , , , , , , This is a regular grammar.  Terminal symbols: The null string, 0, and 1. Nonterminal symbols: , , , Starting symbol: Production rules: , , , , , , , , , , , This is a regular grammar.  See Exercise 3. This language is not regular.   "
},
{
  "id": "exercises-14-2-7",
  "level": "2",
  "url": "s-free-monoids-and-languages.html#exercises-14-2-7",
  "type": "Exercise",
  "number": "14.2.6",
  "title": "",
  "body": "Define the following languages over with phrase structure grammars. Which of these languages are regular?  The strings with more 0's than 1's.  The strings with an even number of 1's.  The strings for which all 0's precede all 1's.    "
},
{
  "id": "exercises-14-2-8",
  "level": "2",
  "url": "s-free-monoids-and-languages.html#exercises-14-2-8",
  "type": "Exercise",
  "number": "14.2.7",
  "title": "",
  "body": "Prove that if a language over is recursive, then its complement is also recursive.  If is in and is recursive, we can answer the question Is s in ? by negating the answer to Is in ?  "
},
{
  "id": "exercises-14-2-9",
  "level": "2",
  "url": "s-free-monoids-and-languages.html#exercises-14-2-9",
  "type": "Exercise",
  "number": "14.2.8",
  "title": "",
  "body": "Use BNF to define the grammars in Exercises 3 and 4.  "
},
{
  "id": "exercises-14-2-10",
  "level": "2",
  "url": "s-free-monoids-and-languages.html#exercises-14-2-10",
  "type": "Exercise",
  "number": "14.2.9",
  "title": "",
  "body": " Prove that if is a countable sequence of countable sets, the union of these sets, is countable.  Using the fact that the countable union of countable sets is countable, prove that if is countable, then is countable.    List the elements of each set in a sequence , , . Then draw arrows as shown below and list the elements of the union in order established by this pattern: , , , , , , , , , , ,  Each of the sets , , , are countable and is the union of these sets; hence is countable.      Exercise 9   Exercise 9    "
},
{
  "id": "s-automata-finite-state-machines",
  "level": "1",
  "url": "s-automata-finite-state-machines.html",
  "type": "Section",
  "number": "14.3",
  "title": "Automata, Finite-State Machines",
  "body": " Automata, Finite-State Machines  Automata  Finite-State Machines  In this section, we will introduce the concept of an abstract machine. The machines we will examine will (in theory) be capable of performing many of the tasks associated with digital computers. One such task is solving the recognition problem for a language. We will concentrate on one class of machines, finite-state machines (finite automata). And we will see that they are precisely the machines that are capable of recognizing strings in a regular grammar.  Given an alphabet , we will imagine a string in to be encoded on a tape that we will call an input tape. When we refer to a tape, we might imagine a strip of material that is divided into segments, each of which can contain either a letter or a blank.  The typical abstract machine includes an input device, the read head, which is capable of reading the symbol from the segment of the input tape that is currently in the read head. Some more advanced machines have a read\/write head that can also write symbols onto the tape. The movement of the input tape after reading a symbol depends on the machine. With a finite-state machine, the next segment of the input tape is always moved into the read head after a symbol has been read. Most machines (including finite-state machines) also have a separate output tape that is written on with a write head. The output symbols come from an output alphabet, , that may or may not be equal to the input alphabet. The most significant component of an abstract machine is its memory structure. This structure can range from a finite number of bits of memory (as in a finite-state machine) to an infinite amount of memory that can be stored in the form of a tape that can be read from and written on (as in a Turing machine).  Finite-State Machine  Finite-State Machine  A finite-state machine with states , input alphabet , output alphabet , and output function and next-state function  A finite-state machine is defined by a quintet where   is the state set, a finite set that corresponds to the set of memory configurations that the machine can have at any time.   is the input alphabet.   is the output alphabet.   is the output function, which specifies which output symbol is written onto the output tape when the machine is in state and the input symbol is read.   is the next-state (or transition) function, which specifies which state the machine should enter when it is in state and it reads the symbol .    Vending Machine as a Finite-State Machine Many mechanical devices, such as simple vending machines, can be thought of as finite-state machines. For simplicity, assume that a vending machine dispenses packets of gum, spearmint (S), peppermint (P), and bubble (B), for cents each. We can define the input alphabet to be and the state set to be , where the deposit of a quarter unlocks the release mechanism of the machine and allows you to select a flavor of gum. We will leave it to the reader to imagine what the output alphabet, output function, and next-state function would be. You are also invited to let your imagination run wild and include such features as a coin-return lever and change maker.  A Parity Checking Machine The following machine is called a parity checker. It recognizes whether or not a string in contains an even number of 1s. The memory structure of this machine reflects the fact that in order to check the parity of a string, we need only keep track of whether an odd or even number of 1's has been detected.  The input alphabet is and the output alphabet is also . The state set is . The following table defines the output and next-state functions.   Note how the value of the most recent output at any time is an indication of the current state of the machine. Therefore, if we start in the even state and read any finite input tape, the last output corresponds to the final state of the parity checker and tells us the parity of the string on the input tape. For example, if the string 11001010 is read from left to right, the output tape, also from left to right, will be 10001100. Since the last character is a 0, we know that the input string has even parity.   An alternate method for defining a finite-state machine is with a transition diagram. A transition diagram is a directed graph that contains a node for each state and edges that indicate the transition and output functions. An edge that is labeled indicates that in state the input results in an output of and the next state is . That is, and . The transition diagram for the parity checker appears in . In later examples, we will see that if there are different inputs, and , while in the same state resulting in the same transitions and outputs, we label a single edge instead of drawing two edges with labels and .   Transition Diagram for a Parity Checker   Transition Diagram for a Parity Checker    One of the most significant features of a finite-state machine is that it retains no information about its past states that can be accessed by the machine itself. For example, after we input a tape encoded with the symbols 01101010 into the parity checker, the current state will be even, but we have no indication within the machine whether or not it has always been in even state. Note how the output tape is not considered part of the machine's memory. In this case, the output tape does contain a history of the parity checker's past states. We assume that the finite-state machine has no way of recovering the output sequence for later use.  A Baseball Machine Consider the following simplified version of the game of baseball. To be precise, this machine describes one half-inning of a simplified baseball game. Suppose that in addition to home plate, there is only one base instead of the usual three bases. Also, assume that there are only two outs per inning instead of the usual three. Our input alphabet will consist of the types of hits that the batter could have: out (O), double play (DP), single (S), and home run (HR). The input DP is meant to represent a batted ball that would result in a double play (two outs), if possible. The input DP can then occur at any time. The output alphabet is the numbers 0, 1, and 2 for the number of runs that can be scored as a result of any input. The state set contains the current situation in the inning, the number of outs, and whether a base runner is currently on the base. The list of possible states is then 00 (for 0 outs and 0 runners), 01, 10, 11, and end (when the half-inning is over). The transition diagram for this machine appears in   Transition Diagram for a simplified game of baseball   Transition Diagram for a simplified game of baseball    Let's concentrate on one state. If the current state is 01, 0 outs and 1 runner on base, each input results in a different combination of output and next-state. If the batter hits the ball poorly (a double play) the output is zero runs and the inning is over (the limit of two outs has been made). A simple out also results in an output of 0 runs and the next state is 11, one out and one runner on base. If the batter hits a single, one run scores (output = 1) while the state remains 01. If a home run is hit, two runs are scored (output = 2) and the next state is 00. If we had allowed three outs per inning, this graph would only be marginally more complicated. The usual game with three bases would be quite a bit more complicated, however.   Recognition in Regular Languages  As we mentioned at the outset of this section, finite-state machines can recognize strings in a regular language. Consider the language over that contains the strings of positive length in which each is followed by and each is followed by . One such string is . This language is regular. A grammar for the language would be nonterminal symbols with starting symbol and production rules , , , , , . A finite-state machine ( ) that recognizes this language can be constructed with one state for each nonterminal symbol and an additional state (Reject) that is entered if any invalid production takes place. At the end of an input tape that encodes a string in , we will know when the string belongs to based on the final output. If the final output is 1, the string belongs to and if it is 0, the string does not belong to . In addition, recognition can be accomplished by examining the final state of the machine. The input string belongs to the language if and only if the final state is .         The construction of this machine is quite easy: note how each production rule translates into an edge between states other than Reject. For example, indicates that in State , an input of places the machine into State . Not all sets of production rules can be as easily translated to a finite-state machine. Another set of production rules for is , , , , and . Techniques for constructing finite-state machines from production rules is not our objective here. Hence we will only expect you to experiment with production rules until appropriate ones are found.   A Binary Adder A finite-state machine can be designed to add positive integers of any size. Given two integers in binary form, and , the machine take as its input sequence the corresponding bits of and reading from right to left with a parity bit added  Notice the special input 111 at the end. All possible inputs except the last one must even parity (contain an even number of ones). The output sequence is the sum of and , starting with the units digit, and comes from the set . The transition diagram for this machine appears in .   Transition Diagram for a binary adder   Transition Diagram for a binary adder      Exercises   Draw a transition diagram for the vending machine described in .     Vending Machine Transitions   Vending Machine Transitions      Construct finite-state machines that recognize the regular languages that you identified in Section 14.2.  What is the input set for the binary adding machine in ?      What input sequence would be used to compute the sum of 1101 and 0111 (binary integers)? What would the output sequence be?  Gray Code Decoder The Gray Code Decoder. The finite-state machine defined by the following figure has an interesting connection with the Gray Code.   Gray Code Decoder   Gray Code Decoder    Given a string , we may ask where appears in . Starting in Copy state, the input string will result in an output string , which is the binary form of the position of in . Recall that positions are numbered from 0 to .  In what positions do 10110, 00100, and 11111 appear in ?  Prove that the Gray Code Decoder always works.       Input: 10110, Output: 11011 10110 is in position 27  Input: 00100, Output: 00111 00100 is in position 7  Input:11111, Output: 10101 11111 is in position 21   Let and recall that for , , where is the reverse of . To prove that the Gray Code Decoder always works, let be the proposition Starting in Copy state, 's output is the position of in ; and starting in Complement state, 's output is the position of in . That p(1) is true is easy to verify for both possible values of , 0 and 1. Now assume that for some , is true and consider .  If , 's output is a zero followed by the output for starting in Copy state. By the induction hypothesis, this is zero followed by the position of in , which is the position of in , by the definition of .  If , 's output is a one followed by the output for starting in Complement state. By the induction hypothesis, this is one followed by the position of in , which is the position of in , by the definition of .      "
},
{
  "id": "def-finite-state-machine",
  "level": "2",
  "url": "s-automata-finite-state-machines.html#def-finite-state-machine",
  "type": "Definition",
  "number": "14.3.1",
  "title": "Finite-State Machine.",
  "body": "Finite-State Machine  Finite-State Machine  A finite-state machine with states , input alphabet , output alphabet , and output function and next-state function  A finite-state machine is defined by a quintet where   is the state set, a finite set that corresponds to the set of memory configurations that the machine can have at any time.   is the input alphabet.   is the output alphabet.   is the output function, which specifies which output symbol is written onto the output tape when the machine is in state and the input symbol is read.   is the next-state (or transition) function, which specifies which state the machine should enter when it is in state and it reads the symbol .   "
},
{
  "id": "ex-vending-machine",
  "level": "2",
  "url": "s-automata-finite-state-machines.html#ex-vending-machine",
  "type": "Example",
  "number": "14.3.2",
  "title": "Vending Machine as a Finite-State Machine.",
  "body": "Vending Machine as a Finite-State Machine Many mechanical devices, such as simple vending machines, can be thought of as finite-state machines. For simplicity, assume that a vending machine dispenses packets of gum, spearmint (S), peppermint (P), and bubble (B), for cents each. We can define the input alphabet to be and the state set to be , where the deposit of a quarter unlocks the release mechanism of the machine and allows you to select a flavor of gum. We will leave it to the reader to imagine what the output alphabet, output function, and next-state function would be. You are also invited to let your imagination run wild and include such features as a coin-return lever and change maker. "
},
{
  "id": "ex-parity-checker",
  "level": "2",
  "url": "s-automata-finite-state-machines.html#ex-parity-checker",
  "type": "Example",
  "number": "14.3.3",
  "title": "A Parity Checking Machine.",
  "body": "A Parity Checking Machine The following machine is called a parity checker. It recognizes whether or not a string in contains an even number of 1s. The memory structure of this machine reflects the fact that in order to check the parity of a string, we need only keep track of whether an odd or even number of 1's has been detected.  The input alphabet is and the output alphabet is also . The state set is . The following table defines the output and next-state functions.   Note how the value of the most recent output at any time is an indication of the current state of the machine. Therefore, if we start in the even state and read any finite input tape, the last output corresponds to the final state of the parity checker and tells us the parity of the string on the input tape. For example, if the string 11001010 is read from left to right, the output tape, also from left to right, will be 10001100. Since the last character is a 0, we know that the input string has even parity.  "
},
{
  "id": "fig-parity-checker",
  "level": "2",
  "url": "s-automata-finite-state-machines.html#fig-parity-checker",
  "type": "Figure",
  "number": "14.3.4",
  "title": "",
  "body": " Transition Diagram for a Parity Checker   Transition Diagram for a Parity Checker   "
},
{
  "id": "ex-baseball-machine",
  "level": "2",
  "url": "s-automata-finite-state-machines.html#ex-baseball-machine",
  "type": "Example",
  "number": "14.3.5",
  "title": "A Baseball Machine.",
  "body": "A Baseball Machine Consider the following simplified version of the game of baseball. To be precise, this machine describes one half-inning of a simplified baseball game. Suppose that in addition to home plate, there is only one base instead of the usual three bases. Also, assume that there are only two outs per inning instead of the usual three. Our input alphabet will consist of the types of hits that the batter could have: out (O), double play (DP), single (S), and home run (HR). The input DP is meant to represent a batted ball that would result in a double play (two outs), if possible. The input DP can then occur at any time. The output alphabet is the numbers 0, 1, and 2 for the number of runs that can be scored as a result of any input. The state set contains the current situation in the inning, the number of outs, and whether a base runner is currently on the base. The list of possible states is then 00 (for 0 outs and 0 runners), 01, 10, 11, and end (when the half-inning is over). The transition diagram for this machine appears in   Transition Diagram for a simplified game of baseball   Transition Diagram for a simplified game of baseball    Let's concentrate on one state. If the current state is 01, 0 outs and 1 runner on base, each input results in a different combination of output and next-state. If the batter hits the ball poorly (a double play) the output is zero runs and the inning is over (the limit of two outs has been made). A simple out also results in an output of 0 runs and the next state is 11, one out and one runner on base. If the batter hits a single, one run scores (output = 1) while the state remains 01. If a home run is hit, two runs are scored (output = 2) and the next state is 00. If we had allowed three outs per inning, this graph would only be marginally more complicated. The usual game with three bases would be quite a bit more complicated, however.  "
},
{
  "id": "ex-problem-recognition-in-reg-languages",
  "level": "2",
  "url": "s-automata-finite-state-machines.html#ex-problem-recognition-in-reg-languages",
  "type": "Example",
  "number": "14.3.7",
  "title": "Recognition in Regular Languages.",
  "body": "Recognition in Regular Languages  As we mentioned at the outset of this section, finite-state machines can recognize strings in a regular language. Consider the language over that contains the strings of positive length in which each is followed by and each is followed by . One such string is . This language is regular. A grammar for the language would be nonterminal symbols with starting symbol and production rules , , , , , . A finite-state machine ( ) that recognizes this language can be constructed with one state for each nonterminal symbol and an additional state (Reject) that is entered if any invalid production takes place. At the end of an input tape that encodes a string in , we will know when the string belongs to based on the final output. If the final output is 1, the string belongs to and if it is 0, the string does not belong to . In addition, recognition can be accomplished by examining the final state of the machine. The input string belongs to the language if and only if the final state is .         The construction of this machine is quite easy: note how each production rule translates into an edge between states other than Reject. For example, indicates that in State , an input of places the machine into State . Not all sets of production rules can be as easily translated to a finite-state machine. Another set of production rules for is , , , , and . Techniques for constructing finite-state machines from production rules is not our objective here. Hence we will only expect you to experiment with production rules until appropriate ones are found.  "
},
{
  "id": "ex-binary-adder",
  "level": "2",
  "url": "s-automata-finite-state-machines.html#ex-binary-adder",
  "type": "Example",
  "number": "14.3.9",
  "title": "A Binary Adder.",
  "body": "A Binary Adder A finite-state machine can be designed to add positive integers of any size. Given two integers in binary form, and , the machine take as its input sequence the corresponding bits of and reading from right to left with a parity bit added  Notice the special input 111 at the end. All possible inputs except the last one must even parity (contain an even number of ones). The output sequence is the sum of and , starting with the units digit, and comes from the set . The transition diagram for this machine appears in .   Transition Diagram for a binary adder   Transition Diagram for a binary adder    "
},
{
  "id": "exercises-14-3-2",
  "level": "2",
  "url": "s-automata-finite-state-machines.html#exercises-14-3-2",
  "type": "Exercise",
  "number": "14.3.1",
  "title": "",
  "body": " Draw a transition diagram for the vending machine described in .     Vending Machine Transitions   Vending Machine Transitions    "
},
{
  "id": "exercises-14-3-3",
  "level": "2",
  "url": "s-automata-finite-state-machines.html#exercises-14-3-3",
  "type": "Exercise",
  "number": "14.3.2",
  "title": "",
  "body": " Construct finite-state machines that recognize the regular languages that you identified in Section 14.2. "
},
{
  "id": "exercises-14-3-4",
  "level": "2",
  "url": "s-automata-finite-state-machines.html#exercises-14-3-4",
  "type": "Exercise",
  "number": "14.3.3",
  "title": "",
  "body": "What is the input set for the binary adding machine in ?    "
},
{
  "id": "exercises-14-3-5",
  "level": "2",
  "url": "s-automata-finite-state-machines.html#exercises-14-3-5",
  "type": "Exercise",
  "number": "14.3.4",
  "title": "",
  "body": " What input sequence would be used to compute the sum of 1101 and 0111 (binary integers)? What would the output sequence be? "
},
{
  "id": "exercises-14-3-6",
  "level": "2",
  "url": "s-automata-finite-state-machines.html#exercises-14-3-6",
  "type": "Exercise",
  "number": "14.3.5",
  "title": "",
  "body": "Gray Code Decoder The Gray Code Decoder. The finite-state machine defined by the following figure has an interesting connection with the Gray Code.   Gray Code Decoder   Gray Code Decoder    Given a string , we may ask where appears in . Starting in Copy state, the input string will result in an output string , which is the binary form of the position of in . Recall that positions are numbered from 0 to .  In what positions do 10110, 00100, and 11111 appear in ?  Prove that the Gray Code Decoder always works.       Input: 10110, Output: 11011 10110 is in position 27  Input: 00100, Output: 00111 00100 is in position 7  Input:11111, Output: 10101 11111 is in position 21   Let and recall that for , , where is the reverse of . To prove that the Gray Code Decoder always works, let be the proposition Starting in Copy state, 's output is the position of in ; and starting in Complement state, 's output is the position of in . That p(1) is true is easy to verify for both possible values of , 0 and 1. Now assume that for some , is true and consider .  If , 's output is a zero followed by the output for starting in Copy state. By the induction hypothesis, this is zero followed by the position of in , which is the position of in , by the definition of .  If , 's output is a one followed by the output for starting in Complement state. By the induction hypothesis, this is one followed by the position of in , which is the position of in , by the definition of .    "
},
{
  "id": "s-monoid-of-fsm",
  "level": "1",
  "url": "s-monoid-of-fsm.html",
  "type": "Section",
  "number": "14.4",
  "title": "The Monoid of a Finite-State Machine",
  "body": " The Monoid of a Finite-State Machine  Monoid of a Finite-State Machine  In this section, we will see how every finite-state machine has a monoid associated with it. For any finite-state machine, the elements of its associated monoid correspond to certain input sequences. Because only a finite number of combinations of states and inputs is possible for a finite-state machine there is only a finite number of input sequences that summarize the machine. This idea is illustrated best with a few examples.  Consider the parity checker. The following table summarizes the effect on the parity checker of strings in and . The row labeled Even contains the final state and final output as a result of each input string in and when the machine starts in the even state. Similarly, the row labeled Odd contains the same information for input sequences when the machine starts in the odd state.  Note how, as indicated in the last row, the strings in have the same effect as certain strings in . For this reason, we can summarize the machine in terms of how it is affected by strings of length 1. The actual monoid that we will now describe consists of a set of functions, and the operation on the functions will be based on the concatenation operation.  Let be the final effect (state and output) on the parity checker of the input 0. Similarly, is defined as the final effect on the parity checker of the input 1. More precisely, while  In general, we define the operation on a set of such functions as follows: if , are input sequences and and , are functions as above, then , that is, the result of the function that summarizes the effect on the machine by the concatenation of with . Since, for example, 01 has the same effect on the parity checker as 1, . We don't stop our calculation at because we want to use the shortest string of inputs to describe the final result. A complete table for the monoid of the parity checker is  What is the identity of this monoid? The monoid of the parity checker is isomorphic to the monoid .  This operation may remind you of the composition operation on functions, but there are two principal differences. The domain of is not the codomain of and the functions are read from left to right unlike in composition, where they are normally read from right to left.  You may have noticed that the output of the parity checker echoes the state of the machine and that we could have looked only at the effect on the machine as the final state. The following example has the same property, hence we will only consider the final state.  The transition diagram for the machine that recognizes strings in that have no consecutive 1's appears in . Note how it is similar to the graph in . Only a reject state has been added, for the case when an input of 1 occurs while in State . We construct a similar table to the one in the previous example to study the effect of certain strings on this machine. This time, we must include strings of length 3 before we recognize that no new effects can be found.   No Consecutive Ones Monoid   No Consecutive Ones Monoid      The following table summarizes how combinations of the strings affect this machine.   All the results in this table can be obtained using the previous table. For example,   Note that none of the elements that we have listed in this table serves as the identity for our operation. This problem can always be remedied by including the function that corresponds to the input of the null string, . Since the null string is the identity for concatenation of strings, for all input strings .  The Unit-time Delay Machine A finite-state machine called the unit-time delay machine does not echo its current state, but prints its previous state. For this reason, when we find the monoid of the unit-time delay machine, we must consider both state and output. The transition diagram of this machine appears in .    Unit time delay graph       Again, since no new outcomes were obtained from strings of length 3, only strings of length 2 or less contribute to the monoid of the machine. The table for the strings of positive length shows that we must add to obtain a monoid.   .   Exercises  For each of the transition diagrams in , write out tables for their associated monoids. Identify the identity in terms of a string of positive length, if possible.   Exercise 1   Exercise 1 of section 14.4    Where the output echoes the current state, the output can be ignored.      We can see that , , etc. Therefore, we have the following monoid:   Notice that is the identity of this monoid.       We have the following monoid:   Notice that is the identity of this monoid.   What common monoids are isomorphic to the monoids obtained in the previous exercise?   Can two finite-state machines with nonisomorphic transition diagrams have isomorphic monoids? Yes, just consider the unit time delay machine of . Its monoid is described by the table at the end of Section 14.4 where the row and column are omitted. Next consider the machine in . The monoid of this machine is:   Hence both of these machines have the same monoid, however, their transition diagrams are nonisomorphic since the first has two vertices and the second has seven.    "
},
{
  "id": "ex-no-consecutive-ones-monoid",
  "level": "2",
  "url": "s-monoid-of-fsm.html#ex-no-consecutive-ones-monoid",
  "type": "Example",
  "number": "14.4.1",
  "title": "",
  "body": "The transition diagram for the machine that recognizes strings in that have no consecutive 1's appears in . Note how it is similar to the graph in . Only a reject state has been added, for the case when an input of 1 occurs while in State . We construct a similar table to the one in the previous example to study the effect of certain strings on this machine. This time, we must include strings of length 3 before we recognize that no new effects can be found.   No Consecutive Ones Monoid   No Consecutive Ones Monoid      The following table summarizes how combinations of the strings affect this machine.   All the results in this table can be obtained using the previous table. For example,   Note that none of the elements that we have listed in this table serves as the identity for our operation. This problem can always be remedied by including the function that corresponds to the input of the null string, . Since the null string is the identity for concatenation of strings, for all input strings . "
},
{
  "id": "ex-unit-time-delay",
  "level": "2",
  "url": "s-monoid-of-fsm.html#ex-unit-time-delay",
  "type": "Example",
  "number": "14.4.3",
  "title": "The Unit-time Delay Machine.",
  "body": "The Unit-time Delay Machine A finite-state machine called the unit-time delay machine does not echo its current state, but prints its previous state. For this reason, when we find the monoid of the unit-time delay machine, we must consider both state and output. The transition diagram of this machine appears in .    Unit time delay graph       Again, since no new outcomes were obtained from strings of length 3, only strings of length 2 or less contribute to the monoid of the machine. The table for the strings of positive length shows that we must add to obtain a monoid.   . "
},
{
  "id": "exercises-14-4-2",
  "level": "2",
  "url": "s-monoid-of-fsm.html#exercises-14-4-2",
  "type": "Exercise",
  "number": "14.4.1",
  "title": "",
  "body": "For each of the transition diagrams in , write out tables for their associated monoids. Identify the identity in terms of a string of positive length, if possible.   Exercise 1   Exercise 1 of section 14.4    Where the output echoes the current state, the output can be ignored.      We can see that , , etc. Therefore, we have the following monoid:   Notice that is the identity of this monoid.       We have the following monoid:   Notice that is the identity of this monoid.  "
},
{
  "id": "exercises-14-4-3",
  "level": "2",
  "url": "s-monoid-of-fsm.html#exercises-14-4-3",
  "type": "Exercise",
  "number": "14.4.2",
  "title": "",
  "body": "What common monoids are isomorphic to the monoids obtained in the previous exercise? "
},
{
  "id": "exercises-14-4-4",
  "level": "2",
  "url": "s-monoid-of-fsm.html#exercises-14-4-4",
  "type": "Exercise",
  "number": "14.4.3",
  "title": "",
  "body": " Can two finite-state machines with nonisomorphic transition diagrams have isomorphic monoids? Yes, just consider the unit time delay machine of . Its monoid is described by the table at the end of Section 14.4 where the row and column are omitted. Next consider the machine in . The monoid of this machine is:   Hence both of these machines have the same monoid, however, their transition diagrams are nonisomorphic since the first has two vertices and the second has seven.  "
},
{
  "id": "s-machine-of-monoid",
  "level": "1",
  "url": "s-machine-of-monoid.html",
  "type": "Section",
  "number": "14.5",
  "title": "The Machine of a Monoid",
  "body": " The Machine of a Monoid  Machine of a Monoid  Any finite monoid can be represented in the form of a finite-state machine with input and state sets equal to . The output of the machine will be ignored here, since it would echo the current state of the machine. Machines of this type are called state machines . It can be shown that whatever can be done with a finite-state machine can be done with a state machine; however, there is a trade-off. Usually, state machines that perform a specific function are more complex than general finite-state machines.  Machine of a Monoid  Machine of a Monoid  The machine of monoid  If is a finite monoid, then the machine of , denoted , is the state machine with state set , input set , and next-state function defined by .  We will construct the machine of the monoid . As mentioned above, the state set and the input set are both . The next state function is defined by . The transition diagram for appears in . Note how it is identical to the transition diagram of the parity checker, which has an associated monoid that was isomorphic to    The machine of    The machine of     The transition diagram of the monoids and appear in .   The machines of and    The machines of and     Let be the monoid that we obtained from the unit-time delay machine ( ). We have seen that the machine of the monoid of the parity checker is essentially the parity checker. Will we obtain a unit-time delay machine when we construct the machine of ? We can't expect to get exactly the same machine because the unit-time delay machine is not a state machine and the machine of a monoid is a state machine. However, we will see that our new machine is capable of telling us what input was received in the previous time period. The operation table for the monoid serves as a table to define the transition function for the machine. The row headings are the state values, while the column headings are the inputs. If we were to draw a transition diagram with all possible inputs, the diagram would be too difficult to read. Since is generated by the two elements, and , we will include only those inputs. Suppose that we wanted to read the transition function for the input . Since , in any state The transition diagram appears in .   Unit time delay machine   Unit time delay machine    If we start reading a string of 0's and 1's while in state and are in state at any one time, the input from the previous time period (not the input that sent us into , the one before that) is . In states and , no previous input exists.   Exercises  Draw the transition diagrams for the machines of the following monoids:    The direct product of with itself.      (a)   solution to 14.5.1a     (b)   solution to 14.5.1b     Even though a monoid may be infinite, we can visualize it as an infinite-state machine provided that it is generated by a finite number of elements. For example, the monoid is generated by 0 and 1. A section of its transition diagram can be obtained by allowing input only from the generating set. The monoid of integers under addition is generated by the set . The transition diagram for this monoid can be visualized by drawing a small portion of it, as in . The same is true for the additive monoid of integers, as seen in .   An infinite machine   An infinite machine     An infinite machine   An infinite machine     Draw a transition diagram for  Draw a transition diagram for .  Draw a transition diagram for with generating set .     "
},
{
  "id": "s-machine-of-monoid-3",
  "level": "2",
  "url": "s-machine-of-monoid.html#s-machine-of-monoid-3",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "state machines "
},
{
  "id": "def-machine-of-monoid",
  "level": "2",
  "url": "s-machine-of-monoid.html#def-machine-of-monoid",
  "type": "Definition",
  "number": "14.5.1",
  "title": "Machine of a Monoid.",
  "body": "Machine of a Monoid  Machine of a Monoid  The machine of monoid  If is a finite monoid, then the machine of , denoted , is the state machine with state set , input set , and next-state function defined by . "
},
{
  "id": "ex-14-5-1",
  "level": "2",
  "url": "s-machine-of-monoid.html#ex-14-5-1",
  "type": "Example",
  "number": "14.5.2",
  "title": "",
  "body": "We will construct the machine of the monoid . As mentioned above, the state set and the input set are both . The next state function is defined by . The transition diagram for appears in . Note how it is identical to the transition diagram of the parity checker, which has an associated monoid that was isomorphic to    The machine of    The machine of    "
},
{
  "id": "ex-transition-examples",
  "level": "2",
  "url": "s-machine-of-monoid.html#ex-transition-examples",
  "type": "Example",
  "number": "14.5.4",
  "title": "",
  "body": "The transition diagram of the monoids and appear in .   The machines of and    The machines of and    "
},
{
  "id": "ex-14-5-2",
  "level": "2",
  "url": "s-machine-of-monoid.html#ex-14-5-2",
  "type": "Example",
  "number": "14.5.6",
  "title": "",
  "body": "Let be the monoid that we obtained from the unit-time delay machine ( ). We have seen that the machine of the monoid of the parity checker is essentially the parity checker. Will we obtain a unit-time delay machine when we construct the machine of ? We can't expect to get exactly the same machine because the unit-time delay machine is not a state machine and the machine of a monoid is a state machine. However, we will see that our new machine is capable of telling us what input was received in the previous time period. The operation table for the monoid serves as a table to define the transition function for the machine. The row headings are the state values, while the column headings are the inputs. If we were to draw a transition diagram with all possible inputs, the diagram would be too difficult to read. Since is generated by the two elements, and , we will include only those inputs. Suppose that we wanted to read the transition function for the input . Since , in any state The transition diagram appears in .   Unit time delay machine   Unit time delay machine    If we start reading a string of 0's and 1's while in state and are in state at any one time, the input from the previous time period (not the input that sent us into , the one before that) is . In states and , no previous input exists. "
},
{
  "id": "exercises-14-5-2",
  "level": "2",
  "url": "s-machine-of-monoid.html#exercises-14-5-2",
  "type": "Exercise",
  "number": "14.5.1",
  "title": "",
  "body": "Draw the transition diagrams for the machines of the following monoids:    The direct product of with itself.      (a)   solution to 14.5.1a     (b)   solution to 14.5.1b    "
},
{
  "id": "exercises-14-5-3",
  "level": "2",
  "url": "s-machine-of-monoid.html#exercises-14-5-3",
  "type": "Exercise",
  "number": "14.5.2",
  "title": "",
  "body": "Even though a monoid may be infinite, we can visualize it as an infinite-state machine provided that it is generated by a finite number of elements. For example, the monoid is generated by 0 and 1. A section of its transition diagram can be obtained by allowing input only from the generating set. The monoid of integers under addition is generated by the set . The transition diagram for this monoid can be visualized by drawing a small portion of it, as in . The same is true for the additive monoid of integers, as seen in .   An infinite machine   An infinite machine     An infinite machine   An infinite machine     Draw a transition diagram for  Draw a transition diagram for .  Draw a transition diagram for with generating set .   "
},
{
  "id": "s-cyclic-groups",
  "level": "1",
  "url": "s-cyclic-groups.html",
  "type": "Section",
  "number": "15.1",
  "title": "Cyclic Groups",
  "body": " Cyclic Groups  Groups are classified according to their size and structure. A group's structure is revealed by a study of its subgroups and other properties (e.g., whether it is abelian) that might give an overview of it. Cyclic groups have the simplest structure of all groups.  Cyclic Group  Cyclic Group   Group is cyclic if there exists such that the cyclic subgroup generated by , , equals all of . That is, , in which case is called a generator of . The reader should note that additive notation is used for .  A Finite Cyclic Group , where is addition modulo 12, is a cyclic group. To verify this statement, all we need to do is demonstrate that some element of is a generator. One such element is 5; that is, . One more obvious generator is 1. In fact, 1 is a generator of every . The reader is asked to prove that if an element is a generator, then its inverse is also a generator. Thus, and are the other generators of . The remaining eight elements of the group are not generators.   Examples of string art    Examples of string art    (a) is an example of string art that illustrates how 5 generates . Twelve tacks are placed evenly around a circle and numbered 0 through 11. A string is tied to tack 0, and is then looped around every fifth tack. As a result, the numbers of the tacks that are reached are exactly the ordered multiples of 5 modulo 12: 5, 10, 3, ... , 7, 0. Note that if every seventh tack were used, the same artwork would be produced. If every third tack were connected, as in (b), the resulting loop would only use four tacks; thus 3 does not generate .   The Group of Integers is Cyclic The additive group of integers, , is cyclic: This observation does not mean that every integer is the product of an integer times 1. It means that   Cyclic Implies Abelian  If is cyclic, then it is abelian.  Let be any generator of and let . By the definition of the generator of a group, there exist integers and such that and . Thus, using ,   One of the first steps in proving a property of cyclic groups is to use the fact that there exists a generator. Then every element of the group can be expressed as some multiple of the generator. Take special note of how this is used in theorems of this section.  Up to now we have used only additive notation to discuss cyclic groups. actually justifies this practice since it is customary to use additive notation when discussing abelian groups. Of course, some concrete groups for which we employ multiplicative notation are cyclic. If one of its elements, , is a generator,   A Cyclic Multiplicative Group The group of positive integers modulo 11 with modulo 11 multiplication, , is cyclic. One of its generators is 6: , , , , , and , the identity of the group.  A Non-cyclic Group The real numbers with addition, is a noncyclic group. The proof of this statement requires a bit more generality since we are saying that for all , is a proper subset of . If is nonzero, the multiples of are distributed over the real line, as in . It is clear then that there are many real numbers, like , that are not in .   Elements of    Elements of the cylic subgroup generated by a real number     The next two proofs make use of the .  The following theorem shows that a cyclic group can never be very complicated.  Possible Cyclic Group Structures  If is a cyclic group, then is either finite or countably infinite. If is finite and , it is isomorphic to . If is infinite, it is isomorphic to .  Case 1: . If is a generator of and , define by for all .  Since is finite, we can use the fact that the elements of are the first nonnegative multiples of . From this observation, we see that is a surjection. A surjection between finite sets of the same cardinality must be a bijection. Finally, if , Therefore is an isomorphism.  Case 2: . We will leave this case as an exercise.   Subgroups of Cyclic Groups  Every subgroup of a cyclic group is cyclic.  Let be cyclic with generator and let . If , has as a generator. We may now assume that and . Let be the least positive integer such that belongs to . This is the key step. It lets us get our hands on a generator of . We will now show that generates . Certainly, , but suppose that . Then there exists such that . Now, since is in , there exists such that . We now apply the division property and divide by . , where . We note that cannot be zero for otherwise we would have . Therefore, . This contradicts our choice of because .  All subgroups of The only proper subgroups of are and . They are both cyclic: , while . The generators of are 1, 3, 7, and 9.  All subgroups of With the exception of , all subgroups of are isomorphic to . If , then is the cyclic subgroup generated by the least positive element of . It is infinite and so by it is isomorphic to .  We now cite a useful theorem for computing the order of cyclic subgroups of a cyclic group:  The order of elements of a finite cyclic group Order of elements of a finite cyclic group If is a cyclic group of order and is a generator of , the order of is , where is the greatest common divisor of and .  The proof of this theorem is left to the reader.  Computation of an order in a cyclic group To compute the order of in , we first observe that 1 is a generator of and . The greatest common divisor of 18 and 30 is 6. Hence, the order of is 30\/6, or 5.  At this point, we will introduce the idea of a fast adder, a relatively modern application (Winograd, 1965) of an ancient theorem, Sun Tzu's Theorem. We will present only an overview of the theory and rely primarily on examples.  Out of necessity, integer addition with a computer is addition modulo , for some larger number. Consider the case where is small, like 64. Then addition involves the addition of six-digit binary numbers. Consider the process of adding 31 and 1. Assume the computer's adder takes as input two bit strings and and outputs , the sum of and . Then, if and , will be (0, 0, 0, 0, 0, 1), or 32. The output cannot be determined until all other outputs have been determined. If addition is done with a finite-state machine, as in , the time required to get will be six time units, where one time unit is the time it takes to get one output from the machine. In general, the time required to obtain will be proportional to the number of bits. Theoretically, this time can be decreased, but the explanation would require a long digression and our relative results would not change that much. We will use the rule that the number of time units needed to perform addition modulo is proportional to .  Now we will introduce a hypothetical problem that we will use to illustrate the idea of a fast adder. Suppose that we had to add 1,000 numbers modulo . By the rule above, since , each addition would take 15 time units. If the sum is initialized to zero, 1,000 additions would be needed; thus, 15,000 time units would be needed to do the additions. We can improve this time dramatically by applying Sun Tzu's Theorem. Recall that is the remainder upon division of by .  Sun Tzu's Theorem  Sun Tzu's Theorem  Chinese Remainder Theorem  STT Sun Tzu's Theorem  Let , , , be integers that have no common factor greater than one between any pair of them; i. e., they are relatively prime. Let . Define by where for , and . Then is an isomorphism from into .  Sun Tzu's Theorem can be stated in several different forms, and its proof can be found in many abstract algebra texts. Older texts most likely will refer to the theorem as the Chinese Remainder Theorem.  As we saw in Chapter 11, is isomorphic to . This is the smallest case to which Sun Tzu's Theorem can be applied. An isomorphism between and is  Let's consider a somewhat larger case. We start by selecting a modulus that can be factored into a product of relatively prime integers: . In this case the factors are , , and . They need not be powers of primes, but it is easy to break the factors into this form to assure relatively prime numbers. To add in , we need time units. Let . Sun Tzu's Theorem gives us an isomorphism between and . The basic idea behind the fast adder, illustrated in , is to make use of this isomorphism. The notation x += a is interpreted as the instruction to add the value of a to the variable x .   Fast Adder Scheme   Fast Adder Scheme    Assume we have several integers to be added. Here, we assume . We compute the sum s to compare our result with this true sum.   Although our sum is an integer calculation, we will put our calculation in the context of the integers modulo 21600. The isomophism from into is defined in Sage as theta . In addition we demonstrate that the operations in these groups are preserved by theta .   We initialize the sums in each factor of the range of theta to zero and decompose each summand into a triple .   Addition in can be done in parallel so that each new subtotal in the form of the triple takes only as long to compute as it takes to add in the largest modulus, time units, if calculations are done in parallel. By the time rule that we have established, the addition of 20 numbers can be done in time units, as opposed to time units if we do the calculations in . However the result is a triple in . The function that performs the inverse of theta is built into most mathematics programs, including Sage. In Sage the function is crt , short for Chinese Remainder Theorem, the other common name of Sun Tzu's Theorem. We use this function to compute the inverse of our triple, which is an element of . The result isn't the true sum because the modulus 21600 is not large enough. However, we verify that our result is congruent to the true sum modulo 21600.   In order to get the true sum from our scheme, the modulus would need to be increased by moving from 21600 to, for example, . Mapping into the new group, will take slightly longer, as will the inversion process with crt , but adding the summands that are in the form of quadruples can be done with no additional time.  The computation of that is done by the Sage function crt can be accomplished in a variety of ways. All of them ultimately are simplified by the fact that is also an isomorphism. One approach is to use the isomorphism property to realize that the value of is . The arithmetic in this expression is in the domain of and is more time consuming, but it need only be done once. This is why the fast adder is only practical in situations where many additions must be performed to get a single sum.  The inverse images of the unit vectors can be computed ahead of time.   The result we computed earlier can be computed directly by in the larger modulus.   To further illustrate the potential of fast adders, consider increasing the modulus to . Each addition using the usual modulo addition with full adders would take 72 time units. By decomposing each summand into 15-tuples according to Sun Tzu's Theorem, the time is reduced to time units per addition.   Exercises  What generators besides 1 does have? The only other generator is .  Suppose is a cyclic group with generator . If you build a graph of with vertices from the elements of and edge set , what would the graph look like? If is a group of even order, what would a graph with edge set look like?  Prove that if and is cyclic, has at least two generators. If , , and , then , , , are distinct elements of . Furthermore, , If , generates :   Similarly, if is infinite and , then generates .   If you wanted to list the generators of you would only have to test the first positive integers. Why?  Which of the following groups are cyclic? Explain.       where        No. Assume that generates . Then . But this gives us at most integer multiples of , not every element in .  No. Similar reasoning to part a.  Yes. 6 is a generator of .  No.  Yes, is a generator of the group.     For each group and element, determine the order of the cyclic subgroup generated by the element:   , 15   , (apply Exercise 8)   , 2   How can be applied to list the generators of ? What are the generators of ? Of ?  implies that generates if and only if the greatest common divisor of and is 1. Therefore the list of generators of are the integers in that are relatively prime to . The generators of are all of the nonzero elements except 5, 10, 15, and 20. The generators of are the odd integers in since 256 is .  Prove that if the greatest common divisor of and is 1, then (1, 1) is a generator of , and hence, is isomorphic to .   Illustrate how the fast adder can be used to add the numbers 21, 5, 7, and 15 using the isomorphism between and .  If the same isomorphism is used to add the numbers 25, 26, and 40, what would the result be, why would it be incorrect, and how would the answer differ from the answer in part a?       maps the given integers as follows: The final sum, 48, is obtained by using the facts that and   .  Using the same isomorphism:  .  The actual sum is 91. Our result is incorrect, since 91 is not in . Notice that 91 and 14 differ by 77. Any error that we get using this technique will be a multiple of 77.    Prove that if is a cyclic group of order with generator , and , then .   "
},
{
  "id": "def-cyclic-group-15",
  "level": "2",
  "url": "s-cyclic-groups.html#def-cyclic-group-15",
  "type": "Definition",
  "number": "15.1.1",
  "title": "Cyclic Group.",
  "body": "Cyclic Group  Cyclic Group   Group is cyclic if there exists such that the cyclic subgroup generated by , , equals all of . That is, , in which case is called a generator of . The reader should note that additive notation is used for . "
},
{
  "id": "ex-a-finite-cyclic-group",
  "level": "2",
  "url": "s-cyclic-groups.html#ex-a-finite-cyclic-group",
  "type": "Example",
  "number": "15.1.2",
  "title": "A Finite Cyclic Group.",
  "body": "A Finite Cyclic Group , where is addition modulo 12, is a cyclic group. To verify this statement, all we need to do is demonstrate that some element of is a generator. One such element is 5; that is, . One more obvious generator is 1. In fact, 1 is a generator of every . The reader is asked to prove that if an element is a generator, then its inverse is also a generator. Thus, and are the other generators of . The remaining eight elements of the group are not generators.   Examples of string art    Examples of string art    (a) is an example of string art that illustrates how 5 generates . Twelve tacks are placed evenly around a circle and numbered 0 through 11. A string is tied to tack 0, and is then looped around every fifth tack. As a result, the numbers of the tacks that are reached are exactly the ordered multiples of 5 modulo 12: 5, 10, 3, ... , 7, 0. Note that if every seventh tack were used, the same artwork would be produced. If every third tack were connected, as in (b), the resulting loop would only use four tacks; thus 3 does not generate .  "
},
{
  "id": "ex-integers-are-cyclic",
  "level": "2",
  "url": "s-cyclic-groups.html#ex-integers-are-cyclic",
  "type": "Example",
  "number": "15.1.4",
  "title": "The Group of Integers is Cyclic.",
  "body": "The Group of Integers is Cyclic The additive group of integers, , is cyclic: This observation does not mean that every integer is the product of an integer times 1. It means that  "
},
{
  "id": "cyclic-implies-abelian",
  "level": "2",
  "url": "s-cyclic-groups.html#cyclic-implies-abelian",
  "type": "Theorem",
  "number": "15.1.5",
  "title": "Cyclic Implies Abelian.",
  "body": "Cyclic Implies Abelian  If is cyclic, then it is abelian.  Let be any generator of and let . By the definition of the generator of a group, there exist integers and such that and . Thus, using ,  "
},
{
  "id": "ex-cyclic-multiplicative",
  "level": "2",
  "url": "s-cyclic-groups.html#ex-cyclic-multiplicative",
  "type": "Example",
  "number": "15.1.6",
  "title": "A Cyclic Multiplicative Group.",
  "body": "A Cyclic Multiplicative Group The group of positive integers modulo 11 with modulo 11 multiplication, , is cyclic. One of its generators is 6: , , , , , and , the identity of the group. "
},
{
  "id": "ex-non-cyclic-group",
  "level": "2",
  "url": "s-cyclic-groups.html#ex-non-cyclic-group",
  "type": "Example",
  "number": "15.1.7",
  "title": "A Non-cyclic Group.",
  "body": "A Non-cyclic Group The real numbers with addition, is a noncyclic group. The proof of this statement requires a bit more generality since we are saying that for all , is a proper subset of . If is nonzero, the multiples of are distributed over the real line, as in . It is clear then that there are many real numbers, like , that are not in .   Elements of    Elements of the cylic subgroup generated by a real number    "
},
{
  "id": "theorem-cyclic-groups",
  "level": "2",
  "url": "s-cyclic-groups.html#theorem-cyclic-groups",
  "type": "Theorem",
  "number": "15.1.9",
  "title": "Possible Cyclic Group Structures.",
  "body": "Possible Cyclic Group Structures  If is a cyclic group, then is either finite or countably infinite. If is finite and , it is isomorphic to . If is infinite, it is isomorphic to .  Case 1: . If is a generator of and , define by for all .  Since is finite, we can use the fact that the elements of are the first nonnegative multiples of . From this observation, we see that is a surjection. A surjection between finite sets of the same cardinality must be a bijection. Finally, if , Therefore is an isomorphism.  Case 2: . We will leave this case as an exercise.  "
},
{
  "id": "theorem-subgroups-of-cyclic-groups",
  "level": "2",
  "url": "s-cyclic-groups.html#theorem-subgroups-of-cyclic-groups",
  "type": "Theorem",
  "number": "15.1.10",
  "title": "Subgroups of Cyclic Groups.",
  "body": "Subgroups of Cyclic Groups  Every subgroup of a cyclic group is cyclic.  Let be cyclic with generator and let . If , has as a generator. We may now assume that and . Let be the least positive integer such that belongs to . This is the key step. It lets us get our hands on a generator of . We will now show that generates . Certainly, , but suppose that . Then there exists such that . Now, since is in , there exists such that . We now apply the division property and divide by . , where . We note that cannot be zero for otherwise we would have . Therefore, . This contradicts our choice of because . "
},
{
  "id": "ex-subgroups-of-z10",
  "level": "2",
  "url": "s-cyclic-groups.html#ex-subgroups-of-z10",
  "type": "Example",
  "number": "15.1.11",
  "title": "All subgroups of <span xmlns:pf=\"https:\/\/prefigure.org\" class=\"process-math\">\\(\\mathbb{Z}_{10}\\)<\/span>.",
  "body": "All subgroups of The only proper subgroups of are and . They are both cyclic: , while . The generators of are 1, 3, 7, and 9. "
},
{
  "id": "ex-subgroups-of-z",
  "level": "2",
  "url": "s-cyclic-groups.html#ex-subgroups-of-z",
  "type": "Example",
  "number": "15.1.12",
  "title": "All subgroups of <span xmlns:pf=\"https:\/\/prefigure.org\" class=\"process-math\">\\(\\mathbb{Z}\\)<\/span>.",
  "body": "All subgroups of With the exception of , all subgroups of are isomorphic to . If , then is the cyclic subgroup generated by the least positive element of . It is infinite and so by it is isomorphic to . "
},
{
  "id": "theorem-order-in-cyclic",
  "level": "2",
  "url": "s-cyclic-groups.html#theorem-order-in-cyclic",
  "type": "Theorem",
  "number": "15.1.13",
  "title": "The order of elements of a finite cyclic group.",
  "body": "The order of elements of a finite cyclic group Order of elements of a finite cyclic group If is a cyclic group of order and is a generator of , the order of is , where is the greatest common divisor of and .  The proof of this theorem is left to the reader. "
},
{
  "id": "ex-order-of-element-cyclic",
  "level": "2",
  "url": "s-cyclic-groups.html#ex-order-of-element-cyclic",
  "type": "Example",
  "number": "15.1.14",
  "title": "Computation of an order in a cyclic group.",
  "body": "Computation of an order in a cyclic group To compute the order of in , we first observe that 1 is a generator of and . The greatest common divisor of 18 and 30 is 6. Hence, the order of is 30\/6, or 5. "
},
{
  "id": "th-crt",
  "level": "2",
  "url": "s-cyclic-groups.html#th-crt",
  "type": "Theorem",
  "number": "15.1.15",
  "title": "Sun Tzu’s Theorem.",
  "body": "Sun Tzu's Theorem  Sun Tzu's Theorem  Chinese Remainder Theorem  STT Sun Tzu's Theorem  Let , , , be integers that have no common factor greater than one between any pair of them; i. e., they are relatively prime. Let . Define by where for , and . Then is an isomorphism from into . "
},
{
  "id": "fig-parallel-sum",
  "level": "2",
  "url": "s-cyclic-groups.html#fig-parallel-sum",
  "type": "Figure",
  "number": "15.1.16",
  "title": "",
  "body": " Fast Adder Scheme   Fast Adder Scheme   "
},
{
  "id": "exercises-15-1-2",
  "level": "2",
  "url": "s-cyclic-groups.html#exercises-15-1-2",
  "type": "Exercise",
  "number": "15.1.1",
  "title": "",
  "body": "What generators besides 1 does have? The only other generator is . "
},
{
  "id": "exercises-15-1-3",
  "level": "2",
  "url": "s-cyclic-groups.html#exercises-15-1-3",
  "type": "Exercise",
  "number": "15.1.2",
  "title": "",
  "body": "Suppose is a cyclic group with generator . If you build a graph of with vertices from the elements of and edge set , what would the graph look like? If is a group of even order, what would a graph with edge set look like? "
},
{
  "id": "exercises-15-1-4",
  "level": "2",
  "url": "s-cyclic-groups.html#exercises-15-1-4",
  "type": "Exercise",
  "number": "15.1.3",
  "title": "",
  "body": "Prove that if and is cyclic, has at least two generators. If , , and , then , , , are distinct elements of . Furthermore, , If , generates :   Similarly, if is infinite and , then generates .  "
},
{
  "id": "exercises-15-1-5",
  "level": "2",
  "url": "s-cyclic-groups.html#exercises-15-1-5",
  "type": "Exercise",
  "number": "15.1.4",
  "title": "",
  "body": "If you wanted to list the generators of you would only have to test the first positive integers. Why? "
},
{
  "id": "exercises-15-1-6",
  "level": "2",
  "url": "s-cyclic-groups.html#exercises-15-1-6",
  "type": "Exercise",
  "number": "15.1.5",
  "title": "",
  "body": "Which of the following groups are cyclic? Explain.       where        No. Assume that generates . Then . But this gives us at most integer multiples of , not every element in .  No. Similar reasoning to part a.  Yes. 6 is a generator of .  No.  Yes, is a generator of the group.   "
},
{
  "id": "exercises-15-1-7",
  "level": "2",
  "url": "s-cyclic-groups.html#exercises-15-1-7",
  "type": "Exercise",
  "number": "15.1.6",
  "title": "",
  "body": " For each group and element, determine the order of the cyclic subgroup generated by the element:   , 15   , (apply Exercise 8)   , 2  "
},
{
  "id": "exercises-15-1-8",
  "level": "2",
  "url": "s-cyclic-groups.html#exercises-15-1-8",
  "type": "Exercise",
  "number": "15.1.7",
  "title": "",
  "body": "How can be applied to list the generators of ? What are the generators of ? Of ?  implies that generates if and only if the greatest common divisor of and is 1. Therefore the list of generators of are the integers in that are relatively prime to . The generators of are all of the nonzero elements except 5, 10, 15, and 20. The generators of are the odd integers in since 256 is . "
},
{
  "id": "exercises-15-1-9",
  "level": "2",
  "url": "s-cyclic-groups.html#exercises-15-1-9",
  "type": "Exercise",
  "number": "15.1.8",
  "title": "",
  "body": "Prove that if the greatest common divisor of and is 1, then (1, 1) is a generator of , and hence, is isomorphic to . "
},
{
  "id": "exercises-15-1-10",
  "level": "2",
  "url": "s-cyclic-groups.html#exercises-15-1-10",
  "type": "Exercise",
  "number": "15.1.9",
  "title": "",
  "body": " Illustrate how the fast adder can be used to add the numbers 21, 5, 7, and 15 using the isomorphism between and .  If the same isomorphism is used to add the numbers 25, 26, and 40, what would the result be, why would it be incorrect, and how would the answer differ from the answer in part a?       maps the given integers as follows: The final sum, 48, is obtained by using the facts that and   .  Using the same isomorphism:  .  The actual sum is 91. Our result is incorrect, since 91 is not in . Notice that 91 and 14 differ by 77. Any error that we get using this technique will be a multiple of 77.   "
},
{
  "id": "exercises-15-1-11",
  "level": "2",
  "url": "s-cyclic-groups.html#exercises-15-1-11",
  "type": "Exercise",
  "number": "15.1.10",
  "title": "",
  "body": "Prove that if is a cyclic group of order with generator , and , then . "
},
{
  "id": "s-cosets-and-factor-groups",
  "level": "1",
  "url": "s-cosets-and-factor-groups.html",
  "type": "Section",
  "number": "15.2",
  "title": "Cosets and Factor Groups",
  "body": " Cosets and Factor Groups  Cosets and Factor Groups  Consider the group . As we saw in the previous section, we can picture its cyclic properties with the string art of . Here we will be interested in the non-generators, like 3. The solid lines in show that only one-third of the tacks have been reached by starting at zero and jumping to every third tack. The numbers of these tacks correspond to .   String art cosets   String art cosets    What happens if you start at one of the unused tacks and again jump to every third tack? The two broken paths on show that identical squares are produced. The tacks are thus partitioned into very similar subsets. The subsets of that they correspond to are , , and . These subsets are called cosets. In particular, they are called cosets of the subgroup . We will see that under certain conditions, cosets of a subgroup can form a group of their own. Before pursuing this example any further we will examine the general situation.  Coset  Coset  the left and right cosets generated by  If is a group, and , the left coset of generated by is and the right coset of generated by is     itself is both a left and right coset since .  If is abelian, and the left-right distinction for cosets can be dropped. We will normally use left coset notation in that situation.   Coset Representative Coset Representative  Any element of a coset is called a representative of that coset.  One might wonder whether is in any way a special representative of since it seems to define the coset. It is not, as we shall see.  A Duality Principle A duality principle can be formulated concerning cosets because left and right cosets are defined in such similar ways. Any theorem about left and right cosets will yield a second theorem when left and right are exchanged for right and left.   If , then , and if , then . In light of the remark above, we need only prove the first part of this theorem. Suppose that . We need only find a way of expressing as times an element of . Then we will have proven that . By the definition of , since and are in , there exist and in such that and . Given these two equations, and Since , , and we are done with this part of the proof. In order to show that , one can follow essentially the same steps, which we will let the reader fill in.  In , you can start at either 1 or 7 and obtain the same path by taking jumps of three tacks in each step. Thus,  The set of left (or right) cosets of a subgroup partition a group in a special way:  Cosets Partition a Group Partition of a group by cosets  If is a group and , the set of left cosets of is a partition of . In addition, all of the left cosets of have the same cardinality. The same is true for right cosets.  That every element of belongs to a left coset is clear because for all . If and are left cosets, we will prove that they are either equal or disjoint. If and are not disjoint, is nonempty and some element belongs to the intersection. Then by , and . Hence .  We complete the proof by showing that each left coset has the same cardinality as . To do this, we simply observe that if , defined by is a bijection and hence . We will leave the proof of this statement to the reader.  The function has a nice interpretation in terms of our opening example. If , the graph of is rotated to coincide with one of the three cosets of .   A Coset Counting Formula  Coset Counting Formula  If and , the number of distinct left cosets of equals . For this reason we use to denote the set of left cosets of in  This follows from the partitioning of into equal sized sets, one of which is .  The set of integer multiples of four, , is a subgroup of . Four distinct cosets of partition the integers. They are , , , and , where, for example, . can also be written .  Distinguished Representatives Although we have seen that any representative can describe a coset, it is often convenient to select a distinguished representative from each coset. The advantage to doing this is that there is a unique name for each coset in terms of its distinguished representative. In numeric examples such as the one above, the distinguished representative is usually the smallest nonnegative representative. Remember, this is purely a convenience and there is absolutely nothing wrong in writing , , or in place of because .  Before completing the main thrust of this section, we will make note of a significant implication of . Since a finite group is divided into cosets of a common size by any subgroup, we can conclude:  Lagrange's Theorem  Lagrange's Theorem  The order of a subgroup of a finite group must divide the order of the group.   One immediate implication of Lagrange's Theorem is that if is prime, has no proper subgroups.  We will now describe the operation on cosets which will, under certain circumstances, result in a group. For most of this section, we will assume that is an abelian group. This is one sufficient (but not necessary) condition that guarantees that the set of left cosets will form a group.  Operation on Cosets  Cosets Operation on  Let and be left cosets of , a subgroup of with representatives and , respectively. Then The operation is called the operation induced on left cosets by .  In , later in this section, we will prove that if is an abelian group, is indeed an operation. In practice, if the group is an additive group, the symbol is replaced by , as in the following example.  Computing with cosets of  Consider the cosets described in . For brevity, we rename , , , and with the symbols , , , and . Let's do a typical calculation, . We will see that the result is always going to be , no matter what representatives we select. For example, , , and . Our choice of the representatives and were completely arbitrary.  In general, can be computed in many ways, and so it is necessary to show that the choice of representatives does not affect the result. When the result we get for is always independent of our choice of representatives, we say that is well defined. Addition of cosets is a well-defined operation on the left cosets of 4 and is summarized in the following table. Do you notice anything familiar?   Cosets of the integers in the group of Real numbers  Consider the group of real numbers, , and its subgroup of integers, . Every element of has the same cardinality as . Let . if can be written for some . Hence and belong to the same coset if they differ by an integer. (See for a generalization of this fact.)  Now consider the coset . Real numbers that differ by an integer from 0.25 are and . If any real number is selected, there exists a representative of its coset that is greater than or equal to 0 and less than 1. We will call that representative the distinguished representative of the coset. For example, 43.125 belongs to the coset represented by 0.125; has 0.618 as its distinguished representative. The operation on is commonly called addition modulo 1. A few typical calculations in are .  Cosets in a Direct Product  Consider , where . Since is of order 8, each element of is a coset containing two ordered pairs. We will leave it to the reader to verify that the four distinct cosets are , , and . The reader can also verify that is isomorphic to , since is cyclic. An educated guess should give you a generator.   Consider the group . Let be , the cyclic subgroup of generate by (1,0,1,0). Since the order of is 2 and , has elements. A typical coset is Note that since , , the identity for the operation on . The orders of non-identity elements of this factor group are all 2, and it can be shown that the factor group is isomorphic to .   Coset operation is well-defined (Abelian Case)  If is an abelian group, and , the operation induced on cosets of by the operation of is well defined. Suppose that , , and , . are two choices for representatives of cosets and . That is to say that , . We will show that and are representatives of the same coset. 1 implies that and , thus we have and . Then there exists such that and and so by various group properties and the assumption that is abelian, which lets us reverse the order in which and appear in the chain of equalities. This last expression for implies that since because is a subgroup of . Thus, we get the same coset for both pairs of representatives.   Let be a group and . If the operation induced on left cosets of by the operation of is well defined, then the set of left cosets forms a group under that operation.  Let , and be the left cosets with representatives , , and , respectively. The values of and are determined by and , respectively. By the associativity of in , these two group elements are equal and so the two coset expressions must be equal. Therefore, the induced operation is associative. As for the identity and inverse properties, there is no surprise. The identity coset is , or , the coset that contains 's identity. If is a coset with representative ; that is, if , then is . .  Factor Group  Factor Group  The factor group G mod H. Let be a group and . If the set of left cosets of forms a group, then that group is called the factor group of modulo . It is denoted .  If is abelian, then every subgroup of yields a factor group. We will delay further consideration of the non-abelian case to Section 15.4.  On Notation It is customary to use the same symbol for the operation of as for the operation on . The reason we used distinct symbols in this section was to make the distinction clear between the two operations.   Exercises  Consider and the subsets of , and . Why is the operation induced on these subsets by modulo 10 addition not well defined?  An example of a valid correct answer: Call the subsets and respectively. If we choose and we get . On the other hand, if we choose and , we get . Therefore, the induced operation is not well defined on .  Can you think of a group , with a subgroup such that and ? Is your answer unique?   For each group and subgroup, what is isomorphic to?   and . Compare to .   and .   = and .     The four distinct cosets in are , , , and . None of these cosets generates ; therefore is not cyclic. Hence must be isomorphic to .  The factor group is isomorphic to . Each coset of is a line in the complex plane that is parallel to the x-axis: , where is an isomorphism.    . The four cosets are: , , , and . 1 generates all four cosets. The factor group is isomorphic to because is a generator.    For each group and subgroup, what is isomorphic to?   and .   and .   and .    Assume that is a group, , and . Prove that if and only if .      Real addition modulo , , can be described as the operation induced on cosets of by ordinary addition. Describe a system of distinguished representatives for the elements of .  Consider the trigonometric function sine. Given that for all and , show how the distinguished representatives of can be useful in developing an algorithm for calculating the sine of a number.    Complete the proof of by proving that if , defined by is a bijection.    "
},
{
  "id": "fig-string-art-cosets",
  "level": "2",
  "url": "s-cosets-and-factor-groups.html#fig-string-art-cosets",
  "type": "Figure",
  "number": "15.2.1",
  "title": "",
  "body": " String art cosets   String art cosets   "
},
{
  "id": "def-coset",
  "level": "2",
  "url": "s-cosets-and-factor-groups.html#def-coset",
  "type": "Definition",
  "number": "15.2.2",
  "title": "Coset.",
  "body": "Coset  Coset  the left and right cosets generated by  If is a group, and , the left coset of generated by is and the right coset of generated by is "
},
{
  "id": "s-cosets-and-factor-groups-7",
  "level": "2",
  "url": "s-cosets-and-factor-groups.html#s-cosets-and-factor-groups-7",
  "type": "Note",
  "number": "15.2.3",
  "title": "",
  "body": "   itself is both a left and right coset since .  If is abelian, and the left-right distinction for cosets can be dropped. We will normally use left coset notation in that situation.  "
},
{
  "id": "def-coset-representative",
  "level": "2",
  "url": "s-cosets-and-factor-groups.html#def-coset-representative",
  "type": "Definition",
  "number": "15.2.4",
  "title": "Coset Representative.",
  "body": "Coset Representative Coset Representative  Any element of a coset is called a representative of that coset. "
},
{
  "id": "s-cosets-and-factor-groups-10",
  "level": "2",
  "url": "s-cosets-and-factor-groups.html#s-cosets-and-factor-groups-10",
  "type": "Remark",
  "number": "15.2.5",
  "title": "A Duality Principle.",
  "body": "A Duality Principle A duality principle can be formulated concerning cosets because left and right cosets are defined in such similar ways. Any theorem about left and right cosets will yield a second theorem when left and right are exchanged for right and left. "
},
{
  "id": "theorem-15-2-1",
  "level": "2",
  "url": "s-cosets-and-factor-groups.html#theorem-15-2-1",
  "type": "Theorem",
  "number": "15.2.6",
  "title": "",
  "body": " If , then , and if , then . In light of the remark above, we need only prove the first part of this theorem. Suppose that . We need only find a way of expressing as times an element of . Then we will have proven that . By the definition of , since and are in , there exist and in such that and . Given these two equations, and Since , , and we are done with this part of the proof. In order to show that , one can follow essentially the same steps, which we will let the reader fill in. "
},
{
  "id": "ex-15-2-1",
  "level": "2",
  "url": "s-cosets-and-factor-groups.html#ex-15-2-1",
  "type": "Example",
  "number": "15.2.7",
  "title": "",
  "body": "In , you can start at either 1 or 7 and obtain the same path by taking jumps of three tacks in each step. Thus, "
},
{
  "id": "theorem-cosets-partition",
  "level": "2",
  "url": "s-cosets-and-factor-groups.html#theorem-cosets-partition",
  "type": "Theorem",
  "number": "15.2.8",
  "title": "Cosets Partition a Group.",
  "body": "Cosets Partition a Group Partition of a group by cosets  If is a group and , the set of left cosets of is a partition of . In addition, all of the left cosets of have the same cardinality. The same is true for right cosets.  That every element of belongs to a left coset is clear because for all . If and are left cosets, we will prove that they are either equal or disjoint. If and are not disjoint, is nonempty and some element belongs to the intersection. Then by , and . Hence .  We complete the proof by showing that each left coset has the same cardinality as . To do this, we simply observe that if , defined by is a bijection and hence . We will leave the proof of this statement to the reader. "
},
{
  "id": "corollary-counting-cosets",
  "level": "2",
  "url": "s-cosets-and-factor-groups.html#corollary-counting-cosets",
  "type": "Corollary",
  "number": "15.2.9",
  "title": "A Coset Counting Formula.",
  "body": " A Coset Counting Formula  Coset Counting Formula  If and , the number of distinct left cosets of equals . For this reason we use to denote the set of left cosets of in  This follows from the partitioning of into equal sized sets, one of which is . "
},
{
  "id": "ex-finite-number-of-infinite-cosets",
  "level": "2",
  "url": "s-cosets-and-factor-groups.html#ex-finite-number-of-infinite-cosets",
  "type": "Example",
  "number": "15.2.10",
  "title": "",
  "body": "The set of integer multiples of four, , is a subgroup of . Four distinct cosets of partition the integers. They are , , , and , where, for example, . can also be written . "
},
{
  "id": "conv-distinguishedi-representatives",
  "level": "2",
  "url": "s-cosets-and-factor-groups.html#conv-distinguishedi-representatives",
  "type": "Notation",
  "number": "15.2.11",
  "title": "Distinguished Representatives.",
  "body": "Distinguished Representatives Although we have seen that any representative can describe a coset, it is often convenient to select a distinguished representative from each coset. The advantage to doing this is that there is a unique name for each coset in terms of its distinguished representative. In numeric examples such as the one above, the distinguished representative is usually the smallest nonnegative representative. Remember, this is purely a convenience and there is absolutely nothing wrong in writing , , or in place of because . "
},
{
  "id": "th-lagrange-theorem",
  "level": "2",
  "url": "s-cosets-and-factor-groups.html#th-lagrange-theorem",
  "type": "Theorem",
  "number": "15.2.12",
  "title": "Lagrange’s Theorem.",
  "body": "Lagrange's Theorem  Lagrange's Theorem  The order of a subgroup of a finite group must divide the order of the group.  "
},
{
  "id": "def-operation-on-cosets",
  "level": "2",
  "url": "s-cosets-and-factor-groups.html#def-operation-on-cosets",
  "type": "Definition",
  "number": "15.2.13",
  "title": "Operation on Cosets.",
  "body": "Operation on Cosets  Cosets Operation on  Let and be left cosets of , a subgroup of with representatives and , respectively. Then The operation is called the operation induced on left cosets by . "
},
{
  "id": "ex-cosets-of-4z",
  "level": "2",
  "url": "s-cosets-and-factor-groups.html#ex-cosets-of-4z",
  "type": "Example",
  "number": "15.2.14",
  "title": "Computing with cosets of <span xmlns:pf=\"https:\/\/prefigure.org\" class=\"process-math\">\\(4\\mathbb{Z}\\)<\/span>.",
  "body": "Computing with cosets of  Consider the cosets described in . For brevity, we rename , , , and with the symbols , , , and . Let's do a typical calculation, . We will see that the result is always going to be , no matter what representatives we select. For example, , , and . Our choice of the representatives and were completely arbitrary. "
},
{
  "id": "cosets-of-a-in-r",
  "level": "2",
  "url": "s-cosets-and-factor-groups.html#cosets-of-a-in-r",
  "type": "Example",
  "number": "15.2.15",
  "title": "Cosets of the integers in the group of Real numbers.",
  "body": "Cosets of the integers in the group of Real numbers  Consider the group of real numbers, , and its subgroup of integers, . Every element of has the same cardinality as . Let . if can be written for some . Hence and belong to the same coset if they differ by an integer. (See for a generalization of this fact.)  Now consider the coset . Real numbers that differ by an integer from 0.25 are and . If any real number is selected, there exists a representative of its coset that is greater than or equal to 0 and less than 1. We will call that representative the distinguished representative of the coset. For example, 43.125 belongs to the coset represented by 0.125; has 0.618 as its distinguished representative. The operation on is commonly called addition modulo 1. A few typical calculations in are . "
},
{
  "id": "ex-cosets-in-direct-product",
  "level": "2",
  "url": "s-cosets-and-factor-groups.html#ex-cosets-in-direct-product",
  "type": "Example",
  "number": "15.2.16",
  "title": "Cosets in a Direct Product.",
  "body": "Cosets in a Direct Product  Consider , where . Since is of order 8, each element of is a coset containing two ordered pairs. We will leave it to the reader to verify that the four distinct cosets are , , and . The reader can also verify that is isomorphic to , since is cyclic. An educated guess should give you a generator. "
},
{
  "id": "ex-cosets-in-z2-to-4",
  "level": "2",
  "url": "s-cosets-and-factor-groups.html#ex-cosets-in-z2-to-4",
  "type": "Example",
  "number": "15.2.17",
  "title": "",
  "body": " Consider the group . Let be , the cyclic subgroup of generate by (1,0,1,0). Since the order of is 2 and , has elements. A typical coset is Note that since , , the identity for the operation on . The orders of non-identity elements of this factor group are all 2, and it can be shown that the factor group is isomorphic to . "
},
{
  "id": "theorem-coset-operation-well-defined",
  "level": "2",
  "url": "s-cosets-and-factor-groups.html#theorem-coset-operation-well-defined",
  "type": "Theorem",
  "number": "15.2.18",
  "title": "Coset operation is well-defined (Abelian Case).",
  "body": " Coset operation is well-defined (Abelian Case)  If is an abelian group, and , the operation induced on cosets of by the operation of is well defined. Suppose that , , and , . are two choices for representatives of cosets and . That is to say that , . We will show that and are representatives of the same coset. 1 implies that and , thus we have and . Then there exists such that and and so by various group properties and the assumption that is abelian, which lets us reverse the order in which and appear in the chain of equalities. This last expression for implies that since because is a subgroup of . Thus, we get the same coset for both pairs of representatives. "
},
{
  "id": "theorem-15-2-4",
  "level": "2",
  "url": "s-cosets-and-factor-groups.html#theorem-15-2-4",
  "type": "Theorem",
  "number": "15.2.19",
  "title": "",
  "body": " Let be a group and . If the operation induced on left cosets of by the operation of is well defined, then the set of left cosets forms a group under that operation.  Let , and be the left cosets with representatives , , and , respectively. The values of and are determined by and , respectively. By the associativity of in , these two group elements are equal and so the two coset expressions must be equal. Therefore, the induced operation is associative. As for the identity and inverse properties, there is no surprise. The identity coset is , or , the coset that contains 's identity. If is a coset with representative ; that is, if , then is . . "
},
{
  "id": "def-factor-group",
  "level": "2",
  "url": "s-cosets-and-factor-groups.html#def-factor-group",
  "type": "Definition",
  "number": "15.2.20",
  "title": "Factor Group.",
  "body": "Factor Group  Factor Group  The factor group G mod H. Let be a group and . If the set of left cosets of forms a group, then that group is called the factor group of modulo . It is denoted . "
},
{
  "id": "s-cosets-and-factor-groups-33",
  "level": "2",
  "url": "s-cosets-and-factor-groups.html#s-cosets-and-factor-groups-33",
  "type": "Note",
  "number": "15.2.21",
  "title": "",
  "body": "If is abelian, then every subgroup of yields a factor group. We will delay further consideration of the non-abelian case to Section 15.4. "
},
{
  "id": "s-cosets-and-factor-groups-34",
  "level": "2",
  "url": "s-cosets-and-factor-groups.html#s-cosets-and-factor-groups-34",
  "type": "Remark",
  "number": "15.2.22",
  "title": "On Notation.",
  "body": "On Notation It is customary to use the same symbol for the operation of as for the operation on . The reason we used distinct symbols in this section was to make the distinction clear between the two operations. "
},
{
  "id": "exercises-15-2-2",
  "level": "2",
  "url": "s-cosets-and-factor-groups.html#exercises-15-2-2",
  "type": "Exercise",
  "number": "15.2.1",
  "title": "",
  "body": "Consider and the subsets of , and . Why is the operation induced on these subsets by modulo 10 addition not well defined?  An example of a valid correct answer: Call the subsets and respectively. If we choose and we get . On the other hand, if we choose and , we get . Therefore, the induced operation is not well defined on . "
},
{
  "id": "exercises-15-2-3",
  "level": "2",
  "url": "s-cosets-and-factor-groups.html#exercises-15-2-3",
  "type": "Exercise",
  "number": "15.2.2",
  "title": "",
  "body": "Can you think of a group , with a subgroup such that and ? Is your answer unique?  "
},
{
  "id": "exercises-15-2-4",
  "level": "2",
  "url": "s-cosets-and-factor-groups.html#exercises-15-2-4",
  "type": "Exercise",
  "number": "15.2.3",
  "title": "",
  "body": "For each group and subgroup, what is isomorphic to?   and . Compare to .   and .   = and .     The four distinct cosets in are , , , and . None of these cosets generates ; therefore is not cyclic. Hence must be isomorphic to .  The factor group is isomorphic to . Each coset of is a line in the complex plane that is parallel to the x-axis: , where is an isomorphism.    . The four cosets are: , , , and . 1 generates all four cosets. The factor group is isomorphic to because is a generator.   "
},
{
  "id": "exercises-15-2-5",
  "level": "2",
  "url": "s-cosets-and-factor-groups.html#exercises-15-2-5",
  "type": "Exercise",
  "number": "15.2.4",
  "title": "",
  "body": "For each group and subgroup, what is isomorphic to?   and .   and .   and .   "
},
{
  "id": "exercises-15-2-6",
  "level": "2",
  "url": "s-cosets-and-factor-groups.html#exercises-15-2-6",
  "type": "Exercise",
  "number": "15.2.5",
  "title": "",
  "body": "Assume that is a group, , and . Prove that if and only if .    "
},
{
  "id": "exercise-15-2-6",
  "level": "2",
  "url": "s-cosets-and-factor-groups.html#exercise-15-2-6",
  "type": "Exercise",
  "number": "15.2.6",
  "title": "",
  "body": " Real addition modulo , , can be described as the operation induced on cosets of by ordinary addition. Describe a system of distinguished representatives for the elements of .  Consider the trigonometric function sine. Given that for all and , show how the distinguished representatives of can be useful in developing an algorithm for calculating the sine of a number.   "
},
{
  "id": "exercises-15-2-8",
  "level": "2",
  "url": "s-cosets-and-factor-groups.html#exercises-15-2-8",
  "type": "Exercise",
  "number": "15.2.7",
  "title": "",
  "body": "Complete the proof of by proving that if , defined by is a bijection.  "
},
{
  "id": "s-permutation-groups",
  "level": "1",
  "url": "s-permutation-groups.html",
  "type": "Section",
  "number": "15.3",
  "title": "Permutation Groups",
  "body": " Permutation Groups  Permutation Groups  The Symmetric Groups  At the risk of boggling the reader's mind, we will now examine groups whose elements are functions. Recall that a permutation on a set is a bijection from into . Suppose that . There are different permutations on . We will call the set of all 6 permutations . They are listed in the following table. The matrix form for describing a function on a finite set is to list the domain across the top row and the image of each element directly below it. For example .   Elements of               The operation that will give a group structure is function composition. Consider the product  : .  The images of 1, 2, and 3 under and are identical. Thus, by the definition of equality for functions, we can say . The complete table for the operation of function composition is given in .   Operation Table for      We don't even need the table to verify that we have a group. Based on the following observations, the set of all permutations on any finite set will be a group.   Function composition is always associative.  The identity for the group is . If is any one of the permutations on and , Therefore .  A permutation, by definition, is a bijection. In Chapter 7 we proved that this implies that it must have an inverse and the inverse itself is a bijection and hence a permutation. Hence all elements of have an inverse in . If a permutation is displayed in matrix form, its inverse can be obtained by exchanging the two rows and rearranging the columns so that the top row is in order. The first step is actually sufficient to obtain the inverse, but the sorting of the top row makes it easier to recognize the inverse.  For example, let's consider a typical permutation on , . .    From , we can see that is non-abelian. Remember, non-abelian is the negation of abelian. The existence of two elements that don't commute is sufficient to make a group non-abelian. In this group, and is one such pair: while , so . Caution: Don't take this to mean that every pair of elements has to have this property. There are several pairs of elements in that do commute. In fact, the identity, , must commute with everything. Also every element must commute with its inverse.  Symmetric Group Symmetric Group  The group of permutations of the set  The group of permutations on a set with elements  Let be a nonempty set. The set of all permutations on with the operation of function composition is called the symmetric group on , denoted .  The cardinality of a finite set is more significant than the elements, and we will denote by the symmetric group on any set of cardinality , .  The significance of Our opening example, , is the smallest non-abelian group. For that reason, all of its proper subgroups are abelian: in fact, they are all cyclic. shows the Hasse diagram for the subgroups of .   Lattice diagram of subgroups of    Lattice diagram of subgroups of     Smallest Symmetric Groups The only abelian symmetric groups are and , with 1 and 2 elements, respectively. The elements of are and . is isomorphic to .   For , and for , is non-abelian. The first part of the theorem follows from the extended rule of products (see Chapter 2). We leave the details of proof of the second part to the reader after the following hint. Consider in where , , , and for . Therefore the cycle representation of is . Now define in a similar manner so that when you compare and you get different results.   Cycle Notation Cycle Notation  A second way of describing a permutation is by means of cycles, which we will introduce first with an example. Consider defined using the now-familiar matrix notation: Consider the images of 1 when is applied repeatedly. The images , , are . In (a), this situation is represented by a graph with vertices 1, 8, 3, and 7 and shows that the values that you get by repeatedly applying cycle through those values. This is why we refer to this part of as a cycle of length 4. Of course starting at 8, 3, or 7 also produces the same cycle with only the starting value changing.   Representations of a cycle of length 4   Representations of a cycle of length 4    (a) illustrates how the cycle can be represented in a visual manner, but it is a bit awkward to write. Part (b) of the figure presents a more universally recognized way to write a cycle. In (b), a cycle is represented by a list where the image of any number in the list is its successor. In addition, the last number in the list has as its image the first number.  The other elements of the domain of are never reached if you start in the cycle , and so looking at the images of these other numbers will produce numbers that are disjoint from the set . The other disjoint cycles of are (2), (4, 6), and (5). We can express as a product of disjoint cycles: or , where the absence of 2 and 5 implies that and .  Disjoint Cycles Disjoint Cycles We say that two cycles are disjoint if no number appears in both cycles, as is the case in our expressions for above. Disjoint cycles can be written in any order. Thus, we could also say that .  Composition of Permutations Permutations Composition We will now consider the composition of permutations written in cyclic form by an example. Suppose that and are elements of . To calculate , we start with simple concatenation: Although this is a valid expression for , our goal is to express the composition as a product of disjoint cycles as and were individually written. We will start by determining the cycle that contains 1. When combining any number of cycles, they are always read from right to left, as with all functions. The first cycle in does not contain 1; thus we move on to the second. The image of 1 under that cycle is 5. Now we move on to the next cycle, looking for 5, which doesn't appear. The fourth cycle does not contain a 5 either; so .  At this point, we would have written on paper. We repeat the steps to determine . This time the second cycle of moves 5 to 6 and then the third cycle moves 6 to 4. Therefore, . We continue until the cycle (1, 5, 4, 3) is completed by determining that . The process is then repeated starting with any number that does not appear in the cycle(s) that have already been completed.  The final result for our example is . Since and , and we need not include the one-cycle (2) in the final result, although it can be included.    Some Compositions      .  Notice that cyclic notation does not indicate the set which is being permuted. The examples above could be in , where the image of 5 is 5. This ambiguity is usually overcome by making the context clear at the start of a discussion.   Transposition Transposition  A transposition is a cycle of length 2.  About transpositions and are transpositions in . However, and are not transpositions; thus, the set of transpositions is not closed under composition. Since and are both equal to the identity permutation, and are their own inverses. In fact, every transposition is its own inverse.  Decomposition into Cycles  Every cycle of length greater than 2 can be expressed as a product of transpositions.  We need only indicate how the product of transpositions can be obtained. It is easy to verify that a cycle of length , , is equal to the following product of transpositions:    Of course, a product of cycles can be written as a product of transpositions just as easily by applying the rule above to each cycle. For example, Unlike the situation with disjoint cycles, we are not free to change the order of these transpositions.   Parity of Permutations and the Alternating Group  A decomposition of permutations into transpositions makes it possible to classify them and identify an important family of groups.  The proofs of the following theorem appears in many abstract algebra texts.  Every permutation on a finite set can be expressed as the product of an even number of transpositions or an odd number of transpositions, but not both.  suggests that can be partitioned into its even and odd elements. For example, the even permutations of are , and . They form a subgroup, of .  In general:  The Alternating Group  Alternating Group  The Alternating Group  Let . The set of even permutations in is a proper subgroup of called the alternating group on , denoted .  We justify our statement that is a group:   Let . The alternating group is indeed a group and has order . In this proof, the symbols and stand for transpositions and , are even nonnegative integers. If , we can write the two permutations as products of even numbers of transpositions, and . Then Since is even, , and is closed with respect to function composition. With this, we have proven that is a subgroup of by .  To prove the final assertion, let be the set of odd permutations and let . Define by . Suppose that . Then and by the right cancellation law, . Hence, is an injection. Next we show that is also a surjection. If , is the image of an element of . Specifically, is the image of . Since is a bijection, .  The Sliding Tile Puzzle  Consider the sliding-tile puzzles pictured in . Each numbered square is a tile and the dark square is a gap. Any tile that is adjacent to the gap can slide into the gap. In most versions of this puzzle, the tiles are locked into a frame so that they can be moved only in the manner described above. The object of the puzzle is to arrange the tiles as they appear in Configuration (a). Configurations (b) and (c) are typical starting points. We propose to show why the puzzle can be solved starting with (b), but not with (c).   Configurations of the sliding tile puzzle   Configurations of the sliding tile puzzle    We will associate a change in the configuration of the puzzle with an element of . Imagine that a tile numbered 16 fills in the gap. For any configuration of the puzzle, the identity , is the function that leave the configurate as is. In general, if , and , is the position to which the tile in position is moved by that appears in the position of in configuration (a). If we call the functions that, starting with configuration (a), result in configurations (b) and (c) by the names and , respectively, and   How can we interpret the movement of one tile as a permutation? Consider what happens when the 12 tile of slides into the gap. The result is a configuration that we would interpret as , a single transposition. Now if we slide the 8 tile into the 12 position, the result is or . Hence, by exchanging the tiles 8 and 16, we have implemented the function .   The configuration    The configuration    Every time you slide a tile into the gap, the new permutation is a transposition composed with the old permutation. Now observe that to start with initial configuration and terminate after a finite number of moves with the gap in its original position, you must make an even number of moves. Thus, configuration corresponding any permutation that leaves 16 fixed cannot be solved if the permutation is odd. Note that is an odd permutation; thus, Puzzle (c) can't be solved. The proof that all even permutations, such as , can be solved is left to the interested reader to pursue.   Dihedral Groups  Dihedral Group  Realizations of Groups  By now we've seen several instances where a group can appear through an isomorphic copy of itself in various settings. The simplest such example is the cyclic group of order 2. When this group is mentioned, we might naturally think of the group , but the groups and are isomorphic to it. None of these groups are necessarily more natural or important than the others. Which one you use depends on the situation you are in and all are referred to as realizations of the cyclic group of order 2. The next family of groups we will study, the dihedral groups, has two natural realizations, first as permutations and second as geometric symmetries.  The family of dihedral groups is indexed by the positive integers greater than or equal to 3. For , will have elements. We first describe the elements and the operation on them using geometry.  We can describe in terms of symmetries of a regular -gon ( : equilateral triangle, : square, : regular pentagon, ). Here we will only concentrate on the case of . If a square is fixed in space, there are several motions of the square that will, at the end of the motion, not change the apparent position of the square. The actual changes in position can be seen if the corners of the square are labeled. In , the initial labeling scheme is shown, along with the four axes of symmetry of the square.   Axes of symmetry of the square   Axes of symmetry of the square    It might be worthwhile making a square like this with a sheet of paper. Be careful to label the back so that the numbers match the front. Two motions of the square will be considered equivalent if the square is in the same position after performing either motion. There are eight distinct motions. The first four are , , , and clockwise rotations of the square, and the other four are the flips along the axes , , , and . We will call the rotations , , and , respectively, and the flips , , , and , respectively. illustrates and . For future reference, we also include the permutations to which they correspond.   Two elements of    Two elements of    What is the operation on this set of symmetries? We will call the operation followed by and use the symbol to represent it. The operation will be to combine motions, applying motions from right to left, as with functions. We will illustrate how is computed by finding . Starting with the initial configuration, if you perform the motion, and then immediately perform on the result, we get the same configuration as if we just performed , which is to flip the square along the line . Therefore, . An important observation is that , meaning that this group is nonabelian. The reader is encouraged to verify this on their own.  We can also realize the dihedral groups as permutations. For any symmetric motion of the square we can associate with it a permutation. In the case of , the images of each of the numbers 1 through 4 are the positions on the square that each of the corners 1 through 4 are moved to. For example, since corner 4 moves to position 1 when you perform , the corresponding function will map 4 to 1. In addition, 1 gets mapped to 2, 2 to 3 and 3 to 4. Therefore, is the cycle . The flip transposes two pairs of corners and corresponds to . If we want to combine these two permutations, using the same names as with motions, we get Notice that this permutation corresponds with the flip .  Although isn't cyclic (since it isn't abelian), it can be generated from the two elements and :   It is quite easy to describe any of the dihedral groups in a similar fashion. Here is the formal definition  Dihedral Group  Dihedral Group Definition  The th dihedral group  Let be a positive integer greater than or equal to 3. If , an -cycle, and Then is the th dihedral group.    Caution You might notice that we use a script , , for the dihedral groups. Occasionally you might see an ordinary in other sources for the dihedral groups. Don't confuse it with the set of divisors of , which we denote by . Normally the context of the discussion should make the meaning of clear.  A Letter-facing Machine  An application of is in the design of a letter-facing machine. Imagine letters entering a conveyor belt to be postmarked. They are placed on the conveyor belt at random so that two sides are parallel to the belt. Suppose that a postmarker can recognize a stamp in the top right corner of the envelope, on the side facing up. In , a sequence of machines is shown that will recognize a stamp on any letter, no matter what position in which the letter starts. The letter stands for a postmarker. The letters and stand for rotating and flipping machines that perform the motions of and .   A letter facer   A letter facer    The arrows pointing up indicate that if a letter is postmarked, it is taken off the conveyor belt for delivery. If a letter reaches the end, it must not have a stamp. Letter-facing machines like this have been designed (see ). One economic consideration is that -machines tend to cost more than -machines. -machines also tend to damage more letters. Taking these facts into consideration, the reader is invited to design a better letter-facing machine. Assume that -machines cost and -machines cost . Be sure that all corners of incoming letters will be examined as they go down the conveyor belt.     Exercises  Given , , and , compute                                             Write , , and from Exercise 1 as products of disjoint cycles and determine whether each is odd or even.  Do the left cosets of over form a group under the induced operation on left cosets of ? What about the left cosets of ?  is a group of order two. The operation on left cosets of is not well defined and so a group cannot be formed from left cosets of .  In its realization as permutations, the dihedral group is equal to . Can you give a geometric explanation why? Why isn't equal to ?   Complete the list of elements of and write out a table for the group in its realization as symmetries.  List the subgroups of in a lattice diagram. Are they all cyclic? To what simpler groups are the subgroups of isomorphic?    Where is the identity function, , and The operation table for the group is   A lattice diagram of its subgroups is   Subgroups of    Subgroups of    All proper subgroups are cyclic except and . Each 2-element subgroup is isomorphic to ; is isomorphic to ; and and are isomorphic to .  Design a better letter-facing machine (see ). How can you verify that a letter-facing machine does indeed check every corner of a letter? Can it be done on paper without actually sending letters through it?  Prove by induction that if and each , is a transposition, then  One solution is to cite Exercise 3 at the end of Section 11.3. It can be directly applied to this problem. An induction proof of the problem at hand would be almost identical to the proof of the more general statement.   How many elements are there in ? Describe them geometrically.  Complete the proof of . Part I: That follows from the Rule of Products.  Part II: Let be the function defined on by , , , and for ; and let be defined by , , , and for . Note that and are elements of . Next, , while , hence and is non-abelian for any .  How many left cosets does , have?  Prove that in .   Prove that the tile puzzles corresponding to are solvable.  If , how can you determine whether 's puzzle is solvable?     Prove that is isomorphic to , the group of rook matrices (see Section 11.2 exercises).  Prove that for each , is isomorphic to .     Both groups are non-abelian and of order 6; so they must be isomorphic, since only one such group exists up to isomorphism. The function defined by is an isomorphism,  Recall that since every function is a relation, it is natural to translate functions to Boolean matrices. Suppose that . We will define its image, , by That is a bijection follows from the existence of . If is a rook matrix, For , Therefore, is an isomorphism.     "
},
{
  "id": "table-permutations-3",
  "level": "2",
  "url": "s-permutation-groups.html#table-permutations-3",
  "type": "Table",
  "number": "15.3.1",
  "title": "Elements of <span xmlns:pf=\"https:\/\/prefigure.org\" class=\"process-math\">\\(S_3\\)<\/span>",
  "body": " Elements of              "
},
{
  "id": "table-s3",
  "level": "2",
  "url": "s-permutation-groups.html#table-s3",
  "type": "Table",
  "number": "15.3.2",
  "title": "Operation Table for <span xmlns:pf=\"https:\/\/prefigure.org\" class=\"process-math\">\\(S_3\\)<\/span>",
  "body": " Operation Table for    "
},
{
  "id": "list-sn-is-a-group",
  "level": "2",
  "url": "s-permutation-groups.html#list-sn-is-a-group",
  "type": "List",
  "number": "15.3.3",
  "title": "",
  "body": " We don't even need the table to verify that we have a group. Based on the following observations, the set of all permutations on any finite set will be a group.   Function composition is always associative.  The identity for the group is . If is any one of the permutations on and , Therefore .  A permutation, by definition, is a bijection. In Chapter 7 we proved that this implies that it must have an inverse and the inverse itself is a bijection and hence a permutation. Hence all elements of have an inverse in . If a permutation is displayed in matrix form, its inverse can be obtained by exchanging the two rows and rearranging the columns so that the top row is in order. The first step is actually sufficient to obtain the inverse, but the sorting of the top row makes it easier to recognize the inverse.  For example, let's consider a typical permutation on , . .   "
},
{
  "id": "ss-symmetric-groups-8",
  "level": "2",
  "url": "s-permutation-groups.html#ss-symmetric-groups-8",
  "type": "Note",
  "number": "15.3.4",
  "title": "",
  "body": "From , we can see that is non-abelian. Remember, non-abelian is the negation of abelian. The existence of two elements that don't commute is sufficient to make a group non-abelian. In this group, and is one such pair: while , so . Caution: Don't take this to mean that every pair of elements has to have this property. There are several pairs of elements in that do commute. In fact, the identity, , must commute with everything. Also every element must commute with its inverse. "
},
{
  "id": "def-symmetric-group",
  "level": "2",
  "url": "s-permutation-groups.html#def-symmetric-group",
  "type": "Definition",
  "number": "15.3.5",
  "title": "Symmetric Group.",
  "body": "Symmetric Group Symmetric Group  The group of permutations of the set  The group of permutations on a set with elements  Let be a nonempty set. The set of all permutations on with the operation of function composition is called the symmetric group on , denoted .  The cardinality of a finite set is more significant than the elements, and we will denote by the symmetric group on any set of cardinality , . "
},
{
  "id": "subgroups-s3",
  "level": "2",
  "url": "s-permutation-groups.html#subgroups-s3",
  "type": "Example",
  "number": "15.3.6",
  "title": "The significance of <span xmlns:pf=\"https:\/\/prefigure.org\" class=\"process-math\">\\(S_3\\)<\/span>.",
  "body": "The significance of Our opening example, , is the smallest non-abelian group. For that reason, all of its proper subgroups are abelian: in fact, they are all cyclic. shows the Hasse diagram for the subgroups of .   Lattice diagram of subgroups of    Lattice diagram of subgroups of    "
},
{
  "id": "ex-smallest-sns",
  "level": "2",
  "url": "s-permutation-groups.html#ex-smallest-sns",
  "type": "Example",
  "number": "15.3.8",
  "title": "Smallest Symmetric Groups.",
  "body": "Smallest Symmetric Groups The only abelian symmetric groups are and , with 1 and 2 elements, respectively. The elements of are and . is isomorphic to . "
},
{
  "id": "theorem-sn-order",
  "level": "2",
  "url": "s-permutation-groups.html#theorem-sn-order",
  "type": "Theorem",
  "number": "15.3.9",
  "title": "",
  "body": " For , and for , is non-abelian. The first part of the theorem follows from the extended rule of products (see Chapter 2). We leave the details of proof of the second part to the reader after the following hint. Consider in where , , , and for . Therefore the cycle representation of is . Now define in a similar manner so that when you compare and you get different results. "
},
{
  "id": "fig-cycles-4",
  "level": "2",
  "url": "s-permutation-groups.html#fig-cycles-4",
  "type": "Figure",
  "number": "15.3.10",
  "title": "",
  "body": " Representations of a cycle of length 4   Representations of a cycle of length 4   "
},
{
  "id": "note-disjoint-cycles",
  "level": "2",
  "url": "s-permutation-groups.html#note-disjoint-cycles",
  "type": "Note",
  "number": "15.3.11",
  "title": "Disjoint Cycles.",
  "body": "Disjoint Cycles Disjoint Cycles We say that two cycles are disjoint if no number appears in both cycles, as is the case in our expressions for above. Disjoint cycles can be written in any order. Thus, we could also say that . "
},
{
  "id": "note-composing-permutations",
  "level": "2",
  "url": "s-permutation-groups.html#note-composing-permutations",
  "type": "Note",
  "number": "15.3.12",
  "title": "Composition of Permutations.",
  "body": "Composition of Permutations Permutations Composition We will now consider the composition of permutations written in cyclic form by an example. Suppose that and are elements of . To calculate , we start with simple concatenation: Although this is a valid expression for , our goal is to express the composition as a product of disjoint cycles as and were individually written. We will start by determining the cycle that contains 1. When combining any number of cycles, they are always read from right to left, as with all functions. The first cycle in does not contain 1; thus we move on to the second. The image of 1 under that cycle is 5. Now we move on to the next cycle, looking for 5, which doesn't appear. The fourth cycle does not contain a 5 either; so .  At this point, we would have written on paper. We repeat the steps to determine . This time the second cycle of moves 5 to 6 and then the third cycle moves 6 to 4. Therefore, . We continue until the cycle (1, 5, 4, 3) is completed by determining that . The process is then repeated starting with any number that does not appear in the cycle(s) that have already been completed.  The final result for our example is . Since and , and we need not include the one-cycle (2) in the final result, although it can be included.  "
},
{
  "id": "ex-some-compositions-cycle",
  "level": "2",
  "url": "s-permutation-groups.html#ex-some-compositions-cycle",
  "type": "Example",
  "number": "15.3.13",
  "title": "Some Compositions.",
  "body": "Some Compositions      .  Notice that cyclic notation does not indicate the set which is being permuted. The examples above could be in , where the image of 5 is 5. This ambiguity is usually overcome by making the context clear at the start of a discussion.  "
},
{
  "id": "def-transposition",
  "level": "2",
  "url": "s-permutation-groups.html#def-transposition",
  "type": "Definition",
  "number": "15.3.14",
  "title": "Transposition.",
  "body": "Transposition Transposition  A transposition is a cycle of length 2. "
},
{
  "id": "obs-transpositions",
  "level": "2",
  "url": "s-permutation-groups.html#obs-transpositions",
  "type": "Observation",
  "number": "15.3.15",
  "title": "About transpositions.",
  "body": "About transpositions and are transpositions in . However, and are not transpositions; thus, the set of transpositions is not closed under composition. Since and are both equal to the identity permutation, and are their own inverses. In fact, every transposition is its own inverse. "
},
{
  "id": "theorem-products-of-transpositions",
  "level": "2",
  "url": "s-permutation-groups.html#theorem-products-of-transpositions",
  "type": "Theorem",
  "number": "15.3.16",
  "title": "Decomposition into Cycles.",
  "body": "Decomposition into Cycles  Every cycle of length greater than 2 can be expressed as a product of transpositions.  We need only indicate how the product of transpositions can be obtained. It is easy to verify that a cycle of length , , is equal to the following product of transpositions:   "
},
{
  "id": "theorem-odd-even",
  "level": "2",
  "url": "s-permutation-groups.html#theorem-odd-even",
  "type": "Theorem",
  "number": "15.3.17",
  "title": "",
  "body": "Every permutation on a finite set can be expressed as the product of an even number of transpositions or an odd number of transpositions, but not both. "
},
{
  "id": "def-alternating-group",
  "level": "2",
  "url": "s-permutation-groups.html#def-alternating-group",
  "type": "Definition",
  "number": "15.3.18",
  "title": "The Alternating Group.",
  "body": "The Alternating Group  Alternating Group  The Alternating Group  Let . The set of even permutations in is a proper subgroup of called the alternating group on , denoted . "
},
{
  "id": "theorem-alternating-group",
  "level": "2",
  "url": "s-permutation-groups.html#theorem-alternating-group",
  "type": "Theorem",
  "number": "15.3.19",
  "title": "",
  "body": " Let . The alternating group is indeed a group and has order . In this proof, the symbols and stand for transpositions and , are even nonnegative integers. If , we can write the two permutations as products of even numbers of transpositions, and . Then Since is even, , and is closed with respect to function composition. With this, we have proven that is a subgroup of by .  To prove the final assertion, let be the set of odd permutations and let . Define by . Suppose that . Then and by the right cancellation law, . Hence, is an injection. Next we show that is also a surjection. If , is the image of an element of . Specifically, is the image of . Since is a bijection, . "
},
{
  "id": "ex-sliding-tile",
  "level": "2",
  "url": "s-permutation-groups.html#ex-sliding-tile",
  "type": "Example",
  "number": "15.3.20",
  "title": "The Sliding Tile Puzzle.",
  "body": "The Sliding Tile Puzzle  Consider the sliding-tile puzzles pictured in . Each numbered square is a tile and the dark square is a gap. Any tile that is adjacent to the gap can slide into the gap. In most versions of this puzzle, the tiles are locked into a frame so that they can be moved only in the manner described above. The object of the puzzle is to arrange the tiles as they appear in Configuration (a). Configurations (b) and (c) are typical starting points. We propose to show why the puzzle can be solved starting with (b), but not with (c).   Configurations of the sliding tile puzzle   Configurations of the sliding tile puzzle    We will associate a change in the configuration of the puzzle with an element of . Imagine that a tile numbered 16 fills in the gap. For any configuration of the puzzle, the identity , is the function that leave the configurate as is. In general, if , and , is the position to which the tile in position is moved by that appears in the position of in configuration (a). If we call the functions that, starting with configuration (a), result in configurations (b) and (c) by the names and , respectively, and   How can we interpret the movement of one tile as a permutation? Consider what happens when the 12 tile of slides into the gap. The result is a configuration that we would interpret as , a single transposition. Now if we slide the 8 tile into the 12 position, the result is or . Hence, by exchanging the tiles 8 and 16, we have implemented the function .   The configuration    The configuration    Every time you slide a tile into the gap, the new permutation is a transposition composed with the old permutation. Now observe that to start with initial configuration and terminate after a finite number of moves with the gap in its original position, you must make an even number of moves. Thus, configuration corresponding any permutation that leaves 16 fixed cannot be solved if the permutation is odd. Note that is an odd permutation; thus, Puzzle (c) can't be solved. The proof that all even permutations, such as , can be solved is left to the interested reader to pursue. "
},
{
  "id": "obs-realization",
  "level": "2",
  "url": "s-permutation-groups.html#obs-realization",
  "type": "Observation",
  "number": "15.3.23",
  "title": "Realizations of Groups.",
  "body": "Realizations of Groups  By now we've seen several instances where a group can appear through an isomorphic copy of itself in various settings. The simplest such example is the cyclic group of order 2. When this group is mentioned, we might naturally think of the group , but the groups and are isomorphic to it. None of these groups are necessarily more natural or important than the others. Which one you use depends on the situation you are in and all are referred to as realizations of the cyclic group of order 2. The next family of groups we will study, the dihedral groups, has two natural realizations, first as permutations and second as geometric symmetries. "
},
{
  "id": "fig-d4-square",
  "level": "2",
  "url": "s-permutation-groups.html#fig-d4-square",
  "type": "Figure",
  "number": "15.3.24",
  "title": "",
  "body": " Axes of symmetry of the square   Axes of symmetry of the square   "
},
{
  "id": "fig-d4-square-2",
  "level": "2",
  "url": "s-permutation-groups.html#fig-d4-square-2",
  "type": "Figure",
  "number": "15.3.25",
  "title": "",
  "body": " Two elements of    Two elements of   "
},
{
  "id": "ss-dihedral-group-13",
  "level": "2",
  "url": "s-permutation-groups.html#ss-dihedral-group-13",
  "type": "Definition",
  "number": "15.3.26",
  "title": "Dihedral Group.",
  "body": "Dihedral Group  Dihedral Group Definition  The th dihedral group  Let be a positive integer greater than or equal to 3. If , an -cycle, and Then is the th dihedral group.   "
},
{
  "id": "ss-dihedral-group-14",
  "level": "2",
  "url": "s-permutation-groups.html#ss-dihedral-group-14",
  "type": "Note",
  "number": "15.3.27",
  "title": "Caution.",
  "body": "Caution You might notice that we use a script , , for the dihedral groups. Occasionally you might see an ordinary in other sources for the dihedral groups. Don't confuse it with the set of divisors of , which we denote by . Normally the context of the discussion should make the meaning of clear. "
},
{
  "id": "ex-letter-facing-machine",
  "level": "2",
  "url": "s-permutation-groups.html#ex-letter-facing-machine",
  "type": "Example",
  "number": "15.3.28",
  "title": "A Letter-facing Machine.",
  "body": "A Letter-facing Machine  An application of is in the design of a letter-facing machine. Imagine letters entering a conveyor belt to be postmarked. They are placed on the conveyor belt at random so that two sides are parallel to the belt. Suppose that a postmarker can recognize a stamp in the top right corner of the envelope, on the side facing up. In , a sequence of machines is shown that will recognize a stamp on any letter, no matter what position in which the letter starts. The letter stands for a postmarker. The letters and stand for rotating and flipping machines that perform the motions of and .   A letter facer   A letter facer    The arrows pointing up indicate that if a letter is postmarked, it is taken off the conveyor belt for delivery. If a letter reaches the end, it must not have a stamp. Letter-facing machines like this have been designed (see ). One economic consideration is that -machines tend to cost more than -machines. -machines also tend to damage more letters. Taking these facts into consideration, the reader is invited to design a better letter-facing machine. Assume that -machines cost and -machines cost . Be sure that all corners of incoming letters will be examined as they go down the conveyor belt.  "
},
{
  "id": "exercises-15-3-2",
  "level": "2",
  "url": "s-permutation-groups.html#exercises-15-3-2",
  "type": "Exercise",
  "number": "15.3.5.1",
  "title": "",
  "body": "Given , , and , compute                                            "
},
{
  "id": "exercises-15-3-3",
  "level": "2",
  "url": "s-permutation-groups.html#exercises-15-3-3",
  "type": "Exercise",
  "number": "15.3.5.2",
  "title": "",
  "body": "Write , , and from Exercise 1 as products of disjoint cycles and determine whether each is odd or even. "
},
{
  "id": "exercises-15-3-4",
  "level": "2",
  "url": "s-permutation-groups.html#exercises-15-3-4",
  "type": "Exercise",
  "number": "15.3.5.3",
  "title": "",
  "body": "Do the left cosets of over form a group under the induced operation on left cosets of ? What about the left cosets of ?  is a group of order two. The operation on left cosets of is not well defined and so a group cannot be formed from left cosets of . "
},
{
  "id": "exercises-15-3-5",
  "level": "2",
  "url": "s-permutation-groups.html#exercises-15-3-5",
  "type": "Exercise",
  "number": "15.3.5.4",
  "title": "",
  "body": "In its realization as permutations, the dihedral group is equal to . Can you give a geometric explanation why? Why isn't equal to ? "
},
{
  "id": "exercise-d4-details",
  "level": "2",
  "url": "s-permutation-groups.html#exercise-d4-details",
  "type": "Exercise",
  "number": "15.3.5.5",
  "title": "",
  "body": " Complete the list of elements of and write out a table for the group in its realization as symmetries.  List the subgroups of in a lattice diagram. Are they all cyclic? To what simpler groups are the subgroups of isomorphic?    Where is the identity function, , and The operation table for the group is   A lattice diagram of its subgroups is   Subgroups of    Subgroups of    All proper subgroups are cyclic except and . Each 2-element subgroup is isomorphic to ; is isomorphic to ; and and are isomorphic to . "
},
{
  "id": "exercises-15-3-7",
  "level": "2",
  "url": "s-permutation-groups.html#exercises-15-3-7",
  "type": "Exercise",
  "number": "15.3.5.6",
  "title": "",
  "body": "Design a better letter-facing machine (see ). How can you verify that a letter-facing machine does indeed check every corner of a letter? Can it be done on paper without actually sending letters through it? "
},
{
  "id": "exercises-15-3-8",
  "level": "2",
  "url": "s-permutation-groups.html#exercises-15-3-8",
  "type": "Exercise",
  "number": "15.3.5.7",
  "title": "",
  "body": "Prove by induction that if and each , is a transposition, then  One solution is to cite Exercise 3 at the end of Section 11.3. It can be directly applied to this problem. An induction proof of the problem at hand would be almost identical to the proof of the more general statement.  "
},
{
  "id": "exercises-15-3-9",
  "level": "2",
  "url": "s-permutation-groups.html#exercises-15-3-9",
  "type": "Exercise",
  "number": "15.3.5.8",
  "title": "",
  "body": "How many elements are there in ? Describe them geometrically. "
},
{
  "id": "exercises-15-3-10",
  "level": "2",
  "url": "s-permutation-groups.html#exercises-15-3-10",
  "type": "Exercise",
  "number": "15.3.5.9",
  "title": "",
  "body": "Complete the proof of . Part I: That follows from the Rule of Products.  Part II: Let be the function defined on by , , , and for ; and let be defined by , , , and for . Note that and are elements of . Next, , while , hence and is non-abelian for any . "
},
{
  "id": "exercises-15-3-11",
  "level": "2",
  "url": "s-permutation-groups.html#exercises-15-3-11",
  "type": "Exercise",
  "number": "15.3.5.10",
  "title": "",
  "body": "How many left cosets does , have? "
},
{
  "id": "exercises-15-3-12",
  "level": "2",
  "url": "s-permutation-groups.html#exercises-15-3-12",
  "type": "Exercise",
  "number": "15.3.5.11",
  "title": "",
  "body": "Prove that in . "
},
{
  "id": "exercises-15-3-13",
  "level": "2",
  "url": "s-permutation-groups.html#exercises-15-3-13",
  "type": "Exercise",
  "number": "15.3.5.12",
  "title": "",
  "body": " Prove that the tile puzzles corresponding to are solvable.  If , how can you determine whether 's puzzle is solvable?   "
},
{
  "id": "exercises-15-3-14",
  "level": "2",
  "url": "s-permutation-groups.html#exercises-15-3-14",
  "type": "Exercise",
  "number": "15.3.5.13",
  "title": "",
  "body": " Prove that is isomorphic to , the group of rook matrices (see Section 11.2 exercises).  Prove that for each , is isomorphic to .     Both groups are non-abelian and of order 6; so they must be isomorphic, since only one such group exists up to isomorphism. The function defined by is an isomorphism,  Recall that since every function is a relation, it is natural to translate functions to Boolean matrices. Suppose that . We will define its image, , by That is a bijection follows from the existence of . If is a rook matrix, For , Therefore, is an isomorphism.  "
},
{
  "id": "s-normal-subgroups-homomorphisms",
  "level": "1",
  "url": "s-normal-subgroups-homomorphisms.html",
  "type": "Section",
  "number": "15.4",
  "title": "Normal Subgroups and Group Homomorphisms",
  "body": " Normal Subgroups and Group Homomorphisms  Normal Subgroups  Homomorphism Group  Our goal in this section is to answer an open question from earlier in this chapter and introduce a related concept. The question is: When are left cosets of a subgroup a group under the induced operation? This question is open for non-abelian groups. Now that we have some examples to work with, we can try a few experiments.  Normal Subgroups  Normal Subgroups  Cosets of We have seen that is a subgroup of , and its left cosets are itself and . Whether is a group boils down to determining whether the induced operation is well defined. Consider the operation table for in .   Operation table for    Operation table for    We have shaded in all occurrences of the elements of in gray. We will call these elements the gray elements and the elements of the white ones.  Now consider the process of computing the coset product . The product is obtained by selecting one white element and one gray element. Note that white times gray is always gray. Thus, is well defined. Similarly, the other three possible products are well defined. The table for the factor group is   Clearly, is isomorphic to . Notice that and are also the right cosets of . This is significant.  Cosets of another subgroup of Now let's try the left cosets of in . There are three of them. Will we get a complicated version of ? The left cosets are , , and .  The reader might be expecting something to go wrong eventually, and here it is. To determine we can choose from four pairs of representatives: This time, we don't get the same coset for each pair of representatives. Therefore, the induced operation is not well defined and no factor group is produced.  This last example changes our course of action. If we had gotten a factor group from , we might have hoped to prove that every collection of left cosets forms a group. Now our question is: How can we determine whether we will get a factor group? Of course, this question is equivalent to: When is the induced operation well defined? There was only one step in the proof of , where we used the fact that was abelian. We repeat the equations here: since was abelian.  The last step was made possible by the fact that . As the proof continued, we used the fact that was in and so is for some in . All that we really needed in the abelian step was that . Then, since is closed under 's operation, is an element of . The consequence of this observation is that we define a certain kind of subgroup that guarantees that the inducted operation is well-defined.  Normal Subgroup  Normal Subgroup.  is a normal subgroup of  If is a group, , then is a normal subgroup of , denoted , if and only if every left coset of is a right coset of ; i. e.   If , then the operation induced on left cosets of by the operation of is well defined if and only if any one of the following conditions is true:  is a normal subgroup of .  If , , then there exists such that .  If , , then .    We leave the proof of this theorem to the reader.   Be careful, the following corollary is not an ...if and only if... statement.   If , then the operation induced on left cosets of by the operation of is well defined if either of the following two conditions is true.  is abelian.   .   A non-normal subgroup The right cosets of are , , and . These are not the same as the left cosets of . In addition, . Thus, is not normal.  The improper subgroups and of any group are normal subgroups. is isomorphic to . All other normal subgroups of a group, if they exist, are called proper normal subgroups .  By Condition b of , is a normal subgroup of and is isomorphic to .  Subgroups of , a group in its own right with 60 elements, has many proper subgroups, but none are normal. Although this could be done by brute force, the number of elements in the group would make the process tedious. A far more elegant way to approach the verification of this statement is to use the following fact about the cycle structure of permutations. If is a permutation with a certain cycle structure, , where the length of is , then for any , , which is the conjugate of by , will have a cycle structure with exactly the same cycle lengths. For example if we take and conjugate by ,   Notice that the condition for normality of a subgroup of is that the conjugate of any element of by an element of must be remain in .  To verify that has no proper normal subgroups, you can start by cataloging the different cycle structures that occur in and how many elements have those structures. Then consider what happens when you conjugate these different cycle structures with elements of . An outline of the process is in the exercises.  Let be the set of two by two invertible matrices of real numbers. That is, We saw in Chapter 11 that is a group with matrix multiplication.  This group has many subgroups, but consider just two: and . It is fairly simple to apply one of the conditions we have observed for normallity that a normal subgroup of , while is not normal in .   Homomorphisms  Think of the word isomorphism. Chances are, one of the first images that comes to mind is an equation something like An isomorphism must be a bijection, but the equation above is the algebraic property of an isomorphism. Here we will examine functions that satisfy equations of this type.  Homomorphism Homomorphism  Let and be groups. is a homomorphism if for all .  Many homomorphisms are useful since they point out similarities between the two groups (or, on the universal level, two algebraic systems) involved.  Decreasing modularity Define by . Therefore, , , , , , and . If . We could actually show that is a homomorphism by checking all different cases for the formula but we will use a line of reasoning that generalizes. We have already encountered Sun Tzu's Theorem, which implies that the function defined by . We need only observe that equating the first coordinates of both sides of the equation gives us precisely the homomorphism property.  Group Homomorphism Properties  If is a homomorphism, then:   .   for all .  If , then .     Let be any element of . Then . By cancellation, .  Again, let . . Hence, by the uniqueness of inverses, .  Let . Then there exists such that , . Recall that a compact necessary and sufficient condition for is that for all . Now we apply the same condition in : since , and so we can conclude that .     Since a homomorphism need not be a surjection and part (c) of is true for the case of , the range of , , is a subgroup of   If we define by , then is a homomorphism. The image of the subgroup is the single coset , the identity of the factor group. Homomorphisms of this type are called natural homomorphisms. The following theorems will verify that is a homomorphism and also show the connection between homomorphisms and normal subgroups. The reader can find more detail and proofs in most abstract algebra texts.   If , then the function defined by is a homomorphism. We leave the proof of this theorem to the reader.  Natural Homomorphism  Natural Homomorphism If , then the function defined by is called the natural homomorphism.  Based on , every normal subgroup gives us a homomorphism. Next, we see that the converse is true.  Kernel of a homomorphism Kernel  the kernel of homomorphism Let be a homomorphism, and let and be the identities of and , respectively. The kernel of is the set   Let be a homomorphism from into . The kernel of is a normal subgroup of . Let . We can see that is a subgroup of by letting and verify that by computing . To prove normality, we let be any element of and . We compute to verify that .   Based on this most recent theorem, every homomorphism gives us a normal subgroup.   Fundamental Theorem of Group Homomorphisms  Fundamental Theorem of Group Homomorphisms  Let be a homomorphism. Then is isomorphic to .    Define by . The three previous theorems imply the following:  defined by is a homomorphism.  .   is isomorphic to .    Let be the same group of two by two invertible real matrices as in . Define by . We will let the reader verify that is a homomorphism. The theorems above imply the following.   . This verifies our statement in . As in that example, let .   is isomorphic to .   defined, naturally, by is a homomorphism.    For the remainder of this section, we will be examining certain kinds of homomorphisms that will play a part in our major application to homomorphisms, coding theory.  Consider defined by . If ,   Since implies that and , the kernel of is . By previous theorems, is isomorphic to .   We can generalize the previous example as follows: If and is an matrix of 0's and 1's (elements of ), then defined by is a homomorphism. This is true because matrix multiplication is distributive over addition. The only new idea here is that computation is done in . If and , is true by basic matrix laws. Therefore, .    Exercises  Which of the following functions are homomorphisms? What are the kernels of those functions that are homomorphisms?   defined by .   where .   , where .   defined by .     Yes, the kernel is  No, since , but A follow-up might be to ask what happens if 5 is replaced with some other positive integer in this part.  Yes, the kernel is  No. A counterexample, among many, would be to consider the two transpositions and . Compare and .    Which of the following functions are homomorphisms? What are the kernels of those functions that are homomorphisms?   , defined by .   defined by .   , where .   defined by .    Show that has one proper normal subgroup, but that is not normal.   is a normal subgroup of . To see you could use the table given in the solution of of Section 15.3 and verify that for all and . A more efficient approach is to prove the general theorem that if is a subgroup with exactly two distinct left cosets, than is normal. is not a normal subgroup of . and if we choose and then   Prove that the function in is a homomorphism.   Define the two functions and by , and Describe the function . Is it a homomorphism?   and so is the trivial homomorphism, but a homomorphism nevertheless.  Express in in matrix form.  Prove that if is an abelian group, then defines a homomorphism from into . Is ever an isomorphism?  Let . Hence, is a homomorphism. In order for to be an isomorphism, it must be the case that no element other than the identity is its own inverse.   Prove that if is a homomorphism, and , then . Is it also true that ?   Prove that if is a homomorphism, and , then . Proof: Recall that the inverse image of under is .  Closure: Let , then . Since is a subgroup of ,  Identity: By (a), .  Inverse: Let . Then and by (b), and so .  Following up on , prove that is a simple group; i. e., it has no proper normal subgroups.  Make a list of the different cycle structures that occur in and how many elements have those structures.  Within each set of permutations with different cycle structures, identify which subsets are closed with respect to the conjugation operation. With this you will have a partition of into conjugate classes where for each class, , if and only if such that .  Use the fact that a normal subgroup of needs to be a union of conjugate classes and verify that no such union exists.      "
},
{
  "id": "ex-alternating-cosets",
  "level": "2",
  "url": "s-normal-subgroups-homomorphisms.html#ex-alternating-cosets",
  "type": "Example",
  "number": "15.4.1",
  "title": "Cosets of <span xmlns:pf=\"https:\/\/prefigure.org\" class=\"process-math\">\\(A_3\\)<\/span>.",
  "body": "Cosets of We have seen that is a subgroup of , and its left cosets are itself and . Whether is a group boils down to determining whether the induced operation is well defined. Consider the operation table for in .   Operation table for    Operation table for    We have shaded in all occurrences of the elements of in gray. We will call these elements the gray elements and the elements of the white ones.  Now consider the process of computing the coset product . The product is obtained by selecting one white element and one gray element. Note that white times gray is always gray. Thus, is well defined. Similarly, the other three possible products are well defined. The table for the factor group is   Clearly, is isomorphic to . Notice that and are also the right cosets of . This is significant. "
},
{
  "id": "ex-3-cosets-S3",
  "level": "2",
  "url": "s-normal-subgroups-homomorphisms.html#ex-3-cosets-S3",
  "type": "Example",
  "number": "15.4.3",
  "title": "Cosets of another subgroup of <span xmlns:pf=\"https:\/\/prefigure.org\" class=\"process-math\">\\(S_3\\)<\/span>.",
  "body": "Cosets of another subgroup of Now let's try the left cosets of in . There are three of them. Will we get a complicated version of ? The left cosets are , , and .  The reader might be expecting something to go wrong eventually, and here it is. To determine we can choose from four pairs of representatives: This time, we don't get the same coset for each pair of representatives. Therefore, the induced operation is not well defined and no factor group is produced. "
},
{
  "id": "ss-normal-subgroups-5",
  "level": "2",
  "url": "s-normal-subgroups-homomorphisms.html#ss-normal-subgroups-5",
  "type": "Observation",
  "number": "15.4.4",
  "title": "",
  "body": "This last example changes our course of action. If we had gotten a factor group from , we might have hoped to prove that every collection of left cosets forms a group. Now our question is: How can we determine whether we will get a factor group? Of course, this question is equivalent to: When is the induced operation well defined? There was only one step in the proof of , where we used the fact that was abelian. We repeat the equations here: since was abelian.  The last step was made possible by the fact that . As the proof continued, we used the fact that was in and so is for some in . All that we really needed in the abelian step was that . Then, since is closed under 's operation, is an element of . The consequence of this observation is that we define a certain kind of subgroup that guarantees that the inducted operation is well-defined. "
},
{
  "id": "def-normal-subgroup",
  "level": "2",
  "url": "s-normal-subgroups-homomorphisms.html#def-normal-subgroup",
  "type": "Definition",
  "number": "15.4.5",
  "title": "Normal Subgroup.",
  "body": "Normal Subgroup  Normal Subgroup.  is a normal subgroup of  If is a group, , then is a normal subgroup of , denoted , if and only if every left coset of is a right coset of ; i. e. "
},
{
  "id": "theorem-normality-conditions",
  "level": "2",
  "url": "s-normal-subgroups-homomorphisms.html#theorem-normality-conditions",
  "type": "Theorem",
  "number": "15.4.6",
  "title": "",
  "body": " If , then the operation induced on left cosets of by the operation of is well defined if and only if any one of the following conditions is true:  is a normal subgroup of .  If , , then there exists such that .  If , , then .    We leave the proof of this theorem to the reader.  "
},
{
  "id": "corollary-normality",
  "level": "2",
  "url": "s-normal-subgroups-homomorphisms.html#corollary-normality",
  "type": "Corollary",
  "number": "15.4.7",
  "title": "",
  "body": " If , then the operation induced on left cosets of by the operation of is well defined if either of the following two conditions is true.  is abelian.   .  "
},
{
  "id": "ex-not-normal",
  "level": "2",
  "url": "s-normal-subgroups-homomorphisms.html#ex-not-normal",
  "type": "Example",
  "number": "15.4.8",
  "title": "A non-normal subgroup.",
  "body": "A non-normal subgroup The right cosets of are , , and . These are not the same as the left cosets of . In addition, . Thus, is not normal. "
},
{
  "id": "ss-normal-subgroups-11",
  "level": "2",
  "url": "s-normal-subgroups-homomorphisms.html#ss-normal-subgroups-11",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "proper normal subgroups "
},
{
  "id": "an-is-normal",
  "level": "2",
  "url": "s-normal-subgroups-homomorphisms.html#an-is-normal",
  "type": "Example",
  "number": "15.4.9",
  "title": "",
  "body": "By Condition b of , is a normal subgroup of and is isomorphic to . "
},
{
  "id": "a5-is-simple",
  "level": "2",
  "url": "s-normal-subgroups-homomorphisms.html#a5-is-simple",
  "type": "Example",
  "number": "15.4.10",
  "title": "Subgroups of <span xmlns:pf=\"https:\/\/prefigure.org\" class=\"process-math\">\\(A_5\\)<\/span>.",
  "body": "Subgroups of , a group in its own right with 60 elements, has many proper subgroups, but none are normal. Although this could be done by brute force, the number of elements in the group would make the process tedious. A far more elegant way to approach the verification of this statement is to use the following fact about the cycle structure of permutations. If is a permutation with a certain cycle structure, , where the length of is , then for any , , which is the conjugate of by , will have a cycle structure with exactly the same cycle lengths. For example if we take and conjugate by ,   Notice that the condition for normality of a subgroup of is that the conjugate of any element of by an element of must be remain in .  To verify that has no proper normal subgroups, you can start by cataloging the different cycle structures that occur in and how many elements have those structures. Then consider what happens when you conjugate these different cycle structures with elements of . An outline of the process is in the exercises. "
},
{
  "id": "ex-matrix-subgroups-15",
  "level": "2",
  "url": "s-normal-subgroups-homomorphisms.html#ex-matrix-subgroups-15",
  "type": "Example",
  "number": "15.4.11",
  "title": "",
  "body": "Let be the set of two by two invertible matrices of real numbers. That is, We saw in Chapter 11 that is a group with matrix multiplication.  This group has many subgroups, but consider just two: and . It is fairly simple to apply one of the conditions we have observed for normallity that a normal subgroup of , while is not normal in . "
},
{
  "id": "def-homomorphism",
  "level": "2",
  "url": "s-normal-subgroups-homomorphisms.html#def-homomorphism",
  "type": "Definition",
  "number": "15.4.12",
  "title": "Homomorphism.",
  "body": "Homomorphism Homomorphism  Let and be groups. is a homomorphism if for all . "
},
{
  "id": "ex-15-4-h1",
  "level": "2",
  "url": "s-normal-subgroups-homomorphisms.html#ex-15-4-h1",
  "type": "Example",
  "number": "15.4.13",
  "title": "Decreasing modularity.",
  "body": "Decreasing modularity Define by . Therefore, , , , , , and . If . We could actually show that is a homomorphism by checking all different cases for the formula but we will use a line of reasoning that generalizes. We have already encountered Sun Tzu's Theorem, which implies that the function defined by . We need only observe that equating the first coordinates of both sides of the equation gives us precisely the homomorphism property. "
},
{
  "id": "theorem-homomorphism-properties",
  "level": "2",
  "url": "s-normal-subgroups-homomorphisms.html#theorem-homomorphism-properties",
  "type": "Theorem",
  "number": "15.4.14",
  "title": "Group Homomorphism Properties.",
  "body": "Group Homomorphism Properties  If is a homomorphism, then:   .   for all .  If , then .     Let be any element of . Then . By cancellation, .  Again, let . . Hence, by the uniqueness of inverses, .  Let . Then there exists such that , . Recall that a compact necessary and sufficient condition for is that for all . Now we apply the same condition in : since , and so we can conclude that .   "
},
{
  "id": "corollary-15-4-1",
  "level": "2",
  "url": "s-normal-subgroups-homomorphisms.html#corollary-15-4-1",
  "type": "Corollary",
  "number": "15.4.15",
  "title": "",
  "body": " Since a homomorphism need not be a surjection and part (c) of is true for the case of , the range of , , is a subgroup of  "
},
{
  "id": "ex-natural-homomorphism",
  "level": "2",
  "url": "s-normal-subgroups-homomorphisms.html#ex-natural-homomorphism",
  "type": "Example",
  "number": "15.4.16",
  "title": "",
  "body": "If we define by , then is a homomorphism. The image of the subgroup is the single coset , the identity of the factor group. Homomorphisms of this type are called natural homomorphisms. The following theorems will verify that is a homomorphism and also show the connection between homomorphisms and normal subgroups. The reader can find more detail and proofs in most abstract algebra texts. "
},
{
  "id": "theorem-natural-homomorphism",
  "level": "2",
  "url": "s-normal-subgroups-homomorphisms.html#theorem-natural-homomorphism",
  "type": "Theorem",
  "number": "15.4.17",
  "title": "",
  "body": " If , then the function defined by is a homomorphism. We leave the proof of this theorem to the reader. "
},
{
  "id": "def-natural-homomorphism",
  "level": "2",
  "url": "s-normal-subgroups-homomorphisms.html#def-natural-homomorphism",
  "type": "Definition",
  "number": "15.4.18",
  "title": "Natural Homomorphism.",
  "body": "Natural Homomorphism  Natural Homomorphism If , then the function defined by is called the natural homomorphism. "
},
{
  "id": "def-group-kernel",
  "level": "2",
  "url": "s-normal-subgroups-homomorphisms.html#def-group-kernel",
  "type": "Definition",
  "number": "15.4.19",
  "title": "Kernel of a homomorphism.",
  "body": "Kernel of a homomorphism Kernel  the kernel of homomorphism Let be a homomorphism, and let and be the identities of and , respectively. The kernel of is the set  "
},
{
  "id": "theorem-kernel-normal",
  "level": "2",
  "url": "s-normal-subgroups-homomorphisms.html#theorem-kernel-normal",
  "type": "Theorem",
  "number": "15.4.20",
  "title": "",
  "body": "Let be a homomorphism from into . The kernel of is a normal subgroup of . Let . We can see that is a subgroup of by letting and verify that by computing . To prove normality, we let be any element of and . We compute to verify that .  "
},
{
  "id": "theorem-fund-group-homomorphism",
  "level": "2",
  "url": "s-normal-subgroups-homomorphisms.html#theorem-fund-group-homomorphism",
  "type": "Theorem",
  "number": "15.4.21",
  "title": "Fundamental Theorem of Group Homomorphisms.",
  "body": " Fundamental Theorem of Group Homomorphisms  Fundamental Theorem of Group Homomorphisms  Let be a homomorphism. Then is isomorphic to .  "
},
{
  "id": "ex-apply-fundamental-theorem",
  "level": "2",
  "url": "s-normal-subgroups-homomorphisms.html#ex-apply-fundamental-theorem",
  "type": "Example",
  "number": "15.4.22",
  "title": "",
  "body": " Define by . The three previous theorems imply the following:  defined by is a homomorphism.  .   is isomorphic to .  "
},
{
  "id": "ex-matrix-homomorphism",
  "level": "2",
  "url": "s-normal-subgroups-homomorphisms.html#ex-matrix-homomorphism",
  "type": "Example",
  "number": "15.4.23",
  "title": "",
  "body": " Let be the same group of two by two invertible real matrices as in . Define by . We will let the reader verify that is a homomorphism. The theorems above imply the following.   . This verifies our statement in . As in that example, let .   is isomorphic to .   defined, naturally, by is a homomorphism.   "
},
{
  "id": "ex-bit-matrix-homomorphism",
  "level": "2",
  "url": "s-normal-subgroups-homomorphisms.html#ex-bit-matrix-homomorphism",
  "type": "Example",
  "number": "15.4.24",
  "title": "",
  "body": "Consider defined by . If ,   Since implies that and , the kernel of is . By previous theorems, is isomorphic to .  "
},
{
  "id": "exercises-15-4-2",
  "level": "2",
  "url": "s-normal-subgroups-homomorphisms.html#exercises-15-4-2",
  "type": "Exercise",
  "number": "15.4.3.1",
  "title": "",
  "body": "Which of the following functions are homomorphisms? What are the kernels of those functions that are homomorphisms?   defined by .   where .   , where .   defined by .     Yes, the kernel is  No, since , but A follow-up might be to ask what happens if 5 is replaced with some other positive integer in this part.  Yes, the kernel is  No. A counterexample, among many, would be to consider the two transpositions and . Compare and .   "
},
{
  "id": "exercises-15-4-3",
  "level": "2",
  "url": "s-normal-subgroups-homomorphisms.html#exercises-15-4-3",
  "type": "Exercise",
  "number": "15.4.3.2",
  "title": "",
  "body": "Which of the following functions are homomorphisms? What are the kernels of those functions that are homomorphisms?   , defined by .   defined by .   , where .   defined by .   "
},
{
  "id": "exercises-15-4-4",
  "level": "2",
  "url": "s-normal-subgroups-homomorphisms.html#exercises-15-4-4",
  "type": "Exercise",
  "number": "15.4.3.3",
  "title": "",
  "body": "Show that has one proper normal subgroup, but that is not normal.   is a normal subgroup of . To see you could use the table given in the solution of of Section 15.3 and verify that for all and . A more efficient approach is to prove the general theorem that if is a subgroup with exactly two distinct left cosets, than is normal. is not a normal subgroup of . and if we choose and then  "
},
{
  "id": "exercises-15-4-5",
  "level": "2",
  "url": "s-normal-subgroups-homomorphisms.html#exercises-15-4-5",
  "type": "Exercise",
  "number": "15.4.3.4",
  "title": "",
  "body": "Prove that the function in is a homomorphism.  "
},
{
  "id": "exercises-15-4-6",
  "level": "2",
  "url": "s-normal-subgroups-homomorphisms.html#exercises-15-4-6",
  "type": "Exercise",
  "number": "15.4.3.5",
  "title": "",
  "body": "Define the two functions and by , and Describe the function . Is it a homomorphism?   and so is the trivial homomorphism, but a homomorphism nevertheless. "
},
{
  "id": "exercises-15-4-7",
  "level": "2",
  "url": "s-normal-subgroups-homomorphisms.html#exercises-15-4-7",
  "type": "Exercise",
  "number": "15.4.3.6",
  "title": "",
  "body": "Express in in matrix form. "
},
{
  "id": "exercises-15-4-8",
  "level": "2",
  "url": "s-normal-subgroups-homomorphisms.html#exercises-15-4-8",
  "type": "Exercise",
  "number": "15.4.3.7",
  "title": "",
  "body": "Prove that if is an abelian group, then defines a homomorphism from into . Is ever an isomorphism?  Let . Hence, is a homomorphism. In order for to be an isomorphism, it must be the case that no element other than the identity is its own inverse.  "
},
{
  "id": "exercises-15-4-9",
  "level": "2",
  "url": "s-normal-subgroups-homomorphisms.html#exercises-15-4-9",
  "type": "Exercise",
  "number": "15.4.3.8",
  "title": "",
  "body": "Prove that if is a homomorphism, and , then . Is it also true that ?  "
},
{
  "id": "exercises-15-4-10",
  "level": "2",
  "url": "s-normal-subgroups-homomorphisms.html#exercises-15-4-10",
  "type": "Exercise",
  "number": "15.4.3.9",
  "title": "",
  "body": "Prove that if is a homomorphism, and , then . Proof: Recall that the inverse image of under is .  Closure: Let , then . Since is a subgroup of ,  Identity: By (a), .  Inverse: Let . Then and by (b), and so . "
},
{
  "id": "exercises-15-4-11",
  "level": "2",
  "url": "s-normal-subgroups-homomorphisms.html#exercises-15-4-11",
  "type": "Exercise",
  "number": "15.4.3.10",
  "title": "",
  "body": "Following up on , prove that is a simple group; i. e., it has no proper normal subgroups.  Make a list of the different cycle structures that occur in and how many elements have those structures.  Within each set of permutations with different cycle structures, identify which subsets are closed with respect to the conjugation operation. With this you will have a partition of into conjugate classes where for each class, , if and only if such that .  Use the fact that a normal subgroup of needs to be a union of conjugate classes and verify that no such union exists.    "
},
{
  "id": "s-coding-theory-groups",
  "level": "1",
  "url": "s-coding-theory-groups.html",
  "type": "Section",
  "number": "15.5",
  "title": "Coding Theory, Linear Codes",
  "body": " Coding Theory, Linear Codes  Coding Theory  Codes Linear  A Transmission Problem  In this section, we will introduce the basic ideas involved in coding theory and consider solutions of a coding problem by means of linear codes.  Imagine a situation in which information is being transmitted between two points. The information takes the form of high and low pulses (for example, radio waves or electric currents), which we will label 1 and 0, respectively. As these pulses are sent and received, they are grouped together in blocks of fixed length. The length determines how much information can be contained in one block. If the length is , there are different values that a block can have. If the information being sent takes the form of text, each block might be a character. In that case, the length of a block may be seven, so that block values can represent letters (both upper and lower case), digits, punctuation, and so on. During the transmission of data, noise can alter the signal so that what is received differs from what is sent. illustrates the problem that can be encountered if information is transmitted between two points.   A noisy transmission   A noisy transmission    Noise is a fact of life for anyone who tries to transmit information. Fortunately, in most situations we could expect a high percentage of the pulses that are sent to be received properly. However, when large numbers of pulses are transmitted, there are usually some errors due to noise. For the remainder of the discussion, we will make assumptions about the nature of the noise and the message that we want to send. Henceforth, we will refer to the pulses as bits.  We will assume that our information is being sent along a binary symmetric channel . By this, we mean that any single bit that is transmitted will be received improperly with a certain fixed probability, , independent of the bit value. The magnitude of is usually quite small. To illustrate the process, we will assume that , which, in the real world, would be considered somewhat large. Since , we can expect of all bits to be properly received.  In addition to assuming throughout, we will also suppose that our message consists of 3,000 bits of information. Two factors will be considered in evaluating a method of transmission. The first is the probability that the message is received with no errors. The second is the number of bits that will be transmitted in order to send the message. This quantity is called the rate of transmission: As you might expect, as we devise methods to improve the probability of success, the rate will decrease.  Suppose that we ignore the noise and transmit the message without any coding. The probability of success is . Therefore we only successfully receive the message in a totally correct form less than of the time. The rate of certainly doesn't offset this poor probability.  Our strategy for improving our chances of success will be to send an encoded message. The encoding will be done in such a way that small errors can be identified and corrected. This idea is illustrated in .   The Coding Process   The Coding Process    In all of our examples, the functions that will correspond to our encoding devices will involve multiplication of messages by matrices using mod 2 arithmetic. First we will introduce some geometric ideas to make the process more intuitive.   Introduction  Although we'll be using algebra to help improve communications, the basic solution can be imagined from a geometric point of view. For any positive integer , we define a distance function on the elements of the group . This distance is called the Hamming Distance .  Hamming Distance  Hamming Distance  Hamming distance between and  Given two elements of , and , the Hamming Distance, between them is the number of positions in which they differ.   For example, since these two elements of differ in just the last position; and . Notice that we can compute the distance between two bit strings by adding them coordinatewise in the Cartesian product and counting the number 's that appear in the sum. For example . The sum has two 's, so the distance between is . In addition, the location of the 's in the sum tell us where the two bit strings differ.  When we look at groups like from a point of view, we refer to these sets as metric spaces or simply spaces . In the case of , there are just points in the space and the maximum distance between the points is . More generally has points and the maximum distance between points in that space is . Looking at the group from this geometric point of view is essentially the same as the -cube we considered in discussing Hamiltonian graphs. In this section we will use -tuples such as interchangeably with strings of bits such as .  For any distance in a space, the ball of radius centered at a point , denoted , is the set of all points whose distance from is or less. For example, in the space ,  The ultimate goal of our encoding will be to take a set of possible messages, the message space , and distribute them in a larger space, the code space , in such a way that the encoded message, called a code word is at least a certain distance away from any other code word. The minimum distance between the code words will determine whether we can correct errors or just detect them. Now let's turn to some examples.   Error Detection  Suppose that each block of three bits is encoded with the function , where The fourth bit of is called the parity-check bit. When the encoded block is received, the four bits will probably all be correct (they are correct approximately of the time under our assumed parameters), but the added bit that is sent will make it possible to detect single bit errors in the block. Note that when is transmitted, the sum of its components is , since in .  If any single bit is garbled by noise, the sum of the received bits will be 1. A parity error occurs if the sum of the received bits is 1. Since more than one error is unlikely when is small, a high percentage of all errors can be detected.  At the receiving end, the decoding function acts on the four-bit block with the function , where Notice that the fourth bit of is an indicator of whether there is a parity error - 0 if no error, and 1 if an error. If no parity error occurs, the first three bits are recorded as part of the message. If a parity error occurs, we will assume that a retransmission of that block can be requested. This request can take the form of automatically having the parity-check bit of sent back to the source. If 1 is received, the previous block is retransmitted; if 0 is received, the next block is sent. This assumption of two-way communication is significant, but it is desirable to make this coding system useful. For our calculations, it is reasonable to expect that the probability of a transmission error in the opposite direction is also 0.001. Without going into the details, we will report that the probability of success in sending 3000 bits is approximately 0.990 and the rate is approximately . The rate includes the transmission of the parity-check bit to the source and is only approximate because the resent blocks will decrease the rate below somewhat.   The -cube with code words displayed as larger vertices   The -cube with code words displayed as larger vertices.    Let's consider the geometry of this code. If we examine the -cube in , the code words are the strings of four bits with an even number of ones. These vertices are the larger ones. Notice that the ball of radius 1 centered around any of the code words consists of that code word and the smaller vertices that are connected to the code word with an edge of the -cube. Since there are no other code-words in the ball, a single bit error produces a non-code word and so an error can be detected.   Error Correction  Next, we will consider coding functions that allow us to correct errors at the receiving end so that only one-way communication is needed. Before we begin, recall that every element of , , is its own inverse; that is, . Therefore, .  The Triple Repetition Code  Triple Repetition Code  Suppose we take each individual bit in our message and encode it by repeating it three times. In other words, if is a single bit, . The code words for this code are and . Let's look at the geometry behind this code. The message space has just two points, but the code space is , which has 8 points, the vertices of the -cube, which appears in .   The -cube with code words displayed as circular vertices   The -cube with code words displayed as circular vertices.    In the figure for this code, the code words are circular vertices. If we identify the balls of radius 1 centered around the two code words, you might notice that the two balls do not intersect. Each has a different vertex with triangular, square and pentagonal shapes. From a geometric point of view, this is why we can correct a single bit error. If any string of three bits in the code space is received it is in one of the two balls and the code word in that ball had to have been the one that was transmitted.  Regarding the actual correction process, the shapes have a meaning, as outlined in the following list.  Circle: No correction needed  Pentagon: Correct the first bit  Square: Correct the second bit  Triangle: Correct the third bit    Of course, once the correction is made, only the first bit is extracted from the code word since all bits will be equal. The simplicity of the final result masks an important property of all error correcting codes we consider. All of the possible points in the code space can be partitioned in such a way that each block in the partition corresponds with a specific correction that we can make to recover the correct code word.  If you have read about cosets, you will see that the partition we refer to is the set of left cosets of the set of code words.    Linear Code Triple repetition is effective, but not very efficient since its rate is quite low, . Next we consider a slightly more efficient error correcting code based on matrix multiplication. Any such code that is computed with a matrix multiplication is called a linear code . We should point out that both the parity check code and the triple repetition code are linear codes. For the parity check code, the encoding function can be thought of as acting on a row vector by multiplying times a matrix: For triple repetition, the encoding function can be thought of as acting on a matrix by multiplying times a matrix:   A Somewhat More Efficient Linear Code   The encoding that we will consider here takes a block and produces a code word of length 6. As in the triple repetition code, each code word will differ from each other code word by at least three bits. As a result, any single error will not push a code word close enough to another code word to cause confusion. Now for the details.  Let We call the generator matrix for the code, and let be our message. Define by where    Notice that since matrix multiplication is distributive over addition, we have for all . This equality, may look familiar from the definition of an isomorphism, but in this case the function is not onto. If you've read about homomorphisms, this is indeed an example of one.  One way to see that any two distinct code words have a distance from one another of at least 3 is to consider the images of any two distinct messages. If and are distinct elements of , then has at least one coordinate equal to 1. Now consider the difference between and : Whether has 1, 2, or 3 ones, must have at least three ones. This can be seen by considering the three cases separately. For example, if has a single one, two of the parity bits are also 1. Therefore, and differ in at least three bits. By the same logic as with triple repetition, a single bit error in any code word produces an element of the code space that is contained in on of the balls of radius 1 centered about a code word.  Now consider the problem of decoding received transmissions. Imagine that a code word, , is transmitted, and is received. At the receiving end, we know the formula for , and if no error has occurred in transmission, The three equations on the right are called parity-check equations. If any of them are not true, an error has occurred. This error checking can be described in matrix form.  Let   Syndrome The matrix is called the parity-check matrix for this code. Now define by . We call the syndrome of the received block. For example, and  Note that has a similar property as , that . If the syndrome of a block is , we can be almost certain that the message block is .  Next we turn to the method of correcting errors. Despite the fact that there are only eight code words, one for each three-bit block value, the set of possible received blocks is , with 64 elements. Suppose that is not a code word, but that it differs from a code word by exactly one bit. In other words, it is the result of a single error in transmission. Suppose that is the code word that is closest to and that they differ in the first bit. Then and This is the first row of !  Note that we haven't specified or , only that they differ in the first bit. Therefore, if is received, there was probably an error in the first bit and , the transmitted code word was probably and the message block was . The same analysis can be done if and differ in any of the other five bits.  In general, if the syndrome of a received string of bits is the th row of the parity check matrix, the error has occurred in the th bit.     Probability Epilog. For the two error correction examples we've looked at, we can compare their probabilities of successfully receiving all 3000 bits correctly over a binary symmetric channel with  For the triple repetition code, the probability is and the rate of this code is which means we need to transmit 9000 bits.  For the second code, the probability of success is , and rate for this code is , which means we need to transmit 6000 bits.  Clearly, there is a trade-off between accuracy and speed.  Another Linear Code  Consider the linear code with generator matrix Since is , this code encodes three bits into five bits. The natural question to ask is what detection or correction does it afford? We can answer this question by constructing the parity check matrix. We observe that if the encoding function is where addition is mod 2 addition. If we receive five bits and no error has occurred, the following two equations would be true. Notice that in general, the number of parity check equations is equal to the number of extra bits that are added by the encoding function. These equations are equivalent to the single matrix equation , where   At a glance, we can see that this code will not correct most single bit errors. Suppose an error is added in the transmission of the five bits. Specifically, suppose that 1 is added (mod 2) in position , where and the other coordinates of are 0. Then when we compute the syndrome of our received transmission, we see that But is the row of . If the syndrome is we know that the error occurred in position 1 and we can correct it. However, if the error is in any other position we can't pinpoint its location. If the syndrome is , then the error could have occurred in either position 2 or position 3. This code does detect all single bit errors but only corrects one fifth of them.      Exercises  If the error-detecting code is being used, how would you act on the following received blocks?           Error detected, since an odd number of 1's was received; ask for retransmission.  No error detected; accept this block.  No error detected; accept this block.    Determine the parity check matrix for the triple repetition code.  If the error-correcting code from this section is being used, how would you decode the following blocks? Expect an error that cannot be fixed with one of these.                 Syndrome = . Corrected coded message is and original message was .  Syndrome = . Corrected coded message is and original message was .  Syndrome = . No error, coded message is and original message was .  Syndrome = . Corrected coded message is and original message was .  Syndrome = . This syndrome occurs only if two bits have been switched. No reliable correction is possible.  Syndrome = . Corrected coded message is and original message was .    Suppose that the code words of a coding function have the property that any two of them have a Hamming distance of at least 5. How many bit errors could be corrected with such a code?  Consider the linear code defined by the generator matrix    What size blocks does this code encode and what is the length of the code words?  What are the code words for this code?  With this code, can you detect single bit errors? Can you correct all, some, or no single bit errors?    Blocks of two bits are encoded into code words of length 4.  The code words are 0000, 1010, 0111 and 1101.  Since the first two code words have a Hamming distance of 2, not all single bit errors can be corrected. For example, if 0000 is transmitted and the first bit is switch, then 1000 is received and we can't tell for sure whether this came from 0000 or 1010. To see what can be corrected, we note that is encoded to and so if is recieved and no error has occurred,   We can extract the parity check matrix from this set of equations. It is The rows of this matrix correspond with the syndromes for errors in bits 1 through 4, which are all nonzero, so we can detect any single bit error. Notice that the syndromes for bits 1 and 3 are identical. This reflects the fact that errors in these bits can't be corrected. However, the syndromes for bits 2 and 4 are unique and so we can correct them. Therefore the second bit of the original message can be sent with more confidence than the first.     Rectangular codes Rectangular codes To build a rectangular code, you partition your message into blocks of length and then factor into and arrange the bits in a rectangular array as in the figure below. Then you add parity bits along the right side and bottom of the rows and columns. The code word is then read row by row.   For example, if is 4, then our only choice is a 2 by 2 array. The message 1101 would be encoded as and the code word is the string 11001110.    Suppose that you were sent four bit messages using this code and you received the following strings. What were the messages, assuming no more than one error in the transmission of coded data?  11011000  01110010  10001111   If you encoded bits in this manner, what would be the rate of the code?  Rectangular codes are linear codes. For the 2 by 2 rectangular code, what are the generator and parity check matrices?    Suppose that the code in is expanded to add the column to the generator matrix , can all single bit errors be corrected? Explain your answer. Yes, you can correct all single bit errors because the parity check matrix for the expanded code is Since each possible syndrome of single bit errors is unique we can correct any error.   Suppose that a linear code has parity check matrix Determine the generator matrix, , and in so doing, identify the number of bits in each message block and the number of parity bits. There is a parity check equation for each parity bit.  Perfect Codes A code with minimum distance is called perfect if every string of bits is within Hamming distance of some code word. For such a code, the spheres of radius around the code words partition the set of all strings. This is analogous to packing objects into a box with no wasted space. Using just the number of bit strings of length and the number of strings in a sphere of radius , for what values of is it possible to find a perfect code of distance ? You don't have to actually find the codes.    Prove that the code words of a linear code are a subgroup of the code space.  Prove that if is a left coset of the set of code words, then all elements of will have the same syndrome.     "
},
{
  "id": "fig-noise",
  "level": "2",
  "url": "s-coding-theory-groups.html#fig-noise",
  "type": "Figure",
  "number": "15.5.1",
  "title": "",
  "body": " A noisy transmission   A noisy transmission   "
},
{
  "id": "s-coding-theory-groups-4-5",
  "level": "2",
  "url": "s-coding-theory-groups.html#s-coding-theory-groups-4-5",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Noise "
},
{
  "id": "s-coding-theory-groups-4-6",
  "level": "2",
  "url": "s-coding-theory-groups.html#s-coding-theory-groups-4-6",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "binary symmetric channel "
},
{
  "id": "fig-coding-process",
  "level": "2",
  "url": "s-coding-theory-groups.html#fig-coding-process",
  "type": "Figure",
  "number": "15.5.2",
  "title": "",
  "body": " The Coding Process   The Coding Process   "
},
{
  "id": "ss-coding-geometry-2",
  "level": "2",
  "url": "s-coding-theory-groups.html#ss-coding-geometry-2",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Hamming Distance "
},
{
  "id": "def-hammingdistance",
  "level": "2",
  "url": "s-coding-theory-groups.html#def-hammingdistance",
  "type": "Definition",
  "number": "15.5.3",
  "title": "Hamming Distance.",
  "body": "Hamming Distance  Hamming Distance  Hamming distance between and  Given two elements of , and , the Hamming Distance, between them is the number of positions in which they differ.  "
},
{
  "id": "ss-coding-geometry-5",
  "level": "2",
  "url": "s-coding-theory-groups.html#ss-coding-geometry-5",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "metric spaces spaces "
},
{
  "id": "ss-coding-geometry-7",
  "level": "2",
  "url": "s-coding-theory-groups.html#ss-coding-geometry-7",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "message space code space code word "
},
{
  "id": "ss-error-detection-3",
  "level": "2",
  "url": "s-coding-theory-groups.html#ss-error-detection-3",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "parity error "
},
{
  "id": "fig-paritycube",
  "level": "2",
  "url": "s-coding-theory-groups.html#fig-paritycube",
  "type": "Figure",
  "number": "15.5.4",
  "title": "",
  "body": " The -cube with code words displayed as larger vertices   The -cube with code words displayed as larger vertices.   "
},
{
  "id": "ss-error-correction-3",
  "level": "2",
  "url": "s-coding-theory-groups.html#ss-error-correction-3",
  "type": "Example",
  "number": "15.5.5",
  "title": "The Triple Repetition Code.",
  "body": "The Triple Repetition Code  Triple Repetition Code  Suppose we take each individual bit in our message and encode it by repeating it three times. In other words, if is a single bit, . The code words for this code are and . Let's look at the geometry behind this code. The message space has just two points, but the code space is , which has 8 points, the vertices of the -cube, which appears in .   The -cube with code words displayed as circular vertices   The -cube with code words displayed as circular vertices.    In the figure for this code, the code words are circular vertices. If we identify the balls of radius 1 centered around the two code words, you might notice that the two balls do not intersect. Each has a different vertex with triangular, square and pentagonal shapes. From a geometric point of view, this is why we can correct a single bit error. If any string of three bits in the code space is received it is in one of the two balls and the code word in that ball had to have been the one that was transmitted.  Regarding the actual correction process, the shapes have a meaning, as outlined in the following list.  Circle: No correction needed  Pentagon: Correct the first bit  Square: Correct the second bit  Triangle: Correct the third bit    Of course, once the correction is made, only the first bit is extracted from the code word since all bits will be equal. The simplicity of the final result masks an important property of all error correcting codes we consider. All of the possible points in the code space can be partitioned in such a way that each block in the partition corresponds with a specific correction that we can make to recover the correct code word.  If you have read about cosets, you will see that the partition we refer to is the set of left cosets of the set of code words.   "
},
{
  "id": "ss-error-correction-4",
  "level": "2",
  "url": "s-coding-theory-groups.html#ss-error-correction-4",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "linear code "
},
{
  "id": "ss-error-correction-5",
  "level": "2",
  "url": "s-coding-theory-groups.html#ss-error-correction-5",
  "type": "Example",
  "number": "15.5.7",
  "title": "A Somewhat More Efficient Linear Code.",
  "body": "A Somewhat More Efficient Linear Code   The encoding that we will consider here takes a block and produces a code word of length 6. As in the triple repetition code, each code word will differ from each other code word by at least three bits. As a result, any single error will not push a code word close enough to another code word to cause confusion. Now for the details.  Let We call the generator matrix for the code, and let be our message. Define by where    Notice that since matrix multiplication is distributive over addition, we have for all . This equality, may look familiar from the definition of an isomorphism, but in this case the function is not onto. If you've read about homomorphisms, this is indeed an example of one.  One way to see that any two distinct code words have a distance from one another of at least 3 is to consider the images of any two distinct messages. If and are distinct elements of , then has at least one coordinate equal to 1. Now consider the difference between and : Whether has 1, 2, or 3 ones, must have at least three ones. This can be seen by considering the three cases separately. For example, if has a single one, two of the parity bits are also 1. Therefore, and differ in at least three bits. By the same logic as with triple repetition, a single bit error in any code word produces an element of the code space that is contained in on of the balls of radius 1 centered about a code word.  Now consider the problem of decoding received transmissions. Imagine that a code word, , is transmitted, and is received. At the receiving end, we know the formula for , and if no error has occurred in transmission, The three equations on the right are called parity-check equations. If any of them are not true, an error has occurred. This error checking can be described in matrix form.  Let   Syndrome The matrix is called the parity-check matrix for this code. Now define by . We call the syndrome of the received block. For example, and  Note that has a similar property as , that . If the syndrome of a block is , we can be almost certain that the message block is .  Next we turn to the method of correcting errors. Despite the fact that there are only eight code words, one for each three-bit block value, the set of possible received blocks is , with 64 elements. Suppose that is not a code word, but that it differs from a code word by exactly one bit. In other words, it is the result of a single error in transmission. Suppose that is the code word that is closest to and that they differ in the first bit. Then and This is the first row of !  Note that we haven't specified or , only that they differ in the first bit. Therefore, if is received, there was probably an error in the first bit and , the transmitted code word was probably and the message block was . The same analysis can be done if and differ in any of the other five bits.  In general, if the syndrome of a received string of bits is the th row of the parity check matrix, the error has occurred in the th bit.    "
},
{
  "id": "ss-error-correction-6",
  "level": "2",
  "url": "s-coding-theory-groups.html#ss-error-correction-6",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Probability Epilog. "
},
{
  "id": "ex-another-linear-code",
  "level": "2",
  "url": "s-coding-theory-groups.html#ex-another-linear-code",
  "type": "Example",
  "number": "15.5.8",
  "title": "Another Linear Code.",
  "body": "Another Linear Code  Consider the linear code with generator matrix Since is , this code encodes three bits into five bits. The natural question to ask is what detection or correction does it afford? We can answer this question by constructing the parity check matrix. We observe that if the encoding function is where addition is mod 2 addition. If we receive five bits and no error has occurred, the following two equations would be true. Notice that in general, the number of parity check equations is equal to the number of extra bits that are added by the encoding function. These equations are equivalent to the single matrix equation , where   At a glance, we can see that this code will not correct most single bit errors. Suppose an error is added in the transmission of the five bits. Specifically, suppose that 1 is added (mod 2) in position , where and the other coordinates of are 0. Then when we compute the syndrome of our received transmission, we see that But is the row of . If the syndrome is we know that the error occurred in position 1 and we can correct it. However, if the error is in any other position we can't pinpoint its location. If the syndrome is , then the error could have occurred in either position 2 or position 3. This code does detect all single bit errors but only corrects one fifth of them.   "
},
{
  "id": "exercises-15-5-2",
  "level": "2",
  "url": "s-coding-theory-groups.html#exercises-15-5-2",
  "type": "Exercise",
  "number": "15.5.4.1",
  "title": "",
  "body": "If the error-detecting code is being used, how would you act on the following received blocks?           Error detected, since an odd number of 1's was received; ask for retransmission.  No error detected; accept this block.  No error detected; accept this block.   "
},
{
  "id": "exercises-15-5-3",
  "level": "2",
  "url": "s-coding-theory-groups.html#exercises-15-5-3",
  "type": "Exercise",
  "number": "15.5.4.2",
  "title": "",
  "body": "Determine the parity check matrix for the triple repetition code. "
},
{
  "id": "exercises-15-5-4",
  "level": "2",
  "url": "s-coding-theory-groups.html#exercises-15-5-4",
  "type": "Exercise",
  "number": "15.5.4.3",
  "title": "",
  "body": "If the error-correcting code from this section is being used, how would you decode the following blocks? Expect an error that cannot be fixed with one of these.                 Syndrome = . Corrected coded message is and original message was .  Syndrome = . Corrected coded message is and original message was .  Syndrome = . No error, coded message is and original message was .  Syndrome = . Corrected coded message is and original message was .  Syndrome = . This syndrome occurs only if two bits have been switched. No reliable correction is possible.  Syndrome = . Corrected coded message is and original message was .   "
},
{
  "id": "exercises-15-5-5",
  "level": "2",
  "url": "s-coding-theory-groups.html#exercises-15-5-5",
  "type": "Exercise",
  "number": "15.5.4.4",
  "title": "",
  "body": "Suppose that the code words of a coding function have the property that any two of them have a Hamming distance of at least 5. How many bit errors could be corrected with such a code? "
},
{
  "id": "exercises-15-5-6",
  "level": "2",
  "url": "s-coding-theory-groups.html#exercises-15-5-6",
  "type": "Exercise",
  "number": "15.5.4.5",
  "title": "",
  "body": "Consider the linear code defined by the generator matrix    What size blocks does this code encode and what is the length of the code words?  What are the code words for this code?  With this code, can you detect single bit errors? Can you correct all, some, or no single bit errors?    Blocks of two bits are encoded into code words of length 4.  The code words are 0000, 1010, 0111 and 1101.  Since the first two code words have a Hamming distance of 2, not all single bit errors can be corrected. For example, if 0000 is transmitted and the first bit is switch, then 1000 is received and we can't tell for sure whether this came from 0000 or 1010. To see what can be corrected, we note that is encoded to and so if is recieved and no error has occurred,   We can extract the parity check matrix from this set of equations. It is The rows of this matrix correspond with the syndromes for errors in bits 1 through 4, which are all nonzero, so we can detect any single bit error. Notice that the syndromes for bits 1 and 3 are identical. This reflects the fact that errors in these bits can't be corrected. However, the syndromes for bits 2 and 4 are unique and so we can correct them. Therefore the second bit of the original message can be sent with more confidence than the first.    "
},
{
  "id": "exercises-15-5-7",
  "level": "2",
  "url": "s-coding-theory-groups.html#exercises-15-5-7",
  "type": "Exercise",
  "number": "15.5.4.6",
  "title": "Rectangular codes.",
  "body": "Rectangular codes Rectangular codes To build a rectangular code, you partition your message into blocks of length and then factor into and arrange the bits in a rectangular array as in the figure below. Then you add parity bits along the right side and bottom of the rows and columns. The code word is then read row by row.   For example, if is 4, then our only choice is a 2 by 2 array. The message 1101 would be encoded as and the code word is the string 11001110.    Suppose that you were sent four bit messages using this code and you received the following strings. What were the messages, assuming no more than one error in the transmission of coded data?  11011000  01110010  10001111   If you encoded bits in this manner, what would be the rate of the code?  Rectangular codes are linear codes. For the 2 by 2 rectangular code, what are the generator and parity check matrices?   "
},
{
  "id": "exercises-15-5-8",
  "level": "2",
  "url": "s-coding-theory-groups.html#exercises-15-5-8",
  "type": "Exercise",
  "number": "15.5.4.7",
  "title": "",
  "body": "Suppose that the code in is expanded to add the column to the generator matrix , can all single bit errors be corrected? Explain your answer. Yes, you can correct all single bit errors because the parity check matrix for the expanded code is Since each possible syndrome of single bit errors is unique we can correct any error.  "
},
{
  "id": "exercises-15-5-9",
  "level": "2",
  "url": "s-coding-theory-groups.html#exercises-15-5-9",
  "type": "Exercise",
  "number": "15.5.4.8",
  "title": "",
  "body": "Suppose that a linear code has parity check matrix Determine the generator matrix, , and in so doing, identify the number of bits in each message block and the number of parity bits. There is a parity check equation for each parity bit. "
},
{
  "id": "exercises-15-5-10",
  "level": "2",
  "url": "s-coding-theory-groups.html#exercises-15-5-10",
  "type": "Exercise",
  "number": "15.5.4.9",
  "title": "",
  "body": "Perfect Codes A code with minimum distance is called perfect if every string of bits is within Hamming distance of some code word. For such a code, the spheres of radius around the code words partition the set of all strings. This is analogous to packing objects into a box with no wasted space. Using just the number of bit strings of length and the number of strings in a sphere of radius , for what values of is it possible to find a perfect code of distance ? You don't have to actually find the codes. "
},
{
  "id": "exercises-15-5-11",
  "level": "2",
  "url": "s-coding-theory-groups.html#exercises-15-5-11",
  "type": "Exercise",
  "number": "15.5.4.10",
  "title": "",
  "body": "  Prove that the code words of a linear code are a subgroup of the code space.  Prove that if is a left coset of the set of code words, then all elements of will have the same syndrome.   "
},
{
  "id": "s-rings-basic",
  "level": "1",
  "url": "s-rings-basic.html",
  "type": "Section",
  "number": "16.1",
  "title": "Rings, Basic Definitions and Concepts",
  "body": " Rings, Basic Definitions and Concepts  Basic Definitions  We would like to investigate algebraic systems whose structure imitates that of the integers.  Ring  Ring  a ring with domain and operations and  A ring is a set together with two binary operations, addition and multiplication, denoted by the symbols and such that the following axioms are satisfied:   is an abelian group.  Multiplication is associative on .  Multiplication is distributive over addition; that is, for all , the left distributive law, , and the right distributive law, .       A ring is denoted or as just plain if the operations are understood.  The symbols and stand for arbitrary operations, not just regular addition and multiplication. These symbols are referred to by the usual names. For simplicity, we may write instead of if it is not ambiguous.  For the abelian group , we use additive notation. In particular, the group identity is designated by 0 rather than by and is customarily called the zero of the ring. The group inverse is also written in additive notation: rather than .     We now look at some examples of rings. Certainly all the additive abelian groups of Chapter 11 are likely candidates for rings.  The ring of integers is a ring, where and stand for regular addition and multiplication on . From Chapter 11, we already know that is an abelian group, so we need only check parts 2 and 3 of the definition of a ring. From elementary algebra, we know that the associative law under multiplication and the distributive laws are true for . This is our main example of an infinite ring.  The ring of integers modulo is a ring. The properties of modular arithmetic on were described in Section 11.4, and they give us the information we need to convince ourselves that is a ring. This example is our main example of finite rings of different orders.  Commutative Ring  Ring Commutative  A ring in which multiplication is a commutative operation is called a commutative ring.  It is common practice to use the word abelian when referring to the commutative law under addition and the word commutative when referring to the commutative law under the operation of multiplication.  Unity of a Ring  Unity of a Ring  Ring with unity  A ring that has a multiplicative identity is called a ring with unity. The multiplicative identity itself is called the unity of the ring. More formally, if there exists an element , such that for all , , then is called a ring with unity .  The rings in our first two examples were commutative rings with unity, the unity in both cases being the number 1. The ring is a noncommutative ring with unity, the unity being the two by two identity matrix.  An example of a ring that is not a ring with unity is the ring of even integers, .    Direct Products of Rings  Products of rings are analogous to products of groups or products of Boolean algebras.  Let , be rings. Let and .  From Chapter 11 we know that is an abelian group under the operation of componentwise addition: We also define multiplication on componentwise:  To show that is a ring under the above operations, we need only show that the (multiplicative) associative law and the distributive laws hold. This is indeed the case, and we leave it as an exercise. If each of the is commutative, then is commutative, and if each contains a unity, then is a ring with unity, which is the -tuple consisting of the unities of each of the 's.   Since and are rings, then is a ring, where, for example, .  To determine the unity in the ring , we look for the element such that for all elements , , or, equivalently, So we want such that in the ring . The only element in that satisfies this equation is . Similarly, we obtain value of 1 for . So the unity of , which is unique by Exercise 15 of this section, is . We leave to the reader to verify that this ring is commutative.    Multiplicative Inverses in Rings  We now consider the extremely important concept of multiplicative inverses. Certainly many basic equations in elementary algebra (e.g., ) are solved with this concept.   The equation has a solution in the ring but does not have a solution in since, to solve this equation, we multiply both sides of the equation by the multiplicative inverse of 2. This number, exists in but does not exist in . We formalize this important idea in a definition which by now should be quite familiar to you.  Multiplicative Inverses  Multiplicative Inverses  Units of a ring  the set of units of a ring Let be a ring with unity, 1. If and there exists an element such that , then is said to have a multiplicative inverse, . A ring element that possesses a multiplicative inverse is a unit of the ring. The set of all units of a ring is denoted by .  By , the multiplicative inverse of a ring element is unique, if it exists. For this reason, we can use the notation for the multiplicative inverse of , if it exists.  In the rings and every nonzero element has a multiplicative inverse. The only elements in that have multiplicative inverses are -1 and 1. That is, , , and .  Let us find the multiplicative inverses, when they exist, of each element of the ring . If , we want an element such that . We do not have to check whether since is commutative. If we try each of the six elements, 0, 1, 2, 3, 4, and 5, of , we find that none of them satisfies the above equation, so 3 does not have a multiplicative inverse in . However, since , 5 does have a multiplicative inverse in , namely itself: . The following table summarizes all results for . It shouldn't be a surprise that the zero of a ring is never going to have a multiplicative inverse.   More universal concepts, isomorphisms and subrings  Isomorphism is a universal concept that is important in every algebraic structure. Two rings are isomorphic as rings if and only if they have the same cardinality and if they behave exactly the same under corresponding operations. They are essentially the same ring. For this to be true, they must behave the same as groups (under + ) and they must behave the same under the operation of multiplication.  Ring Isomorphism Ring Isomorphism.  Let and be rings. Then is isomorphic to if and only if there exists a function, , called a ring isomorphism, such that  is a bijection   for all  for all .    Conditions 1 and 2 tell us that is a group isomorphism.  This leads us to the problem of how to show that two rings are not isomorphic. This is a universal concept. It is true for any algebraic structure and was discussed in Chapter 11. To show that two rings are not isomorphic, we must demonstrate that they behave differently under one of the operations. We illustrate through several examples.  Consider the rings and . In Chapter 11 we showed that as groups, the two sets and 2 with addition were isomorphic. The group isomorphism that proved this was the function , defined by . Is a ring isomorphism? We need only check whether for all . In fact, this condition is not satisfied: Therefore, is not a ring isomorphism. This does not necessarily mean that the two rings and 2 are not isomorphic, but simply that doesn't satisfy the conditions. We could imagine that some other function does. We could try to find another function that is a ring isomorphism, or we could try to show that and 2 are not isomorphic as rings. To do the latter, we must find something different about the ring structure of and 2 .  We already know that they behave identically under addition, so if they are different as rings, it must have something to do with how they behave under the operation of multiplication. Let's begin to develop a checklist of how the two rings could differ:  Do they have the same cardinality? Yes, they are both countable.  Are they both commutative? Yes.  Are they both rings with unity? No.   is a ring with unity, namely the number 1. 2 is not a ring with unity, . Hence, they are not isomorphic as rings.  Next consider whether and are isomorphic. Because of the previous example, we might guess that they are not. However, checklist items 1 through 3 above do not help us. Why? We add another checklist item:  4. Find an equation that makes sense in both rings, which is solvable in one and not the other.  The equation , or , makes sense in both rings. However, this equation has a nonzero solution, , in , but does not have a nonzero solution in . Thus we have an equation solvable in one ring that cannot be solved in the other, so they cannot be isomorphic.  Another universal concept that applies to the theory of rings is that of a subsystem. A subring of a ring is any nonempty subset of that is a ring under the operations of . First, for to be a subring of the ring , must be a subgroup of the group . Also, must be closed under , satisfy the associative law under , and satisfy the distributive laws. But since is a ring, the associative and distributive laws are true for every element in , and, in particular, for all elements in , since . We have just proven the following theorem:   A nonempty subset of a ring is a subring of R if and only if:   is a subgroup of the group   is closed under multiplication: if , then .   The set of even integers, is a subring of the ring since is a subgroup of the group and since it is also closed with respect to multiplication:  Several of the basic facts that we are familiar with are true for any ring. The following theorem lists a few of the elementary properties of rings.  Some Basic Properties  Let be a ring, with . Then            , the last equality valid by the left distributive axiom. Hence if we add to both sides of the equality above, we obtain . Similarly, we can prove that .  Before we begin the proof of this part, recall that the inverse of each element of the group is unique. Hence the inverse of the element is unique and it is denoted . Therefore, to prove that , we need only show that inverts . Similarly, it can be shown that .  We leave the proof of part 3 to the reader as an exercise.      We will compute in the ring . , since the additive inverse of 4 (mod 6) is 2. Of course, we could have done the calculation directly as  Integral Domains and Zero Divisors  As the example above illustrates, is a modest beginning in the study of which algebraic manipulations are possible when working with rings. A fact in elementary algebra that is used frequently in problem solving is the cancellation law. We know that the cancellation laws are true under addition for any ring, based on group theory. Are the cancellation laws true under multiplication, where the group axioms can't be counted on? More specifically, let be a ring and let with . When can we cancel the 's in the equation ? We can do so if exists, but we cannot assume that has a multiplicative inverse. The answer to this question is found with the following definition and the theorem that follows.  Zero Divisor Zero Divisor Let be a ring. If and are two nonzero elements of such that , then and are called zero divisors.  In the ring , the numbers 4 and 2 are zero divisors since . In addition, 6 is a zero divisor because .  In the ring the matrices and are zero divisors since .  has no zero divisors.   Now, here is why zero divisors are related to cancellation.  Multiplicative Cancellation  The multiplicative cancellation laws hold in a ring if and only if has no zero divisors.  We prove the theorem using the left cancellation axiom, namely that if and , then for all . The proof using the right cancellation axiom is its mirror image.  Assume the left cancellation law holds in and assume that and are two elements in such that . We must show that either or . To do this, assume that and show that must be 0. .   Conversely, assume that has no zero divisors and we will prove that the left cancellation law must hold. To do this, assume that , , such that and show that .   Hence, the only time that the cancellation laws hold in a ring is when there are no zero divisors. The commutative rings with unity in which the two conditions are true are given a special name.  Integral Domain Integral Domain a generic integral domain A commutative ring with unity containing no zero divisors is called an integral domain.  In this chapter, Integral domains will be denoted generically by the letter . We state the following two useful facts without proof.  If , , then is a zero divisor if and only if and are not relatively prime; i.e., .   If p is a prime, then has no zero divisors.   , with a prime, , , and are all integral domains. The key example of an infinite integral domain is . In fact, it is from that the term integral domain is derived. Our main example of a finite integral domain is , when is prime.  We close this section with the verification of an observation that was made in Chapter 11, namely that the product of two algebraic systems may not be an algebraic system of the same type.  Both and are integral domains. Consider the direct product . It's true that is a commutative ring with unity (see Exercise 13). However, , so has zero divisors and is therefore not an integral domain.    Exercises  Review the definition of rings to show that the following are rings. The operations involved are the usual operations defined on the sets. Which of these rings are commutative? Which are rings with unity? For the rings with unity, determine the unity and all units.               All but ring d are commutative. All of the rings have a unity element. The number 1 is the unity for all of the rings except d. The unity for is the two by two identity matrix. The units are as follows:               Follow the instructions for Exercise 1 and the following rings:                   Show that the following pairs of rings are not isomorphic:   and   and .    Consider commutativity  Solve in both rings.     Show that the following pairs of rings are not isomorphic:  and .  and .     Show that is a subring of the ring  Find all subrings of .  Find all subrings of .     We already know that is a subgroup of the group . We need only show that is closed with respect to multiplication. Let . , since .  The proper subrings are and ; while and are improper subrings.  The proper subrings are , , and : while and are improper subrings.    Verify the validity of by finding examples of elements , , and , in the following rings, where and yet :             Determine all solutions of the equation in . Can there be any more than two solutions to this equation (or any quadratic equation) in ?  Find all solutions of the equation in part a in . Why are there more than two solutions?     The left-hand side of the equation factors into the product . Since is an integral domain, and are the only possible solutions.  Over , 2, 3, 6, and 11 are solutions. Although the equation factors into , this product can be zero without making either 2 or 3. For example. If = 6 we get . Notice that 4 and 3 are zero divisors.    Solve the equation in the following rings. Interpret 4 as , where 1 is the unity of the ring.  in   in   in  in    The relation is isomorphic to on rings is an equivalence relation. Explain the meaning of this statement. Let , , and be any rings, then   is isomorphic to and so is isomorphic to is a reflexive relation on rings.   is isomorphic to   is isomorphic to , and so is isomorphic to is a symmetric relation on rings,   is isomorphic to , and is isomorphic to implies that is isomorphic to , and so is isomorphic to is a transitive relation on rings.  We haven't proven these properties here, just stated them. The combination of these observations implies that is isomorphic to is an equivalence relation on rings.   Let , , , be rings. Prove the multiplicative, associative, and distributive laws for the ring  If each of the is commutative, is R commutative?  Under what conditions will be a ring with unity?  What will the units of be when it has a unity?      Prove that the ring is commutative and has unity.  Determine all zero divisors for the ring .  Give another example illustrating the fact that the product of two integral domains may not be an integral domain. Is there an example where the product is an integral domain?     Commutativity is clear from examination of a multiplication table for . More generally, we could prove a theorem that the direct product of two or more commutative rings is commutative. is the unity of .    Another example is . You never get an integral domain in this situation. By the definition an integral domain must contain a zero so we always have in .    Boolean Rings Let be a nonempty set.  Verify that is a commutative ring with unity.  What are the units of this ring?     For any ring , expand for .  If is commutative, prove that for all .        .     Let be a commutative ring with unity. Prove by induction that for ,  Simplify in .  Simplify in .    Prove part 3 of .  Let be a finite set. Prove that the Boolean ring is isomorphic to the ring . where .   "
},
{
  "id": "def-ring",
  "level": "2",
  "url": "s-rings-basic.html#def-ring",
  "type": "Definition",
  "number": "16.1.1",
  "title": "Ring.",
  "body": "Ring  Ring  a ring with domain and operations and  A ring is a set together with two binary operations, addition and multiplication, denoted by the symbols and such that the following axioms are satisfied:   is an abelian group.  Multiplication is associative on .  Multiplication is distributive over addition; that is, for all , the left distributive law, , and the right distributive law, .    "
},
{
  "id": "ss-ring-definitions-4",
  "level": "2",
  "url": "s-rings-basic.html#ss-ring-definitions-4",
  "type": "Note",
  "number": "16.1.2",
  "title": "",
  "body": "  A ring is denoted or as just plain if the operations are understood.  The symbols and stand for arbitrary operations, not just regular addition and multiplication. These symbols are referred to by the usual names. For simplicity, we may write instead of if it is not ambiguous.  For the abelian group , we use additive notation. In particular, the group identity is designated by 0 rather than by and is customarily called the zero of the ring. The group inverse is also written in additive notation: rather than .    "
},
{
  "id": "ex-ring-of-integers",
  "level": "2",
  "url": "s-rings-basic.html#ex-ring-of-integers",
  "type": "Example",
  "number": "16.1.3",
  "title": "The ring of integers.",
  "body": "The ring of integers is a ring, where and stand for regular addition and multiplication on . From Chapter 11, we already know that is an abelian group, so we need only check parts 2 and 3 of the definition of a ring. From elementary algebra, we know that the associative law under multiplication and the distributive laws are true for . This is our main example of an infinite ring. "
},
{
  "id": "ex-ring-of-integers-mod-n",
  "level": "2",
  "url": "s-rings-basic.html#ex-ring-of-integers-mod-n",
  "type": "Example",
  "number": "16.1.4",
  "title": "The ring of integers modulo <span xmlns:pf=\"https:\/\/prefigure.org\" class=\"process-math\">\\(n\\)<\/span>.",
  "body": "The ring of integers modulo is a ring. The properties of modular arithmetic on were described in Section 11.4, and they give us the information we need to convince ourselves that is a ring. This example is our main example of finite rings of different orders. "
},
{
  "id": "def-commutative-ring",
  "level": "2",
  "url": "s-rings-basic.html#def-commutative-ring",
  "type": "Definition",
  "number": "16.1.5",
  "title": "Commutative Ring.",
  "body": "Commutative Ring  Ring Commutative  A ring in which multiplication is a commutative operation is called a commutative ring. "
},
{
  "id": "def-unity",
  "level": "2",
  "url": "s-rings-basic.html#def-unity",
  "type": "Definition",
  "number": "16.1.6",
  "title": "Unity of a Ring.",
  "body": "Unity of a Ring  Unity of a Ring  Ring with unity  A ring that has a multiplicative identity is called a ring with unity. The multiplicative identity itself is called the unity of the ring. More formally, if there exists an element , such that for all , , then is called a ring with unity . "
},
{
  "id": "ex-rings-with-unity",
  "level": "2",
  "url": "s-rings-basic.html#ex-rings-with-unity",
  "type": "Example",
  "number": "16.1.7",
  "title": "",
  "body": "The rings in our first two examples were commutative rings with unity, the unity in both cases being the number 1. The ring is a noncommutative ring with unity, the unity being the two by two identity matrix.  An example of a ring that is not a ring with unity is the ring of even integers, . "
},
{
  "id": "ex-finite-direct-product-ring",
  "level": "2",
  "url": "s-rings-basic.html#ex-finite-direct-product-ring",
  "type": "Example",
  "number": "16.1.8",
  "title": "",
  "body": " Since and are rings, then is a ring, where, for example, .  To determine the unity in the ring , we look for the element such that for all elements , , or, equivalently, So we want such that in the ring . The only element in that satisfies this equation is . Similarly, we obtain value of 1 for . So the unity of , which is unique by Exercise 15 of this section, is . We leave to the reader to verify that this ring is commutative.  "
},
{
  "id": "ex-simple-equation-16-5",
  "level": "2",
  "url": "s-rings-basic.html#ex-simple-equation-16-5",
  "type": "Example",
  "number": "16.1.9",
  "title": "",
  "body": " The equation has a solution in the ring but does not have a solution in since, to solve this equation, we multiply both sides of the equation by the multiplicative inverse of 2. This number, exists in but does not exist in . We formalize this important idea in a definition which by now should be quite familiar to you. "
},
{
  "id": "def-multiplicative-inverses",
  "level": "2",
  "url": "s-rings-basic.html#def-multiplicative-inverses",
  "type": "Definition",
  "number": "16.1.10",
  "title": "Multiplicative Inverses.",
  "body": "Multiplicative Inverses  Multiplicative Inverses  Units of a ring  the set of units of a ring Let be a ring with unity, 1. If and there exists an element such that , then is said to have a multiplicative inverse, . A ring element that possesses a multiplicative inverse is a unit of the ring. The set of all units of a ring is denoted by . "
},
{
  "id": "ex-some-units-1",
  "level": "2",
  "url": "s-rings-basic.html#ex-some-units-1",
  "type": "Example",
  "number": "16.1.11",
  "title": "",
  "body": "In the rings and every nonzero element has a multiplicative inverse. The only elements in that have multiplicative inverses are -1 and 1. That is, , , and . "
},
{
  "id": "ex-some-units-2",
  "level": "2",
  "url": "s-rings-basic.html#ex-some-units-2",
  "type": "Example",
  "number": "16.1.12",
  "title": "",
  "body": "Let us find the multiplicative inverses, when they exist, of each element of the ring . If , we want an element such that . We do not have to check whether since is commutative. If we try each of the six elements, 0, 1, 2, 3, 4, and 5, of , we find that none of them satisfies the above equation, so 3 does not have a multiplicative inverse in . However, since , 5 does have a multiplicative inverse in , namely itself: . The following table summarizes all results for . It shouldn't be a surprise that the zero of a ring is never going to have a multiplicative inverse. "
},
{
  "id": "def-ring-isomorphism",
  "level": "2",
  "url": "s-rings-basic.html#def-ring-isomorphism",
  "type": "Definition",
  "number": "16.1.13",
  "title": "Ring Isomorphism.",
  "body": "Ring Isomorphism Ring Isomorphism.  Let and be rings. Then is isomorphic to if and only if there exists a function, , called a ring isomorphism, such that  is a bijection   for all  for all .   "
},
{
  "id": "ex-non-isomorphic-rings-1",
  "level": "2",
  "url": "s-rings-basic.html#ex-non-isomorphic-rings-1",
  "type": "Example",
  "number": "16.1.14",
  "title": "",
  "body": "Consider the rings and . In Chapter 11 we showed that as groups, the two sets and 2 with addition were isomorphic. The group isomorphism that proved this was the function , defined by . Is a ring isomorphism? We need only check whether for all . In fact, this condition is not satisfied: Therefore, is not a ring isomorphism. This does not necessarily mean that the two rings and 2 are not isomorphic, but simply that doesn't satisfy the conditions. We could imagine that some other function does. We could try to find another function that is a ring isomorphism, or we could try to show that and 2 are not isomorphic as rings. To do the latter, we must find something different about the ring structure of and 2 .  We already know that they behave identically under addition, so if they are different as rings, it must have something to do with how they behave under the operation of multiplication. Let's begin to develop a checklist of how the two rings could differ:  Do they have the same cardinality? Yes, they are both countable.  Are they both commutative? Yes.  Are they both rings with unity? No.   is a ring with unity, namely the number 1. 2 is not a ring with unity, . Hence, they are not isomorphic as rings. "
},
{
  "id": "ex-non-isomorphic-rings-2",
  "level": "2",
  "url": "s-rings-basic.html#ex-non-isomorphic-rings-2",
  "type": "Example",
  "number": "16.1.15",
  "title": "",
  "body": "Next consider whether and are isomorphic. Because of the previous example, we might guess that they are not. However, checklist items 1 through 3 above do not help us. Why? We add another checklist item:  4. Find an equation that makes sense in both rings, which is solvable in one and not the other.  The equation , or , makes sense in both rings. However, this equation has a nonzero solution, , in , but does not have a nonzero solution in . Thus we have an equation solvable in one ring that cannot be solved in the other, so they cannot be isomorphic. "
},
{
  "id": "theorem-subring-conditions",
  "level": "2",
  "url": "s-rings-basic.html#theorem-subring-conditions",
  "type": "Theorem",
  "number": "16.1.16",
  "title": "",
  "body": " A nonempty subset of a ring is a subring of R if and only if:   is a subgroup of the group   is closed under multiplication: if , then .  "
},
{
  "id": "ex-subring-of-z",
  "level": "2",
  "url": "s-rings-basic.html#ex-subring-of-z",
  "type": "Example",
  "number": "16.1.17",
  "title": "",
  "body": "The set of even integers, is a subring of the ring since is a subgroup of the group and since it is also closed with respect to multiplication: "
},
{
  "id": "theorem-basic-group-properties",
  "level": "2",
  "url": "s-rings-basic.html#theorem-basic-group-properties",
  "type": "Theorem",
  "number": "16.1.18",
  "title": "Some Basic Properties.",
  "body": "Some Basic Properties  Let be a ring, with . Then            , the last equality valid by the left distributive axiom. Hence if we add to both sides of the equality above, we obtain . Similarly, we can prove that .  Before we begin the proof of this part, recall that the inverse of each element of the group is unique. Hence the inverse of the element is unique and it is denoted . Therefore, to prove that , we need only show that inverts . Similarly, it can be shown that .  We leave the proof of part 3 to the reader as an exercise.     "
},
{
  "id": "ex-some-basics-applied-161",
  "level": "2",
  "url": "s-rings-basic.html#ex-some-basics-applied-161",
  "type": "Example",
  "number": "16.1.19",
  "title": "",
  "body": "We will compute in the ring . , since the additive inverse of 4 (mod 6) is 2. Of course, we could have done the calculation directly as "
},
{
  "id": "def-zero-divisor",
  "level": "2",
  "url": "s-rings-basic.html#def-zero-divisor",
  "type": "Definition",
  "number": "16.1.20",
  "title": "Zero Divisor.",
  "body": "Zero Divisor Zero Divisor Let be a ring. If and are two nonzero elements of such that , then and are called zero divisors. "
},
{
  "id": "ex-some-zero-divisors",
  "level": "2",
  "url": "s-rings-basic.html#ex-some-zero-divisors",
  "type": "Example",
  "number": "16.1.21",
  "title": "",
  "body": "In the ring , the numbers 4 and 2 are zero divisors since . In addition, 6 is a zero divisor because .  In the ring the matrices and are zero divisors since .  has no zero divisors.  "
},
{
  "id": "theorem-zero-divisors-cancellation",
  "level": "2",
  "url": "s-rings-basic.html#theorem-zero-divisors-cancellation",
  "type": "Theorem",
  "number": "16.1.22",
  "title": "Multiplicative Cancellation.",
  "body": "Multiplicative Cancellation  The multiplicative cancellation laws hold in a ring if and only if has no zero divisors.  We prove the theorem using the left cancellation axiom, namely that if and , then for all . The proof using the right cancellation axiom is its mirror image.  Assume the left cancellation law holds in and assume that and are two elements in such that . We must show that either or . To do this, assume that and show that must be 0. .   Conversely, assume that has no zero divisors and we will prove that the left cancellation law must hold. To do this, assume that , , such that and show that .  "
},
{
  "id": "def-integral-domain",
  "level": "2",
  "url": "s-rings-basic.html#def-integral-domain",
  "type": "Definition",
  "number": "16.1.23",
  "title": "Integral Domain.",
  "body": "Integral Domain Integral Domain a generic integral domain A commutative ring with unity containing no zero divisors is called an integral domain. "
},
{
  "id": "theorem-zn-zero-divisors",
  "level": "2",
  "url": "s-rings-basic.html#theorem-zn-zero-divisors",
  "type": "Theorem",
  "number": "16.1.24",
  "title": "",
  "body": "If , , then is a zero divisor if and only if and are not relatively prime; i.e., . "
},
{
  "id": "corollary-zp-zero-divisors",
  "level": "2",
  "url": "s-rings-basic.html#corollary-zp-zero-divisors",
  "type": "Corollary",
  "number": "16.1.25",
  "title": "",
  "body": " If p is a prime, then has no zero divisors. "
},
{
  "id": "ex-some-integral-domains",
  "level": "2",
  "url": "s-rings-basic.html#ex-some-integral-domains",
  "type": "Example",
  "number": "16.1.26",
  "title": "",
  "body": " , with a prime, , , and are all integral domains. The key example of an infinite integral domain is . In fact, it is from that the term integral domain is derived. Our main example of a finite integral domain is , when is prime. "
},
{
  "id": "ex-direct-product-not-id",
  "level": "2",
  "url": "s-rings-basic.html#ex-direct-product-not-id",
  "type": "Example",
  "number": "16.1.27",
  "title": "",
  "body": "Both and are integral domains. Consider the direct product . It's true that is a commutative ring with unity (see Exercise 13). However, , so has zero divisors and is therefore not an integral domain. "
},
{
  "id": "exercises-16-1-2",
  "level": "2",
  "url": "s-rings-basic.html#exercises-16-1-2",
  "type": "Exercise",
  "number": "16.1.6.1",
  "title": "",
  "body": "Review the definition of rings to show that the following are rings. The operations involved are the usual operations defined on the sets. Which of these rings are commutative? Which are rings with unity? For the rings with unity, determine the unity and all units.               All but ring d are commutative. All of the rings have a unity element. The number 1 is the unity for all of the rings except d. The unity for is the two by two identity matrix. The units are as follows:              "
},
{
  "id": "exercises-16-1-3",
  "level": "2",
  "url": "s-rings-basic.html#exercises-16-1-3",
  "type": "Exercise",
  "number": "16.1.6.2",
  "title": "",
  "body": "Follow the instructions for Exercise 1 and the following rings:                  "
},
{
  "id": "exercises-16-1-4",
  "level": "2",
  "url": "s-rings-basic.html#exercises-16-1-4",
  "type": "Exercise",
  "number": "16.1.6.3",
  "title": "",
  "body": "Show that the following pairs of rings are not isomorphic:   and   and .    Consider commutativity  Solve in both rings.    "
},
{
  "id": "exercises-16-1-5",
  "level": "2",
  "url": "s-rings-basic.html#exercises-16-1-5",
  "type": "Exercise",
  "number": "16.1.6.4",
  "title": "",
  "body": "Show that the following pairs of rings are not isomorphic:  and .  and .   "
},
{
  "id": "exercises-16-1-6",
  "level": "2",
  "url": "s-rings-basic.html#exercises-16-1-6",
  "type": "Exercise",
  "number": "16.1.6.5",
  "title": "",
  "body": " Show that is a subring of the ring  Find all subrings of .  Find all subrings of .     We already know that is a subgroup of the group . We need only show that is closed with respect to multiplication. Let . , since .  The proper subrings are and ; while and are improper subrings.  The proper subrings are , , and : while and are improper subrings.   "
},
{
  "id": "exercises-16-1-7",
  "level": "2",
  "url": "s-rings-basic.html#exercises-16-1-7",
  "type": "Exercise",
  "number": "16.1.6.6",
  "title": "",
  "body": "Verify the validity of by finding examples of elements , , and , in the following rings, where and yet :           "
},
{
  "id": "exercises-16-1-8",
  "level": "2",
  "url": "s-rings-basic.html#exercises-16-1-8",
  "type": "Exercise",
  "number": "16.1.6.7",
  "title": "",
  "body": " Determine all solutions of the equation in . Can there be any more than two solutions to this equation (or any quadratic equation) in ?  Find all solutions of the equation in part a in . Why are there more than two solutions?     The left-hand side of the equation factors into the product . Since is an integral domain, and are the only possible solutions.  Over , 2, 3, 6, and 11 are solutions. Although the equation factors into , this product can be zero without making either 2 or 3. For example. If = 6 we get . Notice that 4 and 3 are zero divisors.   "
},
{
  "id": "exercises-16-1-9",
  "level": "2",
  "url": "s-rings-basic.html#exercises-16-1-9",
  "type": "Exercise",
  "number": "16.1.6.8",
  "title": "",
  "body": "Solve the equation in the following rings. Interpret 4 as , where 1 is the unity of the ring.  in   in   in  in   "
},
{
  "id": "exercises-16-1-10",
  "level": "2",
  "url": "s-rings-basic.html#exercises-16-1-10",
  "type": "Exercise",
  "number": "16.1.6.9",
  "title": "",
  "body": "The relation is isomorphic to on rings is an equivalence relation. Explain the meaning of this statement. Let , , and be any rings, then   is isomorphic to and so is isomorphic to is a reflexive relation on rings.   is isomorphic to   is isomorphic to , and so is isomorphic to is a symmetric relation on rings,   is isomorphic to , and is isomorphic to implies that is isomorphic to , and so is isomorphic to is a transitive relation on rings.  We haven't proven these properties here, just stated them. The combination of these observations implies that is isomorphic to is an equivalence relation on rings. "
},
{
  "id": "exercises-16-1-11",
  "level": "2",
  "url": "s-rings-basic.html#exercises-16-1-11",
  "type": "Exercise",
  "number": "16.1.6.10",
  "title": "",
  "body": " Let , , , be rings. Prove the multiplicative, associative, and distributive laws for the ring  If each of the is commutative, is R commutative?  Under what conditions will be a ring with unity?  What will the units of be when it has a unity?   "
},
{
  "id": "exercises-16-1-12",
  "level": "2",
  "url": "s-rings-basic.html#exercises-16-1-12",
  "type": "Exercise",
  "number": "16.1.6.11",
  "title": "",
  "body": "  Prove that the ring is commutative and has unity.  Determine all zero divisors for the ring .  Give another example illustrating the fact that the product of two integral domains may not be an integral domain. Is there an example where the product is an integral domain?     Commutativity is clear from examination of a multiplication table for . More generally, we could prove a theorem that the direct product of two or more commutative rings is commutative. is the unity of .    Another example is . You never get an integral domain in this situation. By the definition an integral domain must contain a zero so we always have in .   "
},
{
  "id": "exercises-16-1-13",
  "level": "2",
  "url": "s-rings-basic.html#exercises-16-1-13",
  "type": "Exercise",
  "number": "16.1.6.12",
  "title": "Boolean Rings.",
  "body": "Boolean Rings Let be a nonempty set.  Verify that is a commutative ring with unity.  What are the units of this ring?   "
},
{
  "id": "exercises-16-1-14",
  "level": "2",
  "url": "s-rings-basic.html#exercises-16-1-14",
  "type": "Exercise",
  "number": "16.1.6.13",
  "title": "",
  "body": " For any ring , expand for .  If is commutative, prove that for all .        .   "
},
{
  "id": "exercises-16-1-15",
  "level": "2",
  "url": "s-rings-basic.html#exercises-16-1-15",
  "type": "Exercise",
  "number": "16.1.6.14",
  "title": "",
  "body": " Let be a commutative ring with unity. Prove by induction that for ,  Simplify in .  Simplify in .   "
},
{
  "id": "exercises-16-1-16",
  "level": "2",
  "url": "s-rings-basic.html#exercises-16-1-16",
  "type": "Exercise",
  "number": "16.1.6.15",
  "title": "",
  "body": "Prove part 3 of . "
},
{
  "id": "exercises-16-1-17",
  "level": "2",
  "url": "s-rings-basic.html#exercises-16-1-17",
  "type": "Exercise",
  "number": "16.1.6.16",
  "title": "",
  "body": "Let be a finite set. Prove that the Boolean ring is isomorphic to the ring . where . "
},
{
  "id": "s-fields",
  "level": "1",
  "url": "s-fields.html",
  "type": "Section",
  "number": "16.2",
  "title": "Fields",
  "body": " Fields  Although the algebraic structures of rings and integral domains are widely used and play an important part in the applications of mathematics, we still cannot solve the simple equation , in all rings or all integral domains, for that matter. Yet this is one of the first equations we learn to solve in elementary algebra and its solubility is basic to innumerable questions. If we wish to solve a wide range of problems in a system we need at least all of the laws true for rings and the cancellation laws together with the ability to solve the equation , . We summarize the above in a definition and list theorems that will place this concept in the context of the previous section.  Field Field A field is a commutative ring with unity such that each nonzero element has a multiplicative inverse.  In this chapter, we denote a field generically by the letter . The letters , and are also conventionally used for fields.  Some common fields The most common infinite fields are , , and .  Since every field is a ring, all facts and concepts that are true for rings are true for any field.  Field Integral Domain  Every field is an integral domain.  The proof is fairly easy and a good exercise, so we provide a hint. Starting with the assumption that if we assume that then the existence of makes it possible to infer that .  Of course the converse of is not true. Consider . However, the next theorem proves the converse in finite fields.  Finite Integral Domain Field Every finite integral domain is a field. We leave the details to the reader, but observe that if is a finite integral domain, we can list all elements as , where . Now, to show that any has a multiplicative inverse, consider the products . What can you say about these products?  If is a prime, . An immediate implication of this fact is the following corollary.  If p is a prime, then is a field.   A field of order 4  gives us a large number of finite fields, but we must be cautious. This does not tell us that all finite fields are of the form , a prime. To see this, let's try to construct a field of order 4.  First the field must contain the additive and multiplicative identities, 0 and 1, so, without loss of generality, we can assume that the field we are looking for is of the form . Since there are only two nonisomorphic groups of order 4, we have only two choices for the group table for . If the additive group is isomorphic to then two of the nonzero elements of would not be their own additive inverse (as are 1 and 3 in ). Let's assume is one of those elements and . An isomorphism between the additive groups and would require that in correspond with 2 in . We could continue our argument and infer that , producing a zero divisor, which we need to avoid if is to be a field. We leave the remainder of the argument to the reader. We can thus complete the addition table so that is isomorphic to :   Next, since 1 is the unity of , the partial multiplication table must look like:  Hence, to complete the table, we have only four entries to find, and, since must be commutative, this reduces our task to filling in three entries. Next, each nonzero element of must have a unique multiplicative inverse. The inverse of must be either itself or . If , then . (Why?) But . And if , then is equal to or . In either case, by the cancellation law, we obtain or , which is impossible. Therefore we are forced to conclude that and . To determine the final two products of the table, simply note that, because the equation has only two solutions, 0 and 1 in any field. We also know that cannot be 1 because doesn't invert itself and cannot be 0 because can't be a zero divisor. This leaves us with one possible conclusion, that and similarly . Hence, our multiplication table for is:  We leave it to the reader to verify that , as described above, is a field. Hence, we have produced a field of order 4. This construction would be difficult to repeat for larger fields. In section 16.4 we will introduce a different approach to constructing fields that will be far more efficient.  Even though not all finite fields are isomorphic to for some prime , it can be shown that every field must have either:  a subfield isomorphic to for some prime , or  a subfield isomorphic to .    One can think of all fields as being constructed from either or .   is a field, and it contains a subfield isomorphic to , namely itself.  The field that we constructed in has a subfield isomorphic to for some prime . From the tables, we note that the subset of under the given operations of behaves exactly like . Hence, has a subfield isomorphic to .  We close this section with a brief discussion of isomorphic fields. Again, since a field is a ring, the definition of isomorphism of fields is the same as that of rings. It can be shown that if is a field isomorphism, then ; that is, inverses are mapped onto inverses under any field isomorphism. A major question to try to solve is: How many different non-isomorphic finite fields are there of any given order? If is a prime, it seems clear from our discussions that all fields of order are isomorphic to . But how many nonisomorphic fields are there, if any, of order 4, 6, 8, 9, etc? The answer is given in the following theorem, whose proof is beyond the scope of this text.   Any finite field has order for a prime and a positive integer .  For any prime and any positive integer there is a field of order .  Any two fields of order are isomorphic.    Galois The field of order is frequently referred to as the Galois field of order and it is denoted by . Evariste Galois (1811-32) was a pioneer in the field of abstract algebra.   French stamp honoring Evariste Galois   French stamp honoring Evariste Galois    This theorem tells us that there is a field of order , and there is only one such field up to isomorphism. That is, all such fields of order 4 are isomorphic to , which we constructed in the example above.   Exercises  Write out the addition, multiplication, and inverse tables for each of the following fields'.          Show that the set of units of the fields in Exercise 1 form a group under the operation of the multiplication of the given field. Recall that a unit is an element which has a multiplicative inverse.  Complete the proof of that every finite integral domain is a field.  Write out the operation tables for . Is a ring? An integral domain? A field? Explain.  Determine all values from the given field that satisfy the given equation:   in , and   in and  in     0 in , 1 in , 3 in  2 in , 3 in  2 in    Prove that if and are prime, then is never a field.  Can be a field for any prime and any positive integer ?    Determine all solutions to the following equations over . That is, find all elements of that satisfy the equations.             0 and 1  1  1  none    Determine the number of different fields, if any, of all orders 2 through 15. Wherever possible, describe these fields via a known field.  Let .  Prove that is a field.  Show that is a subfield of . For this reason, is called an extension field of .  Show that all the roots of the equation lie in the extension field .  Do the roots of the equation lie in this field? Explain.     "
},
{
  "id": "def-field",
  "level": "2",
  "url": "s-fields.html#def-field",
  "type": "Definition",
  "number": "16.2.1",
  "title": "Field.",
  "body": "Field Field A field is a commutative ring with unity such that each nonzero element has a multiplicative inverse. "
},
{
  "id": "ex-common-fields",
  "level": "2",
  "url": "s-fields.html#ex-common-fields",
  "type": "Example",
  "number": "16.2.2",
  "title": "Some common fields.",
  "body": "Some common fields The most common infinite fields are , , and . "
},
{
  "id": "s-fields-6",
  "level": "2",
  "url": "s-fields.html#s-fields-6",
  "type": "Remark",
  "number": "16.2.3",
  "title": "",
  "body": "Since every field is a ring, all facts and concepts that are true for rings are true for any field. "
},
{
  "id": "theorem-field-implies-integral-domain",
  "level": "2",
  "url": "s-fields.html#theorem-field-implies-integral-domain",
  "type": "Theorem",
  "number": "16.2.4",
  "title": "Field <span xmlns:pf=\"https:\/\/prefigure.org\" class=\"process-math\">\\(\\Rightarrow\\)<\/span> Integral Domain.",
  "body": "Field Integral Domain  Every field is an integral domain.  The proof is fairly easy and a good exercise, so we provide a hint. Starting with the assumption that if we assume that then the existence of makes it possible to infer that . "
},
{
  "id": "theorem-finite-id-is-field",
  "level": "2",
  "url": "s-fields.html#theorem-finite-id-is-field",
  "type": "Theorem",
  "number": "16.2.5",
  "title": "Finite Integral Domain <span xmlns:pf=\"https:\/\/prefigure.org\" class=\"process-math\">\\(\\Rightarrow\\)<\/span> Field.",
  "body": "Finite Integral Domain Field Every finite integral domain is a field. We leave the details to the reader, but observe that if is a finite integral domain, we can list all elements as , where . Now, to show that any has a multiplicative inverse, consider the products . What can you say about these products? "
},
{
  "id": "corollary-zp-is-field",
  "level": "2",
  "url": "s-fields.html#corollary-zp-is-field",
  "type": "Corollary",
  "number": "16.2.6",
  "title": "",
  "body": "If p is a prime, then is a field.  "
},
{
  "id": "ex-field-of-order-4",
  "level": "2",
  "url": "s-fields.html#ex-field-of-order-4",
  "type": "Example",
  "number": "16.2.7",
  "title": "A field of order 4.",
  "body": "A field of order 4  gives us a large number of finite fields, but we must be cautious. This does not tell us that all finite fields are of the form , a prime. To see this, let's try to construct a field of order 4.  First the field must contain the additive and multiplicative identities, 0 and 1, so, without loss of generality, we can assume that the field we are looking for is of the form . Since there are only two nonisomorphic groups of order 4, we have only two choices for the group table for . If the additive group is isomorphic to then two of the nonzero elements of would not be their own additive inverse (as are 1 and 3 in ). Let's assume is one of those elements and . An isomorphism between the additive groups and would require that in correspond with 2 in . We could continue our argument and infer that , producing a zero divisor, which we need to avoid if is to be a field. We leave the remainder of the argument to the reader. We can thus complete the addition table so that is isomorphic to :   Next, since 1 is the unity of , the partial multiplication table must look like:  Hence, to complete the table, we have only four entries to find, and, since must be commutative, this reduces our task to filling in three entries. Next, each nonzero element of must have a unique multiplicative inverse. The inverse of must be either itself or . If , then . (Why?) But . And if , then is equal to or . In either case, by the cancellation law, we obtain or , which is impossible. Therefore we are forced to conclude that and . To determine the final two products of the table, simply note that, because the equation has only two solutions, 0 and 1 in any field. We also know that cannot be 1 because doesn't invert itself and cannot be 0 because can't be a zero divisor. This leaves us with one possible conclusion, that and similarly . Hence, our multiplication table for is:  We leave it to the reader to verify that , as described above, is a field. Hence, we have produced a field of order 4. This construction would be difficult to repeat for larger fields. In section 16.4 we will introduce a different approach to constructing fields that will be far more efficient. "
},
{
  "id": "ex-Q-in-R",
  "level": "2",
  "url": "s-fields.html#ex-Q-in-R",
  "type": "Example",
  "number": "16.2.8",
  "title": "",
  "body": " is a field, and it contains a subfield isomorphic to , namely itself. "
},
{
  "id": "ex-z2-in-f",
  "level": "2",
  "url": "s-fields.html#ex-z2-in-f",
  "type": "Example",
  "number": "16.2.9",
  "title": "",
  "body": "The field that we constructed in has a subfield isomorphic to for some prime . From the tables, we note that the subset of under the given operations of behaves exactly like . Hence, has a subfield isomorphic to . "
},
{
  "id": "theorem-number-of-fields",
  "level": "2",
  "url": "s-fields.html#theorem-number-of-fields",
  "type": "Theorem",
  "number": "16.2.10",
  "title": "",
  "body": " Any finite field has order for a prime and a positive integer .  For any prime and any positive integer there is a field of order .  Any two fields of order are isomorphic.   "
},
{
  "id": "fig-galois-stamp",
  "level": "2",
  "url": "s-fields.html#fig-galois-stamp",
  "type": "Figure",
  "number": "16.2.11",
  "title": "",
  "body": " French stamp honoring Evariste Galois   French stamp honoring Evariste Galois   "
},
{
  "id": "exercises-16-2-2",
  "level": "2",
  "url": "s-fields.html#exercises-16-2-2",
  "type": "Exercise",
  "number": "16.2.1",
  "title": "",
  "body": "Write out the addition, multiplication, and inverse tables for each of the following fields'.         "
},
{
  "id": "exercises-16-2-3",
  "level": "2",
  "url": "s-fields.html#exercises-16-2-3",
  "type": "Exercise",
  "number": "16.2.2",
  "title": "",
  "body": "Show that the set of units of the fields in Exercise 1 form a group under the operation of the multiplication of the given field. Recall that a unit is an element which has a multiplicative inverse. "
},
{
  "id": "exercises-16-2-4",
  "level": "2",
  "url": "s-fields.html#exercises-16-2-4",
  "type": "Exercise",
  "number": "16.2.3",
  "title": "",
  "body": "Complete the proof of that every finite integral domain is a field. "
},
{
  "id": "exercises-16-2-5",
  "level": "2",
  "url": "s-fields.html#exercises-16-2-5",
  "type": "Exercise",
  "number": "16.2.4",
  "title": "",
  "body": "Write out the operation tables for . Is a ring? An integral domain? A field? Explain. "
},
{
  "id": "exercises-16-2-6",
  "level": "2",
  "url": "s-fields.html#exercises-16-2-6",
  "type": "Exercise",
  "number": "16.2.5",
  "title": "",
  "body": "Determine all values from the given field that satisfy the given equation:   in , and   in and  in     0 in , 1 in , 3 in  2 in , 3 in  2 in  "
},
{
  "id": "exercises-16-2-7",
  "level": "2",
  "url": "s-fields.html#exercises-16-2-7",
  "type": "Exercise",
  "number": "16.2.6",
  "title": "",
  "body": " Prove that if and are prime, then is never a field.  Can be a field for any prime and any positive integer ?   "
},
{
  "id": "exercises-16-2-8",
  "level": "2",
  "url": "s-fields.html#exercises-16-2-8",
  "type": "Exercise",
  "number": "16.2.7",
  "title": "",
  "body": "Determine all solutions to the following equations over . That is, find all elements of that satisfy the equations.             0 and 1  1  1  none   "
},
{
  "id": "exercises-16-2-9",
  "level": "2",
  "url": "s-fields.html#exercises-16-2-9",
  "type": "Exercise",
  "number": "16.2.8",
  "title": "",
  "body": "Determine the number of different fields, if any, of all orders 2 through 15. Wherever possible, describe these fields via a known field. "
},
{
  "id": "exercises-16-2-10",
  "level": "2",
  "url": "s-fields.html#exercises-16-2-10",
  "type": "Exercise",
  "number": "16.2.9",
  "title": "",
  "body": "Let .  Prove that is a field.  Show that is a subfield of . For this reason, is called an extension field of .  Show that all the roots of the equation lie in the extension field .  Do the roots of the equation lie in this field? Explain.   "
},
{
  "id": "s-polynomial-rings",
  "level": "1",
  "url": "s-polynomial-rings.html",
  "type": "Section",
  "number": "16.3",
  "title": "Polynomial Rings",
  "body": " Polynomial Rings  In the previous sections we examined the solutions of a few equations over different rings and fields. To solve the equation over the field of the real numbers means to find all solutions of this equation that are in this particular field . This statement can be replaced as follows: Determine all such that the polynomial is equal to zero when evaluated at . In this section, we will concentrate on the theory of polynomials. We will develop concepts using the general setting of polynomials over rings since results proven over rings are true for fields (and integral domains). The reader should keep in mind that in most cases we are just formalizing concepts that he or she learned in high school algebra over the field of reals.  Polynomial over a Ring  Polynomial over a Ring  the degree of polynomial  the set of all polynomials in over  Let be a ring. A polynomial, , over is an expression of the form where , and . If , then the degree of is . If , then the degree of is undefined, but for convenience we say that . If the degree of is , we write . The set of all polynomials in the indeterminate with coefficients in is denoted by .   The symbol is an object called an indeterminate , which is not an element of the ring .  Note that . The elements of are called constant polynomials, with the nonzero elements of being the polynomials of degree 0.  is called the ground, or base, ring for .  In the definition above, we have written the terms in increasing degree starting with the constant. The ordering of terms can be reversed without changing the polynomial. For example, and are the same polynomial.  A term of the form in a polynomial is understood to be .  It is understood that if , then coefficients of powers of higher than are equal to the zero of the base ring.   Polynomial Addition  Polynomial Addition  Let and be elements in so that and for all i. Let be the maximum of and . Then , where for .  Polynomial Multiplication  Polynomial Multiplication  Let and . Then   The important fact to keep in mind is that addition and multiplication in depends on addition and multiplication in . The powers of merely serve the purpose of place holders. All computations involving coefficients are done over the given ring. Powers of the indeterminate are computed formally applying the rule of adding exponents when multiplying powers.   , , and are all polynomials in . Their degrees are 0, 2, and 4, respectively.  Addition and multiplication of polynomials are performed as in high school algebra. However, we must do our computations in the ground ring of the polynomials.  In , if and , then and However, for the same polynomials as above, and in the more familiar setting of , we have and   Let and . We will compute in . Of course this product can be obtained by the usual methods of high school algebra. We will, for illustrative purposes, use the above definition. Using the notation of the above definition, , , , , , and . We want to compute the coefficients , , , , and . We will compute , the coefficient of the term of the product, and leave the remainder to the reader (see Exercise 2 of this section). Since the degrees of both factors is 2, for . The coefficient of is  The proofs of the following theorem are not difficult but rather long, so we omit them.  Properties of Polynomial Rings Let be a ring. Then:  is a ring under the operations of polynomial addition and multiplication.  If is a commutative ring, then is a commutative ring.  If is a ring with unity, , then is a ring with unity (the unity in is ).  If is an integral domain, then is an integral domain.  If is a field, then is not a field. However, is an integral domain.    Next we turn to division of polynomials, which is not an operation since the result is a pair of polynomials, not a single one. From high school algebra we all learned the standard procedure for dividing a polynomial by a second polynomial . This process of polynomial long division is referred to as the division property for polynomials. Under this scheme we continue to divide until the result is a quotient and a remainder whose degree is strictly less than that of the divisor . This property is valid over any field. Before giving a formal description, we consider some examples.  Polynomial Division Let and be two polynomials in . Let us divide by . Keep in mind that we are in and that, in particular, in . This is a case where reordering the terms in decreasing degree is preferred.    fig-poly-divison-1    Therefore, or equivalently, That is, where and . Notice that , which is strictly less than .   Let and be polynomials in . Let us divide by :    fig-poly-divison-2    Thus . Since we have 0 as a remainder, must be a factor of . Also, since is a factor of , 1 is a zero (or root) of . Of course we could have determined that 1 is a root of simply by computing .  Before we summarize the main results suggested by the previous examples, we should probably consider what could have happened if we had attempted to perform divisions of polynomials in the ring rather than in the polynomials over the field . For example, and are both elements of the ring , yet The quotient and remainder are not a polynomials over but polynomials over the field of rational numbers. For this reason it would be wise to describe all results over a field rather than over an arbitrary ring so that we don't have to expand our possible set of coefficients.  Division Property for Polynomials  Division Property for Polynomials  Let be a field and let and be two elements of with . Then there exist unique polynomials and in such that , where . This theorem can be proven by induction on .  The Factor Theorem  Factor Theorem  Let be a field. An element is a zero of if and only if is a factor of in .   Assume that is a zero of . We wish to show that is a factor of . To do so, apply the division property to and . Hence, there exist unique polynomials and from such that and the , so , that is, is a constant. Also, the fact that is a zero of means that . So becomes . Hence , so , and is a factor of . The reader should note that a critical point of the proof of this half of the theorem was the part of the division property that stated that .  We leave this half to the reader as an exercise.    A nonzero polynomial of degree can have at most zeros. Let be a zero of . Then , , by the Factor Theorem. If is a zero of , then again by Factor Theorem, , . Continue this process, which must terminate in at most steps since the degree of would be .  From , we can get yet another insight into the problems associated with solving polynomial equations; that is, finding the zeros of a polynomial. The initial important idea here is that the zero is from the ground field . Second, is a zero only if is a factor of in ; that is, only when can be factored (or reduced) to the product of times some other polynomial in .  Consider the polynomial taken as being in [x]. From high school algebra we know that has two zeros (or roots), namely , and can be factored as . However, we are working in , these two factors are not in the set of polynomials over the rational numbers, since . Therefore, does not have a zero in since it cannot be factored over . When this happens, we say that the polynomial is irreducible over .  The problem of factoring polynomials is tied hand-in-hand with that of the reducibility of polynomials. We give a precise definition of this concept.  Reducibility over a Field  Irreducibility of a Polynomial  Reducible Polynomial  Polynomial Irreducible  Let be a field and let be a nonconstant polynomial. is reducible over if and only if it can be factored as a product of two nonconstant polynomials in . A polynomial is irreducible over if it is not reducible over .   The polynomial is reducible over since  Is the polynomial reducible over ? Since a factorization of a cubic polynomial can only be as a product of linear and quadratic factors, or as a product of three linear factors, is reducible if and only if it has at least one linear factor. From the Factor Theorem, is a factor of over if and only if is a zero of . So is reducible over if and only if it has a zero in . Since has only two elements, 0 and 1, this is easy enough to check. and , so neither 0 nor 1 is a zero of over . Hence, is irreducible over .  From high school algebra we know that has three zeros from some field. Can we find this field? To be more precise, can we construct the field that contains and all zeros of ? We will consider this task in the next section.  We close this section with a final analogy. Prime numbers play an important role in mathematics. The concept of irreducible polynomials (over a field) is analogous to that of a prime number. Just think of the definition of a prime number. A useful fact concerning primes is: If is a prime and if , then or . We leave it to the reader to think about the veracity of the following: If is an irreducible polynomial over , and , then or .   Exercises  Let and . Compute the following sums and products in the indicated rings.   and in   and in   in   in   in    ,  ,          Complete the calculations started in .  Prove that:  The ring is a subring of the ring .  The ring is a subring of the .  The ring is a subring of the ring .     If , and are in since is a ring in its own right. Therefore, is a subring of . The proofs of parts b and c are similar.     Find all zeros of in .  Find all zeros of in .    Determine which of the following are reducible over . Explain.     .   .   . (Be careful.)     Reducible,  Reducible,  Irreducible. If you could factor this polynomial, one factor would be either or , which would give you a root of 0 or 1, respectively. By substitution of 0 and 1 into this polynomial, it clearly has no roots.  Reducible,    Prove the second half of .  Give an example of the contention made in the last paragraph of this section. We illustrate this property of polynomials by showing that it is not true for a nonprime polynomial in . Suppose that , which can be reduced to , , and . Since , . However, is not a factor of either or .  Determine all zeros of in .  Show that is irreducible over but reducible over the field of real numbers. The only possible proper factors of are and , which are not in but are in [x].  The definition of a vector space given in Chapter 13 holds over any field , not just over the field of real numbers, where the elements of are called scalars.  Show that is a vector space over .  Find a basis for over .  What is the dimension of over ?    Prove , the Division Property for Polynomials For , let be the proposition: For all and with , there exist unique polynomials and such that , and either or .  Basis: is true, for if has degree 0, it is a nonzero constant, and so either if is not a constant, or if is also a constant.  Induction: Assume that for some , is true for all , If has degree , then there are two cases to consider. If , , and we are done. Otherwise, if , we perform long division as follows, where LDT's stand for terms of lower degree than .  Therefore, Since is less than , we can apply the induction hypothesis: with .  Therefore, with . This establishes the existence of a quotient and remainder. The uniqueness of and as stated in the theorem is proven as follows: if is also equal to with deg , then Since , the degree of both sides of the last equation is less than . Therefore, it must be that , or And so .     Show that the field of real numbers is a vector space over . Find a basis for this vector space. What is dim over ?  Repeat part a for an arbitrary field F.  Show that is a vector space over .     "
},
{
  "id": "def-polynomial-over-r",
  "level": "2",
  "url": "s-polynomial-rings.html#def-polynomial-over-r",
  "type": "Definition",
  "number": "16.3.1",
  "title": "Polynomial over a Ring.",
  "body": "Polynomial over a Ring  Polynomial over a Ring  the degree of polynomial  the set of all polynomials in over  Let be a ring. A polynomial, , over is an expression of the form where , and . If , then the degree of is . If , then the degree of is undefined, but for convenience we say that . If the degree of is , we write . The set of all polynomials in the indeterminate with coefficients in is denoted by . "
},
{
  "id": "s-polynomial-rings-4",
  "level": "2",
  "url": "s-polynomial-rings.html#s-polynomial-rings-4",
  "type": "Note",
  "number": "16.3.2",
  "title": "",
  "body": " The symbol is an object called an indeterminate , which is not an element of the ring .  Note that . The elements of are called constant polynomials, with the nonzero elements of being the polynomials of degree 0.  is called the ground, or base, ring for .  In the definition above, we have written the terms in increasing degree starting with the constant. The ordering of terms can be reversed without changing the polynomial. For example, and are the same polynomial.  A term of the form in a polynomial is understood to be .  It is understood that if , then coefficients of powers of higher than are equal to the zero of the base ring.  "
},
{
  "id": "def-polynomial-addition",
  "level": "2",
  "url": "s-polynomial-rings.html#def-polynomial-addition",
  "type": "Definition",
  "number": "16.3.3",
  "title": "Polynomial Addition.",
  "body": "Polynomial Addition  Polynomial Addition  Let and be elements in so that and for all i. Let be the maximum of and . Then , where for . "
},
{
  "id": "def-polynomial-multiplication",
  "level": "2",
  "url": "s-polynomial-rings.html#def-polynomial-multiplication",
  "type": "Definition",
  "number": "16.3.4",
  "title": "Polynomial Multiplication.",
  "body": "Polynomial Multiplication  Polynomial Multiplication  Let and . Then  "
},
{
  "id": "ex-some-polynomials",
  "level": "2",
  "url": "s-polynomial-rings.html#ex-some-polynomials",
  "type": "Example",
  "number": "16.3.5",
  "title": "",
  "body": " , , and are all polynomials in . Their degrees are 0, 2, and 4, respectively. "
},
{
  "id": "ex-polynomial-operations",
  "level": "2",
  "url": "s-polynomial-rings.html#ex-polynomial-operations",
  "type": "Example",
  "number": "16.3.6",
  "title": "",
  "body": "In , if and , then and However, for the same polynomials as above, and in the more familiar setting of , we have and  "
},
{
  "id": "ex-polyomial-mult",
  "level": "2",
  "url": "s-polynomial-rings.html#ex-polyomial-mult",
  "type": "Example",
  "number": "16.3.7",
  "title": "",
  "body": "Let and . We will compute in . Of course this product can be obtained by the usual methods of high school algebra. We will, for illustrative purposes, use the above definition. Using the notation of the above definition, , , , , , and . We want to compute the coefficients , , , , and . We will compute , the coefficient of the term of the product, and leave the remainder to the reader (see Exercise 2 of this section). Since the degrees of both factors is 2, for . The coefficient of is "
},
{
  "id": "theorem-polynomial-ring-properties",
  "level": "2",
  "url": "s-polynomial-rings.html#theorem-polynomial-ring-properties",
  "type": "Theorem",
  "number": "16.3.8",
  "title": "Properties of Polynomial Rings.",
  "body": "Properties of Polynomial Rings Let be a ring. Then:  is a ring under the operations of polynomial addition and multiplication.  If is a commutative ring, then is a commutative ring.  If is a ring with unity, , then is a ring with unity (the unity in is ).  If is an integral domain, then is an integral domain.  If is a field, then is not a field. However, is an integral domain.   "
},
{
  "id": "ex-poly-division-1",
  "level": "2",
  "url": "s-polynomial-rings.html#ex-poly-division-1",
  "type": "Example",
  "number": "16.3.9",
  "title": "Polynomial Division.",
  "body": "Polynomial Division Let and be two polynomials in . Let us divide by . Keep in mind that we are in and that, in particular, in . This is a case where reordering the terms in decreasing degree is preferred.    fig-poly-divison-1    Therefore, or equivalently, That is, where and . Notice that , which is strictly less than . "
},
{
  "id": "ex-poly-division-2",
  "level": "2",
  "url": "s-polynomial-rings.html#ex-poly-division-2",
  "type": "Example",
  "number": "16.3.11",
  "title": "",
  "body": " Let and be polynomials in . Let us divide by :    fig-poly-divison-2    Thus . Since we have 0 as a remainder, must be a factor of . Also, since is a factor of , 1 is a zero (or root) of . Of course we could have determined that 1 is a root of simply by computing . "
},
{
  "id": "theorem-poly-divison-property",
  "level": "2",
  "url": "s-polynomial-rings.html#theorem-poly-divison-property",
  "type": "Theorem",
  "number": "16.3.13",
  "title": "Division Property for Polynomials.",
  "body": "Division Property for Polynomials  Division Property for Polynomials  Let be a field and let and be two elements of with . Then there exist unique polynomials and in such that , where . This theorem can be proven by induction on . "
},
{
  "id": "theorem-polynomial-factor",
  "level": "2",
  "url": "s-polynomial-rings.html#theorem-polynomial-factor",
  "type": "Theorem",
  "number": "16.3.14",
  "title": "The Factor Theorem.",
  "body": "The Factor Theorem  Factor Theorem  Let be a field. An element is a zero of if and only if is a factor of in .   Assume that is a zero of . We wish to show that is a factor of . To do so, apply the division property to and . Hence, there exist unique polynomials and from such that and the , so , that is, is a constant. Also, the fact that is a zero of means that . So becomes . Hence , so , and is a factor of . The reader should note that a critical point of the proof of this half of the theorem was the part of the division property that stated that .  We leave this half to the reader as an exercise.  "
},
{
  "id": "theorem-max-zeros",
  "level": "2",
  "url": "s-polynomial-rings.html#theorem-max-zeros",
  "type": "Theorem",
  "number": "16.3.15",
  "title": "",
  "body": " A nonzero polynomial of degree can have at most zeros. Let be a zero of . Then , , by the Factor Theorem. If is a zero of , then again by Factor Theorem, , . Continue this process, which must terminate in at most steps since the degree of would be . "
},
{
  "id": "ex-hs-factorization-revisited",
  "level": "2",
  "url": "s-polynomial-rings.html#ex-hs-factorization-revisited",
  "type": "Example",
  "number": "16.3.16",
  "title": "",
  "body": "Consider the polynomial taken as being in [x]. From high school algebra we know that has two zeros (or roots), namely , and can be factored as . However, we are working in , these two factors are not in the set of polynomials over the rational numbers, since . Therefore, does not have a zero in since it cannot be factored over . When this happens, we say that the polynomial is irreducible over . "
},
{
  "id": "def-reducible-over-field",
  "level": "2",
  "url": "s-polynomial-rings.html#def-reducible-over-field",
  "type": "Definition",
  "number": "16.3.17",
  "title": "Reducibility over a Field.",
  "body": "Reducibility over a Field  Irreducibility of a Polynomial  Reducible Polynomial  Polynomial Irreducible  Let be a field and let be a nonconstant polynomial. is reducible over if and only if it can be factored as a product of two nonconstant polynomials in . A polynomial is irreducible over if it is not reducible over . "
},
{
  "id": "ex-reducible-over-z2",
  "level": "2",
  "url": "s-polynomial-rings.html#ex-reducible-over-z2",
  "type": "Example",
  "number": "16.3.18",
  "title": "",
  "body": " The polynomial is reducible over since "
},
{
  "id": "ex-irreducible-over-z2",
  "level": "2",
  "url": "s-polynomial-rings.html#ex-irreducible-over-z2",
  "type": "Example",
  "number": "16.3.19",
  "title": "",
  "body": "Is the polynomial reducible over ? Since a factorization of a cubic polynomial can only be as a product of linear and quadratic factors, or as a product of three linear factors, is reducible if and only if it has at least one linear factor. From the Factor Theorem, is a factor of over if and only if is a zero of . So is reducible over if and only if it has a zero in . Since has only two elements, 0 and 1, this is easy enough to check. and , so neither 0 nor 1 is a zero of over . Hence, is irreducible over . "
},
{
  "id": "exercises-16-3-2",
  "level": "2",
  "url": "s-polynomial-rings.html#exercises-16-3-2",
  "type": "Exercise",
  "number": "16.3.1",
  "title": "",
  "body": "Let and . Compute the following sums and products in the indicated rings.   and in   and in   in   in   in    ,  ,         "
},
{
  "id": "exercises-16-3-3",
  "level": "2",
  "url": "s-polynomial-rings.html#exercises-16-3-3",
  "type": "Exercise",
  "number": "16.3.2",
  "title": "",
  "body": "Complete the calculations started in . "
},
{
  "id": "exercises-16-3-4",
  "level": "2",
  "url": "s-polynomial-rings.html#exercises-16-3-4",
  "type": "Exercise",
  "number": "16.3.3",
  "title": "",
  "body": "Prove that:  The ring is a subring of the ring .  The ring is a subring of the .  The ring is a subring of the ring .     If , and are in since is a ring in its own right. Therefore, is a subring of . The proofs of parts b and c are similar.   "
},
{
  "id": "exercises-16-3-5",
  "level": "2",
  "url": "s-polynomial-rings.html#exercises-16-3-5",
  "type": "Exercise",
  "number": "16.3.4",
  "title": "",
  "body": " Find all zeros of in .  Find all zeros of in .   "
},
{
  "id": "exercises-16-3-6",
  "level": "2",
  "url": "s-polynomial-rings.html#exercises-16-3-6",
  "type": "Exercise",
  "number": "16.3.5",
  "title": "",
  "body": "Determine which of the following are reducible over . Explain.     .   .   . (Be careful.)     Reducible,  Reducible,  Irreducible. If you could factor this polynomial, one factor would be either or , which would give you a root of 0 or 1, respectively. By substitution of 0 and 1 into this polynomial, it clearly has no roots.  Reducible,   "
},
{
  "id": "exercises-16-3-7",
  "level": "2",
  "url": "s-polynomial-rings.html#exercises-16-3-7",
  "type": "Exercise",
  "number": "16.3.6",
  "title": "",
  "body": "Prove the second half of . "
},
{
  "id": "exercises-16-3-8",
  "level": "2",
  "url": "s-polynomial-rings.html#exercises-16-3-8",
  "type": "Exercise",
  "number": "16.3.7",
  "title": "",
  "body": "Give an example of the contention made in the last paragraph of this section. We illustrate this property of polynomials by showing that it is not true for a nonprime polynomial in . Suppose that , which can be reduced to , , and . Since , . However, is not a factor of either or . "
},
{
  "id": "exercises-16-3-9",
  "level": "2",
  "url": "s-polynomial-rings.html#exercises-16-3-9",
  "type": "Exercise",
  "number": "16.3.8",
  "title": "",
  "body": "Determine all zeros of in . "
},
{
  "id": "exercises-16-3-10",
  "level": "2",
  "url": "s-polynomial-rings.html#exercises-16-3-10",
  "type": "Exercise",
  "number": "16.3.9",
  "title": "",
  "body": "Show that is irreducible over but reducible over the field of real numbers. The only possible proper factors of are and , which are not in but are in [x]. "
},
{
  "id": "exercises-16-3-11",
  "level": "2",
  "url": "s-polynomial-rings.html#exercises-16-3-11",
  "type": "Exercise",
  "number": "16.3.10",
  "title": "",
  "body": "The definition of a vector space given in Chapter 13 holds over any field , not just over the field of real numbers, where the elements of are called scalars.  Show that is a vector space over .  Find a basis for over .  What is the dimension of over ?   "
},
{
  "id": "exercises-16-3-12",
  "level": "2",
  "url": "s-polynomial-rings.html#exercises-16-3-12",
  "type": "Exercise",
  "number": "16.3.11",
  "title": "",
  "body": "Prove , the Division Property for Polynomials For , let be the proposition: For all and with , there exist unique polynomials and such that , and either or .  Basis: is true, for if has degree 0, it is a nonzero constant, and so either if is not a constant, or if is also a constant.  Induction: Assume that for some , is true for all , If has degree , then there are two cases to consider. If , , and we are done. Otherwise, if , we perform long division as follows, where LDT's stand for terms of lower degree than .  Therefore, Since is less than , we can apply the induction hypothesis: with .  Therefore, with . This establishes the existence of a quotient and remainder. The uniqueness of and as stated in the theorem is proven as follows: if is also equal to with deg , then Since , the degree of both sides of the last equation is less than . Therefore, it must be that , or And so .  "
},
{
  "id": "exercises-16-3-13",
  "level": "2",
  "url": "s-polynomial-rings.html#exercises-16-3-13",
  "type": "Exercise",
  "number": "16.3.12",
  "title": "",
  "body": "  Show that the field of real numbers is a vector space over . Find a basis for this vector space. What is dim over ?  Repeat part a for an arbitrary field F.  Show that is a vector space over .   "
},
{
  "id": "s-field-extensions",
  "level": "1",
  "url": "s-field-extensions.html",
  "type": "Section",
  "number": "16.4",
  "title": "Field Extensions",
  "body": " Field Extensions  From high school algebra we realize that to solve a polynomial equation means to find its roots (or, equivalently, to find the zeros of the polynomials). From and we know that the zeros may not lie in the given ground field. Hence, to solve a polynomial really involves two steps: first, find the zeros, and second, find the field in which the zeros lie. For economy's sake we would like this field to be the smallest field that contains all the zeros of the given polynomial. To illustrate this concept, let us reconsider the examples from the previous section..  Extending the Rational Numbers  Let . It is important to remember that we are considering over , no other field. We would like to find all zeros of and the smallest field, call it for now, that contains them. The zeros are , neither of which is an element of . The set we are looking for must satisfy the conditions:   must be a field.   must contain as a subfield,   must contain all zeros of   By the last condition must be an element of , and, if is to be a field, the sum, product, difference, and quotient of elements in must be in . So operations involving this number, such as , , , , , and must all be elements of . Further, since contains as a subset, any element of combined with under any field operation must be an element of . Hence, every element of the form , where and can be any elements in , is an element of . We leave to the reader to show that is a field (see Exercise 1 of this section). We note that the second zero of , namely , is an element of this set. To see this, simply take and . The field is frequently denoted as , and it is referred to as an extension field of . Note that the polynomial factors into linear factors, or splits , in ; that is, all coefficients of both factors are elements of the field .  Extending Consider the polynomial . Let's repeat the steps from the previous example to factor . First, and , so none of the elements of are zeros of . Hence, the zeros of must lie in an extension field of . By , can have at most two zeros. Let be a zero of . Then the extension field of must contain, besides , , , , , and so on. But, since , we have , or equivalently, (remember, we are working in an extension of ). We can use this recurrence relation to reduce powers of . So far our extension field, , of must contain the set , and we claim that this the complete extension. For to be a field, all possible sums, products, and differences of elements in must be in . Let's try a few: Since , , which is in . Adding three 's together doesn't give us anything new: In fact, is in for all possible positive integers . Next, Therefore, and .  It is not difficult to see that is in for all positive . Does contain all zeros of ? Remember, can have at most two distinct zeros and we called one of them , so if there is a second, it must be . To see if is indeed a zero of , simply compute :   Therefore, is also a zero of . Hence, is the smallest field that contains as a subfield and contains all zeros of . This extension field is denoted by . Note that splits in ; that is, it factors into linear factors in . We also observe that is a field containing exactly four elements. By , we expected that would be of order for some prime and positive integer . Also recall that all fields of order are isomorphic. Hence, we have described all fields of order by finding the extension field of a polynomial that is irreducible over .  The reader might feel somewhat uncomfortable with the results obtained in . In particular, what is ? Can we describe it through a known quantity? All we know about is that it is a zero of and that . We could also say that , but we really expected more. However, should we expect more? In , is a number we are more comfortable with, but all we really know about it is that is the number such that . Similarly, the zero that the reader will obtain in Exercise 2 of this section is the imaginary number . Here again, this is simply a symbol, and all we know about it is that . Hence, the result obtained in is not really that strange.  The reader should be aware that we have just scratched the surface in the development of topics in polynomial rings. One area of significant applications is in coding theory.  An Error Correcting Polynomial Code  Polynomial Code  Code Polynomial  An important observation regarding the previous example is that the nonzero elements of can be represented two ways. First as a linear combination of 1 and . There are four such linear combinations, one of which is zero. Second, as powers of . There are three distinct powers and the each match up with a nonzero linear combination:   Next, we briefly describe the field and how an error correcting code can be build on a the same observation about that field.  First, we start with the irreducible polynomial over . There is another such cubic polynomial, but its choice produces essentially the same result. Just as we did in the previous example, we assume we have a zero of and call it . Since we have assumed that , we get the recurrence relation for powers that lets us reduce the seven powers , , to linear combinations of 1, , and . Higher powers will reduce to these seven, which make up the elements of a field with elements when we add zero to the set. We leave as an exercise for you to set up a table relating powers of with the linear combinations.  With this information we are now in a position to take blocks of four bits and encode them with three parity bits to create an error correcting code. If the bits are , then we reduce the expression using the recurrence relation to an expression . Since we are equating equals within , we have , or . The encoded message is , which is a representation of 0 in . If the transmitted sequence of bits is received as we reduce using the recurrence. If there was no transmission error, the result is zero. If the reduced result is zero it is most likely that the original message was . If bit is switched in the transmission, then Therefore if we reduce with the recurrence, we get the linear combination of 1, , and that is equal to and so we can identify the location of the error and correct it.   Exercises   Use the definition of a field to show that is a field.  Use the definition of vector space to show that is a vector space over .  Prove that is a basis for the vector space over , and, therefore, the dimension of over is 2.   If is nonzero, then it has a multiplicative inverse: The denominator, , is nonzero since is irrational. Since and are both rational numbers, is a unit of . The field containing is denoted and so    Determine the splitting field of over . This means consider the polynomial and find the smallest field that contains and all the zeros of . Denote this field by .   is more commonly referred to by a different name. What is it?  Show that is a basis for the vector space over . What is the dimension of this vector space (over )?    Determine the splitting field of over . has zeros and .  contains the zeros but does not contain , since neither are expressible in the form . If we consider the set , then this field contains as well as , and is denoted . Taking into account the form of and in the description above, we can expand to    Factor into linear factors in .  Write out the field tables for the field and compare the results to the tables of .  Cite a theorem and use it to show why the results of part b were to be expected.     Show that is irreducible over .  Determine the splitting field of over .  By , you have described all fields of order .      is reducible if and only if it has a factor of the form . By , is a factor if and only if is a zero. Neither 0 nor 1 is a zero of over .  Since is irreducible over , all zeros of must lie in an extension field of . Let c be a zero of . can be described several different ways. One way is to note that since , for all n. Therefore, includes 0, , , . But since . Furthermore, , , , and . Higher powers of repeat preceding powers. Therefore,   The three zeros of are , and .  Cite Theorem , part 3.     List all polynomials of degree 1, 2, 3, and 4 over .  From your list in part a, identify all irreducible polynomials of degree 1, 2, 3, and 4.  Determine the splitting fields of each of the polynomials in part b.  What is the order of each of the splitting fields obtained in part c? Explain your results using .     Is the polynomial code described in this section a linear code?    "
},
{
  "id": "ex-irreducible-over-Q-again",
  "level": "2",
  "url": "s-field-extensions.html#ex-irreducible-over-Q-again",
  "type": "Example",
  "number": "16.4.1",
  "title": "Extending the Rational Numbers.",
  "body": "Extending the Rational Numbers  Let . It is important to remember that we are considering over , no other field. We would like to find all zeros of and the smallest field, call it for now, that contains them. The zeros are , neither of which is an element of . The set we are looking for must satisfy the conditions:   must be a field.   must contain as a subfield,   must contain all zeros of   By the last condition must be an element of , and, if is to be a field, the sum, product, difference, and quotient of elements in must be in . So operations involving this number, such as , , , , , and must all be elements of . Further, since contains as a subset, any element of combined with under any field operation must be an element of . Hence, every element of the form , where and can be any elements in , is an element of . We leave to the reader to show that is a field (see Exercise 1 of this section). We note that the second zero of , namely , is an element of this set. To see this, simply take and . The field is frequently denoted as , and it is referred to as an extension field of . Note that the polynomial factors into linear factors, or splits , in ; that is, all coefficients of both factors are elements of the field . "
},
{
  "id": "ex-extension-of-z2",
  "level": "2",
  "url": "s-field-extensions.html#ex-extension-of-z2",
  "type": "Example",
  "number": "16.4.2",
  "title": "Extending <span xmlns:pf=\"https:\/\/prefigure.org\" class=\"process-math\">\\(\\mathbb{Z}_2\\)<\/span>.",
  "body": "Extending Consider the polynomial . Let's repeat the steps from the previous example to factor . First, and , so none of the elements of are zeros of . Hence, the zeros of must lie in an extension field of . By , can have at most two zeros. Let be a zero of . Then the extension field of must contain, besides , , , , , and so on. But, since , we have , or equivalently, (remember, we are working in an extension of ). We can use this recurrence relation to reduce powers of . So far our extension field, , of must contain the set , and we claim that this the complete extension. For to be a field, all possible sums, products, and differences of elements in must be in . Let's try a few: Since , , which is in . Adding three 's together doesn't give us anything new: In fact, is in for all possible positive integers . Next, Therefore, and .  It is not difficult to see that is in for all positive . Does contain all zeros of ? Remember, can have at most two distinct zeros and we called one of them , so if there is a second, it must be . To see if is indeed a zero of , simply compute :   Therefore, is also a zero of . Hence, is the smallest field that contains as a subfield and contains all zeros of . This extension field is denoted by . Note that splits in ; that is, it factors into linear factors in . We also observe that is a field containing exactly four elements. By , we expected that would be of order for some prime and positive integer . Also recall that all fields of order are isomorphic. Hence, we have described all fields of order by finding the extension field of a polynomial that is irreducible over . "
},
{
  "id": "ex-ex-polynomial-code",
  "level": "2",
  "url": "s-field-extensions.html#ex-ex-polynomial-code",
  "type": "Example",
  "number": "16.4.3",
  "title": "An Error Correcting Polynomial Code.",
  "body": "An Error Correcting Polynomial Code  Polynomial Code  Code Polynomial  An important observation regarding the previous example is that the nonzero elements of can be represented two ways. First as a linear combination of 1 and . There are four such linear combinations, one of which is zero. Second, as powers of . There are three distinct powers and the each match up with a nonzero linear combination:   Next, we briefly describe the field and how an error correcting code can be build on a the same observation about that field.  First, we start with the irreducible polynomial over . There is another such cubic polynomial, but its choice produces essentially the same result. Just as we did in the previous example, we assume we have a zero of and call it . Since we have assumed that , we get the recurrence relation for powers that lets us reduce the seven powers , , to linear combinations of 1, , and . Higher powers will reduce to these seven, which make up the elements of a field with elements when we add zero to the set. We leave as an exercise for you to set up a table relating powers of with the linear combinations.  With this information we are now in a position to take blocks of four bits and encode them with three parity bits to create an error correcting code. If the bits are , then we reduce the expression using the recurrence relation to an expression . Since we are equating equals within , we have , or . The encoded message is , which is a representation of 0 in . If the transmitted sequence of bits is received as we reduce using the recurrence. If there was no transmission error, the result is zero. If the reduced result is zero it is most likely that the original message was . If bit is switched in the transmission, then Therefore if we reduce with the recurrence, we get the linear combination of 1, , and that is equal to and so we can identify the location of the error and correct it. "
},
{
  "id": "exercises-16-4-2",
  "level": "2",
  "url": "s-field-extensions.html#exercises-16-4-2",
  "type": "Exercise",
  "number": "16.4.1",
  "title": "",
  "body": " Use the definition of a field to show that is a field.  Use the definition of vector space to show that is a vector space over .  Prove that is a basis for the vector space over , and, therefore, the dimension of over is 2.   If is nonzero, then it has a multiplicative inverse: The denominator, , is nonzero since is irrational. Since and are both rational numbers, is a unit of . The field containing is denoted and so  "
},
{
  "id": "exercises-16-4-3",
  "level": "2",
  "url": "s-field-extensions.html#exercises-16-4-3",
  "type": "Exercise",
  "number": "16.4.2",
  "title": "",
  "body": " Determine the splitting field of over . This means consider the polynomial and find the smallest field that contains and all the zeros of . Denote this field by .   is more commonly referred to by a different name. What is it?  Show that is a basis for the vector space over . What is the dimension of this vector space (over )?   "
},
{
  "id": "exercises-16-4-4",
  "level": "2",
  "url": "s-field-extensions.html#exercises-16-4-4",
  "type": "Exercise",
  "number": "16.4.3",
  "title": "",
  "body": "Determine the splitting field of over . has zeros and .  contains the zeros but does not contain , since neither are expressible in the form . If we consider the set , then this field contains as well as , and is denoted . Taking into account the form of and in the description above, we can expand to  "
},
{
  "id": "exercises-16-4-5",
  "level": "2",
  "url": "s-field-extensions.html#exercises-16-4-5",
  "type": "Exercise",
  "number": "16.4.4",
  "title": "",
  "body": " Factor into linear factors in .  Write out the field tables for the field and compare the results to the tables of .  Cite a theorem and use it to show why the results of part b were to be expected.   "
},
{
  "id": "exercises-16-4-6",
  "level": "2",
  "url": "s-field-extensions.html#exercises-16-4-6",
  "type": "Exercise",
  "number": "16.4.5",
  "title": "",
  "body": " Show that is irreducible over .  Determine the splitting field of over .  By , you have described all fields of order .      is reducible if and only if it has a factor of the form . By , is a factor if and only if is a zero. Neither 0 nor 1 is a zero of over .  Since is irreducible over , all zeros of must lie in an extension field of . Let c be a zero of . can be described several different ways. One way is to note that since , for all n. Therefore, includes 0, , , . But since . Furthermore, , , , and . Higher powers of repeat preceding powers. Therefore,   The three zeros of are , and .  Cite Theorem , part 3.   "
},
{
  "id": "exercises-16-4-7",
  "level": "2",
  "url": "s-field-extensions.html#exercises-16-4-7",
  "type": "Exercise",
  "number": "16.4.6",
  "title": "",
  "body": " List all polynomials of degree 1, 2, 3, and 4 over .  From your list in part a, identify all irreducible polynomials of degree 1, 2, 3, and 4.  Determine the splitting fields of each of the polynomials in part b.  What is the order of each of the splitting fields obtained in part c? Explain your results using .    "
},
{
  "id": "exercises-16-4-8",
  "level": "2",
  "url": "s-field-extensions.html#exercises-16-4-8",
  "type": "Exercise",
  "number": "16.4.7",
  "title": "",
  "body": "Is the polynomial code described in this section a linear code?  "
},
{
  "id": "s-power-series",
  "level": "1",
  "url": "s-power-series.html",
  "type": "Section",
  "number": "16.5",
  "title": "Power Series",
  "body": " Power Series  Definition  Earlier in this chapter, we found that a polynomial of degree over a ring is an expression of the form where , each of the are elements of and . In Section 8.5 we defined a generating function of a sequence with terms , , as the infinite sum  The main difference between these two expressions, disregarding notation, is that the latter is an infinite expression and the former is a finite expression. In this section we will extend the algebra of polynomials to the algebra of infinite expressions like , which are called power series.  Power Series  Power Series  the set of all powers series in  Let be a ring. A power series over is an expression of the form where . The set of all such expressions is denoted by .   Our first observation in our comparison of and is that every polynomial is a power series and so . This is true because a polynomial of degree in , can be thought of as an infinite expression where for . In addition, we will see that is a ring with subring .  is given a ring structure by defining addition and multiplication on power series as we did in , with the modification that, since we are dealing with infinite expressions, the sums and products will remain infinite expressions that we can determine term by term, as was done in with polynomials.  Power Series Addition Given power series their sum is .  Power Series Multiplication Given power series their product is .  Some Power Series Calcuations  Let be elements in . Let us compute and . First the sum: The product is a bit more involved: We can compute any value of , with the amount of time\/work required increasing as increases.   A closed-form expression for would be desirable. Using techniques from Chapter 8, the formula is , which we leave it to the reader to derive. Hence,    Properties, Units  We have seen that addition and multiplication in is virtually identical to that in . The following theorem parallels , establishing the ring properties of .  Properties of Power Series Let be a ring. Then:  is a ring under the operations of power series addition and multiplication, which depend on the operations in R.  If R is a commutative ring, then is a commutative ring.  If R is a ring with unity, , then is a ring with unity (the unity in R[x] is ).  If R is an integral domain, then is an integral domain.  If F is a field, then is not a field. However, is an integral domain.     We are most interested in the situation when the set of coefficients is a field. The theorem above indicates that when is a field, is an integral domain. A reason that is not a field is the same as one that we can cite for , namely that does not have multiplicative inverse in .  With all of these similarities, one might wonder if the rings of polynomials and power series over a field are isomorphic. It turns out that they are not. The difference between and becomes apparent when one studies which elements are units in each. First we prove that the only units in are the nonzero constants; that is, the nonzero elements of .  Polynomial Units  Polynomial Units  Units of Polynomial Rings  Let be a field. Polynomial is a unit in if and only if it is a nonzero constant polynomial.    Let be a unit in . Then has a multiplicative inverse, call it , such that . Hence, the . But . So , and since the degree of a polynomial is always nonnegative, this can only happen when the . Hence, is a constant, an element of , which is a unit if and only if it is nonzero.   If is a nonzero element of , then it is a unit since is a field. Thus it has an inverse, which is also in and so is a unit of .    Before we proceed to categorize the units in , we remind the reader that two power series and are equal if and only if corresponding coefficients are equal, for all .  Power Series Units  Power Series Units  Units of Power Series Rings  Let be a field. Then is a unit of if and only if .    If is a unit of , then there exists in such that .  Since corresponding coefficients in the equation above must be equal, , which implies that .   Assume that . To prove that is a unit of we need to find in such that . If we use the formula for the coefficients and equate coefficients, we get .  Therefore the powers series is an expression whose coefficients lie in and that satisfies the statement . Hence, is the multiplicative inverse of and is a unit.      Let be an element of . Then, by , since , is a unit and has an inverse, call it . To compute , we follow the procedure outlined in the above theorem. Using the formulas for the s, we obtain For , we have Hence, is the multiplicative inverse of .   Certainly contains a wider variety of units than . Yet is not a field, since is not a unit. So concerning the algebraic structure of , we know that it is an integral domain that contains . If we allow our power series to take on negative exponents; that is, consider expressions of the form where all but a finite number of terms with a negative index equal zero. These expressions are called extended power series. The set of all such expressions is a field, call it . This set does contain, for example, the inverse of , which is . It can be shown that each nonzero element of is a unit.    Exercises  Let and be elements of . Let . Apply basic algebra to to derive the formula for the coefficients of . Hence, to show that    Prove that for any integral domain , the following can be proven: is a unit of if and only if is a unit in .  Compare the statement in part a to that in .  Give an example of the statement in part a in .    Use the formula for the product to verify that the expression of is indeed the inverse of .    Determine the inverse of in .  Repeat part a with taken in .  Use the method outlined in Chapter 8 to show that the power series is the rational generating function . What is the inverse of this function? Compare your results with those in part a.     Determine the inverse of in .  Use the procedures in Chapter 8 to find a rational generating function for in part a. Find the multiplicative inverse of this function.     All other terms are zero. Hence,   The last step follows from the formula for the sum of a geometric series.    Let and both in .  What are the first four terms (counting the constant term as the term) of ?  Find a closed form expression for .  What are the first four terms of ?    Write as an extended power series:         .   .     Derive the closed form expression for the coefficients of the product in .   "
},
{
  "id": "def-power-series",
  "level": "2",
  "url": "s-power-series.html#def-power-series",
  "type": "Definition",
  "number": "16.5.1",
  "title": "Power Series.",
  "body": "Power Series  Power Series  the set of all powers series in  Let be a ring. A power series over is an expression of the form where . The set of all such expressions is denoted by .  "
},
{
  "id": "def-power-series-addition",
  "level": "2",
  "url": "s-power-series.html#def-power-series-addition",
  "type": "Definition",
  "number": "16.5.2",
  "title": "Power Series Addition.",
  "body": "Power Series Addition Given power series their sum is . "
},
{
  "id": "def-power-series-multiplication",
  "level": "2",
  "url": "s-power-series.html#def-power-series-multiplication",
  "type": "Definition",
  "number": "16.5.3",
  "title": "Power Series Multiplication.",
  "body": "Power Series Multiplication Given power series their product is . "
},
{
  "id": "ex-ps-calculations",
  "level": "2",
  "url": "s-power-series.html#ex-ps-calculations",
  "type": "Example",
  "number": "16.5.4",
  "title": "Some Power Series Calcuations.",
  "body": "Some Power Series Calcuations  Let be elements in . Let us compute and . First the sum: The product is a bit more involved: We can compute any value of , with the amount of time\/work required increasing as increases.   A closed-form expression for would be desirable. Using techniques from Chapter 8, the formula is , which we leave it to the reader to derive. Hence,  "
},
{
  "id": "theorem-power-series-properties",
  "level": "2",
  "url": "s-power-series.html#theorem-power-series-properties",
  "type": "Theorem",
  "number": "16.5.5",
  "title": "Properties of Power Series.",
  "body": "Properties of Power Series Let be a ring. Then:  is a ring under the operations of power series addition and multiplication, which depend on the operations in R.  If R is a commutative ring, then is a commutative ring.  If R is a ring with unity, , then is a ring with unity (the unity in R[x] is ).  If R is an integral domain, then is an integral domain.  If F is a field, then is not a field. However, is an integral domain.    "
},
{
  "id": "theorem-polynomial-units",
  "level": "2",
  "url": "s-power-series.html#theorem-polynomial-units",
  "type": "Theorem",
  "number": "16.5.6",
  "title": "Polynomial Units.",
  "body": "Polynomial Units  Polynomial Units  Units of Polynomial Rings  Let be a field. Polynomial is a unit in if and only if it is a nonzero constant polynomial.    Let be a unit in . Then has a multiplicative inverse, call it , such that . Hence, the . But . So , and since the degree of a polynomial is always nonnegative, this can only happen when the . Hence, is a constant, an element of , which is a unit if and only if it is nonzero.   If is a nonzero element of , then it is a unit since is a field. Thus it has an inverse, which is also in and so is a unit of .   "
},
{
  "id": "theorem-power-series-units",
  "level": "2",
  "url": "s-power-series.html#theorem-power-series-units",
  "type": "Theorem",
  "number": "16.5.7",
  "title": "Power Series Units.",
  "body": "Power Series Units  Power Series Units  Units of Power Series Rings  Let be a field. Then is a unit of if and only if .    If is a unit of , then there exists in such that .  Since corresponding coefficients in the equation above must be equal, , which implies that .   Assume that . To prove that is a unit of we need to find in such that . If we use the formula for the coefficients and equate coefficients, we get .  Therefore the powers series is an expression whose coefficients lie in and that satisfies the statement . Hence, is the multiplicative inverse of and is a unit.    "
},
{
  "id": "ex-power-series-inversion",
  "level": "2",
  "url": "s-power-series.html#ex-power-series-inversion",
  "type": "Example",
  "number": "16.5.8",
  "title": "",
  "body": " Let be an element of . Then, by , since , is a unit and has an inverse, call it . To compute , we follow the procedure outlined in the above theorem. Using the formulas for the s, we obtain For , we have Hence, is the multiplicative inverse of .  "
},
{
  "id": "exercises-16-5-2",
  "level": "2",
  "url": "s-power-series.html#exercises-16-5-2",
  "type": "Exercise",
  "number": "16.5.3.1",
  "title": "",
  "body": "Let and be elements of . Let . Apply basic algebra to to derive the formula for the coefficients of . Hence, to show that  "
},
{
  "id": "exercises-16-5-3",
  "level": "2",
  "url": "s-power-series.html#exercises-16-5-3",
  "type": "Exercise",
  "number": "16.5.3.2",
  "title": "",
  "body": " Prove that for any integral domain , the following can be proven: is a unit of if and only if is a unit in .  Compare the statement in part a to that in .  Give an example of the statement in part a in .   "
},
{
  "id": "exercises-16-5-4",
  "level": "2",
  "url": "s-power-series.html#exercises-16-5-4",
  "type": "Exercise",
  "number": "16.5.3.3",
  "title": "",
  "body": "Use the formula for the product to verify that the expression of is indeed the inverse of .  "
},
{
  "id": "exercises-16-5-5",
  "level": "2",
  "url": "s-power-series.html#exercises-16-5-5",
  "type": "Exercise",
  "number": "16.5.3.4",
  "title": "",
  "body": " Determine the inverse of in .  Repeat part a with taken in .  Use the method outlined in Chapter 8 to show that the power series is the rational generating function . What is the inverse of this function? Compare your results with those in part a.   "
},
{
  "id": "exercises-16-5-6",
  "level": "2",
  "url": "s-power-series.html#exercises-16-5-6",
  "type": "Exercise",
  "number": "16.5.3.5",
  "title": "",
  "body": " Determine the inverse of in .  Use the procedures in Chapter 8 to find a rational generating function for in part a. Find the multiplicative inverse of this function.     All other terms are zero. Hence,   The last step follows from the formula for the sum of a geometric series.  "
},
{
  "id": "exercises-16-5-7",
  "level": "2",
  "url": "s-power-series.html#exercises-16-5-7",
  "type": "Exercise",
  "number": "16.5.3.6",
  "title": "",
  "body": " Let and both in .  What are the first four terms (counting the constant term as the term) of ?  Find a closed form expression for .  What are the first four terms of ?   "
},
{
  "id": "exercises-16-5-8",
  "level": "2",
  "url": "s-power-series.html#exercises-16-5-8",
  "type": "Exercise",
  "number": "16.5.3.7",
  "title": "",
  "body": "Write as an extended power series:         .   .    "
},
{
  "id": "exercises-16-5-9",
  "level": "2",
  "url": "s-power-series.html#exercises-16-5-9",
  "type": "Exercise",
  "number": "16.5.3.8",
  "title": "",
  "body": "Derive the closed form expression for the coefficients of the product in . "
},
{
  "id": "sec-expect",
  "level": "1",
  "url": "sec-expect.html",
  "type": "Section",
  "number": "17.1",
  "title": "What can we expect",
  "body": " What can we expect   At its heart, the subject of linear algebra is about linear equations and, more specifically, sets of two or more linear equations. Google routinely deals with a set of trillions of equations each of which has trillions of unknowns. We will eventually understand how to deal with that kind of complexity. To begin, however, we will look at a more familiar situation in which there are a small number of equations and a small number of unknowns. In spite of its relative simplicity, this situation is rich enough to demonstrate some fundamental concepts that will motivate much of our exploration.    Some simple examples    In this activity, we consider sets of linear equations having just two unknowns. In this case, we can graph the solutions sets for the equations, which allows us to visualize different types of behavior.    On the grid below, graph the lines   At what point or points , do the lines intersect? How many points satisfy both equations?       On the grid below, graph the lines   At what point or points , do the lines intersect? How many points satisfy both equations?       On the grid below, graph the line .  How many points satisfy this equation?       On the grid below, graph the lines   At what point or points , do the lines intersect? How many points satisfy all three equations?            The graph of the two lines is as shown on the right. There is a single point, the point , at which the lines intersect. Therefore, there is a single point that satisfies both equations.       These two lines are parallel, which means there is no point at which the lines intersect. Therefore, there is no point that satisfies both equations.       There are infinitely many points that lie on this line and that, therefore, satisfy this single equation.       These three lines do not have a common intersection point. Consequently, there is no point satisfying all three equations.           This is exactly one point, the point , that satisfies both equations.    There are no points that satisfy both equations.    There are infinitely many points.    There are no points that satisfy all three equations.       The examples in this introductory activity demonstrate several possible outcomes for the solutions to a set of linear equations. Notice that we are interested in points that satisfy each equation in the set and that these are seen as intersection points of the lines. Similar to the examples considered in the activity, three types of outcomes are seen in .    Three possible graphs for sets of linear equations in two unknowns.   In this figure, we see that   With a single equation, there are infinitely many points satisfying that equation.  Adding a second equation adds another condition we place on the points resulting in a single point that satisfies both equations.  Adding a third equation adds a third condition on the points , and there is no point that satisfies all three equations.   Generally speaking, a single equation will have many solutions, in fact, infinitely many. As we add equations, we add conditions which lead, in a sense we will make precise later, to a smaller number of solutions. Eventually, we have too many equations and find there are no points that satisfy all of them.  This example illustrates a general principle to which we will frequently return.   Solutions to sets of linear equations  Given a set of linear equations, there are either:   infinitely many points,    exactly one point, or    no points  that satisfy every equation in the set.   Notice that we can see a bit more. In , we are looking at equations in two unknowns. Here we see that   One equation has infinitely many solutions.    Two equations have exactly one solution.   Three equations have no solutions.    It seems reasonable to wonder if the number of solutions depends on whether the number of equations is less than, equal to, or greater than the number of unknowns. Of course, one of the examples in the activity shows that there are exceptions to this simple rule, as seen in . For instance, two equations in two unknowns may correspond to parallel lines so that the set of equations has no solutions. It may also happen that a set of three equations in two unknowns has a single solution. However, it seems safe to think that the more equations we have, the smaller the set of solutions will be.   A set of two equations in two unknowns can have no solutions, and a set of three equations can have one solution.       Let's also consider some examples of equations having three unknowns, which we call , , and . Just as solutions to linear equations in two unknowns formed straight lines, solutions to linear equations in three unknowns form planes.  When we consider an equation in three unknowns graphically, we need to add a third coordinate axis, as shown in .       Coordinate systems in two and three dimensions.   As shown in , a linear equation in two unknowns, such as , is a line while a linear equation in three unknowns, such as , is a plane.       The solutions to the equation in two dimensions and in three.   In three unknowns, the set of solutions to one linear equation forms a plane. The set of solutions to a pair of linear equations is seen graphically as the intersection of the two planes. As in , we typically expect this intersection to be a line.       A single plane and the intersection of two planes.   When we add a third equation, we are looking for the intersection of three planes, which we expect to form a point, as in the left of . However, in certain special cases, it may happen that there are no solutions, as seen on the right.       Two examples showing the intersections of three planes.     This activity considers sets of equations having three unknowns. In this case, we know that the solutions of a single equation form a plane. If it helps with visualization, consider using -inch index cards to represent planes.   Is it possible that there are no solutions to two linear equations in three unknowns? Either sketch an example or state a reason why it can't happen.    Is it possible that there is exactly one solution to two linear equations in three unknowns? Either sketch an example or state a reason why it can't happen.    Is it possible that the solutions to four equations in three unknowns form a line? Either sketch an example or state a reason why it can't happen.    What would you usually expect for the set of solutions to four equations in three unknowns?    Suppose we have a set of 500 linear equations in 10 unknowns. Which of the three possibilities would you expect to hold?   Suppose we have a set of 10 linear equations in 500 unknowns. Which of the three possibilities would you expect to hold?       Yes, it is possible if the two planes are parallel to one another.    No, this is not possible. Two planes will either intersect in a line, if they are not parallel, or not intersect at all, if they are parallel.    Yes, it is possible that four planes intersect in a line. One may sketch four planes that intersect in, say, the -axis.    In general, we would expect there to be no solutions to four equations in three unknowns because there are more equations than unknowns.    Since there are more equations than unknowns, we would expect there to be no solutions. We cannot guarantee this, however.    Since there are fewer equations than unknowns, we would expect there to be infinitely many solutions. We cannot guarantee this, however.        Yes.   No.   Yes.   We would expect there to be no solutions.   We would expect there to be no solutions.   We would expect there to be infinitely many solutions.       Systems of linear equations  Now that we have seen some simple examples, let's agree on some terminology to help us think more precisely about sets of equations.  First, we considered a linear equation having the form . It will be convenient for us to rewrite this so that all the unknowns are on one side of the equation: . More generally, the equation of a line can always be expressed in the form which gives us the flexibility to describe all lines. For instance, vertical lines, such as , may be represented in this form.  Notice that each term on the left is the product of a constant and the first power of an unknown. In the future, we will want to consider equations having many more unknowns, which we will sometimes denote as . This leads to the following definition:     linear equation A linear equation in the unknowns may be written in the form , where are real numbers known as coefficients . We also say that are the variables in the equation.  By a system of linear equations or a  linear system  linear system , we mean a set of linear equations written in a common set of unknowns.   For instance, is an example of a linear system.   solution A solution to a linear system is simply a set of numbers that satisfy all the equations in the system.   For instance, we earlier considered the linear system To check that is a solution, we verify that the following equations are true.     solution space  We call the set of all solutions the solution space of the linear system.    Linear equations and their solutions     Which of the following equations are linear? Please provide a justification for your response.   .   .   .    Consider the system of linear equations:    Is a solution?   Is a solution?    Is a solution?   Can you find a solution in which ?   Do you think there are other solutions? Please explain your response.          There are two linear equations in this list.  This is not a linear equation due to the presence of in the third term on the left.  This is a linear equation.  This is a linear equation since it can be rewritten in the form .     We will see that the system of linear equations has infinitely many solutions.  Yes, this is a solution since all three equations are satisfied if we set , , and .  No, this is not a solution since the first equation is not satisfied if and .  This is also not a solution.  If , then we arrive at the system of three linear equations: We have a solution when and . Therefore, is a solution to the original system of equations.  Since we have found more than one solution to the system of equations, we should expect that there are infinitely many. Therefore, there should be many other solutions.         There are two linear equations in this list.  This is not a linear equation.  This is a linear equation.  This is a linear equation.     We will see that the system of linear equations has infinitely many solutions.  This is a solution.  This is a not solution.  This is a not solution.  Yes, is a solution.  Yes, because there should be infinitely many solutions.          Summary  The point of this section is to build some intuition about the behavior of solutions to linear systems through consideration of some simple examples. We will develop a deeper and more precise understanding of these phenomena in our future explorations.   A linear equation is one that may be written in the form .  A linear system is a set of linear equations and a solution is a set of values assigned to the unknowns that make each equation true.   We came to expect that a linear system has either infinitely many solutions, exactly one solution, or no solutions.   When we add more equations to a system, the solution space usually seems to become smaller.    "
},
{
  "id": "sec-expect-3-2",
  "level": "2",
  "url": "sec-expect.html#sec-expect-3-2",
  "type": "Activity",
  "number": "17.1.1",
  "title": "",
  "body": "  In this activity, we consider sets of linear equations having just two unknowns. In this case, we can graph the solutions sets for the equations, which allows us to visualize different types of behavior.    On the grid below, graph the lines   At what point or points , do the lines intersect? How many points satisfy both equations?       On the grid below, graph the lines   At what point or points , do the lines intersect? How many points satisfy both equations?       On the grid below, graph the line .  How many points satisfy this equation?       On the grid below, graph the lines   At what point or points , do the lines intersect? How many points satisfy all three equations?            The graph of the two lines is as shown on the right. There is a single point, the point , at which the lines intersect. Therefore, there is a single point that satisfies both equations.       These two lines are parallel, which means there is no point at which the lines intersect. Therefore, there is no point that satisfies both equations.       There are infinitely many points that lie on this line and that, therefore, satisfy this single equation.       These three lines do not have a common intersection point. Consequently, there is no point satisfying all three equations.           This is exactly one point, the point , that satisfies both equations.    There are no points that satisfy both equations.    There are infinitely many points.    There are no points that satisfy all three equations.      "
},
{
  "id": "fig-three-possibilities",
  "level": "2",
  "url": "sec-expect.html#fig-three-possibilities",
  "type": "Figure",
  "number": "17.1.1",
  "title": "",
  "body": "  Three possible graphs for sets of linear equations in two unknowns.  "
},
{
  "id": "solution-exceptions",
  "level": "2",
  "url": "sec-expect.html#solution-exceptions",
  "type": "Figure",
  "number": "17.1.2",
  "title": "",
  "body": " A set of two equations in two unknowns can have no solutions, and a set of three equations can have one solution.      "
},
{
  "id": "fig-coordinates",
  "level": "2",
  "url": "sec-expect.html#fig-coordinates",
  "type": "Figure",
  "number": "17.1.3",
  "title": "",
  "body": "     Coordinate systems in two and three dimensions.  "
},
{
  "id": "fig-plane-z0",
  "level": "2",
  "url": "sec-expect.html#fig-plane-z0",
  "type": "Figure",
  "number": "17.1.4",
  "title": "",
  "body": "     The solutions to the equation in two dimensions and in three.  "
},
{
  "id": "fig-two-planes",
  "level": "2",
  "url": "sec-expect.html#fig-two-planes",
  "type": "Figure",
  "number": "17.1.5",
  "title": "",
  "body": "     A single plane and the intersection of two planes.  "
},
{
  "id": "fig-three-planes",
  "level": "2",
  "url": "sec-expect.html#fig-three-planes",
  "type": "Figure",
  "number": "17.1.6",
  "title": "",
  "body": "     Two examples showing the intersections of three planes.  "
},
{
  "id": "sec-expect-3-22",
  "level": "2",
  "url": "sec-expect.html#sec-expect-3-22",
  "type": "Activity",
  "number": "17.1.2",
  "title": "",
  "body": "  This activity considers sets of equations having three unknowns. In this case, we know that the solutions of a single equation form a plane. If it helps with visualization, consider using -inch index cards to represent planes.   Is it possible that there are no solutions to two linear equations in three unknowns? Either sketch an example or state a reason why it can't happen.    Is it possible that there is exactly one solution to two linear equations in three unknowns? Either sketch an example or state a reason why it can't happen.    Is it possible that the solutions to four equations in three unknowns form a line? Either sketch an example or state a reason why it can't happen.    What would you usually expect for the set of solutions to four equations in three unknowns?    Suppose we have a set of 500 linear equations in 10 unknowns. Which of the three possibilities would you expect to hold?   Suppose we have a set of 10 linear equations in 500 unknowns. Which of the three possibilities would you expect to hold?       Yes, it is possible if the two planes are parallel to one another.    No, this is not possible. Two planes will either intersect in a line, if they are not parallel, or not intersect at all, if they are parallel.    Yes, it is possible that four planes intersect in a line. One may sketch four planes that intersect in, say, the -axis.    In general, we would expect there to be no solutions to four equations in three unknowns because there are more equations than unknowns.    Since there are more equations than unknowns, we would expect there to be no solutions. We cannot guarantee this, however.    Since there are fewer equations than unknowns, we would expect there to be infinitely many solutions. We cannot guarantee this, however.        Yes.   No.   Yes.   We would expect there to be no solutions.   We would expect there to be no solutions.   We would expect there to be infinitely many solutions.    "
},
{
  "id": "sec-expect-4-5",
  "level": "2",
  "url": "sec-expect.html#sec-expect-4-5",
  "type": "Definition",
  "number": "17.1.7",
  "title": "",
  "body": "   linear equation A linear equation in the unknowns may be written in the form , where are real numbers known as coefficients . We also say that are the variables in the equation.  By a system of linear equations or a  linear system  linear system , we mean a set of linear equations written in a common set of unknowns.  "
},
{
  "id": "sec-expect-4-7",
  "level": "2",
  "url": "sec-expect.html#sec-expect-4-7",
  "type": "Definition",
  "number": "17.1.8",
  "title": "",
  "body": " solution A solution to a linear system is simply a set of numbers that satisfy all the equations in the system.  "
},
{
  "id": "sec-expect-4-9",
  "level": "2",
  "url": "sec-expect.html#sec-expect-4-9",
  "type": "Definition",
  "number": "17.1.9",
  "title": "",
  "body": "  solution space  We call the set of all solutions the solution space of the linear system.  "
},
{
  "id": "sec-expect-4-10",
  "level": "2",
  "url": "sec-expect.html#sec-expect-4-10",
  "type": "Activity",
  "number": "17.1.3",
  "title": "Linear equations and their solutions.",
  "body": " Linear equations and their solutions     Which of the following equations are linear? Please provide a justification for your response.   .   .   .    Consider the system of linear equations:    Is a solution?   Is a solution?    Is a solution?   Can you find a solution in which ?   Do you think there are other solutions? Please explain your response.          There are two linear equations in this list.  This is not a linear equation due to the presence of in the third term on the left.  This is a linear equation.  This is a linear equation since it can be rewritten in the form .     We will see that the system of linear equations has infinitely many solutions.  Yes, this is a solution since all three equations are satisfied if we set , , and .  No, this is not a solution since the first equation is not satisfied if and .  This is also not a solution.  If , then we arrive at the system of three linear equations: We have a solution when and . Therefore, is a solution to the original system of equations.  Since we have found more than one solution to the system of equations, we should expect that there are infinitely many. Therefore, there should be many other solutions.         There are two linear equations in this list.  This is not a linear equation.  This is a linear equation.  This is a linear equation.     We will see that the system of linear equations has infinitely many solutions.  This is a solution.  This is a not solution.  This is a not solution.  Yes, is a solution.  Yes, because there should be infinitely many solutions.       "
},
{
  "id": "sec-finding-solutions",
  "level": "1",
  "url": "sec-finding-solutions.html",
  "type": "Section",
  "number": "17.2",
  "title": "Finding solutions to linear systems",
  "body": " Finding solutions to linear systems   In the previous section, we looked at systems of linear equations from a graphical perspective. Since the equations had only two or three variables, we could study the solution spaces as the intersections of lines and planes.  Because we will eventually consider systems with many equations and many variables, this graphical approach will not generally be a useful strategy. Instead, we will approach this problem algebraically and develop a technique to describe the solution spaces of general linear systems.    Gaussian elimination   Gaussian elimination We will develop an algorithm, which is usually called Gaussian elimination , that allows us to describe the solution space of a linear system. This algorithm plays a central role in much of what is to come.    In this activity, we will consider some simple examples that will guide us in finding a more general approach.    Give a description of the solution space to the linear system:    Give a description of the solution space to the linear system:    Give a description of the solution space to the linear system:   Describe the solution space to the linear equation .  Describe the solution space to the linear equation .       The equations tell us the value of both variables so there is only one solution .    We first use the third equation to determine that . We next substitute this value into the first two equations to obtain Now we can use the second equation to determine that , substitute that value into the first equation, and determine that . This tells us that the linear system has one solution .    Notice that we can rewrite the first equation as and substitute this into the second equation to obtain This tells us that and . The linear system therefore has one solution .    No matter the value of , we have . Therefore, the solution space to the equation is all real numbers . In other words, this equation does not place a restriction on the value of .    By contrast, the equation has no solutions since no value of , when multiplied by , can produce .        .   .   .  Any real number is a solution.  There are no solutions.     These examples lead to a few observations that motivate a general approach to finding solutions of linear systems.   First, finding the solution space to some systems is simple. For example, because each equation in the following system has only one variable, it prescribes a specific value for that variable. We therefore see that there is exactly one solution, which is . We call such a system decoupled . decoupled system     Second, there is a process that can be used to find solutions to certain types of linear systems. For instance, let's consider the system Multiplying both sides of the last equation by gives us Any solution to this linear system must then have .  Once we know that, we can substitute into the first and second equations and simplify to obtain a new system of equations having the same solutions: The second equation, after multiplying both sides by , tells us that . We can then substitute this value into the first equation to determine that .  In this way, we arrive at a decoupled system, which shows that there is exactly one solution, namely .  Our original system,  triangular system  back substitution is called a triangular system due to the shape formed by the coefficients. As this example demonstrates, triangular systems are easily solved by this process, which is called back substitution .    We can use substitution in a more general way to solve linear systems. For example, a natural approach to the system is to use the first equation to express in terms of : and then substitute this into the second equation and simplify: From here, we can substitute into the first equation to arrive at the solution .  However, the two-step process of solving for in terms of and substituting into the second equation may be performed more efficiently by adding a multiple of the first equation to the second. In this case, we will multiply the first equation by -2 and add to the second equation to obtain     which gives us       In this way, the system is transformed into the new triangular system Notice that this process can be reversed. Beginning with the triangular system, we can recover the original system by multiplying the first equation by 2 and adding it to the second. Because of this, the two systems have the same solution space. We will revisit this point later and give what may be a more convincing explanation.  Of course, the choice to multiply the first equation by -2 was made so that the terms involving in the two equations will cancel leading to a triangular system that can be solved using back substitution.   Based on these observations, we take note of three operations that transform a system of linear equations into a new system of equations having the same solution space. Our goal is to create a new system whose solution space is the same as the original system's and may be easily described.   Scaling  We can multiply one equation by a nonzero number. For instance, has the same set of solutions as or .    Interchange  Interchanging equations will not change the set of solutions. For instance, has the same set of solutions as     Replacement  As we saw above, we may multiply one equation by a real number and add it to another equation. We call this process replacement .       Let's illustrate the use of these operations to find the solution space to the system of equations:   We will first transform the system into a triangular system so we start by eliminating from the second and third equations.  We begin with a replacement operation where we multiply the first equation by -2 and add the result to the second equation.         Another replacement operation eliminates from the third equation. We multiply the first equation by 3 and add to the third.         Scale the second equation by multiplying it by .         Eliminate from the third equation by multiplying the second equation by -4 and adding it to the third. Notice that we now have a triangular system that can be solved using back substitution.         After scaling the third equation by , we have found the value for .         We eliminate from the second equation by multiplying the third equation by -1 and adding to the second.         Finally, multiply the second equation by -2 and add to the first to obtain:       Now that we have arrived at a decoupled system, we know that there is exactly one solution to our original system of equations, which is .    One could find the same result by applying a different sequence of replacement and scaling operations. However, we chose this particular sequence guided by our desire to first transform the system into a triangular one. To do this, we eliminated the first variable from all but one equation and then proceeded to the next variables working left to right. Once we had a triangular system, we used back substitution moving through the variables right to left.  We call this process Gaussian elimination and note that it is our primary tool for solving systems of linear equations.   Gaussian Elimination   For each of the following linear systems, use Gaussian elimination to describe the solutions to the following systems of linear equations. In particular, determine whether each linear system has exactly one solution, infinitely many solutions, or no solutions.                   Our aim is to apply a sequence of scaling, interchange, and replacement operations to first put the system into a triangular form. We begin by multiplying the first equation by and adding it to the second equation. Next, we add the first equation to the third. This leads us to: We will now apply a scaling operation to make the coefficient of equal in the second equation. Another replacement operation brings the system into a triangular form.   From here, we begin the process of back substitution seeking a decoupled system. Finally, we have the decoupled system which tells us that the solution space consists of the single solution .  Once again, we begin with a sequence of replacement and scaling operations that lead to the triangular system Back substitution gives us the system The third equation does not impose a restriction on the solutions since it is satisfied for any . The second equation tells us that must equal ; however, there are infinitely many solutions to the first equation that have . Therefore, this system has infinitely many solutions.  After applying two replacement and one scaling operation, we find Another replacement operation leads to the system Since the third equation has no solutions, the original system can have no solutions as well.      There is a single solution .  There are infinitely many solutions.  There are no solutions.       Augmented matrices  After performing Gaussian elimination a few times, you probably noticed that you spent most of the time concentrating on the coefficients and simply recorded the variables as place holders. Based on this observation, we will introduce a shorthand description of linear systems.   augmented matrix When writing a linear system, we always write the variables in the same order in each equation. We then construct an augmented matrix by simply forgetting about the variables and recording the numerical data in a rectangular array. For instance, the system of equations below has the following augmented matrix          The vertical line reminds us where the equals signs appear in the equations. Entries in the matrix to the left of the vertical line correspond to coefficients of the equations. We sometimes choose to focus only on the coefficients of the system in which case we write the coefficient matrix as coefficient matrix    The three operations we perform on systems of equations translate naturally into operations on matrices. For instance, the replacement operation that multiplies the first equation by 2 and adds it to the second may be performed by multiplying the first row of the augmented matrix by 2 and adding it to the second row:       The symbol between the matrices indicates that the two matrices are related by a sequence of scaling, interchange, and replacement operations. Since these operations act on the rows of the matrices, we say that the matrices are row equivalent . Notice that the linear systems corresponding to two row equivalent augmented matrices have the same solution space. row equivalent    Augmented matrices and solution spaces      Write the augmented matrix for the linear system and perform Gaussian elimination to describe the solution space in as much detail as you can.    Suppose that you have a linear system in the variables and whose augmented matrix is row equivalent to Write the linear system corresponding to this augmented matrix and describe its solution set in as much detail as you can.    Suppose that you have a linear system in the variables and whose augmented matrix is row equivalent to Write the linear system corresponding to this augmented matrix and describe its solution set in as much detail as you can.    Suppose that the augmented matrix of a linear system has the following shape where could be any real number.    How many equations are there in this system and how many variables?   Based on our earlier discussion in , do you think it's possible that this system has exactly one solution, infinitely many solutions, or no solutions?   Suppose that this augmented matrix is row equivalent to Make a choice for the names of the variables and write the corresponding linear system. Does the system have exactly one solution, infinitely many solutions, or no solutions?         The augmented matrix for this linear system is This corresponds to the system of equations showing that there is a single solution .  The corresponding system of equations is The third equation is satisfied for any values of and . Therefore, we see that the only solution to the system is .  Here, the corresponding system of equations is Since the third equation is not satisfied for any values of and , there are no solutions to the system.  The system corresponding to this augmented matrix has three equations and five variables. Our first guess is there are infinitely many solutions. If we write out the equations corresponding to the augmented matrix, we find since the third row of the augmented matrix does not restrict the solution space. From here, we see that there are infinitely many solutions: if we make any choice for the variables , , and , we can find values for and that give a solution.      There is a single solution .  There is a single solution .  There are no solutions.  This system has three equations in five variables, and there are infinitely many solutions.       Reduced row echelon form  There is a special class of matrices whose form makes it especially easy to describe the solution space of the corresponding linear system. As we describe the properties of this class of matrices, it may be helpful to consider an example, such as the following matrix.    reduced row echelon form  We say that a matrix is in reduced row echelon form if the following properties are satisfied.   If the entries in a row are all zero, then the same is true of any row below it.    If we move across a row from left to right, the first nonzero entry we encounter is 1. We call this entry the leading entry in the row.    The leading entry in any row is to the right of the leading entries in all the rows above it.    A leading entry is the only nonzero entry in its column.    reduced row echelon matrix We call a matrix in reduced row echelon form a reduced row echelon matrix .   We have been intentionally vague about whether the matrix we are considering is an augmented matrix corresponding to a linear system or a coefficient matrix since we will consider both possibilities in the future.   Identifying reduced row echelon matrices   Consider each of the following augmented matrices. Determine if the matrix is in reduced row echelon form. If it is not, perform a sequence of scaling, interchange, and replacement operations to obtain a row equivalent matrix that is in reduced row echelon form. Then use the reduced row echelon matrix to describe the solution space.                            Because the leading entry in the first row is not a , this is not in reduced row echelon form. If we scale the first row by , however, we have a matrix in reduced row echelon form. We may write the corresponding linear system as which may be rewritten as Since may take on any value, this shows that there are infinitely many solutions.   This matrix is in reduced row echelon form. There is a single solution .   This matrix is also in reduced row echelon form. However, this are no solutions since the third equation is .   This is not in reduced row echelon form because the row of zeroes should be at the bottom of the matrix. We also need another interchange so that the leading entry in the second row is to the right of the leading entry in the first row. Once again, there are infinitely many solutions.   This is not in reduced row echelon form because the leading entry in the second and third rows are not the only nonzero elements in their columns. We can use replacement operations to remedy this and see that the system has the single solution .      The row equivalent reduced row echelon form is and there are infinitely many solutions.   This matrix is in reduced row echelon form. There is a single solution .   This matrix is also in reduced row echelon form. However, this are no solutions since the third equation is .  The row equivalent reduced row echelon form is There are infinitely many solutions.  The row equivalent reduced row echelon form is This system has the single solution .     The examples in the previous activity indicate that there is a sequence of row operations that transforms any matrix into one in reduced row echelon form. Moreover, the conditions that define reduced row echelon matrices guarantee that this matrix is unique.   For any given matrix, there is exactly one reduced row echelon matrix to which it is row equivalent.   Once we have this reduced row echelon matrix, we may describe the set of solutions to the corresponding linear system with relative ease.   Describing the solution space from a reduced row echelon matrix     Consider the reduced row echelon matrix and its corresponding linear system as Let's rewrite the equations as From this description, it is clear that we obtain a solution for any value of the variable . For instance, if , then and so that is a solution. Similarly, if , we see that is also a solution.  Because there is no restriction on the value of , we call it a free variable , and note that the linear system has infinitely many solutions. The variables and are called basic variables as they are determined once we make a choice of the free variable. free variable  basic variable   We will call this description of the solution space, in which the basic variables are written in terms of the free variables, a parametric description of the solution space. parametric description     Consider the matrix The last equation gives , which is true for any . We may safely ignore this equation since it does not impose a restriction on . We then see that there is a unique solution .    Consider the matrix Beginning with the last equation, we see that , which is not true for any . There is no solution to this particular equation and therefore no solution to the system of equations.        Summary  We saw several important concepts in this section.  We can describe the solution space to a linear system by transforming it into a new linear system having the same solution space through a sequence of scaling, interchange, and replacement operations.  We can represent a linear system by an augmented matrix. Using scaling, interchange, and replacement operations, the augmented matrix is row equivalent to exactly one reduced row echelon matrix. The process of constructing this reduced row echelon matrix is called Gaussian elimination.  The reduced row echelon matrix allows us to easily describe the solution space of a linear system.       For each of the linear systems below, write the associated augmented matrix and find the reduced row echelon matrix that is row equivalent to it. Identify the basic and free variables and then describe the solution space of the original linear system using a parametric description, if appropriate.                     The reduced row echelon form is and there is a single solution. Every variable is basic.  The reduced row echelon form is All the variables are basic so there is a unique solution: , , and .  The reduced row echelon form is which gives the equations This shows that is a free variable and , , and are basic variables. There are therefore infinitely many solutions.     The augmented matrix and its reduced row echelon form are The linear system corresponding to the reduced row echelon form is This shows that there is a single solution and that every variable is a basic variable.  We have the augmented matrix This shows that all the variables are basic variables and that there is a unique solution: , , and .  The augmented matrix and its reduced row echelon form are This gives the equations showing that is a free variable and , , and are basic variables. There are therefore infinitely many solutions.     Consider each matrix below and determine if it is in reduced row echelon form. If not, indicate the reason and apply a sequence of row operations to find its reduced row echelon matrix. For each matrix, indicate whether the corresponding linear system has infinitely many solutions, exactly one solution, or no solutions.                           The reduced row echelon form is and there are infinitely many solutions.    The reduced row echelon form is and there are no solutions.    The reduced row echelon form is and this linear system has a single solution.    The reduced row echelon form is and there are infinitely many solutions.        This is not in reduced row echelon form since the leading entry in the second row is not the only nonzero entry in its column. Applying a replacement operation gives, There are infinitely many solutions since is a free variable.    This is not in reduced row echelon form since the leading entries are not all . We can scale those rows to find the reduced row echelon form. There are no solutions since the last row gives the equation .    This is not in reduced row echelon form since the leading entry in the last row is not and is not the only nonzero entry in its column. This linear system has a single solution.    This is not in reduced row echelon form since the leading entries appear in the wrong order. Since is a free variable, there are infinitely many solutions.       Give an example of a reduced row echelon matrix that describes a linear system having the stated properties. If it is not possible to find such an example, explain why not.   Write a reduced row echelon matrix for a linear system having five equations and three variables and having exactly one solution.   Write a reduced row echelon matrix for a linear system having three equations and three variables and having no solution.   Write a reduced row echelon matrix for a linear system having three equations and five variables and having infinitely many solutions.   Write a reduced row echelon matrix for a linear system having three equations and four variables and having exactly one solution.   Write a reduced row echelon matrix for a linear system having four equations and four variables and having exactly one solution.               This is not possible.        Our matrix should have five rows and four columns. Here is an example.   Here is an example.   Here is an example.   This is not possible. If there is a solution, there will always be a free variable so there will be infinitely many solutions.   Here is an example.      For any given matrix, tells us that there is a reduced row echelon matrix that is row equivalent to it. This exercise demonstrates why this is the case. Each of the following matrices satisfies three of the four conditions required of a reduced row echelon matrix as prescribed by . For each, indicate how a sequence of row operations can be applied to form a row equivalent reduced row echelon matrix.                            Interchange the second and third rows.    Scale the second row by .    Perform interchanges among the first three rows.    Perform row replacement operations so that the leading entry in the third row is the only nonzero entry in its column.         Interchange the second and third rows to form     Scale the second row by to form     Perform interchanges among the first three rows so that     Perform row replacement operations so that the leading entry in the third row is the only nonzero entry in its column.        For each of the questions below, provide a justification for your response.   What does the presence of a row whose entries are all zero in an augmented matrix tell us about the solution space of the linear system?   How can you determine if a linear system has no solutions directly from its reduced row echelon matrix?   How can you determine if a linear system has infinitely many solutions directly from its reduced row echelon matrix?   What can you say about the solution space of a linear system if there are more variables than equations and at least one solution exists?      Nothing.  The leading entry of some row appears in the rightmost column of the augmented matrix.  There is a column that is not the rightmost and that does not contain the leading entry of a row. Also, the rightmost column does not contain a leading entry.  There are infinitely many solutions.     It doesn't tell us anything. The equation is true for any values of the variables. Therefore, it does not provide a restriction on the solution space.  There must be an equation having the form so there is a row whose leading entry is in the rightmost column of the augmented matrix.  First, we know there are solutions so no row can have its leading entry in the rightmost column of the augmented matrix. Second, there must be a free variable to have infinitely many solutions. Therefore, there must be at least one column that corresponds to a variable that does not contain the leading entry of a row.  There must be infinitely many solutions since there will be a free variable.     Determine whether the following statements are true or false and explain your reasoning.    If every variable is basic, then the linear system has exactly one solution.   If two augmented matrices are row equivalent to one another, then they describe two linear systems having the same solution spaces.   The presence of a free variable indicates that there are no solutions to the linear system.   If a linear system has exactly one solution, then it must have the same number of equations as variables.    If a linear system has the same number of equations as variables, then it has exactly one solution.     True.  True.  False.  False.  False.     This is true provided that there is some solution to the linear system. We have to avoid the situation when there are infinitely many solutions, and this happens only when there is a free variable.  This is true. When two matrices are row equivalent, there is a sequence of scaling, interchange, and replacement operations that transforms one matrix into the other. These operations do not change the solution space of the matrix.  This is false. The presence of a free variable tells us there are infinitely many solutions.  This is false. The reduced row echelon form of the augmented matrix could look like In this case, there are three equations in two variables, and the system has exactly one solution.  This is false, and here is a reduced row echelon matrix that illustrates why.      "
},
{
  "id": "sec-finding-solutions-3-3",
  "level": "2",
  "url": "sec-finding-solutions.html#sec-finding-solutions-3-3",
  "type": "Preview Activity",
  "number": "17.2.1",
  "title": "",
  "body": "  In this activity, we will consider some simple examples that will guide us in finding a more general approach.    Give a description of the solution space to the linear system:    Give a description of the solution space to the linear system:    Give a description of the solution space to the linear system:   Describe the solution space to the linear equation .  Describe the solution space to the linear equation .       The equations tell us the value of both variables so there is only one solution .    We first use the third equation to determine that . We next substitute this value into the first two equations to obtain Now we can use the second equation to determine that , substitute that value into the first equation, and determine that . This tells us that the linear system has one solution .    Notice that we can rewrite the first equation as and substitute this into the second equation to obtain This tells us that and . The linear system therefore has one solution .    No matter the value of , we have . Therefore, the solution space to the equation is all real numbers . In other words, this equation does not place a restriction on the value of .    By contrast, the equation has no solutions since no value of , when multiplied by , can produce .        .   .   .  Any real number is a solution.  There are no solutions.    "
},
{
  "id": "sec-finding-solutions-3-5",
  "level": "2",
  "url": "sec-finding-solutions.html#sec-finding-solutions-3-5",
  "type": "Observation",
  "number": "17.2.1",
  "title": "",
  "body": " First, finding the solution space to some systems is simple. For example, because each equation in the following system has only one variable, it prescribes a specific value for that variable. We therefore see that there is exactly one solution, which is . We call such a system decoupled . decoupled system   "
},
{
  "id": "sec-finding-solutions-3-6",
  "level": "2",
  "url": "sec-finding-solutions.html#sec-finding-solutions-3-6",
  "type": "Observation",
  "number": "17.2.2",
  "title": "",
  "body": " Second, there is a process that can be used to find solutions to certain types of linear systems. For instance, let's consider the system Multiplying both sides of the last equation by gives us Any solution to this linear system must then have .  Once we know that, we can substitute into the first and second equations and simplify to obtain a new system of equations having the same solutions: The second equation, after multiplying both sides by , tells us that . We can then substitute this value into the first equation to determine that .  In this way, we arrive at a decoupled system, which shows that there is exactly one solution, namely .  Our original system,  triangular system  back substitution is called a triangular system due to the shape formed by the coefficients. As this example demonstrates, triangular systems are easily solved by this process, which is called back substitution .  "
},
{
  "id": "sec-finding-solutions-3-7",
  "level": "2",
  "url": "sec-finding-solutions.html#sec-finding-solutions-3-7",
  "type": "Observation",
  "number": "17.2.3",
  "title": "",
  "body": " We can use substitution in a more general way to solve linear systems. For example, a natural approach to the system is to use the first equation to express in terms of : and then substitute this into the second equation and simplify: From here, we can substitute into the first equation to arrive at the solution .  However, the two-step process of solving for in terms of and substituting into the second equation may be performed more efficiently by adding a multiple of the first equation to the second. In this case, we will multiply the first equation by -2 and add to the second equation to obtain     which gives us       In this way, the system is transformed into the new triangular system Notice that this process can be reversed. Beginning with the triangular system, we can recover the original system by multiplying the first equation by 2 and adding it to the second. Because of this, the two systems have the same solution space. We will revisit this point later and give what may be a more convincing explanation.  Of course, the choice to multiply the first equation by -2 was made so that the terms involving in the two equations will cancel leading to a triangular system that can be solved using back substitution.  "
},
{
  "id": "sec-finding-solutions-3-9",
  "level": "2",
  "url": "sec-finding-solutions.html#sec-finding-solutions-3-9",
  "type": "Example",
  "number": "17.2.4",
  "title": "",
  "body": "  Let's illustrate the use of these operations to find the solution space to the system of equations:   We will first transform the system into a triangular system so we start by eliminating from the second and third equations.  We begin with a replacement operation where we multiply the first equation by -2 and add the result to the second equation.         Another replacement operation eliminates from the third equation. We multiply the first equation by 3 and add to the third.         Scale the second equation by multiplying it by .         Eliminate from the third equation by multiplying the second equation by -4 and adding it to the third. Notice that we now have a triangular system that can be solved using back substitution.         After scaling the third equation by , we have found the value for .         We eliminate from the second equation by multiplying the third equation by -1 and adding to the second.         Finally, multiply the second equation by -2 and add to the first to obtain:       Now that we have arrived at a decoupled system, we know that there is exactly one solution to our original system of equations, which is .   "
},
{
  "id": "sec-finding-solutions-3-12",
  "level": "2",
  "url": "sec-finding-solutions.html#sec-finding-solutions-3-12",
  "type": "Activity",
  "number": "17.2.2",
  "title": "Gaussian Elimination.",
  "body": " Gaussian Elimination   For each of the following linear systems, use Gaussian elimination to describe the solutions to the following systems of linear equations. In particular, determine whether each linear system has exactly one solution, infinitely many solutions, or no solutions.                   Our aim is to apply a sequence of scaling, interchange, and replacement operations to first put the system into a triangular form. We begin by multiplying the first equation by and adding it to the second equation. Next, we add the first equation to the third. This leads us to: We will now apply a scaling operation to make the coefficient of equal in the second equation. Another replacement operation brings the system into a triangular form.   From here, we begin the process of back substitution seeking a decoupled system. Finally, we have the decoupled system which tells us that the solution space consists of the single solution .  Once again, we begin with a sequence of replacement and scaling operations that lead to the triangular system Back substitution gives us the system The third equation does not impose a restriction on the solutions since it is satisfied for any . The second equation tells us that must equal ; however, there are infinitely many solutions to the first equation that have . Therefore, this system has infinitely many solutions.  After applying two replacement and one scaling operation, we find Another replacement operation leads to the system Since the third equation has no solutions, the original system can have no solutions as well.      There is a single solution .  There are infinitely many solutions.  There are no solutions.    "
},
{
  "id": "sec-finding-solutions-4-7",
  "level": "2",
  "url": "sec-finding-solutions.html#sec-finding-solutions-4-7",
  "type": "Activity",
  "number": "17.2.3",
  "title": "Augmented matrices and solution spaces.",
  "body": " Augmented matrices and solution spaces      Write the augmented matrix for the linear system and perform Gaussian elimination to describe the solution space in as much detail as you can.    Suppose that you have a linear system in the variables and whose augmented matrix is row equivalent to Write the linear system corresponding to this augmented matrix and describe its solution set in as much detail as you can.    Suppose that you have a linear system in the variables and whose augmented matrix is row equivalent to Write the linear system corresponding to this augmented matrix and describe its solution set in as much detail as you can.    Suppose that the augmented matrix of a linear system has the following shape where could be any real number.    How many equations are there in this system and how many variables?   Based on our earlier discussion in , do you think it's possible that this system has exactly one solution, infinitely many solutions, or no solutions?   Suppose that this augmented matrix is row equivalent to Make a choice for the names of the variables and write the corresponding linear system. Does the system have exactly one solution, infinitely many solutions, or no solutions?         The augmented matrix for this linear system is This corresponds to the system of equations showing that there is a single solution .  The corresponding system of equations is The third equation is satisfied for any values of and . Therefore, we see that the only solution to the system is .  Here, the corresponding system of equations is Since the third equation is not satisfied for any values of and , there are no solutions to the system.  The system corresponding to this augmented matrix has three equations and five variables. Our first guess is there are infinitely many solutions. If we write out the equations corresponding to the augmented matrix, we find since the third row of the augmented matrix does not restrict the solution space. From here, we see that there are infinitely many solutions: if we make any choice for the variables , , and , we can find values for and that give a solution.      There is a single solution .  There is a single solution .  There are no solutions.  This system has three equations in five variables, and there are infinitely many solutions.    "
},
{
  "id": "reduced-row-echelon",
  "level": "2",
  "url": "sec-finding-solutions.html#reduced-row-echelon",
  "type": "Definition",
  "number": "17.2.5",
  "title": "",
  "body": " reduced row echelon form  We say that a matrix is in reduced row echelon form if the following properties are satisfied.   If the entries in a row are all zero, then the same is true of any row below it.    If we move across a row from left to right, the first nonzero entry we encounter is 1. We call this entry the leading entry in the row.    The leading entry in any row is to the right of the leading entries in all the rows above it.    A leading entry is the only nonzero entry in its column.    reduced row echelon matrix We call a matrix in reduced row echelon form a reduced row echelon matrix .  "
},
{
  "id": "sec-finding-solutions-5-5",
  "level": "2",
  "url": "sec-finding-solutions.html#sec-finding-solutions-5-5",
  "type": "Activity",
  "number": "17.2.4",
  "title": "Identifying reduced row echelon matrices.",
  "body": " Identifying reduced row echelon matrices   Consider each of the following augmented matrices. Determine if the matrix is in reduced row echelon form. If it is not, perform a sequence of scaling, interchange, and replacement operations to obtain a row equivalent matrix that is in reduced row echelon form. Then use the reduced row echelon matrix to describe the solution space.                            Because the leading entry in the first row is not a , this is not in reduced row echelon form. If we scale the first row by , however, we have a matrix in reduced row echelon form. We may write the corresponding linear system as which may be rewritten as Since may take on any value, this shows that there are infinitely many solutions.   This matrix is in reduced row echelon form. There is a single solution .   This matrix is also in reduced row echelon form. However, this are no solutions since the third equation is .   This is not in reduced row echelon form because the row of zeroes should be at the bottom of the matrix. We also need another interchange so that the leading entry in the second row is to the right of the leading entry in the first row. Once again, there are infinitely many solutions.   This is not in reduced row echelon form because the leading entry in the second and third rows are not the only nonzero elements in their columns. We can use replacement operations to remedy this and see that the system has the single solution .      The row equivalent reduced row echelon form is and there are infinitely many solutions.   This matrix is in reduced row echelon form. There is a single solution .   This matrix is also in reduced row echelon form. However, this are no solutions since the third equation is .  The row equivalent reduced row echelon form is There are infinitely many solutions.  The row equivalent reduced row echelon form is This system has the single solution .    "
},
{
  "id": "thm-rref-is-unique",
  "level": "2",
  "url": "sec-finding-solutions.html#thm-rref-is-unique",
  "type": "Theorem",
  "number": "17.2.6",
  "title": "",
  "body": " For any given matrix, there is exactly one reduced row echelon matrix to which it is row equivalent.  "
},
{
  "id": "sec-finding-solutions-5-9",
  "level": "2",
  "url": "sec-finding-solutions.html#sec-finding-solutions-5-9",
  "type": "Example",
  "number": "17.2.7",
  "title": "Describing the solution space from a reduced row echelon matrix.",
  "body": " Describing the solution space from a reduced row echelon matrix     Consider the reduced row echelon matrix and its corresponding linear system as Let's rewrite the equations as From this description, it is clear that we obtain a solution for any value of the variable . For instance, if , then and so that is a solution. Similarly, if , we see that is also a solution.  Because there is no restriction on the value of , we call it a free variable , and note that the linear system has infinitely many solutions. The variables and are called basic variables as they are determined once we make a choice of the free variable. free variable  basic variable   We will call this description of the solution space, in which the basic variables are written in terms of the free variables, a parametric description of the solution space. parametric description     Consider the matrix The last equation gives , which is true for any . We may safely ignore this equation since it does not impose a restriction on . We then see that there is a unique solution .    Consider the matrix Beginning with the last equation, we see that , which is not true for any . There is no solution to this particular equation and therefore no solution to the system of equations.     "
},
{
  "id": "sec-finding-solutions-7-1",
  "level": "2",
  "url": "sec-finding-solutions.html#sec-finding-solutions-7-1",
  "type": "Exercise",
  "number": "17.2.5.1",
  "title": "",
  "body": " For each of the linear systems below, write the associated augmented matrix and find the reduced row echelon matrix that is row equivalent to it. Identify the basic and free variables and then describe the solution space of the original linear system using a parametric description, if appropriate.                     The reduced row echelon form is and there is a single solution. Every variable is basic.  The reduced row echelon form is All the variables are basic so there is a unique solution: , , and .  The reduced row echelon form is which gives the equations This shows that is a free variable and , , and are basic variables. There are therefore infinitely many solutions.     The augmented matrix and its reduced row echelon form are The linear system corresponding to the reduced row echelon form is This shows that there is a single solution and that every variable is a basic variable.  We have the augmented matrix This shows that all the variables are basic variables and that there is a unique solution: , , and .  The augmented matrix and its reduced row echelon form are This gives the equations showing that is a free variable and , , and are basic variables. There are therefore infinitely many solutions.   "
},
{
  "id": "sec-finding-solutions-7-2",
  "level": "2",
  "url": "sec-finding-solutions.html#sec-finding-solutions-7-2",
  "type": "Exercise",
  "number": "17.2.5.2",
  "title": "",
  "body": " Consider each matrix below and determine if it is in reduced row echelon form. If not, indicate the reason and apply a sequence of row operations to find its reduced row echelon matrix. For each matrix, indicate whether the corresponding linear system has infinitely many solutions, exactly one solution, or no solutions.                           The reduced row echelon form is and there are infinitely many solutions.    The reduced row echelon form is and there are no solutions.    The reduced row echelon form is and this linear system has a single solution.    The reduced row echelon form is and there are infinitely many solutions.        This is not in reduced row echelon form since the leading entry in the second row is not the only nonzero entry in its column. Applying a replacement operation gives, There are infinitely many solutions since is a free variable.    This is not in reduced row echelon form since the leading entries are not all . We can scale those rows to find the reduced row echelon form. There are no solutions since the last row gives the equation .    This is not in reduced row echelon form since the leading entry in the last row is not and is not the only nonzero entry in its column. This linear system has a single solution.    This is not in reduced row echelon form since the leading entries appear in the wrong order. Since is a free variable, there are infinitely many solutions.     "
},
{
  "id": "sec-finding-solutions-7-3",
  "level": "2",
  "url": "sec-finding-solutions.html#sec-finding-solutions-7-3",
  "type": "Exercise",
  "number": "17.2.5.3",
  "title": "",
  "body": " Give an example of a reduced row echelon matrix that describes a linear system having the stated properties. If it is not possible to find such an example, explain why not.   Write a reduced row echelon matrix for a linear system having five equations and three variables and having exactly one solution.   Write a reduced row echelon matrix for a linear system having three equations and three variables and having no solution.   Write a reduced row echelon matrix for a linear system having three equations and five variables and having infinitely many solutions.   Write a reduced row echelon matrix for a linear system having three equations and four variables and having exactly one solution.   Write a reduced row echelon matrix for a linear system having four equations and four variables and having exactly one solution.               This is not possible.        Our matrix should have five rows and four columns. Here is an example.   Here is an example.   Here is an example.   This is not possible. If there is a solution, there will always be a free variable so there will be infinitely many solutions.   Here is an example.    "
},
{
  "id": "sec-finding-solutions-7-4",
  "level": "2",
  "url": "sec-finding-solutions.html#sec-finding-solutions-7-4",
  "type": "Exercise",
  "number": "17.2.5.4",
  "title": "",
  "body": " For any given matrix, tells us that there is a reduced row echelon matrix that is row equivalent to it. This exercise demonstrates why this is the case. Each of the following matrices satisfies three of the four conditions required of a reduced row echelon matrix as prescribed by . For each, indicate how a sequence of row operations can be applied to form a row equivalent reduced row echelon matrix.                            Interchange the second and third rows.    Scale the second row by .    Perform interchanges among the first three rows.    Perform row replacement operations so that the leading entry in the third row is the only nonzero entry in its column.         Interchange the second and third rows to form     Scale the second row by to form     Perform interchanges among the first three rows so that     Perform row replacement operations so that the leading entry in the third row is the only nonzero entry in its column.      "
},
{
  "id": "sec-finding-solutions-7-5",
  "level": "2",
  "url": "sec-finding-solutions.html#sec-finding-solutions-7-5",
  "type": "Exercise",
  "number": "17.2.5.5",
  "title": "",
  "body": " For each of the questions below, provide a justification for your response.   What does the presence of a row whose entries are all zero in an augmented matrix tell us about the solution space of the linear system?   How can you determine if a linear system has no solutions directly from its reduced row echelon matrix?   How can you determine if a linear system has infinitely many solutions directly from its reduced row echelon matrix?   What can you say about the solution space of a linear system if there are more variables than equations and at least one solution exists?      Nothing.  The leading entry of some row appears in the rightmost column of the augmented matrix.  There is a column that is not the rightmost and that does not contain the leading entry of a row. Also, the rightmost column does not contain a leading entry.  There are infinitely many solutions.     It doesn't tell us anything. The equation is true for any values of the variables. Therefore, it does not provide a restriction on the solution space.  There must be an equation having the form so there is a row whose leading entry is in the rightmost column of the augmented matrix.  First, we know there are solutions so no row can have its leading entry in the rightmost column of the augmented matrix. Second, there must be a free variable to have infinitely many solutions. Therefore, there must be at least one column that corresponds to a variable that does not contain the leading entry of a row.  There must be infinitely many solutions since there will be a free variable.   "
},
{
  "id": "sec-finding-solutions-7-6",
  "level": "2",
  "url": "sec-finding-solutions.html#sec-finding-solutions-7-6",
  "type": "Exercise",
  "number": "17.2.5.6",
  "title": "",
  "body": " Determine whether the following statements are true or false and explain your reasoning.    If every variable is basic, then the linear system has exactly one solution.   If two augmented matrices are row equivalent to one another, then they describe two linear systems having the same solution spaces.   The presence of a free variable indicates that there are no solutions to the linear system.   If a linear system has exactly one solution, then it must have the same number of equations as variables.    If a linear system has the same number of equations as variables, then it has exactly one solution.     True.  True.  False.  False.  False.     This is true provided that there is some solution to the linear system. We have to avoid the situation when there are infinitely many solutions, and this happens only when there is a free variable.  This is true. When two matrices are row equivalent, there is a sequence of scaling, interchange, and replacement operations that transforms one matrix into the other. These operations do not change the solution space of the matrix.  This is false. The presence of a free variable tells us there are infinitely many solutions.  This is false. The reduced row echelon form of the augmented matrix could look like In this case, there are three equations in two variables, and the system has exactly one solution.  This is false, and here is a reduced row echelon matrix that illustrates why.    "
},
{
  "id": "sec-sage-introduction",
  "level": "1",
  "url": "sec-sage-introduction.html",
  "type": "Section",
  "number": "17.3",
  "title": "Computation with Sage",
  "body": " Computation with Sage   Linear algebra owes its prominence as a powerful scientific tool to the ever-growing power of computers. Carl Cowen, a former president of the Mathematical Association of America, has said, No serious application of linear algebra happens without a computer. Indeed, Cowen notes that, in the 1950s, working with a system of 100 equations in 100 variables was difficult. Today, scientists and mathematicians routinely work on problems that are vastly larger. This is only possible because of today's computing power.  It is therefore important for any student of linear algebra to become comfortable solving linear algebraic problems on a computer. This section will introduce you to a program called Sage that can help. While you may be able to do much of this work on a graphing calculator, you are encouraged to become comfortable with Sage as we will use increasingly powerful features as we encounter their need.    Introduction to Sage  There are several ways to access Sage.   If you are reading this book online, there will be embedded Sage cells at appropriate places in the text. You have the opportunity to type Sage commands into these cells and execute them, provided you are connected to the Internet. Please be aware that your work will be lost if you reload the page.  Here is a Sage cell containing a command that asks Sage to multiply 5 and 3. You may execute the command by pressing the Evaluate button.     You may also go to , sign up for an account, open a new project, and create a Sage worksheet. Once inside the worksheet, you may enter commands as shown here, and evaluate them by pressing Enter on your keyboard while holding down the Shift key.    There is a page of Sage cells at . Any results obtained by evaluating one cell are available in other cells. However, your work will be lost when the page is reloaded.     Throughout the text, we will introduce new Sage commands that allow us to explore linear algebraic concepts. These commands are collected and summarized in the reference found in Appendix A .   Basic Sage commands      Sage uses the standard operators +, -, *, \/, and ^ for the usual arithmetic operations. By entering text in the cell below, ask Sage to evaluate      Notice that we can create new lines by pressing Enter and entering additional commands on them. What happens when you evaluate this Sage cell?   Notice that we only see the result from the last command. With the print command, we may see earlier results, if we wish.     We may give a name to the result of one command and refer to it in a later command.   Suppose you have three tests in your linear algebra class and your scores are 90, 100, and 98. In the Sage cell below, add your scores together and call the result total . On the next line, find the average of your test scores and print it.     If you are not a programmer, you may ignore this part. If you are an experienced programmer, however, you should know that Sage is written in the Python programming language and that you may enter Python code into a Sage cell.           3 + 4*(2^4 - 1)     Only the results of the last line are displayed.     total = 90 + 100 + 98 average = total \/ 3 print(average)     This cells print the integers from 0 to 9.         3 + 4*(2^4 - 1)     Only the results of the last line are displayed.     total = 90 + 100 + 98 average = total \/ 3 print(average)     This cells print the integers from 0 to 9.        Sage and matrices  When we encounter a matrix, tells us that there is exactly one reduced row echelon matrix that is row equivalent to it.  In fact, the uniqueness of this reduced row echelon matrix is what motivates us to define this particular form. When solving a system of linear equations using Gaussian elimination, there are other row equivalent matrices that reveal the structure of the solution space. The reduced row echelon matrix is simply a convenience as it is an agreement we make with one another to seek the same matrix.  An added benefit is that we can ask a computer program, like Sage, to find reduced row echelon matrices for us. We will learn how to do this now that we have a little familiarity with Sage.  First, notice that a matrix has a certain number of rows and columns. For instance, the matrix has three rows and five columns. We consequently refer to this as a matrix.  We may ask Sage to create the matrix by entering When evaluated, Sage will confirm the matrix by writing out the rows of the matrix, each inside square brackets.  Notice that there are three separate things (we call them arguments ) inside the parentheses: the number of rows, the number of columns, and the entries of the matrix listed by row inside square brackets. These three arguments are separated by commas. Notice that there is no way of specifying whether this is an augmented or coefficient matrix so it will be up to us to interpret our results appropriately.   Sage syntax  Some common mistakes are   to forget the square brackets around the list of entries,   to omit an entry from the list or to add an extra one,   to forget to separate the rows, columns, and entries by commas, and   to omit the parentheses around the arguments after matrix .  If you see an error message, carefully proofread your input and try again.   Alternatively, you can create a matrix by simply listing its rows, like this matrix([ [-1, 0, 2, 7], [ 2, 1,-3,-1] ])    Using Sage to find row reduced echelon matrices      Enter the following matrix into Sage.      Give the matrix the name by entering A = matrix( ..., ..., [ ... ]) We may then find its reduced row echelon form by entering A = matrix( ..., ..., [ ... ]) A.rref() A common mistake is to forget the parentheses after rref .  Use Sage to find the reduced row echelon form of the matrix from of this activity.     Use Sage to describe the solution space of the system of linear equations      Consider the two matrices: We say that is an augmentation of because it is obtained from by adding some more columns.  Using Sage, define the matrices and compare their reduced row echelon forms. What do you notice about the relationship between the two reduced row echelon forms?       Using the system of equations in , write the augmented matrix corresponding to the system of equations. What did you find for the reduced row echelon form of the augmented matrix?  Now write the coefficient matrix of this system of equations. What does of this activity tell you about its reduced row echelon form?           matrix(3, 4, [-1,-2, 2,-1, 2, 4,-1, 5, 1, 2, 0, 3])     The reduced row echelon form of the matrix is     Sage tells us that the reduced row echelon form of the corresponding augmented matrix is so there is a unique solution .    The first four columns of the reduced row echelon form of form the reduced row echelon form of .    The reduced row echelon form of the coefficient matrix is            matrix(3, 4, [-1,-2, 2,-1, 2, 4,-1, 5, 1, 2, 0, 3])     The reduced row echelon form of the matrix is     There is a unique solution .    The first four columns of the reduced row echelon form of form the reduced row echelon form of .    The reduced row echelon form of the coefficient matrix is         Sage practices  Here are some practices that you may find helpful when working with matrices in Sage.   Break the matrix entries across lines, one for each row, for better readability by pressing Enter between rows. A = matrix(2, 4, [ 1, 2, -1, 0, -3, 0, 4, 3 ])    Print your original matrix to check that you have entered it correctly. You may want to also print a dividing line to separate matrices. A = matrix(2, 2, [ 1, 2, 2, 2]) print (A) print (\"---------\") A.rref()       The last part of the previous activity, , demonstrates something that will be helpful for us in the future. In that activity, we started with a matrix , which we augmented by adding some columns to obtain a matrix . We then noticed that the reduced row echelon form of is itself an augmentation of the reduced row echelon form of .  To illustrate, we can consider the reduced row echelon form of the augmented matrix:   We can then determine the reduced row echelon form of the coefficient matrix by looking inside the augmented matrix.   If we trace through the steps in the Gaussian elimination algorithm carefully, we see that this is a general principle, which we now state.   Augmentation Principle  If matrix is an augmentation of matrix , then the reduced row echelon form of is an augmentation of the reduced row echelon form of .     Computational effort  At the beginning of this section, we indicated that linear algebra has become more prominent as computers have grown more powerful. Computers, however, still have limits. Let's consider how much effort is expended when we ask to find the reduced row echelon form of a matrix. We will measure, very roughly, the effort by the number of times the algorithm requires us to multiply or add two numbers.  We will assume that our matrix has the same number of rows as columns, which we call . We are mainly interested in the case when is very large, which is when we need to worry about how much effort is required.  Let's first consider the effort required for each of our row operations.   Scaling a row multiplies each of the entries in a row by some number, which requires operations.   Interchanging two rows requires no multiplications or additions so we won't worry about the effort required by an interchange.   A replacement requires us to multiply each entry in a row by some number, which takes operations, and then add the resulting entries to another row, which requires another operations. The total number of operations is .    Our goal is to transform a matrix to its reduced row echelon form, which looks something like this: . We roughly perform one replacement operation for every 0 entry in the reduced row echelon matrix. When is very large, most of the entries in the reduced row echelon form are 0 so we need roughly replacements. Since each replacement operation requires operations, the number of operations resulting from the needed replacements is roughly .  Each row is scaled roughly one time so there are roughly scaling operations, each of which requires operations. The number of operations due to scaling is roughly .  Therefore, the total number of operations is roughly . When is very large, the term is much smaller than the term. We therefore state that    The number of operations required to find the reduced row echelon form of an matrix is roughly proportional to .    This is a very rough measure of the effort required to find the reduced row echelon form; a more careful accounting shows that the number of arithmetic operations is roughly . As we have seen, some matrices require more effort than others, but the upshot of this observation is that the effort is proportional to . We can think of this in the following way: If the size of the matrix grows by a factor of 10, then the effort required grows by a factor of .  While today's computers are powerful, they cannot handle every problem we might ask of them. Eventually, we would like to be able to consider matrices that have (a trillion) rows and columns. In very broad terms, the effort required to find the reduced row echelon matrix will require roughly operations.  To put this into context, imagine we need to solve a linear system with a trillion equations and a trillion variables and that we have a computer that can perform a trillion, , operations every second. Finding the reduced row echelon form would take about years. At this time, the universe is estimated to be approximately years old. If we started the calculation when the universe was born, we'd be about one-millionth of the way through.  This may seem like an absurd situation, but we'll see in how we use the results of such a computation every day. Clearly, we will need some better tools to deal with really big problems like this one.    Summary  We learned some basic features of Sage with an emphasis on finding the reduced row echelon form of a matrix.   Sage can perform basic arithmetic using standard operators. Sage can also save results from one command to be reused in a later command.   We may define matrices in Sage and find the reduced row echelon form using the rref command.   We saw an example of the Augmentation Principle , which we then stated as a general principle.   We saw that the computational effort required to find the reduced row echelon form of an matrix is proportional to .   Appendix A contains a reference outlining the Sage commands that we have encountered.     Consider the linear system Write this system as an augmented matrix and use Sage to find a description of the solution space.    There is exactly one solution .   We can use Sage to find the reduced row echelon form of the corresponding augmented matrix: This shows that there is exactly one solution .    Shown below are some traffic patterns in the downtown area of a large city. The figures give the number of cars per hour traveling along each road. Any car that drives into an intersection must also leave the intersection. This means that the number of cars entering an intersection in an hour is equal to the number of cars leaving the intersection.   Let's begin with the following traffic pattern.   How many cars per hour enter the upper left intersection? How many cars per hour leave this intersection? Use this to form a linear equation in the variables , , , and .      Form three more linear equations from the other three intersections to form a linear system having four equations in four variables. Then use Sage to find the solution space to this system.    Is there exactly one solution or infinitely many solutions? Explain why you would expect this given the information provided.      Another traffic pattern is shown below.      Once again, write a linear system for the quantities , , , and and solve the system using the Sage cell below.   What can you say about the solution of this linear system? Is there exactly one solution or infinitely many solutions? Explain why you would expect this given the information provided.  What is the smallest possible amount of traffic flowing through ?           There is one solution: , , , and .    There are infinitely many solutions described by         We will form a linear system by considering each intersection and equating the number of cars that enter the intersection to the number of cars that leave.   For the intersection in the upper left, the number of cars that enter is while the number of cars that leave is . This gives the equation or .    By studying the other three intersections, we form a linear system consisting of four equations in four variables: If we express these equations in terms of an augmented matrix and use Sage to find its reduced row echelon form, we find     There is exactly one solution: , , , and . This is what we expect since the traffic flow is determined by the upper right intersection. From here, all the other flows are determined as well.       Let's now look at another traffic pattern.   In this case, we find the equations: This gives the augmented matrix:     Here, we may view as a free variable and write This shows that there are infinitely many solutions. This makes sense since the unknown flows , , , and form a loop. For instance, we can add one car to each of , , and provided we remove one from .    Since we must have , it follows that the free variable satisfies . This means that satisfies .         A typical problem in thermodynamics is to find the steady-state temperature distribution inside a thin plate if we know the temperature around the boundary. Let be the temperatures at the six nodes inside the plate as shown below.   The temperature at a node is approximately the average of the four nearest nodes: for instance, , which we may rewrite as .  Set up a linear system to find the temperature at these six points inside the plate. Then use Sage to solve the linear system. If all the entries of the matrix are integers, Sage will compute the reduced row echelon form using rational numbers. To view a decimal approximation of the results, you may use A.rref().numerical_approx(digits=4)    In the real world, the approximation becomes better as we add more and more points into the grid. This is a situation where we may want to solve a linear system having millions of equations and millions of variables.       We set up the equations Setting up an augmented matrix and using Sage to find its reduced row echelon form, we obtain     The fuel inside model rocket motors is a black powder mixture that ideally consists of 60% charcoal, 30% potassium nitrate, and 10% sulfur by weight.  Suppose you work at a company that makes model rocket motors. When you come into work one morning, you learn that yesterday's first shift made a perfect batch of fuel. The second shift, however, misread the recipe and used 50% charcoal, 20% potassium nitrate and 30% sulfur. Then the two batches were mixed together. A chemical analysis shows that there are 100.3 pounds of charcoal in the mixture and 46.9 pounds of potassium nitrate.   Assuming the first shift produced pounds of fuel and the second pounds, set up a linear system in terms of and . How many pounds of fuel did the first shift produce and how many did the second shift produce?   How much sulfur would you expect to find in the mixture?     The first shift produces pounds while the second shift produces pounds. There are pounds of sulfur.      Writing equations for the amount of charcoal and the amount of potassium nitrate, we have Solving this gives pounds and pounds.    The amount of sulfur will be pounds.       This exercise is about balancing chemical reactions.  Chemists denote a molecule of water as , which means it is composed of two atoms of hydrogen (H) and one atom of oxygen (O). The process by which hydrogen burns is described by the chemical reaction This means that molecules of hydrogen combine with molecules of oxygen to produce water molecules. The number of hydrogen atoms is the same before and after the reaction; the same is true of the oxygen atoms.     In terms of , , and , how many hydrogen atoms are there before the reaction? How many hydrogen atoms are there after the reaction? Find a linear equation in , , and by equating these quantities.   Find a second linear equation in , , and by equating the number of oxygen atoms before and after the reaction.   Find the solutions of this linear system. Why are there infinitely many solutions?    In this chemical setting, , , and should be positive integers. Find the solution where , , and are the smallest possible positive integers.      Now consider the reaction where potassium permanganate and manganese sulfate combine with water to produce manganese dioxide, potassium sulfate, and sulfuric acid: As in the previous exercise, find the appropriate values for to balance the chemical reaction.          We have the reaction     We have the reaction          We will equate the number of hydrogen and oxygen atoms before and after the reaction.   Before the reaction, there are two hydrogen atoms for every hydrogen molecule. Since there are hydrogen molecules, we have hydrogen atoms before the reaction.  After the reaction, there are two hydrogen atoms for every water molecule. Since there are hydrogen molecules, we have hydrogen atoms after the reaction. This gives the equation .    Similarly, there are oxygen atoms before the reaction and oxygen atoms after. This gives the equation .    We have the linear system When we create the augmented matrix and find its reduced row echelon form, we see that is a free variable so that We need all the quantities to be integers, and the smallest value of that makes an integer is . Therefore, we have , , and . The reaction is then        Proceeding as in the previous part, we have Solving this linear system, we find Once again, we choose to be the smallest positive integer that causes all of the variables to be integers. This means that so that we have This gives the reaction:        We began this section by stating that increasing computational power has helped linear algebra assume a prominent role as a scientific tool. Later, we looked at one computational limitation: once a matrix gets to be too big, it is not reasonable to apply Gaussian elimination to find its reduced row echelon form.  In this exercise, we will see another limitation: computer arithmetic with real numbers is only an approximation because computers represent real numbers with only a finite number of bits. For instance, the number pi would be approximated inside a computer by, say, Most of the time, this is not a problem. However, when we perform millions or even billions of arithmetic operations, the error in these approximations starts to accumulate and can lead to results that are wildly inaccurate. Here are two examples demonstrating this.   Let's first see an example showing that computer arithmetic really is an approximation. First, consider the linear system If the coefficients are entered into Sage as fractions, Sage will find the exact reduced row echelon form. Find the exact solution to this linear system.   Now let's ask Sage to compute with real numbers. We can do this by representing one of the coefficients as a decimal. For instance, the same linear system can be represented as Most computers do arithmetic using either 32 or 64 bits. To magnify the problem so that we can see it better, we will ask Sage to do arithmetic using only 10 bits as follows. What does Sage give for the solution now? Compare this to the exact solution that you found previously.   Some types of linear systems are particularly sensitive to errors resulting from computers' approximate arithmetic. For instance, suppose we are interested in the linear system Find the solution to this linear system.   Suppose now that the computer has accumulated some error in one of the entries of this system so that it incorrectly stores the system as Find the solution to this linear system.   Notice how a small error in one of the entries in the linear system leads to a solution that has a dramatically large error. Fortunately, this is an issue that has been well studied, and there are techniques that mitigate this type of behavior.         The exact solution is while the approximate solution is     This solution to the first system is and . However, with just a small change in the system, we find the solution and .         The exact solution is However, if we compute with only 10 bits, we find the approximate solution     This solution to the first system is and . However, with just a small change in the system, we find the solution and .       "
},
{
  "id": "sec-sage-introduction-3-4",
  "level": "2",
  "url": "sec-sage-introduction.html#sec-sage-introduction-3-4",
  "type": "Activity",
  "number": "17.3.1",
  "title": "Basic Sage commands.",
  "body": " Basic Sage commands      Sage uses the standard operators +, -, *, \/, and ^ for the usual arithmetic operations. By entering text in the cell below, ask Sage to evaluate      Notice that we can create new lines by pressing Enter and entering additional commands on them. What happens when you evaluate this Sage cell?   Notice that we only see the result from the last command. With the print command, we may see earlier results, if we wish.     We may give a name to the result of one command and refer to it in a later command.   Suppose you have three tests in your linear algebra class and your scores are 90, 100, and 98. In the Sage cell below, add your scores together and call the result total . On the next line, find the average of your test scores and print it.     If you are not a programmer, you may ignore this part. If you are an experienced programmer, however, you should know that Sage is written in the Python programming language and that you may enter Python code into a Sage cell.           3 + 4*(2^4 - 1)     Only the results of the last line are displayed.     total = 90 + 100 + 98 average = total \/ 3 print(average)     This cells print the integers from 0 to 9.         3 + 4*(2^4 - 1)     Only the results of the last line are displayed.     total = 90 + 100 + 98 average = total \/ 3 print(average)     This cells print the integers from 0 to 9.     "
},
{
  "id": "sec-sage-introduction-4-10",
  "level": "2",
  "url": "sec-sage-introduction.html#sec-sage-introduction-4-10",
  "type": "Activity",
  "number": "17.3.2",
  "title": "Using Sage to find row reduced echelon matrices.",
  "body": " Using Sage to find row reduced echelon matrices      Enter the following matrix into Sage.      Give the matrix the name by entering A = matrix( ..., ..., [ ... ]) We may then find its reduced row echelon form by entering A = matrix( ..., ..., [ ... ]) A.rref() A common mistake is to forget the parentheses after rref .  Use Sage to find the reduced row echelon form of the matrix from of this activity.     Use Sage to describe the solution space of the system of linear equations      Consider the two matrices: We say that is an augmentation of because it is obtained from by adding some more columns.  Using Sage, define the matrices and compare their reduced row echelon forms. What do you notice about the relationship between the two reduced row echelon forms?       Using the system of equations in , write the augmented matrix corresponding to the system of equations. What did you find for the reduced row echelon form of the augmented matrix?  Now write the coefficient matrix of this system of equations. What does of this activity tell you about its reduced row echelon form?           matrix(3, 4, [-1,-2, 2,-1, 2, 4,-1, 5, 1, 2, 0, 3])     The reduced row echelon form of the matrix is     Sage tells us that the reduced row echelon form of the corresponding augmented matrix is so there is a unique solution .    The first four columns of the reduced row echelon form of form the reduced row echelon form of .    The reduced row echelon form of the coefficient matrix is            matrix(3, 4, [-1,-2, 2,-1, 2, 4,-1, 5, 1, 2, 0, 3])     The reduced row echelon form of the matrix is     There is a unique solution .    The first four columns of the reduced row echelon form of form the reduced row echelon form of .    The reduced row echelon form of the coefficient matrix is       "
},
{
  "id": "principle-augmentation-principle",
  "level": "2",
  "url": "sec-sage-introduction.html#principle-augmentation-principle",
  "type": "Proposition",
  "number": "17.3.1",
  "title": "Augmentation Principle.",
  "body": " Augmentation Principle  If matrix is an augmentation of matrix , then the reduced row echelon form of is an augmentation of the reduced row echelon form of .  "
},
{
  "id": "subsec-compute-effort-8",
  "level": "2",
  "url": "sec-sage-introduction.html#subsec-compute-effort-8",
  "type": "Observation",
  "number": "17.3.2",
  "title": "",
  "body": "  The number of operations required to find the reduced row echelon form of an matrix is roughly proportional to .   "
},
{
  "id": "sec-sage-introduction-7-1",
  "level": "2",
  "url": "sec-sage-introduction.html#sec-sage-introduction-7-1",
  "type": "Exercise",
  "number": "17.3.5.1",
  "title": "",
  "body": " Consider the linear system Write this system as an augmented matrix and use Sage to find a description of the solution space.    There is exactly one solution .   We can use Sage to find the reduced row echelon form of the corresponding augmented matrix: This shows that there is exactly one solution .  "
},
{
  "id": "sec-sage-introduction-7-2",
  "level": "2",
  "url": "sec-sage-introduction.html#sec-sage-introduction-7-2",
  "type": "Exercise",
  "number": "17.3.5.2",
  "title": "",
  "body": " Shown below are some traffic patterns in the downtown area of a large city. The figures give the number of cars per hour traveling along each road. Any car that drives into an intersection must also leave the intersection. This means that the number of cars entering an intersection in an hour is equal to the number of cars leaving the intersection.   Let's begin with the following traffic pattern.   How many cars per hour enter the upper left intersection? How many cars per hour leave this intersection? Use this to form a linear equation in the variables , , , and .      Form three more linear equations from the other three intersections to form a linear system having four equations in four variables. Then use Sage to find the solution space to this system.    Is there exactly one solution or infinitely many solutions? Explain why you would expect this given the information provided.      Another traffic pattern is shown below.      Once again, write a linear system for the quantities , , , and and solve the system using the Sage cell below.   What can you say about the solution of this linear system? Is there exactly one solution or infinitely many solutions? Explain why you would expect this given the information provided.  What is the smallest possible amount of traffic flowing through ?           There is one solution: , , , and .    There are infinitely many solutions described by         We will form a linear system by considering each intersection and equating the number of cars that enter the intersection to the number of cars that leave.   For the intersection in the upper left, the number of cars that enter is while the number of cars that leave is . This gives the equation or .    By studying the other three intersections, we form a linear system consisting of four equations in four variables: If we express these equations in terms of an augmented matrix and use Sage to find its reduced row echelon form, we find     There is exactly one solution: , , , and . This is what we expect since the traffic flow is determined by the upper right intersection. From here, all the other flows are determined as well.       Let's now look at another traffic pattern.   In this case, we find the equations: This gives the augmented matrix:     Here, we may view as a free variable and write This shows that there are infinitely many solutions. This makes sense since the unknown flows , , , and form a loop. For instance, we can add one car to each of , , and provided we remove one from .    Since we must have , it follows that the free variable satisfies . This means that satisfies .       "
},
{
  "id": "sec-sage-introduction-7-3",
  "level": "2",
  "url": "sec-sage-introduction.html#sec-sage-introduction-7-3",
  "type": "Exercise",
  "number": "17.3.5.3",
  "title": "",
  "body": " A typical problem in thermodynamics is to find the steady-state temperature distribution inside a thin plate if we know the temperature around the boundary. Let be the temperatures at the six nodes inside the plate as shown below.   The temperature at a node is approximately the average of the four nearest nodes: for instance, , which we may rewrite as .  Set up a linear system to find the temperature at these six points inside the plate. Then use Sage to solve the linear system. If all the entries of the matrix are integers, Sage will compute the reduced row echelon form using rational numbers. To view a decimal approximation of the results, you may use A.rref().numerical_approx(digits=4)    In the real world, the approximation becomes better as we add more and more points into the grid. This is a situation where we may want to solve a linear system having millions of equations and millions of variables.       We set up the equations Setting up an augmented matrix and using Sage to find its reduced row echelon form, we obtain   "
},
{
  "id": "sec-sage-introduction-7-4",
  "level": "2",
  "url": "sec-sage-introduction.html#sec-sage-introduction-7-4",
  "type": "Exercise",
  "number": "17.3.5.4",
  "title": "",
  "body": " The fuel inside model rocket motors is a black powder mixture that ideally consists of 60% charcoal, 30% potassium nitrate, and 10% sulfur by weight.  Suppose you work at a company that makes model rocket motors. When you come into work one morning, you learn that yesterday's first shift made a perfect batch of fuel. The second shift, however, misread the recipe and used 50% charcoal, 20% potassium nitrate and 30% sulfur. Then the two batches were mixed together. A chemical analysis shows that there are 100.3 pounds of charcoal in the mixture and 46.9 pounds of potassium nitrate.   Assuming the first shift produced pounds of fuel and the second pounds, set up a linear system in terms of and . How many pounds of fuel did the first shift produce and how many did the second shift produce?   How much sulfur would you expect to find in the mixture?     The first shift produces pounds while the second shift produces pounds. There are pounds of sulfur.      Writing equations for the amount of charcoal and the amount of potassium nitrate, we have Solving this gives pounds and pounds.    The amount of sulfur will be pounds.     "
},
{
  "id": "sec-sage-introduction-7-5",
  "level": "2",
  "url": "sec-sage-introduction.html#sec-sage-introduction-7-5",
  "type": "Exercise",
  "number": "17.3.5.5",
  "title": "",
  "body": " This exercise is about balancing chemical reactions.  Chemists denote a molecule of water as , which means it is composed of two atoms of hydrogen (H) and one atom of oxygen (O). The process by which hydrogen burns is described by the chemical reaction This means that molecules of hydrogen combine with molecules of oxygen to produce water molecules. The number of hydrogen atoms is the same before and after the reaction; the same is true of the oxygen atoms.     In terms of , , and , how many hydrogen atoms are there before the reaction? How many hydrogen atoms are there after the reaction? Find a linear equation in , , and by equating these quantities.   Find a second linear equation in , , and by equating the number of oxygen atoms before and after the reaction.   Find the solutions of this linear system. Why are there infinitely many solutions?    In this chemical setting, , , and should be positive integers. Find the solution where , , and are the smallest possible positive integers.      Now consider the reaction where potassium permanganate and manganese sulfate combine with water to produce manganese dioxide, potassium sulfate, and sulfuric acid: As in the previous exercise, find the appropriate values for to balance the chemical reaction.          We have the reaction     We have the reaction          We will equate the number of hydrogen and oxygen atoms before and after the reaction.   Before the reaction, there are two hydrogen atoms for every hydrogen molecule. Since there are hydrogen molecules, we have hydrogen atoms before the reaction.  After the reaction, there are two hydrogen atoms for every water molecule. Since there are hydrogen molecules, we have hydrogen atoms after the reaction. This gives the equation .    Similarly, there are oxygen atoms before the reaction and oxygen atoms after. This gives the equation .    We have the linear system When we create the augmented matrix and find its reduced row echelon form, we see that is a free variable so that We need all the quantities to be integers, and the smallest value of that makes an integer is . Therefore, we have , , and . The reaction is then        Proceeding as in the previous part, we have Solving this linear system, we find Once again, we choose to be the smallest positive integer that causes all of the variables to be integers. This means that so that we have This gives the reaction:      "
},
{
  "id": "sec-sage-introduction-7-6",
  "level": "2",
  "url": "sec-sage-introduction.html#sec-sage-introduction-7-6",
  "type": "Exercise",
  "number": "17.3.5.6",
  "title": "",
  "body": " We began this section by stating that increasing computational power has helped linear algebra assume a prominent role as a scientific tool. Later, we looked at one computational limitation: once a matrix gets to be too big, it is not reasonable to apply Gaussian elimination to find its reduced row echelon form.  In this exercise, we will see another limitation: computer arithmetic with real numbers is only an approximation because computers represent real numbers with only a finite number of bits. For instance, the number pi would be approximated inside a computer by, say, Most of the time, this is not a problem. However, when we perform millions or even billions of arithmetic operations, the error in these approximations starts to accumulate and can lead to results that are wildly inaccurate. Here are two examples demonstrating this.   Let's first see an example showing that computer arithmetic really is an approximation. First, consider the linear system If the coefficients are entered into Sage as fractions, Sage will find the exact reduced row echelon form. Find the exact solution to this linear system.   Now let's ask Sage to compute with real numbers. We can do this by representing one of the coefficients as a decimal. For instance, the same linear system can be represented as Most computers do arithmetic using either 32 or 64 bits. To magnify the problem so that we can see it better, we will ask Sage to do arithmetic using only 10 bits as follows. What does Sage give for the solution now? Compare this to the exact solution that you found previously.   Some types of linear systems are particularly sensitive to errors resulting from computers' approximate arithmetic. For instance, suppose we are interested in the linear system Find the solution to this linear system.   Suppose now that the computer has accumulated some error in one of the entries of this system so that it incorrectly stores the system as Find the solution to this linear system.   Notice how a small error in one of the entries in the linear system leads to a solution that has a dramatically large error. Fortunately, this is an issue that has been well studied, and there are techniques that mitigate this type of behavior.         The exact solution is while the approximate solution is     This solution to the first system is and . However, with just a small change in the system, we find the solution and .         The exact solution is However, if we compute with only 10 bits, we find the approximate solution     This solution to the first system is and . However, with just a small change in the system, we find the solution and .     "
},
{
  "id": "sec-pivots",
  "level": "1",
  "url": "sec-pivots.html",
  "type": "Section",
  "number": "17.4",
  "title": "Pivots and their influence on solution spaces",
  "body": " Pivots and their influence on solution spaces   By now, we have seen several examples illustrating how the reduced row echelon matrix leads to a convenient description of the solution space to a linear system. In this section, we will use this understanding to make some general observations about how certain features of the reduced row echelon matrix reflect the nature of the solution space.  Remember that a leading entry in a reduced row echelon matrix is the leftmost nonzero entry in a row of the matrix. As we'll see, the positions of these leading entries encode a lot of information about the solution space of the corresponding linear system. For this reason, we make the following definition.   pivot position  A pivot position in a matrix is the position of a leading entry in the reduced row echelon matrix of .   For instance, in this reduced row echelon matrix, the pivot positions are indicated in bold: We can refer to pivot positions by their row and column number saying, for instance, that there is a pivot position in the second row and fourth column.   Some basic observations about pivots      Shown below is a matrix and its reduced row echelon form. Indicate the pivot positions. .   How many pivot positions can there be in one row? In a matrix, what is the largest possible number of pivot positions? Give an example of a matrix that has the largest possible number of pivot positions.   How many pivots can there be in one column? In a matrix, what is the largest possible number of pivot positions? Give an example of a matrix that has the largest possible number of pivot positions.   Give an example of a matrix with a pivot position in every row and every column. What is special about such a matrix?        The pivot positions are indicated below   A row contains at most one pivot position. Therefore, a matrix, which has three rows, contains at most three pivot positions. Here is an example:   A column contains at most one pivot position. Therefore, a matrix, which has three columns, contains at most three pivot positions. Here is an example   A matrix with a pivot position in every row and every column would have the following reduced row echelon form: Such a matrix must necessarily have the same number of rows and columns, which means it is what we call a square matrix.        The pivot positions are indicated below   Three.  Three.  Such a matrix must necessarily have the same number of rows and columns, which means it is what we call a square matrix.      When we have looked at solution spaces of linear systems, we have frequently asked whether there are infinitely many solutions, exactly one solution, or no solutions. We will now break this question into two separate questions.   Two Fundamental Questions  When we encounter a linear system, we often ask   Existence  Is there a solution to the linear system? If so, we say that the system is consistent ; if not, we say it is inconsistent . consistent system  inconsistent system     Uniqueness  If the linear system is consistent, is the solution unique or are there infinitely many solutions?      These two questions represent two sides of a coin that appear in many variations throughout our explorations. In this section, we will study how the location of the pivots influence the answers to these two questions. We begin by considering the first question concerning the existence of solutions.    The existence of solutions       Shown below are three augmented matrices in reduced row echelon form.             For each matrix, identify the pivot positions and determine if the corresponding linear system is consistent. Explain how the location of the pivots determines whether the system is consistent or inconsistent.    Each of the augmented matrices above has a row in which each entry is zero. What, if anything, does the presence of such a row tell us about the consistency of the corresponding linear system?   Give an example of a augmented matrix in reduced row echelon form that represents a consistent system. Indicate the pivot positions in your matrix and explain why these pivot positions guarantee a consistent system.   Give an example of a augmented matrix in reduced row echelon form that represents an inconsistent system. Indicate the pivot positions in your matrix and explain why these pivot positions guarantee an inconsistent system.   Write the reduced row echelon form of the coefficient matrix of the corresponding linear system in ? (Remember that the Augmentation Principle says that the reduced row echelon form of the coefficient matrix simply consists of the first four columns of the augmented matrix.) What do you notice about the pivot positions in this coefficient matrix?   Suppose we have a linear system for which the coefficient matrix has the following reduced row echelon form. What can you say about the consistency of the linear system?        The pivot positions are indicated below.           The first two augmented matrices correspond to consistent linear systems. The third does not, however, since the third row corresponds to the equation .  In general, a linear system is inconsistent exactly when there is a pivot position in the rightmost column of the augmented matrix.  A row in which every entry is zero corresponds to the equation , which is always true. Such an equation has no bearing on the consistency of the linear system.   This corresponds to a consistent system because there is not a pivot in the rightmost column.   This is an inconsistent system because the third row corresponds to the equation , which is never satisfied.   In the coefficient matrix, there is a row without a pivot position so that each entry is . This allows a pivot position to appear in the rightmost column of the augmented matrix.  This linear system must be consistent because the augmented matrix cannot have a pivot position in the rightmost column.      Let's summarize the results of this activity by considering the following reduced row echelon matrix: . In terms of variables , , and , the final equation says . If we evaluate the left-hand side with any values of , , and , we get 0, which means that the equation always holds. Therefore, its presence has no effect on the solution space defined by the other three equations.  The third equation, however, says that . Again, if we evaluate the left-hand side with any values of , , and , we get 0 so this equation cannot be satisfied for any . This means that the entire linear system has no solution and is therefore inconsistent.  An equation like this appears in the reduced row echelon matrix as . The pivot positions make this condition clear: the system is inconsistent if there is a pivot position in the rightmost column of the corresponding augmented matrix.   In fact, we will soon see that the system is consistent if there is not a pivot in the rightmost column of the corresponding augmented matrix. This leaves us with the following    A linear system is inconsistent if and only if there is a pivot position in the rightmost column of the corresponding augmented matrix.    This also says something about the pivot positions of the coefficient matrix. Consider an example of an inconsistent system corresponding to the reduced row echelon form of the following augmented matrix . The Augmentation Principle says that that the reduced row echelon form of the coefficient matrix is which shows that the coefficient matrix has a row without a pivot position. To turn this around, we see that if every row of the coefficient matrix has a pivot position, then the system must be consistent. For instance, if our linear system has a coefficient matrix whose reduced row echelon form is , then we can guarantee that the linear system is consistent because there is no way to obtain a pivot in the rightmost column of the augmented matrix.   If every row of the coefficient matrix has a pivot position, then the corresponding system of linear equations is consistent.     The uniqueness of solutions  Now that we have studied the role that pivot positions play in the existence of solutions, let's turn to the question of uniqueness.     Here are the three augmented matrices in reduced row echelon form that we considered in the previous section.             For each matrix, identify the pivot positions and state whether the corresponding linear system is consistent. If the system is consistent, explain whether the solution is unique or whether there are infinitely many solutions.   If possible, give an example of a augmented matrix that corresponds to a linear system having a unique solution. If it is not possible, explain why.  If possible, give an example of a augmented matrix that corresponds to a linear system having a unique solution. If it is not possible, explain why.   What condition on the pivot positions guarantees that a linear system has a unique solution?   If a linear system has a unique solution, what can we say about the relationship between the number of equations and the number of variables?        The pivot positions are indicated below.           The third linear system is inconsistent. The first system is consistent and has exactly one solution because , , and . The second system is consistent and has infinitely many solutions since we can write the equations as   This is not possible as we see by considering the shape of a typical matrix: In this case, the variable is free meaning there are infinitely many solutions.  This is possible as the following matrix illustrates:   If every column of the coefficient matrix has a pivot position, we can guarantee that the solution is unique.  If the coefficient matrix has a pivot position in every column, there must be at least as many rows as columns. Therefore, the number of equations must be less than or equal to the number of variables.      Let's consider what we've learned in this activity. Since we are interested in the question of whether a consistent linear system has a unique solution or infinitely many, we will only consider consistent systems. By the results of the previous section, this means that there is not a pivot in the rightmost column of the augmented matrix. Here are two possible examples:          In the first example, we have the equations demonstrating the fact that there is a unique solution .  In the second example, we have the equations that we may rewrite in parametric form as . Here we see that and are basic variables that may be expressed in terms of the free variable . In this case, the presence of the free variable leads to infinitely many solutions.  Remember that every column of the coefficient matrix corresponds to a variable in our linear system. In the first example, we see that every column of the coefficient contains a pivot position, which means that every variable is uniquely determined. In the second example, the column of the coefficient matrix corresponding to does not contain a pivot position, which results in appearing as a free variable. This illustrates the following principle.    Suppose that we consider a consistent linear system.  If every column of the coefficient matrix contains a pivot position, then the system has a unique solution.  If there is a column in the coefficient matrix that contains no pivot position, then the system has infinitely many solutions.  Columns that contain a pivot position correspond to basic variables while columns that do not correspond to free variables.      When a linear system has a unique solution, every column of the coefficient matrix has a pivot position. Since every row contains at most one pivot position, there must be at least as many rows as columns in the coefficient matrix. Therefore, the linear system has at least as many equations as variables, which is something we intuitively suspected in .  It is reasonable to ask how we choose the free variables. For instance, if we have a single equation , then we may write or, equivalently, . Clearly, either variable may be considered as a free variable in this case.  As we'll see in the future, we are more interested in the number of free variables rather than in their choice. For convenience, we will adopt the convention that free variables correspond to columns without a pivot position, which allows us to quickly identify them. For example, the variables and appear as free variables in the following linear system: .    Summary  We have seen how the locations of pivot positions, in both the augmented and coefficient matrices, give vital information about the existence and uniqueness of solutions to linear systems. More specifically,     A linear system is inconsistent exactly when a pivot position appears in the rightmost column of the augmented matrix.   If a linear system is consistent, the solution is unique when every column of the coefficient matrix contains a pivot position. There are infinitely many solutions when there is a column of the coefficient matrix without a pivot position.   If a linear system is consistent, the columns of the coefficient matrix containing pivot positions correspond to basic variables and the columns without pivot positions correspond to free variables.       For each of the augmented matrices in reduced row echelon form given below, determine whether the corresponding linear system is consistent and, if so, determine whether the solution is unique. If the system is consistent, identify the free variables and the basic variables and give a description of the solution space in parametric form.    .    .    .    .       The linear system is consistent because there is not a pivot position in the rightmost column, and there are infinitely many solutions.  The linear system is consistent because there is not a pivot position in the rightmost column, and there are infinitely many solutions.  The linear system is inconsistent because there is a pivot position in the rightmost column.  The linear system is consistent, and there is a unique solution.       The linear system is consistent because there is not a pivot position in the rightmost column. The variable is free while , , and are basic. There are infinitely many solutions.  The linear system is consistent because there is not a pivot position in the rightmost column. The variable is free while , , and are basic. There are infinitely many solutions.  The linear system is inconsistent because there is a pivot position in the rightmost column.  The linear system is consistent with , , and all being basic variables. There is a unique solution.      For each of the following linear systems, determine whether the system is consistent, and, if so, determine whether there are infinitely many solutions.                        The linear system is consistent with a unique solution.    The linear system is consistent with infinitely many solutions.    The linear system is inconsistent.         The corresponding augmented matrix is so this linear system is consistent with a unique solution.    The corresponding augmented matrix is so this linear system is consistent with infinitely many solutions.    The corresponding augmented matrix is so this linear system is inconsistent.       Include an example of an appropriate matrix as you justify your responses to the following questions.  Suppose a linear system having six equations and three variables is consistent. Can you guarantee that the solution is unique? Can you guarantee that there are infinitely many solutions?  Suppose that a linear system having three equations and six variables is consistent. Can you guarantee that the solution is unique? Can you guarantee that there are infinitely many solutions?  Suppose that a linear system is consistent and has a unique solution. What can you guarantee about the pivot positions in the augmented matrix?       We cannot guarantee either possibility.  We can guarantee there are infinitely many solutions.  There is a pivot position in each column of the coefficient matrix.       Consider the augmented matrices These examples show that anything is possible: the system may or may not be consistent, and, if it is consistent, there may or may not be a unique solution.  Consider the augmented matrix There must be a column without a pivot position so we can guarantee that there are infinitely many solutions.  Consider the augmented matrix We know that every column of the coefficient matrix has a pivot position and that the rightmost column does not. There must be at least as many equations as variables.      Determine whether the following statements are true or false and provide a justification for your response.  If the coefficient matrix of a linear system has a pivot in the rightmost column, then the system is inconsistent.  If a linear system has two equations and four variables, then it must be consistent.  If a linear system having four equations and three variables is consistent, then the solution is unique.  Suppose that a linear system has four equations and four variables and that the coefficient matrix has four pivots. Then the linear system is consistent and has a unique solution.  Suppose that a linear system has five equations and three variables and that the coefficient matrix has a pivot position in every column. Then the linear system is consistent and has a unique solution.       False.  False.  False.  True.  False.       This statement is false. If the augmented matrix has a pivot in the rightmost column, then the system is inconsistent.  This statement is false as illustrated by the matrix   This statement is false as illustrated by the matrix   This statement is true as illustrated by the matrix   This statement is false as illustrated by the matrix       We began our explorations in by noticing that the solution spaces of linear systems with more equations seem to be smaller. Let's reexamine this idea using what we know about pivot positions.   Remember that the solution space of a single linear equation in three variables is a plane. Can two planes ever intersect in a single point? What are the possible ways in which two planes can intersect? How can our understanding of pivot positions help answer these questions?    Suppose that a consistent linear system has more variables than equations. By considering the possible pivot positions, what can you say with certainty about the solution space?    If a linear system has many more equations than variables, why is it reasonable to expect the system to be inconsistent?         The planes will most likely intersect either in a line or not at all.    There must be infinitely many solutions.    There will most likely be a pivot position in the rightmost column of the augmented matrix.         The intersection of two planes is described by a linear system having two equations and three variables. Two possible reduced row echelon forms are indicating that the planes either intersect in a line or do not intersect at all. In particular, it is not possible for there to be a single point of intersection.    If there are more variables than equations, there must be a column in the coefficient matrix that does not contain a pivot position: so we can guarantee that there are infinitely many solutions.    If there are many more equations than variables, the coefficient matrix will have many rows without a pivot position: This makes it likely that there will be a pivot position in the rightmost column of the augmented matrix, which means that the linear system is inconsistent.       The following linear systems contain either one or two parameters.  For what values of the parameter is the following system consistent? For which of those values is the solution unique? .  For what values of the parameters and l is the following system consistent? For which of those values is the solution unique? .      The system is consistent when , and the solution can never be unique. If , the system is inconsistent.  The system is inconsistent if and . There are infinitely many solutions if and . There is a unique solution if .       We have This shows that the system is consistent when , and the solution can never be unique. If , the system is inconsistent.  We have The system is inconsistent if and . There are infinitely many solutions if and . There is a unique solution if .      Consider the linear system described by the following augmented matrix. .  Find a choice for the parameters , , and that causes the linear system to be inconsistent. Explain why your choice has this property.  Find a choice for the parameters , , and that causes the linear system to have a unique solution. Explain why your choice has this property.  Find a choice for the parameters , , and that causes the linear system to have infinitely many solutions. Explain why your choice has this property.     Some example choices are given here, but there are many more.   .   and .   , , and .     We have   If and , then the system will be inconsistent. We can choose .  We want there to be a pivot position in every column so we choose . For instance , , and works.  We need for the third and fourth columns to lack pivot positions so we choose and . For instance, , , and works.      A linear system where the right hand side of every equation is 0 is called homogeneous . The augmented matrix of a homogeneous system, for instance, has the following form: .   Using the concepts we've seen in this section, explain why a homogeneous linear system must be consistent.    What values for the variables are guaranteed to give a solution? Use this to offer another explanation for why a homogeneous linear system is consistent.   Suppose that a homogeneous linear system has a unique solution.   Give an example of such a system by writing its augmented matrix in reduced row echelon form.   Write just the coefficient matrix for the example you gave in the previous part. What can you say about the pivot positions in the coefficient matrix? Explain why your observation must hold for any homogeneous system having a unique solution.   If a homogeneous system of equations has a unique solution, what can you say about the number of equations compared to the number of variables?         There cannot be a pivot in the rightmost column.  There is a solution when all the variables are set to zero.  There are at least as many equations as variables.       Since all the entries in the rightmost column are zero, there cannot be a pivot in the rightmost column.  We have a solution when all the variables are zero. Therefore, the system must be consistent.  We must have a pivot position in every column of the coefficient matrix so the augmented matrix could look like Since every column of the coefficient matrix has a pivot position, there must be at least as many rows as columns. This means that there must be at least as many equations as variables.      In a previous math class, you have probably seen the fact that, if we are given two points in the plane, then there is a unique line passing through both of them. In this problem, we will begin with the four points on the left below and ask to find a polynomial that passes through these four points as shown on the right.      A degree three polynomial can be written as where , , , and are coefficients that we would like to determine. Since we want the polynomial to pass through the point , we should require that . In this way, we obtain a linear equation for the coefficients , , , and .   Write the four linear equations for the coefficients obtained by requiring that the graph of the polynomial passes through the four points above.   Write the augmented matrix corresponding to this system of equations and use the Sage cell below to solve for the coefficients.    Write the polynomial that you found and check your work by graphing it in the Sage cell below and verifying that it passes through the four points. To plot a function over a range, you may use a command like plot(1 + x- 2*x^2, xmin = -1, xmax = 4) .    Rather than looking for a degree three polynomial, suppose we wanted to find a polynomial that passes through the four points and that has degree two, such as . Solve the linear system for the coefficients. What can you say about the existence and uniqueness of a degree two polynomial passing through these four points?    Rather than looking for a degree three polynomial, suppose we wanted to find a polynomial that passes through the four points and that has degree four, such as . Solve the linear system for the coefficients. What can you say about the existence and uniqueness of a degree four polynomial passing through these four points?    Suppose you had 10 points and you wanted to find a polynomial passing through each of them. What should the degree of the polynomial be to guarantee that there is exactly one such polynomial? Explain your response.       The four equations we find are   The coefficients are , , , and .  The polynomial is   The linear system is inconsistent.  The linear system is consistent with infinitely many solutions.  Nine.       The four equations we find are   This leads to the augmented matrix This gives the coefficients , , , and .  The polynomial passes through the four given points.  We have the augmented matrix This system is therefore inconsistent, which means there is no degree 2 polynomial passing through the four points.  The augmented matrix we find is This shows that there are infinitely many polynomials whose degree is four and that pass through the four points.  If there are 10 points, we should find a polynomial having 10 coefficients. This means that the degree of the polynomial should be 9.      "
},
{
  "id": "sec-pivots-2-3",
  "level": "2",
  "url": "sec-pivots.html#sec-pivots-2-3",
  "type": "Definition",
  "number": "17.4.1",
  "title": "",
  "body": " pivot position  A pivot position in a matrix is the position of a leading entry in the reduced row echelon matrix of .  "
},
{
  "id": "sec-pivots-2-5",
  "level": "2",
  "url": "sec-pivots.html#sec-pivots-2-5",
  "type": "Preview Activity",
  "number": "17.4.1",
  "title": "Some basic observations about pivots.",
  "body": " Some basic observations about pivots      Shown below is a matrix and its reduced row echelon form. Indicate the pivot positions. .   How many pivot positions can there be in one row? In a matrix, what is the largest possible number of pivot positions? Give an example of a matrix that has the largest possible number of pivot positions.   How many pivots can there be in one column? In a matrix, what is the largest possible number of pivot positions? Give an example of a matrix that has the largest possible number of pivot positions.   Give an example of a matrix with a pivot position in every row and every column. What is special about such a matrix?        The pivot positions are indicated below   A row contains at most one pivot position. Therefore, a matrix, which has three rows, contains at most three pivot positions. Here is an example:   A column contains at most one pivot position. Therefore, a matrix, which has three columns, contains at most three pivot positions. Here is an example   A matrix with a pivot position in every row and every column would have the following reduced row echelon form: Such a matrix must necessarily have the same number of rows and columns, which means it is what we call a square matrix.        The pivot positions are indicated below   Three.  Three.  Such a matrix must necessarily have the same number of rows and columns, which means it is what we call a square matrix.     "
},
{
  "id": "fundamental-questions",
  "level": "2",
  "url": "sec-pivots.html#fundamental-questions",
  "type": "Question",
  "number": "17.4.2",
  "title": "Two Fundamental Questions.",
  "body": " Two Fundamental Questions  When we encounter a linear system, we often ask   Existence  Is there a solution to the linear system? If so, we say that the system is consistent ; if not, we say it is inconsistent . consistent system  inconsistent system     Uniqueness  If the linear system is consistent, is the solution unique or are there infinitely many solutions?     "
},
{
  "id": "sec-pivots-3-2",
  "level": "2",
  "url": "sec-pivots.html#sec-pivots-3-2",
  "type": "Activity",
  "number": "17.4.2",
  "title": "",
  "body": "     Shown below are three augmented matrices in reduced row echelon form.             For each matrix, identify the pivot positions and determine if the corresponding linear system is consistent. Explain how the location of the pivots determines whether the system is consistent or inconsistent.    Each of the augmented matrices above has a row in which each entry is zero. What, if anything, does the presence of such a row tell us about the consistency of the corresponding linear system?   Give an example of a augmented matrix in reduced row echelon form that represents a consistent system. Indicate the pivot positions in your matrix and explain why these pivot positions guarantee a consistent system.   Give an example of a augmented matrix in reduced row echelon form that represents an inconsistent system. Indicate the pivot positions in your matrix and explain why these pivot positions guarantee an inconsistent system.   Write the reduced row echelon form of the coefficient matrix of the corresponding linear system in ? (Remember that the Augmentation Principle says that the reduced row echelon form of the coefficient matrix simply consists of the first four columns of the augmented matrix.) What do you notice about the pivot positions in this coefficient matrix?   Suppose we have a linear system for which the coefficient matrix has the following reduced row echelon form. What can you say about the consistency of the linear system?        The pivot positions are indicated below.           The first two augmented matrices correspond to consistent linear systems. The third does not, however, since the third row corresponds to the equation .  In general, a linear system is inconsistent exactly when there is a pivot position in the rightmost column of the augmented matrix.  A row in which every entry is zero corresponds to the equation , which is always true. Such an equation has no bearing on the consistency of the linear system.   This corresponds to a consistent system because there is not a pivot in the rightmost column.   This is an inconsistent system because the third row corresponds to the equation , which is never satisfied.   In the coefficient matrix, there is a row without a pivot position so that each entry is . This allows a pivot position to appear in the rightmost column of the augmented matrix.  This linear system must be consistent because the augmented matrix cannot have a pivot position in the rightmost column.     "
},
{
  "id": "thm-pivot-inconsistency",
  "level": "2",
  "url": "sec-pivots.html#thm-pivot-inconsistency",
  "type": "Proposition",
  "number": "17.4.3",
  "title": "",
  "body": "  A linear system is inconsistent if and only if there is a pivot position in the rightmost column of the corresponding augmented matrix.   "
},
{
  "id": "sec-pivots-3-9",
  "level": "2",
  "url": "sec-pivots.html#sec-pivots-3-9",
  "type": "Proposition",
  "number": "17.4.4",
  "title": "",
  "body": " If every row of the coefficient matrix has a pivot position, then the corresponding system of linear equations is consistent.  "
},
{
  "id": "sec-pivots-4-3",
  "level": "2",
  "url": "sec-pivots.html#sec-pivots-4-3",
  "type": "Activity",
  "number": "17.4.3",
  "title": "",
  "body": "   Here are the three augmented matrices in reduced row echelon form that we considered in the previous section.             For each matrix, identify the pivot positions and state whether the corresponding linear system is consistent. If the system is consistent, explain whether the solution is unique or whether there are infinitely many solutions.   If possible, give an example of a augmented matrix that corresponds to a linear system having a unique solution. If it is not possible, explain why.  If possible, give an example of a augmented matrix that corresponds to a linear system having a unique solution. If it is not possible, explain why.   What condition on the pivot positions guarantees that a linear system has a unique solution?   If a linear system has a unique solution, what can we say about the relationship between the number of equations and the number of variables?        The pivot positions are indicated below.           The third linear system is inconsistent. The first system is consistent and has exactly one solution because , , and . The second system is consistent and has infinitely many solutions since we can write the equations as   This is not possible as we see by considering the shape of a typical matrix: In this case, the variable is free meaning there are infinitely many solutions.  This is possible as the following matrix illustrates:   If every column of the coefficient matrix has a pivot position, we can guarantee that the solution is unique.  If the coefficient matrix has a pivot position in every column, there must be at least as many rows as columns. Therefore, the number of equations must be less than or equal to the number of variables.     "
},
{
  "id": "sec-pivots-4-8",
  "level": "2",
  "url": "sec-pivots.html#sec-pivots-4-8",
  "type": "Principle",
  "number": "17.4.5",
  "title": "",
  "body": "  Suppose that we consider a consistent linear system.  If every column of the coefficient matrix contains a pivot position, then the system has a unique solution.  If there is a column in the coefficient matrix that contains no pivot position, then the system has infinitely many solutions.  Columns that contain a pivot position correspond to basic variables while columns that do not correspond to free variables.     "
},
{
  "id": "sec-pivots-6-1",
  "level": "2",
  "url": "sec-pivots.html#sec-pivots-6-1",
  "type": "Exercise",
  "number": "17.4.4.1",
  "title": "",
  "body": " For each of the augmented matrices in reduced row echelon form given below, determine whether the corresponding linear system is consistent and, if so, determine whether the solution is unique. If the system is consistent, identify the free variables and the basic variables and give a description of the solution space in parametric form.    .    .    .    .       The linear system is consistent because there is not a pivot position in the rightmost column, and there are infinitely many solutions.  The linear system is consistent because there is not a pivot position in the rightmost column, and there are infinitely many solutions.  The linear system is inconsistent because there is a pivot position in the rightmost column.  The linear system is consistent, and there is a unique solution.       The linear system is consistent because there is not a pivot position in the rightmost column. The variable is free while , , and are basic. There are infinitely many solutions.  The linear system is consistent because there is not a pivot position in the rightmost column. The variable is free while , , and are basic. There are infinitely many solutions.  The linear system is inconsistent because there is a pivot position in the rightmost column.  The linear system is consistent with , , and all being basic variables. There is a unique solution.    "
},
{
  "id": "sec-pivots-6-2",
  "level": "2",
  "url": "sec-pivots.html#sec-pivots-6-2",
  "type": "Exercise",
  "number": "17.4.4.2",
  "title": "",
  "body": " For each of the following linear systems, determine whether the system is consistent, and, if so, determine whether there are infinitely many solutions.                        The linear system is consistent with a unique solution.    The linear system is consistent with infinitely many solutions.    The linear system is inconsistent.         The corresponding augmented matrix is so this linear system is consistent with a unique solution.    The corresponding augmented matrix is so this linear system is consistent with infinitely many solutions.    The corresponding augmented matrix is so this linear system is inconsistent.     "
},
{
  "id": "sec-pivots-6-3",
  "level": "2",
  "url": "sec-pivots.html#sec-pivots-6-3",
  "type": "Exercise",
  "number": "17.4.4.3",
  "title": "",
  "body": " Include an example of an appropriate matrix as you justify your responses to the following questions.  Suppose a linear system having six equations and three variables is consistent. Can you guarantee that the solution is unique? Can you guarantee that there are infinitely many solutions?  Suppose that a linear system having three equations and six variables is consistent. Can you guarantee that the solution is unique? Can you guarantee that there are infinitely many solutions?  Suppose that a linear system is consistent and has a unique solution. What can you guarantee about the pivot positions in the augmented matrix?       We cannot guarantee either possibility.  We can guarantee there are infinitely many solutions.  There is a pivot position in each column of the coefficient matrix.       Consider the augmented matrices These examples show that anything is possible: the system may or may not be consistent, and, if it is consistent, there may or may not be a unique solution.  Consider the augmented matrix There must be a column without a pivot position so we can guarantee that there are infinitely many solutions.  Consider the augmented matrix We know that every column of the coefficient matrix has a pivot position and that the rightmost column does not. There must be at least as many equations as variables.    "
},
{
  "id": "sec-pivots-6-4",
  "level": "2",
  "url": "sec-pivots.html#sec-pivots-6-4",
  "type": "Exercise",
  "number": "17.4.4.4",
  "title": "",
  "body": " Determine whether the following statements are true or false and provide a justification for your response.  If the coefficient matrix of a linear system has a pivot in the rightmost column, then the system is inconsistent.  If a linear system has two equations and four variables, then it must be consistent.  If a linear system having four equations and three variables is consistent, then the solution is unique.  Suppose that a linear system has four equations and four variables and that the coefficient matrix has four pivots. Then the linear system is consistent and has a unique solution.  Suppose that a linear system has five equations and three variables and that the coefficient matrix has a pivot position in every column. Then the linear system is consistent and has a unique solution.       False.  False.  False.  True.  False.       This statement is false. If the augmented matrix has a pivot in the rightmost column, then the system is inconsistent.  This statement is false as illustrated by the matrix   This statement is false as illustrated by the matrix   This statement is true as illustrated by the matrix   This statement is false as illustrated by the matrix     "
},
{
  "id": "sec-pivots-6-5",
  "level": "2",
  "url": "sec-pivots.html#sec-pivots-6-5",
  "type": "Exercise",
  "number": "17.4.4.5",
  "title": "",
  "body": " We began our explorations in by noticing that the solution spaces of linear systems with more equations seem to be smaller. Let's reexamine this idea using what we know about pivot positions.   Remember that the solution space of a single linear equation in three variables is a plane. Can two planes ever intersect in a single point? What are the possible ways in which two planes can intersect? How can our understanding of pivot positions help answer these questions?    Suppose that a consistent linear system has more variables than equations. By considering the possible pivot positions, what can you say with certainty about the solution space?    If a linear system has many more equations than variables, why is it reasonable to expect the system to be inconsistent?         The planes will most likely intersect either in a line or not at all.    There must be infinitely many solutions.    There will most likely be a pivot position in the rightmost column of the augmented matrix.         The intersection of two planes is described by a linear system having two equations and three variables. Two possible reduced row echelon forms are indicating that the planes either intersect in a line or do not intersect at all. In particular, it is not possible for there to be a single point of intersection.    If there are more variables than equations, there must be a column in the coefficient matrix that does not contain a pivot position: so we can guarantee that there are infinitely many solutions.    If there are many more equations than variables, the coefficient matrix will have many rows without a pivot position: This makes it likely that there will be a pivot position in the rightmost column of the augmented matrix, which means that the linear system is inconsistent.     "
},
{
  "id": "sec-pivots-6-6",
  "level": "2",
  "url": "sec-pivots.html#sec-pivots-6-6",
  "type": "Exercise",
  "number": "17.4.4.6",
  "title": "",
  "body": " The following linear systems contain either one or two parameters.  For what values of the parameter is the following system consistent? For which of those values is the solution unique? .  For what values of the parameters and l is the following system consistent? For which of those values is the solution unique? .      The system is consistent when , and the solution can never be unique. If , the system is inconsistent.  The system is inconsistent if and . There are infinitely many solutions if and . There is a unique solution if .       We have This shows that the system is consistent when , and the solution can never be unique. If , the system is inconsistent.  We have The system is inconsistent if and . There are infinitely many solutions if and . There is a unique solution if .    "
},
{
  "id": "sec-pivots-6-7",
  "level": "2",
  "url": "sec-pivots.html#sec-pivots-6-7",
  "type": "Exercise",
  "number": "17.4.4.7",
  "title": "",
  "body": " Consider the linear system described by the following augmented matrix. .  Find a choice for the parameters , , and that causes the linear system to be inconsistent. Explain why your choice has this property.  Find a choice for the parameters , , and that causes the linear system to have a unique solution. Explain why your choice has this property.  Find a choice for the parameters , , and that causes the linear system to have infinitely many solutions. Explain why your choice has this property.     Some example choices are given here, but there are many more.   .   and .   , , and .     We have   If and , then the system will be inconsistent. We can choose .  We want there to be a pivot position in every column so we choose . For instance , , and works.  We need for the third and fourth columns to lack pivot positions so we choose and . For instance, , , and works.    "
},
{
  "id": "sec-pivots-6-8",
  "level": "2",
  "url": "sec-pivots.html#sec-pivots-6-8",
  "type": "Exercise",
  "number": "17.4.4.8",
  "title": "",
  "body": " A linear system where the right hand side of every equation is 0 is called homogeneous . The augmented matrix of a homogeneous system, for instance, has the following form: .   Using the concepts we've seen in this section, explain why a homogeneous linear system must be consistent.    What values for the variables are guaranteed to give a solution? Use this to offer another explanation for why a homogeneous linear system is consistent.   Suppose that a homogeneous linear system has a unique solution.   Give an example of such a system by writing its augmented matrix in reduced row echelon form.   Write just the coefficient matrix for the example you gave in the previous part. What can you say about the pivot positions in the coefficient matrix? Explain why your observation must hold for any homogeneous system having a unique solution.   If a homogeneous system of equations has a unique solution, what can you say about the number of equations compared to the number of variables?         There cannot be a pivot in the rightmost column.  There is a solution when all the variables are set to zero.  There are at least as many equations as variables.       Since all the entries in the rightmost column are zero, there cannot be a pivot in the rightmost column.  We have a solution when all the variables are zero. Therefore, the system must be consistent.  We must have a pivot position in every column of the coefficient matrix so the augmented matrix could look like Since every column of the coefficient matrix has a pivot position, there must be at least as many rows as columns. This means that there must be at least as many equations as variables.    "
},
{
  "id": "exercise-poly-fit",
  "level": "2",
  "url": "sec-pivots.html#exercise-poly-fit",
  "type": "Exercise",
  "number": "17.4.4.9",
  "title": "",
  "body": " In a previous math class, you have probably seen the fact that, if we are given two points in the plane, then there is a unique line passing through both of them. In this problem, we will begin with the four points on the left below and ask to find a polynomial that passes through these four points as shown on the right.      A degree three polynomial can be written as where , , , and are coefficients that we would like to determine. Since we want the polynomial to pass through the point , we should require that . In this way, we obtain a linear equation for the coefficients , , , and .   Write the four linear equations for the coefficients obtained by requiring that the graph of the polynomial passes through the four points above.   Write the augmented matrix corresponding to this system of equations and use the Sage cell below to solve for the coefficients.    Write the polynomial that you found and check your work by graphing it in the Sage cell below and verifying that it passes through the four points. To plot a function over a range, you may use a command like plot(1 + x- 2*x^2, xmin = -1, xmax = 4) .    Rather than looking for a degree three polynomial, suppose we wanted to find a polynomial that passes through the four points and that has degree two, such as . Solve the linear system for the coefficients. What can you say about the existence and uniqueness of a degree two polynomial passing through these four points?    Rather than looking for a degree three polynomial, suppose we wanted to find a polynomial that passes through the four points and that has degree four, such as . Solve the linear system for the coefficients. What can you say about the existence and uniqueness of a degree four polynomial passing through these four points?    Suppose you had 10 points and you wanted to find a polynomial passing through each of them. What should the degree of the polynomial be to guarantee that there is exactly one such polynomial? Explain your response.       The four equations we find are   The coefficients are , , , and .  The polynomial is   The linear system is inconsistent.  The linear system is consistent with infinitely many solutions.  Nine.       The four equations we find are   This leads to the augmented matrix This gives the coefficients , , , and .  The polynomial passes through the four given points.  We have the augmented matrix This system is therefore inconsistent, which means there is no degree 2 polynomial passing through the four points.  The augmented matrix we find is This shows that there are infinitely many polynomials whose degree is four and that pass through the four points.  If there are 10 points, we should find a polynomial having 10 coefficients. This means that the degree of the polynomial should be 9.    "
},
{
  "id": "sec-vectors-lin-combs",
  "level": "1",
  "url": "sec-vectors-lin-combs.html",
  "type": "Section",
  "number": "18.1",
  "title": "Vectors and linear combinations",
  "body": " Vectors and linear combinations   It is a remarkable fact that algebra, which is about symbolic equations and their solutions, and geometry are intimately connected. For instance, the solution set of a linear equation in two unknowns, such as , can be represented graphically by a straight line. The aim of this section is to further this connection by introducing vectors, which will help us to apply geometric intuition to our thinking about linear systems.    Vectors   vector A vector is most simply thought of as a matrix with a single column. For instance, and are both vectors. The entries in a vector are called its components. Since the vector has two components, we say that it is a two-dimensional vector; in the same way, the vector is a four-dimensional vector.  We denote the set of all -dimensional vectors by . Consequently, if is a 3-dimensional vector, we say that is in .  While it can be difficult to visualize a four-dimensional vector, we can draw a simple picture describing the two-dimensional vector , as shown in .  A graphical representation of the vector .       We can think of as describing a walk in the plane where we move two units horizontally and one unit vertically. Though we allow ourselves to begin walking from any point in the plane, we will most frequently begin at the origin in which case we arrive at the the point , as shown in the figure.  There are two simple algebraic operations we often perform on vectors.  Scalar Multiplication  scalar multiplication  We multiply a vector by a real number by multiplying each of the components of by . For instance, We will frequently refer to real numbers, such as -3 in this example, as scalars to distinguish them from vectors.   Vector Addition  vector addition  We add two vectors of the same dimension by adding their components. For instance,       Scalar Multiplication and Vector Addition   Suppose that      Find expressions for the vectors and sketch them using .  Sketch the vectors on this grid.         What geometric effect does scalar multiplication have on a vector? Also, describe the effect that multiplying by a negative scalar has.   Sketch the vectors using .  Sketch the vectors on this grid.         Consider vectors that have the form where is any scalar. Sketch a few of these vectors when, say, and . Give a geometric description of this set of vectors.  Sketch the vectors on this grid.        If and are two scalars, then the vector is called a linear combination of the vectors and . Find the vector that is the linear combination when and .    Can the vector be represented as a linear combination of and ? Asked differently, can we find scalars and such that .      Solutions to this preview activity are given in the text below.    The preview activity demonstrates how we may interpret scalar multiplication and vector addition geometrically.  First, we see that scalar multiplication has the effect of stretching or compressing a vector. Multiplying by a negative scalar changes the direction of the vector. In either case, shows that a scalar multiple of a vector lies on the same line defined by .     Scalar multiples of the vector .    To represent the sum , we imagine walking from the origin with the appropriate horizontal and vertical changes given by . From there, we continue our walk using the horizontal and vertical changes prescribed by , after which we arrive at the sum . This is illustrated on the left of where the tail of is placed on the tip of .      Vector addition as a simple walk in the plane is illustrated on the left. The vector sum is represented as the diagonal of a parallelogram on the right.    Alternatively, we may construct the parallelogram with and as two sides. The sum is then the diagonal of the parallelogram, as illustrated on the right of .  We have now seen that the set of vectors having the form is a line. To form the set of vectors , we can begin with the vector and add multiples of . Geometrically, this means that we begin from the tip of and move in a direction parallel to . The effect is to translate the line by the vector , as shown in .     The set of vectors form a line.    At times, it will be useful for us to think of vectors and points interchangeably. That is, we may wish to think of the vector as describing the point and vice-versa. When we say that the vectors having the form form a line, we really mean that the tips of the vectors all lie on the line passing through and parallel to .   Even though these vector operations are new, it is straightforward to check that some familiar properties hold.   Commutativity   .   Distributivity   .     Sage can perform scalar multiplication and vector addition. We define a vector using the vector command; then * and + denote scalar multiplication and vector addition.     Linear combinations  Linear combinations, which we encountered in the preview activity, provide the link between vectors and linear systems. In particular, they will help us apply geometric intuition to problems involving linear systems.   linear combination  weights  The linear combination of the vectors with scalars is the vector The scalars are called the weights of the linear combination.     In this activity, we will look at linear combinations of a pair of vectors, and .   Linear combinations of vectors and .      The weight is initially set to 0. Explain what happens as you vary while keeping . How is this related to scalar multiplication?   What is the linear combination of and when and ? You may find this result using the diagram, but you should also verify it by computing the linear combination.   Describe the vectors that arise when the weight is set to 1 and is varied. How is this related to our investigations in the preview activity?   Can the vector be expressed as a linear combination of and ? If so, what are the weights and ?   Can the vector be expressed as a linear combination of and ? If so, what are the weights and ?   Verify the result from the previous part by algebraically finding the weights and that form the linear combination .   Can the vector be expressed as a linear combination of and ? What about the vector ?   Are there any two-dimensional vectors that cannot be expressed as linear combinations of and ?       When we vary with , the linear combination moves along the line defined by .  When and , we find   When and is allowed to vary, the linear combinations lie on the line through parallel to .  If the weights and , then the linear combination is the vector .  If the weights and , then the linear combination is the vector .  We find the linear system for the weights: If we construct the corresponding augmented matrix and determine its reduced row echelon matrix, we find the weights and .  In the same way, we construct a linear system for the weights whose augmented matrix is which shows that there are weights that produce the desired linear combination. The same will happen for any vector that we ask to write as a linear combination of and .  Every two-dimensional vector can be written as a linear combination of and because the coefficient matrix of the linear system remains the same. Since that coefficient matrix has a pivot position in every row, the augmented matrix can never have a pivot position in the rightmost column.        The linear combinations lie on the line defined by .   .  They lie on the line through parallel to .  Yes, with weights .  Yes, with weights and .  This can be done by writing the appropriate linear system for the weights.  No, any two-dimensional vector can be expressed as a linear combination of and .      This activity illustrates how linear combinations are constructed geometrically: the linear combination is found by walking along a total of times followed by walking along a total of times. When one of the weights is held constant while the other varies, the vector moves along a line.    The previous activity also shows that questions about linear combinations lead naturally to linear systems. Suppose we have vectors and . Let's determine whether we can describe the vector as a linear combination of and . In other words, we would like to know whether there are weights and such that   This leads to the equations   Equating the components of the vectors on each side of the equation, we arrive at the linear system This means that is a linear combination of and if this linear system is consistent.  To solve this linear system, we construct its corresponding augmented matrix and find its reduced row echelon form, giving us the weights and ; that is, . In fact, we know more because the reduced row echelon matrix tells us that these are the only possible weights. Therefore, may be expressed as a linear combination of and in exactly one way.    This example demonstrates the connection between linear combinations and linear systems. Asking whether a vector is a linear combination of vectors is equivalent to asking whether an associated linear system is consistent.  In fact, we may easily describe the associated linear system in terms of the vectors , , and . Notice that the augmented matrix we found in our example was The first two columns of this matrix are and and the rightmost column is . As shorthand, we will write this augmented matrix replacing the columns with their vector representation: . This fact is generally true so we record it in the following proposition.    The vector is a linear combination of the vectors if and only if the linear system corresponding to the augmented matrix is consistent. A solution to this linear system gives weights such that .    The next activity puts this proposition to use.   Linear combinations and linear systems     Given the vectors , can be expressed as a linear combination of , , and ? Rephrase this question by writing a linear system for the weights , , and and use the Sage cell below to answer this question.   Consider the following linear system. Identify vectors , , , and such that the question \"Is this linear system consistent?\" is equivalent to the question \"Can be expressed as a linear combination of , , and ?\"   Consider the vectors . Can be expressed as a linear combination of , , and ? If so, can be written as a linear combination of these vectors in more than one way?    Considering the vectors , , and from the previous part, can we write every three-dimensional vector as a linear combination of these vectors? Explain how the pivot positions of the matrix help answer this question.   Now consider the vectors . Can be expressed as a linear combination of , , and ? If so, can be written as a linear combination of these vectors in more than one way?    Considering the vectors , , and from the previous part, can we write every three-dimensional vector as a linear combination of these vectors? Explain how the pivot positions of the matrix help answer this question.        We find the linear system with corresponding augmented matrix This shows that the linear system is inconsistent so there are no such weights , , and . This means that is not a linear combination of , , and .  We find vectors   This is the same as asking if the linear system corresponding to the following augmented matrix is consistent: From the reduced row echelon form, we see that the system is consistent, which means that can be expressed as a linear combination of , , and . Moreover, there are infinitely many ways in which we can do so.  No, it is not possible to write every three-dimensional as a linear combination of , , and because the matrix does not have a pivot position in every row. This means that, with some choice of vector , we will obtain an inconsistent system.  We find the augmented matrix This shows that can be expressed as a linear combination of , , and in exactly one way.  Every vector can be expressed as a linear combination of , , and in exactly one way because has a pivot position in every row and every column.        The vector cannot be expressed as a linear combination of , , and .  We find vectors   Yes, can be expressed as a linear combination of , , and in infinitely many ways.  No.  Yes, can be expressed as a linear combination of , , and in exactly one way.  Any vector can be expressed as a linear combination of , , and in exactly one way.        Consider the vectors and , as shown in .   Vectors and .      These vectors appear to lie on the same line, a fact that becomes apparent once we notice that . Intuitively, we think of the linear combination as the result of walking times in the direction and times in the direction. With these vectors, we are always walking along the same line so it would seem that any linear combination of these vectors should lie on the same line. In addition, a vector that is not on the line, say , should be not be expressible as a linear combination of and .  We can verify this by checking This shows that the associated linear system is inconsistent, which means that the vector cannot be written as a linear combination of and .  Notice that the reduced row echelon form of the coefficient matrix tells us to expect this. Since there is not a pivot position in the second row of the coefficient matrix , it is possible for a pivot position to appear in the rightmost column of the augmented matrix for some choice of .      Summary  This section has introduced vectors, linear combinations, and their connection to linear systems.   There are two operations we can perform with vectors: scalar multiplication and vector addition. Both of these operations have geometric meaning.   Given a set of vectors and a set of scalars we call weights, we can create a linear combination using scalar multiplication and vector addition.   A solution to the linear system whose augmented matrix is is a set of weights that expresses as a linear combination of .       Consider the vectors    Sketch these vectors below.     Compute the vectors , , , and and add them into the sketch above.  Sketch below the set of vectors having the form where is any scalar.     Sketch below the line . Then identify two vectors and so that this line is described by . Are there other choices for the vectors and ?                   This forms the line passing through parallel to .  There are many possibilities. One is and .               This forms the line passing through parallel to .  There are many possibilities. One is and .     Shown below are two vectors and       Express the labeled points as linear combinations of and .   Sketch the line described parametrically as .      We have   This is the line passing through parallel to .     We have   This is the line passing through parallel to .     Consider the vectors    Find the linear combination with weights , , and .   Can you write the vector as a linear combination of , , and ? If so, describe all the ways in which you can do so.    Can you write the vector as a linear combination using just the first two vectors  ? If so, describe all the ways in which you can do so.    Can you write as a linear combination of and ? If so, in how many ways?      The linear combination    and .   .   .     The linear combination   The appropriate linear system corresponds to the augmented matrix This shows we obtain when the weights are chosen so that   In this case, we want , which means that .  We see that .     Nutritional information about a breakfast cereal is printed on the box. For instance, one serving of Frosted Flakes has 111 calories, 140 milligrams of sodium, and 1.2 grams of protein. We may represent this as a vector . One serving of Cocoa Puffs has 120 calories, 105 milligrams of sodium, and 1.0 grams of protein.    Write the vector describing the nutritional content of Cocoa Puffs.   Suppose you eat servings of Frosted Flakes and servings of Cocoa Puffs. Use the language of vectors and linear combinations to express the quantities of calories, sodium, and protein you have consumed.   How many servings of each cereal have you eaten if you have consumed 342 calories, 385 milligrams of sodium, and 3.4 grams of protein.   Suppose your sister consumed 250 calories, 200 milligrams of sodium, and 4 grams of protein. What can you conclude about her breakfast?      .  The totals consumed are expressed by the vector   You had two servings of Frosted Flakes and one serving of Cocoa Puffs.  Your sister must have eaten something else.      .  The totals consumed are expressed by the vector   We ask to write the vector as a linear combination of the two cereal vectors. This leads to the augmented matrix This means that you had two servings of Frosted Flakes and one serving of Cocoa Puffs.  Now the augmented matrix is which represents an inconsistent system. This means that your sister must have eaten something else.     Consider the vectors     Can you express the vector as a linear combination of , , and ? If so, describe all the ways in which you can do so.   Can you express the vector as a linear combination of , , and ? If so, describe all the ways in which you can do so.   Show that can be written as a linear combination of and .   Explain why any linear combination of , , and , can be rewritten as a linear combination of just and .     The vector may be expressed as a linear combination of , , and provided that the weights are related by and .  The vector cannot be written as a linear combination of , , and .   .   .     We have the system corresponding to the augmented matrix from which we conclude that may be expressed as a linear combination of , , and provided that the weights are related by and .  Here we have which represents an inconsistent system and shows that cannot be written as a linear combination of , , and .  The augmented matrix is which shows that .   .     Consider the vectors For what value(s) of , if any, can the vector be written as a linear combination of and ?    .   We form the augmented matrix and find a triangular matrix that is row equivalent: This represents a consistent system when .    Determine whether the following statements are true or false and provide a justification for your response.  Given two vectors and , the vector is a linear combination of and .  Suppose is a collection of -dimensional vectors and that the matrix has a pivot position in every row. If is any -dimensional vector, then can be written as a linear combination of .  Suppose is a collection of -dimensional vectors and that the matrix has a pivot position in every row and every column. If is any -dimensional vector, then can be written as a linear combination of in exactly one way.  It is possible to find two 3-dimensional vectors and such that every 3-dimensional vector can be written as a linear combination of and .      True  True  True  False     True, because we can choose the weights and .  True, because the augmented matrix can never have a pivot position in the rightmost column.  True, because the augmented matrix can never have a pivot position in the rightmost column and the corresponding linear system cannot have a free variable.  False, because it is possible to choose a vector such that the augmented matrix has a pivot in the rightmost column.     A theme that will later unfold concerns the use of coordinate systems. We can identify the point with the tip of the vector , drawn emanating from the origin. We can then think of the usual Cartesian coordinate system in terms of linear combinations of the vectors For instance, the point is identified with the vector as shown on the left in .       The usual Cartesian coordinate system, defined by the vectors and , is shown on the left along with the representation of the point . The right shows a nonstandard coordinate system defined by vectors and .   If instead we have vectors , we may define a new coordinate system in which a point will correspond to the vector . For instance, the point is shown on the right side of .  Write the point in standard coordinates; that is, find and such that .   Write the point in the new coordinate system; that is, find and such that .   Convert a general point , expressed in the new coordinate system, into standard Cartesian coordinates .   What is the general strategy for converting a point from standard Cartesian coordinates to the new coordinates ? Actually implementing this strategy in general may take a bit of work so just describe the strategy. We will study this in more detail later.      .   .     Solve the linear system corresponding to the augmented matrix      We have   We have . Solving this equation, we find and , which means that .  As in the first part of this problem, we write   We want to solve by constructing the augmented matrix and finding its reduced row echelon form.      "
},
{
  "id": "fig-vector",
  "level": "2",
  "url": "sec-vectors-lin-combs.html#fig-vector",
  "type": "Figure",
  "number": "18.1.1",
  "title": "",
  "body": " A graphical representation of the vector .     "
},
{
  "id": "sec-vectors-lin-combs-3-7",
  "level": "2",
  "url": "sec-vectors-lin-combs.html#sec-vectors-lin-combs-3-7",
  "type": "Preview Activity",
  "number": "18.1.1",
  "title": "Scalar Multiplication and Vector Addition.",
  "body": " Scalar Multiplication and Vector Addition   Suppose that      Find expressions for the vectors and sketch them using .  Sketch the vectors on this grid.         What geometric effect does scalar multiplication have on a vector? Also, describe the effect that multiplying by a negative scalar has.   Sketch the vectors using .  Sketch the vectors on this grid.         Consider vectors that have the form where is any scalar. Sketch a few of these vectors when, say, and . Give a geometric description of this set of vectors.  Sketch the vectors on this grid.        If and are two scalars, then the vector is called a linear combination of the vectors and . Find the vector that is the linear combination when and .    Can the vector be represented as a linear combination of and ? Asked differently, can we find scalars and such that .      Solutions to this preview activity are given in the text below.   "
},
{
  "id": "fig-scalar-mult",
  "level": "2",
  "url": "sec-vectors-lin-combs.html#fig-scalar-mult",
  "type": "Figure",
  "number": "18.1.5",
  "title": "",
  "body": "    Scalar multiples of the vector .  "
},
{
  "id": "fig-vector-sum",
  "level": "2",
  "url": "sec-vectors-lin-combs.html#fig-vector-sum",
  "type": "Figure",
  "number": "18.1.6",
  "title": "",
  "body": "     Vector addition as a simple walk in the plane is illustrated on the left. The vector sum is represented as the diagonal of a parallelogram on the right.  "
},
{
  "id": "fig-parametric-line",
  "level": "2",
  "url": "sec-vectors-lin-combs.html#fig-parametric-line",
  "type": "Figure",
  "number": "18.1.7",
  "title": "",
  "body": "    The set of vectors form a line.  "
},
{
  "id": "sec-vectors-lin-combs-3-14",
  "level": "2",
  "url": "sec-vectors-lin-combs.html#sec-vectors-lin-combs-3-14",
  "type": "Observation",
  "number": "18.1.8",
  "title": "",
  "body": " Even though these vector operations are new, it is straightforward to check that some familiar properties hold.   Commutativity   .   Distributivity   .    "
},
{
  "id": "sec-vectors-lin-combs-4-3",
  "level": "2",
  "url": "sec-vectors-lin-combs.html#sec-vectors-lin-combs-4-3",
  "type": "Definition",
  "number": "18.1.9",
  "title": "",
  "body": " linear combination  weights  The linear combination of the vectors with scalars is the vector The scalars are called the weights of the linear combination.  "
},
{
  "id": "sec-vectors-lin-combs-4-4",
  "level": "2",
  "url": "sec-vectors-lin-combs.html#sec-vectors-lin-combs-4-4",
  "type": "Activity",
  "number": "18.1.2",
  "title": "",
  "body": "  In this activity, we will look at linear combinations of a pair of vectors, and .   Linear combinations of vectors and .      The weight is initially set to 0. Explain what happens as you vary while keeping . How is this related to scalar multiplication?   What is the linear combination of and when and ? You may find this result using the diagram, but you should also verify it by computing the linear combination.   Describe the vectors that arise when the weight is set to 1 and is varied. How is this related to our investigations in the preview activity?   Can the vector be expressed as a linear combination of and ? If so, what are the weights and ?   Can the vector be expressed as a linear combination of and ? If so, what are the weights and ?   Verify the result from the previous part by algebraically finding the weights and that form the linear combination .   Can the vector be expressed as a linear combination of and ? What about the vector ?   Are there any two-dimensional vectors that cannot be expressed as linear combinations of and ?       When we vary with , the linear combination moves along the line defined by .  When and , we find   When and is allowed to vary, the linear combinations lie on the line through parallel to .  If the weights and , then the linear combination is the vector .  If the weights and , then the linear combination is the vector .  We find the linear system for the weights: If we construct the corresponding augmented matrix and determine its reduced row echelon matrix, we find the weights and .  In the same way, we construct a linear system for the weights whose augmented matrix is which shows that there are weights that produce the desired linear combination. The same will happen for any vector that we ask to write as a linear combination of and .  Every two-dimensional vector can be written as a linear combination of and because the coefficient matrix of the linear system remains the same. Since that coefficient matrix has a pivot position in every row, the augmented matrix can never have a pivot position in the rightmost column.        The linear combinations lie on the line defined by .   .  They lie on the line through parallel to .  Yes, with weights .  Yes, with weights and .  This can be done by writing the appropriate linear system for the weights.  No, any two-dimensional vector can be expressed as a linear combination of and .     "
},
{
  "id": "sec-vectors-lin-combs-4-6",
  "level": "2",
  "url": "sec-vectors-lin-combs.html#sec-vectors-lin-combs-4-6",
  "type": "Example",
  "number": "18.1.11",
  "title": "",
  "body": "  The previous activity also shows that questions about linear combinations lead naturally to linear systems. Suppose we have vectors and . Let's determine whether we can describe the vector as a linear combination of and . In other words, we would like to know whether there are weights and such that   This leads to the equations   Equating the components of the vectors on each side of the equation, we arrive at the linear system This means that is a linear combination of and if this linear system is consistent.  To solve this linear system, we construct its corresponding augmented matrix and find its reduced row echelon form, giving us the weights and ; that is, . In fact, we know more because the reduced row echelon matrix tells us that these are the only possible weights. Therefore, may be expressed as a linear combination of and in exactly one way.   "
},
{
  "id": "prop-system-comb",
  "level": "2",
  "url": "sec-vectors-lin-combs.html#prop-system-comb",
  "type": "Proposition",
  "number": "18.1.12",
  "title": "",
  "body": "  The vector is a linear combination of the vectors if and only if the linear system corresponding to the augmented matrix is consistent. A solution to this linear system gives weights such that .   "
},
{
  "id": "sec-vectors-lin-combs-4-11",
  "level": "2",
  "url": "sec-vectors-lin-combs.html#sec-vectors-lin-combs-4-11",
  "type": "Activity",
  "number": "18.1.3",
  "title": "Linear combinations and linear systems.",
  "body": " Linear combinations and linear systems     Given the vectors , can be expressed as a linear combination of , , and ? Rephrase this question by writing a linear system for the weights , , and and use the Sage cell below to answer this question.   Consider the following linear system. Identify vectors , , , and such that the question \"Is this linear system consistent?\" is equivalent to the question \"Can be expressed as a linear combination of , , and ?\"   Consider the vectors . Can be expressed as a linear combination of , , and ? If so, can be written as a linear combination of these vectors in more than one way?    Considering the vectors , , and from the previous part, can we write every three-dimensional vector as a linear combination of these vectors? Explain how the pivot positions of the matrix help answer this question.   Now consider the vectors . Can be expressed as a linear combination of , , and ? If so, can be written as a linear combination of these vectors in more than one way?    Considering the vectors , , and from the previous part, can we write every three-dimensional vector as a linear combination of these vectors? Explain how the pivot positions of the matrix help answer this question.        We find the linear system with corresponding augmented matrix This shows that the linear system is inconsistent so there are no such weights , , and . This means that is not a linear combination of , , and .  We find vectors   This is the same as asking if the linear system corresponding to the following augmented matrix is consistent: From the reduced row echelon form, we see that the system is consistent, which means that can be expressed as a linear combination of , , and . Moreover, there are infinitely many ways in which we can do so.  No, it is not possible to write every three-dimensional as a linear combination of , , and because the matrix does not have a pivot position in every row. This means that, with some choice of vector , we will obtain an inconsistent system.  We find the augmented matrix This shows that can be expressed as a linear combination of , , and in exactly one way.  Every vector can be expressed as a linear combination of , , and in exactly one way because has a pivot position in every row and every column.        The vector cannot be expressed as a linear combination of , , and .  We find vectors   Yes, can be expressed as a linear combination of , , and in infinitely many ways.  No.  Yes, can be expressed as a linear combination of , , and in exactly one way.  Any vector can be expressed as a linear combination of , , and in exactly one way.     "
},
{
  "id": "sec-vectors-lin-combs-4-12",
  "level": "2",
  "url": "sec-vectors-lin-combs.html#sec-vectors-lin-combs-4-12",
  "type": "Example",
  "number": "18.1.13",
  "title": "",
  "body": "  Consider the vectors and , as shown in .   Vectors and .      These vectors appear to lie on the same line, a fact that becomes apparent once we notice that . Intuitively, we think of the linear combination as the result of walking times in the direction and times in the direction. With these vectors, we are always walking along the same line so it would seem that any linear combination of these vectors should lie on the same line. In addition, a vector that is not on the line, say , should be not be expressible as a linear combination of and .  We can verify this by checking This shows that the associated linear system is inconsistent, which means that the vector cannot be written as a linear combination of and .  Notice that the reduced row echelon form of the coefficient matrix tells us to expect this. Since there is not a pivot position in the second row of the coefficient matrix , it is possible for a pivot position to appear in the rightmost column of the augmented matrix for some choice of .   "
},
{
  "id": "sec-vectors-lin-combs-6-1",
  "level": "2",
  "url": "sec-vectors-lin-combs.html#sec-vectors-lin-combs-6-1",
  "type": "Exercise",
  "number": "18.1.4.1",
  "title": "",
  "body": " Consider the vectors    Sketch these vectors below.     Compute the vectors , , , and and add them into the sketch above.  Sketch below the set of vectors having the form where is any scalar.     Sketch below the line . Then identify two vectors and so that this line is described by . Are there other choices for the vectors and ?                   This forms the line passing through parallel to .  There are many possibilities. One is and .               This forms the line passing through parallel to .  There are many possibilities. One is and .   "
},
{
  "id": "sec-vectors-lin-combs-6-2",
  "level": "2",
  "url": "sec-vectors-lin-combs.html#sec-vectors-lin-combs-6-2",
  "type": "Exercise",
  "number": "18.1.4.2",
  "title": "",
  "body": " Shown below are two vectors and       Express the labeled points as linear combinations of and .   Sketch the line described parametrically as .      We have   This is the line passing through parallel to .     We have   This is the line passing through parallel to .   "
},
{
  "id": "sec-vectors-lin-combs-6-3",
  "level": "2",
  "url": "sec-vectors-lin-combs.html#sec-vectors-lin-combs-6-3",
  "type": "Exercise",
  "number": "18.1.4.3",
  "title": "",
  "body": " Consider the vectors    Find the linear combination with weights , , and .   Can you write the vector as a linear combination of , , and ? If so, describe all the ways in which you can do so.    Can you write the vector as a linear combination using just the first two vectors  ? If so, describe all the ways in which you can do so.    Can you write as a linear combination of and ? If so, in how many ways?      The linear combination    and .   .   .     The linear combination   The appropriate linear system corresponds to the augmented matrix This shows we obtain when the weights are chosen so that   In this case, we want , which means that .  We see that .   "
},
{
  "id": "sec-vectors-lin-combs-6-4",
  "level": "2",
  "url": "sec-vectors-lin-combs.html#sec-vectors-lin-combs-6-4",
  "type": "Exercise",
  "number": "18.1.4.4",
  "title": "",
  "body": " Nutritional information about a breakfast cereal is printed on the box. For instance, one serving of Frosted Flakes has 111 calories, 140 milligrams of sodium, and 1.2 grams of protein. We may represent this as a vector . One serving of Cocoa Puffs has 120 calories, 105 milligrams of sodium, and 1.0 grams of protein.    Write the vector describing the nutritional content of Cocoa Puffs.   Suppose you eat servings of Frosted Flakes and servings of Cocoa Puffs. Use the language of vectors and linear combinations to express the quantities of calories, sodium, and protein you have consumed.   How many servings of each cereal have you eaten if you have consumed 342 calories, 385 milligrams of sodium, and 3.4 grams of protein.   Suppose your sister consumed 250 calories, 200 milligrams of sodium, and 4 grams of protein. What can you conclude about her breakfast?      .  The totals consumed are expressed by the vector   You had two servings of Frosted Flakes and one serving of Cocoa Puffs.  Your sister must have eaten something else.      .  The totals consumed are expressed by the vector   We ask to write the vector as a linear combination of the two cereal vectors. This leads to the augmented matrix This means that you had two servings of Frosted Flakes and one serving of Cocoa Puffs.  Now the augmented matrix is which represents an inconsistent system. This means that your sister must have eaten something else.   "
},
{
  "id": "sec-vectors-lin-combs-6-5",
  "level": "2",
  "url": "sec-vectors-lin-combs.html#sec-vectors-lin-combs-6-5",
  "type": "Exercise",
  "number": "18.1.4.5",
  "title": "",
  "body": " Consider the vectors     Can you express the vector as a linear combination of , , and ? If so, describe all the ways in which you can do so.   Can you express the vector as a linear combination of , , and ? If so, describe all the ways in which you can do so.   Show that can be written as a linear combination of and .   Explain why any linear combination of , , and , can be rewritten as a linear combination of just and .     The vector may be expressed as a linear combination of , , and provided that the weights are related by and .  The vector cannot be written as a linear combination of , , and .   .   .     We have the system corresponding to the augmented matrix from which we conclude that may be expressed as a linear combination of , , and provided that the weights are related by and .  Here we have which represents an inconsistent system and shows that cannot be written as a linear combination of , , and .  The augmented matrix is which shows that .   .   "
},
{
  "id": "sec-vectors-lin-combs-6-6",
  "level": "2",
  "url": "sec-vectors-lin-combs.html#sec-vectors-lin-combs-6-6",
  "type": "Exercise",
  "number": "18.1.4.6",
  "title": "",
  "body": " Consider the vectors For what value(s) of , if any, can the vector be written as a linear combination of and ?    .   We form the augmented matrix and find a triangular matrix that is row equivalent: This represents a consistent system when .  "
},
{
  "id": "sec-vectors-lin-combs-6-7",
  "level": "2",
  "url": "sec-vectors-lin-combs.html#sec-vectors-lin-combs-6-7",
  "type": "Exercise",
  "number": "18.1.4.7",
  "title": "",
  "body": " Determine whether the following statements are true or false and provide a justification for your response.  Given two vectors and , the vector is a linear combination of and .  Suppose is a collection of -dimensional vectors and that the matrix has a pivot position in every row. If is any -dimensional vector, then can be written as a linear combination of .  Suppose is a collection of -dimensional vectors and that the matrix has a pivot position in every row and every column. If is any -dimensional vector, then can be written as a linear combination of in exactly one way.  It is possible to find two 3-dimensional vectors and such that every 3-dimensional vector can be written as a linear combination of and .      True  True  True  False     True, because we can choose the weights and .  True, because the augmented matrix can never have a pivot position in the rightmost column.  True, because the augmented matrix can never have a pivot position in the rightmost column and the corresponding linear system cannot have a free variable.  False, because it is possible to choose a vector such that the augmented matrix has a pivot in the rightmost column.   "
},
{
  "id": "sec-vectors-lin-combs-6-8",
  "level": "2",
  "url": "sec-vectors-lin-combs.html#sec-vectors-lin-combs-6-8",
  "type": "Exercise",
  "number": "18.1.4.8",
  "title": "",
  "body": " A theme that will later unfold concerns the use of coordinate systems. We can identify the point with the tip of the vector , drawn emanating from the origin. We can then think of the usual Cartesian coordinate system in terms of linear combinations of the vectors For instance, the point is identified with the vector as shown on the left in .       The usual Cartesian coordinate system, defined by the vectors and , is shown on the left along with the representation of the point . The right shows a nonstandard coordinate system defined by vectors and .   If instead we have vectors , we may define a new coordinate system in which a point will correspond to the vector . For instance, the point is shown on the right side of .  Write the point in standard coordinates; that is, find and such that .   Write the point in the new coordinate system; that is, find and such that .   Convert a general point , expressed in the new coordinate system, into standard Cartesian coordinates .   What is the general strategy for converting a point from standard Cartesian coordinates to the new coordinates ? Actually implementing this strategy in general may take a bit of work so just describe the strategy. We will study this in more detail later.      .   .     Solve the linear system corresponding to the augmented matrix      We have   We have . Solving this equation, we find and , which means that .  As in the first part of this problem, we write   We want to solve by constructing the augmented matrix and finding its reduced row echelon form.    "
},
{
  "id": "sec-matrices-lin-combs",
  "level": "1",
  "url": "sec-matrices-lin-combs.html",
  "type": "Section",
  "number": "18.2",
  "title": "Matrix multiplication and linear combinations",
  "body": " Matrix multiplication and linear combinations   The previous section introduced vectors and linear combinations and demonstrated how they provide a way to think about linear systems geometrically. In particular, we saw that the vector is a linear combination of the vectors precisely when the linear system corresponding to the augmented matrix is consistent.  Our goal in this section is to introduce matrix multiplication, another algebraic operation that deepens the connection between linear systems and linear combinations.    Scalar multiplication and addition of matrices   matrix, shape We first thought of a matrix as a rectangular array of numbers. If we say that the shape of a matrix is , we mean that it has rows and columns. For instance, the shape of the matrix below is : .  We may also think of the columns of a matrix as a set of vectors. For instance, the matrix above may be represented as where . In this way, we see that the matrix is equivalent to an ordered set of 4 vectors in .   matrix, addition  matrix, scalar multiplication This means that we may define scalar multiplication and matrix addition operations using the corresponding column-wise vector operations. For instance,    Matrix operations     Compute the scalar multiple .   Find the sum .   Suppose that and are two matrices. What do we need to know about their shapes before we can form the sum ?   matrix, identity  The matrix , which we call the identity matrix, is the matrix whose entries are zero except for the diagonal entries, all of which are 1. For instance, . If we can form the sum , what must be true about the matrix ?   Find the matrix where .            The shapes must be the same.  The shape of must be .               The shapes must be the same.  The shape of must be .        As this preview activity shows, the operations of scalar multiplication and addition of matrices are natural extensions of their vector counterparts. Some care, however, is required when adding matrices. Since we need the same number of vectors to add and since those vectors must be of the same dimension, two matrices must have the same shape if we wish to form their sum.    Matrix-vector multiplication and linear combinations  A more important operation will be matrix multiplication as it allows us to compactly express linear systems. We now introduce the product of a matrix and a vector with an example.   Matrix-vector multiplication   Suppose we have the matrix and vector : . Their product will be defined to be the linear combination of the columns of using the components of as weights. This means that   Because has two columns, we need two weights to form a linear combination of those columns, which means that must have two components. In other words, the number of columns of must equal the dimension of the vector .  Similarly, the columns of are 3-dimensional so any linear combination of them is 3-dimensional as well. Therefore, will be 3-dimensional.  We then see that if is a matrix, must be a 2-dimensional vector and will be 3-dimensional.    More generally, we have the following definition.   Matrix-vector multiplication  matrix-vector multiplication   The product of a matrix by a vector will be the linear combination of the columns of using the components of as weights. More specifically, if then .  If is an matrix, then must be an -dimensional vector, and the product will be an -dimensional vector.    The next activity explores some properties of matrix multiplication.   Matrix-vector multiplication     Find the matrix product .   Suppose that is the matrix . If is defined, what is the dimension of the vector and what is the dimension of ?   A vector whose entries are all zero is denoted by . If is a matrix, what is the product ?   Suppose that is the identity matrix and . Find the product and explain why is called the identity matrix.   Suppose we write the matrix in terms of its columns as . If the vector , what is the product ?    Suppose that . Is there a vector such that ?       We have   The dimension of must be the same as the number of columns of so is three-dimensional. The dimension of equals the number of rows of so is four-dimensional.  We have .  We have ; that is, multiplying a vector by produces the same vector.  The product .  If , then we have with corresponding augmented matrix This means that is the unique solution to the equation .         The dimension of must three, and the dimension of must be four.   .   .   .   is the unique solution.     Multiplication of a matrix and a vector is defined as a linear combination of the columns of . However, there is a shortcut for computing such a product. Let's look at our previous example and focus on the first row of the product. .  To find the first component of the product, we consider the first row of the matrix. We then multiply the first entry in that row by the first component of the vector, the second entry by the second component of the vector, and so on, and add the results. In this way, we see that the third component of the product would be obtained from the third row of the matrix by computing .  You are encouraged to evaluate the product of the previous activity using this shortcut and compare the result to what you found while completing that activity.    Sage can find the product of a matrix and vector using the * operator. For example,    Use Sage to evaluate the product from of the previous activity.    In Sage, define the matrix and vectors .   What do you find when you evaluate ?  What do you find when you evaluate and and compare your results?  What do you find when you evaluate and and compare your results?       We define A = matrix(3, 4, [1, 2, 0, -1, 2, 4, -3, -2, -1, -2, 6, 1]) v = vector([3, 1, -1, 1]) A*v   We define A = matrix(2, 3, [-2, 0, 3, 1, 4, 2]) zero = vector([0, 0]) v = vector([-2, 3]) w = vector([1, 2])    .   .         We define A = matrix(3, 4, [1, 2, 0, -1, 2, 4, -3, -2, -1, -2, 6, 1]) v = vector([3, 1, -1, 1]) A*v   We define A = matrix(2, 3, [-2, 0, 3, 1, 4, 2]) zero = vector([0, 0]) v = vector([-2, 3]) w = vector([1, 2])    .   .        This activity demonstrates several general properties satisfied by matrix multiplication that we record here.   Linearity of matrix multiplication   If is a matrix, and vectors of the appropriate dimensions, and a scalar, then    .    .    .        Matrix-vector multiplication and linear systems  So far, we have begun with a matrix and a vector and formed their product . We would now like to turn this around: beginning with a matrix and a vector , we will ask if we can find a vector such that . This will naturally lead back to linear systems.  To see the connection between the matrix equation and linear systems, let's write the matrix in terms of its columns and in terms of its components. .  We know that the matrix product forms a linear combination of the columns of . Therefore, the equation is merely a compact way of writing the equation for the weights : . We have seen this equation before: Remember that says that the solutions of this equation are the same as the solutions to the linear system whose augmented matrix is .  This gives us three different ways of looking at the same solution space.    If and , then the following statements are equivalent.   The vector satisfies the equation .   The vector is a linear combination of the columns of with weights : .   The components of form a solution to the linear system corresponding to the augmented matrix       When the matrix , we will frequently write and say that the matrix is augmented by the vector .  The equation gives a notationally compact way to write a linear system. Moreover, this notation will allow us to focus on important features of the system that determine its solution space.    We will describe the solution space of the equation   By , this equation may be equivalently expressed as , which is the linear system corresponding to the augmented matrix . The reduced row echelon form of the augmented matrix is which corresponds to the linear system The variable is free so we may write the solution space parametrically as   Since we originally asked to describe the solutions to the equation , we will express the solution in terms of the vector : As before, we call this a parametric description of the solution space.  This shows that the solutions may be written in the form , for appropriate vectors and . Geometrically, the solution space is a line in through moving parallel to .     The equation      Consider the linear system Identify the matrix and vector to express this system in the form .   If and are as below, write the linear system corresponding to the equation and describe its solution space, using a parametric description if appropriate:    Describe the solution space of the equation .   Suppose is an matrix. What can you guarantee about the solution space of the equation ?       and .  Form the augmented matrix so that   We have the augmented matrix Since this system is inconsistent, there are no solutions to the matrix equation.  We know that there is at least one solution, namely, .       and .     There are no solutions.  There is at least one solution, namely, .       Matrix-matrix products  In this section, we have developed some algebraic operations on matrices with the aim of simplifying our description of linear systems. We now introduce a final operation, the product of two matrices, that will become important when we study linear transformations in .   Matrix-matrix multiplication   Given matrices and , we form their product by first writing in terms of its columns and then defining       Given the matrices , we have .      It is important to note that we can only multiply matrices if the shapes of the matrices are compatible. More specifically, when constructing the product , the matrix multiplies the columns of . Therefore, the number of columns of must equal the number of rows of . When this condition is met, the number of rows of is the number of rows of , and the number of columns of is the number of columns of .      Consider the matrices .   Before computing, first explain why the shapes of and enable us to form the product . Then describe the shape of .   Compute the product .  Sage can multiply matrices using the * operator. Define the matrices and in the Sage cell below and check your work by computing .   Are we able to form the matrix product ? If so, use the Sage cell above to find . Is it generally true that ?  Suppose we form the three matrices. . Compare what happens when you compute and . State your finding as a general principle.   Compare the results of evaluating and and state your finding as a general principle.  When we are dealing with real numbers, we know if and , then . Define matrices and compute and . If , is it necessarily true that ?  Again, with real numbers, we know that if , then either or . Define and compute . If , is it necessarily true that either or ?       The product exists because the number of columns of equals the number of rows of . The dimensions of are .  We have .  Define A = matrix(2, 3, [1, 3, 2, -3, 4, -1]) B = matrix(3, 2, [3, 0, 1, 2, -2, -1]) A*B   Yes, we can form the product because the number of columns of equals the number of rows of . This product will be , however, so it must be true that .  We find that .  We find that .  It is not generally true that if , as illustrated by this example.  It is not generally true that or if , as illustrated by this example.      The product exists because the number of columns of equals the number of rows of . The dimensions of are .  We have .  Define A = matrix(2, 3, [1, 3, 2, -3, 4, -1]) B = matrix(3, 2, [3, 0, 1, 2, -2, -1]) A*B   It is not generally true that .  We find that .  We find that .  It is not generally true that if .  It is not generally true that or if .     This activity demonstrated some general properties about products of matrices, which mirror some properties about operations with real numbers.   Properties of Matrix-matrix Multiplication  If , , and are matrices such that the following operations are defined, it follows that  Associativity:   .  Distributivity:   .   .      At the same time, there are a few properties that hold for real numbers that do not hold for matrices.   Caution  The following properties hold for real numbers but not for matrices.  Commutativity:  It is not generally true that .   Cancellation:  It is not generally true that implies that .   Zero divisors:  It is not generally true that implies that either or .        Summary  In this section, we have found an especially simple way to express linear systems using matrix multiplication.   If is an matrix and an -dimensional vector, then is the linear combination of the columns of using the components of as weights. The vector is -dimensional.   The solution space to the equation is the same as the solution space to the linear system corresponding to the augmented matrix .  If is an matrix and is an matrix, we can form the product , which is an matrix whose columns are the products of and the columns of .       Consider the system of linear equations .  Find the matrix and vector that expresses this linear system in the form .  Give a description of the solution space to the equation .       and .  There is a unique solution .       and .  We have so there is a unique solution .      Suppose that is a matrix, and that is a vector. If is defined, what is the dimension of ? What is the dimension of ?    and .   If is defined, then and .    Suppose that is a matrix whose columns are and ; that is, .   What is the dimension of the vectors and ?   What is the product in terms of and ? What is the product ? What is the product ?   If we know that what is the matrix ?      Both and are three-dimensional.   and . Also, .        Both and are three-dimensional.   and . Also, .        Suppose that the matrix where and are shown in .      Two vectors and that form the columns of the matrix .     What is the shape of the matrix ?  On , indicate the vectors .    Find all vectors such that .   Find all vectors such that .       is a matrix.  We have       There is a unique vector .  There is a unique vector .      is a matrix.  We have       There is a unique vector .  There is a unique vector .     Suppose that .  Describe the solution space to the equation .  Find a matrix with no zero entries such that .         There are many possibilities. For instance,      We construct the augmented matrix which shows that   We form by choosing two vectors in the solution space to . For instance,      Consider the matrix .   Find the product where .   Give a description of the vectors such that .     Find the reduced row echelon form of and identify the pivot positions.   Can you find a vector such that is inconsistent?   For a general 3-dimensional vector , what can you say about the solution space of the equation ?      .   .  We see that   No  The solution space will form a line in , as in part b.      .  We consider the augmented matrix so that .  We see that   There is no vector such that is inconsistent because the augmented matrix cannot have a pivot position in the rightmost column.  The coefficient matrix will have exactly one column without a pivot position. Therefore, the solution space will form a line in , as in part b.     The operations that we perform in Gaussian elimination can be accomplished using matrix multiplication. This observation is the basis of an important technique that we will investigate in a subsequent chapter.  Let's consider the matrix .  Suppose that . Verify that is the matrix that results when the second row of is scaled by a factor of 7. What matrix would scale the third row by -3?  Suppose that . Verify that is the matrix that results from interchanging the first and second rows. What matrix would interchange the first and third rows?  Suppose that . Verify that is the matrix that results from multiplying the first row of by and adding it to the second row. What matrix would multiply the first row by 3 and add it to the third row?  When we performed Gaussian elimination, our first goal was to perform row operations that brought the matrix into a triangular form. For our matrix , find the row operations needed to find a row equivalent matrix in triangular form. By expressing these row operations in terms of matrix multiplication, find a matrix such that .       To scale the third row by , we would use the matrix .  To interchange the first and third rows, we would use .  To multiply the first row by 3 and add to the third row, we would use .   .     We compute that To scale the third row by , we would use the matrix .  We compute that To interchange the first and third rows, we would use .  We compute that To multiply the first row by 3 and add to the third row, we would use .  Keeping track of the operations we perform in Gaussian elimination, we define Then . So .     In this exercise, you will construct the inverse of a matrix, a subject that we will investigate more fully in the next chapter. Suppose that is the matrix: .  Find the vectors and such that the matrix satisfies .   In general, it is not true that . Check that it is true, however, for the specific and that appear in this problem.   Suppose that . What do you find when you evaluate ?  Suppose that we want to solve the equation . We know how to do this using Gaussian elimination; let's use our matrix to find a different way: . In other words, the solution to the equation is .  Consider the equation . Find the solution in two different ways, first using Gaussian elimination and then as , and verify that you have found the same result.           It is true, in this case, that .   .   .     We find and by solving the equations This gives   It is true, in this case, that .  We have seen that .  If , then is the solution to the equation .     Determine whether the following statements are true or false and provide a justification for your response.  If is defined, then the number of components of equals the number of rows of .  The solution space to the equation is equivalent to the solution space to the linear system whose augmented matrix is .  If a linear system of equations has 8 equations and 5 unknowns, then the shape of the matrix in the corresponding equation is .  If has a pivot position in every row, then every equation is consistent.  If is a matrix, then is inconsistent for some vector .     False  True  False  True  True     False, the number of components of equals the number of columns of .  True. This is a result of .  False. The shape of is .  True, because the augmented matrix cannot have a pivot position in the rightmost column.  True. Because there is not a pivot position in every row of , the augmented matrix will have a pivot position in the rightmost column for some vectors .     Suppose that is a matrix and that the equation has a unique solution for some vector .  What does this say about the pivot positions of the matrix ? Write the reduced row echelon form of .  Can you find another vector such that is inconsistent?  What can you say about the solution space to the equation ?  Suppose . Explain why every four-dimensional vector can be written as a linear combination of the vectors , , , and in exactly one way.       .  No   .  Every equation has exactly one solution.     Since the solution is unique, there must be a pivot position in every column of . This means that there are four pivot positions so there is a pivot position in every row. In other words, .  No, because has a pivot in every row.  There is only one solution, which is .  If is a four-dimensional vector, then has a unique solution since has a pivot position in every row and every column. This means that can be written as a linear combination of the columns of in exactly one way.     Define the matrix .  Describe the solution space to the homogeneous equation using a parametric description, if appropriate. What does this solution space represent geometrically?   Describe the solution space to the equation where . What does this solution space represent geometrically and how does it compare to the previous solution space?   We will now explain the relationship between the previous two solution spaces. Suppose that is a solution to the homogeneous equation; that is . Suppose also that is a solution to the equation ; that is, .  Use the Linearity Principle expressed in to explain why is a solution to the equation . You may do this by evaluating .  That is, if we find one solution to an equation , we may add any solution to the homogeneous equation to and still have a solution to the equation . In other words, the solution space to the equation is given by translating the solution space to the homogeneous equation by the vector .      .   .  Notice that .     We have the augmented matrix so that . The solution space is a line that passes through the origin.  Now we have so that . This is a line through parallel to . Notice that this line is parallel to the line in part a.  Notice that . This shows that if we add a vector from the solution space to to a vector from the solution space to , we obtain a vector in the solution space to .     Suppose that a city is starting a bicycle sharing program with bicycles at locations and . Bicycles that are rented at one location may be returned to either location at the end of the day. Over time, the city finds that 80% of bicycles rented at location are returned to with the other 20% returned to . Similarly, 50% of bicycles rented at location are returned to and 50% to .  To keep track of the bicycles, we form a vector where is the number of bicycles at location at the beginning of day and is the number of bicycles at . The information above tells us how to determine the distribution of bicycles the following day: Expressed in matrix-vector form, these expressions give where .   Let's check that this makes sense.  Suppose that there are 1000 bicycles at location and none at on day 1. This means we have . Find the number of bicycles at both locations on day 2 by evaluating .  Suppose that there are 1000 bicycles at location and none at on day 1. Form the vector and determine the number of bicycles at the two locations the next day by finding .     Suppose that one day there are 1050 bicycles at location and 450 at location . How many bicycles were there at each location the previous day?    Suppose that there are 500 bicycles at location and 500 at location on Monday. How many bicycles are there at the two locations on Tuesday? on Wednesday? on Thursday?     We see that   .   .     .   represents the distribution of bicycles on Tuesday, represents the distribution of bicycles on Wednesday, and on Thursday.     We see that   .   .    We solve the equation to obtain .  If represents the distribution of bicycles on Monday, then represents the distribution of bicycles on Tuesday, represents the distribution of bicycles on Wednesday, and on Thursday.     This problem is a continuation of the previous problem.  Let us define vectors . Show that .  Suppose that where and are scalars. Use the Linearity Principle expressed in to explain why .  Continuing in this way, explain why .   Suppose that there are initially 500 bicycles at location and 500 at location . Write the vector and find the scalars and such that .  Use the previous part of this problem to determine , and .  After a very long time, how are all the bicycles distributed?      and .  We have    and so on.  We find that .  Then   After a very long time, the second term becomes increasingly close to zero and so       and .  We have    and so on.  We find that .  Then   After a very long time, the second term becomes increasingly close to zero and so      "
},
{
  "id": "sec-matrices-lin-combs-3-5",
  "level": "2",
  "url": "sec-matrices-lin-combs.html#sec-matrices-lin-combs-3-5",
  "type": "Preview Activity",
  "number": "18.2.1",
  "title": "Matrix operations.",
  "body": " Matrix operations     Compute the scalar multiple .   Find the sum .   Suppose that and are two matrices. What do we need to know about their shapes before we can form the sum ?   matrix, identity  The matrix , which we call the identity matrix, is the matrix whose entries are zero except for the diagonal entries, all of which are 1. For instance, . If we can form the sum , what must be true about the matrix ?   Find the matrix where .            The shapes must be the same.  The shape of must be .               The shapes must be the same.  The shape of must be .       "
},
{
  "id": "sec-matrices-lin-combs-4-3",
  "level": "2",
  "url": "sec-matrices-lin-combs.html#sec-matrices-lin-combs-4-3",
  "type": "Example",
  "number": "18.2.1",
  "title": "Matrix-vector multiplication.",
  "body": " Matrix-vector multiplication   Suppose we have the matrix and vector : . Their product will be defined to be the linear combination of the columns of using the components of as weights. This means that   Because has two columns, we need two weights to form a linear combination of those columns, which means that must have two components. In other words, the number of columns of must equal the dimension of the vector .  Similarly, the columns of are 3-dimensional so any linear combination of them is 3-dimensional as well. Therefore, will be 3-dimensional.  We then see that if is a matrix, must be a 2-dimensional vector and will be 3-dimensional.   "
},
{
  "id": "sec-matrices-lin-combs-4-5",
  "level": "2",
  "url": "sec-matrices-lin-combs.html#sec-matrices-lin-combs-4-5",
  "type": "Definition",
  "number": "18.2.2",
  "title": "Matrix-vector multiplication.",
  "body": " Matrix-vector multiplication  matrix-vector multiplication   The product of a matrix by a vector will be the linear combination of the columns of using the components of as weights. More specifically, if then .  If is an matrix, then must be an -dimensional vector, and the product will be an -dimensional vector.   "
},
{
  "id": "sec-matrices-lin-combs-4-7",
  "level": "2",
  "url": "sec-matrices-lin-combs.html#sec-matrices-lin-combs-4-7",
  "type": "Activity",
  "number": "18.2.2",
  "title": "Matrix-vector multiplication.",
  "body": " Matrix-vector multiplication     Find the matrix product .   Suppose that is the matrix . If is defined, what is the dimension of the vector and what is the dimension of ?   A vector whose entries are all zero is denoted by . If is a matrix, what is the product ?   Suppose that is the identity matrix and . Find the product and explain why is called the identity matrix.   Suppose we write the matrix in terms of its columns as . If the vector , what is the product ?    Suppose that . Is there a vector such that ?       We have   The dimension of must be the same as the number of columns of so is three-dimensional. The dimension of equals the number of rows of so is four-dimensional.  We have .  We have ; that is, multiplying a vector by produces the same vector.  The product .  If , then we have with corresponding augmented matrix This means that is the unique solution to the equation .         The dimension of must three, and the dimension of must be four.   .   .   .   is the unique solution.    "
},
{
  "id": "sec-matrices-lin-combs-4-11",
  "level": "2",
  "url": "sec-matrices-lin-combs.html#sec-matrices-lin-combs-4-11",
  "type": "Activity",
  "number": "18.2.3",
  "title": "",
  "body": "  Sage can find the product of a matrix and vector using the * operator. For example,    Use Sage to evaluate the product from of the previous activity.    In Sage, define the matrix and vectors .   What do you find when you evaluate ?  What do you find when you evaluate and and compare your results?  What do you find when you evaluate and and compare your results?       We define A = matrix(3, 4, [1, 2, 0, -1, 2, 4, -3, -2, -1, -2, 6, 1]) v = vector([3, 1, -1, 1]) A*v   We define A = matrix(2, 3, [-2, 0, 3, 1, 4, 2]) zero = vector([0, 0]) v = vector([-2, 3]) w = vector([1, 2])    .   .         We define A = matrix(3, 4, [1, 2, 0, -1, 2, 4, -3, -2, -1, -2, 6, 1]) v = vector([3, 1, -1, 1]) A*v   We define A = matrix(2, 3, [-2, 0, 3, 1, 4, 2]) zero = vector([0, 0]) v = vector([-2, 3]) w = vector([1, 2])    .   .       "
},
{
  "id": "prop-matrix-mult-prop",
  "level": "2",
  "url": "sec-matrices-lin-combs.html#prop-matrix-mult-prop",
  "type": "Proposition",
  "number": "18.2.3",
  "title": "Linearity of matrix multiplication.",
  "body": " Linearity of matrix multiplication   If is a matrix, and vectors of the appropriate dimensions, and a scalar, then    .    .    .     "
},
{
  "id": "prop-matrix-eq-solution",
  "level": "2",
  "url": "sec-matrices-lin-combs.html#prop-matrix-eq-solution",
  "type": "Proposition",
  "number": "18.2.4",
  "title": "",
  "body": "  If and , then the following statements are equivalent.   The vector satisfies the equation .   The vector is a linear combination of the columns of with weights : .   The components of form a solution to the linear system corresponding to the augmented matrix      "
},
{
  "id": "sec-matrices-lin-combs-5-9",
  "level": "2",
  "url": "sec-matrices-lin-combs.html#sec-matrices-lin-combs-5-9",
  "type": "Example",
  "number": "18.2.5",
  "title": "",
  "body": "  We will describe the solution space of the equation   By , this equation may be equivalently expressed as , which is the linear system corresponding to the augmented matrix . The reduced row echelon form of the augmented matrix is which corresponds to the linear system The variable is free so we may write the solution space parametrically as   Since we originally asked to describe the solutions to the equation , we will express the solution in terms of the vector : As before, we call this a parametric description of the solution space.  This shows that the solutions may be written in the form , for appropriate vectors and . Geometrically, the solution space is a line in through moving parallel to .   "
},
{
  "id": "sec-matrices-lin-combs-5-10",
  "level": "2",
  "url": "sec-matrices-lin-combs.html#sec-matrices-lin-combs-5-10",
  "type": "Activity",
  "number": "18.2.4",
  "title": "The equation <span xmlns:pf=\"https:\/\/prefigure.org\" class=\"process-math\">\\(A\\xvec = \\bvec\\)<\/span>.",
  "body": " The equation      Consider the linear system Identify the matrix and vector to express this system in the form .   If and are as below, write the linear system corresponding to the equation and describe its solution space, using a parametric description if appropriate:    Describe the solution space of the equation .   Suppose is an matrix. What can you guarantee about the solution space of the equation ?       and .  Form the augmented matrix so that   We have the augmented matrix Since this system is inconsistent, there are no solutions to the matrix equation.  We know that there is at least one solution, namely, .       and .     There are no solutions.  There is at least one solution, namely, .    "
},
{
  "id": "sec-matrices-lin-combs-6-3",
  "level": "2",
  "url": "sec-matrices-lin-combs.html#sec-matrices-lin-combs-6-3",
  "type": "Definition",
  "number": "18.2.6",
  "title": "Matrix-matrix multiplication.",
  "body": " Matrix-matrix multiplication   Given matrices and , we form their product by first writing in terms of its columns and then defining    "
},
{
  "id": "sec-matrices-lin-combs-6-4",
  "level": "2",
  "url": "sec-matrices-lin-combs.html#sec-matrices-lin-combs-6-4",
  "type": "Example",
  "number": "18.2.7",
  "title": "",
  "body": "  Given the matrices , we have .   "
},
{
  "id": "sec-matrices-lin-combs-6-5",
  "level": "2",
  "url": "sec-matrices-lin-combs.html#sec-matrices-lin-combs-6-5",
  "type": "Observation",
  "number": "18.2.8",
  "title": "",
  "body": "  It is important to note that we can only multiply matrices if the shapes of the matrices are compatible. More specifically, when constructing the product , the matrix multiplies the columns of . Therefore, the number of columns of must equal the number of rows of . When this condition is met, the number of rows of is the number of rows of , and the number of columns of is the number of columns of .   "
},
{
  "id": "sec-matrices-lin-combs-6-6",
  "level": "2",
  "url": "sec-matrices-lin-combs.html#sec-matrices-lin-combs-6-6",
  "type": "Activity",
  "number": "18.2.5",
  "title": "",
  "body": "  Consider the matrices .   Before computing, first explain why the shapes of and enable us to form the product . Then describe the shape of .   Compute the product .  Sage can multiply matrices using the * operator. Define the matrices and in the Sage cell below and check your work by computing .   Are we able to form the matrix product ? If so, use the Sage cell above to find . Is it generally true that ?  Suppose we form the three matrices. . Compare what happens when you compute and . State your finding as a general principle.   Compare the results of evaluating and and state your finding as a general principle.  When we are dealing with real numbers, we know if and , then . Define matrices and compute and . If , is it necessarily true that ?  Again, with real numbers, we know that if , then either or . Define and compute . If , is it necessarily true that either or ?       The product exists because the number of columns of equals the number of rows of . The dimensions of are .  We have .  Define A = matrix(2, 3, [1, 3, 2, -3, 4, -1]) B = matrix(3, 2, [3, 0, 1, 2, -2, -1]) A*B   Yes, we can form the product because the number of columns of equals the number of rows of . This product will be , however, so it must be true that .  We find that .  We find that .  It is not generally true that if , as illustrated by this example.  It is not generally true that or if , as illustrated by this example.      The product exists because the number of columns of equals the number of rows of . The dimensions of are .  We have .  Define A = matrix(2, 3, [1, 3, 2, -3, 4, -1]) B = matrix(3, 2, [3, 0, 1, 2, -2, -1]) A*B   It is not generally true that .  We find that .  We find that .  It is not generally true that if .  It is not generally true that or if .    "
},
{
  "id": "sec-matrices-lin-combs-8-1",
  "level": "2",
  "url": "sec-matrices-lin-combs.html#sec-matrices-lin-combs-8-1",
  "type": "Exercise",
  "number": "18.2.6.1",
  "title": "",
  "body": " Consider the system of linear equations .  Find the matrix and vector that expresses this linear system in the form .  Give a description of the solution space to the equation .       and .  There is a unique solution .       and .  We have so there is a unique solution .    "
},
{
  "id": "sec-matrices-lin-combs-8-2",
  "level": "2",
  "url": "sec-matrices-lin-combs.html#sec-matrices-lin-combs-8-2",
  "type": "Exercise",
  "number": "18.2.6.2",
  "title": "",
  "body": " Suppose that is a matrix, and that is a vector. If is defined, what is the dimension of ? What is the dimension of ?    and .   If is defined, then and .  "
},
{
  "id": "sec-matrices-lin-combs-8-3",
  "level": "2",
  "url": "sec-matrices-lin-combs.html#sec-matrices-lin-combs-8-3",
  "type": "Exercise",
  "number": "18.2.6.3",
  "title": "",
  "body": " Suppose that is a matrix whose columns are and ; that is, .   What is the dimension of the vectors and ?   What is the product in terms of and ? What is the product ? What is the product ?   If we know that what is the matrix ?      Both and are three-dimensional.   and . Also, .        Both and are three-dimensional.   and . Also, .      "
},
{
  "id": "sec-matrices-lin-combs-8-4",
  "level": "2",
  "url": "sec-matrices-lin-combs.html#sec-matrices-lin-combs-8-4",
  "type": "Exercise",
  "number": "18.2.6.4",
  "title": "",
  "body": " Suppose that the matrix where and are shown in .      Two vectors and that form the columns of the matrix .     What is the shape of the matrix ?  On , indicate the vectors .    Find all vectors such that .   Find all vectors such that .       is a matrix.  We have       There is a unique vector .  There is a unique vector .      is a matrix.  We have       There is a unique vector .  There is a unique vector .   "
},
{
  "id": "sec-matrices-lin-combs-8-5",
  "level": "2",
  "url": "sec-matrices-lin-combs.html#sec-matrices-lin-combs-8-5",
  "type": "Exercise",
  "number": "18.2.6.5",
  "title": "",
  "body": " Suppose that .  Describe the solution space to the equation .  Find a matrix with no zero entries such that .         There are many possibilities. For instance,      We construct the augmented matrix which shows that   We form by choosing two vectors in the solution space to . For instance,    "
},
{
  "id": "sec-matrices-lin-combs-8-6",
  "level": "2",
  "url": "sec-matrices-lin-combs.html#sec-matrices-lin-combs-8-6",
  "type": "Exercise",
  "number": "18.2.6.6",
  "title": "",
  "body": " Consider the matrix .   Find the product where .   Give a description of the vectors such that .     Find the reduced row echelon form of and identify the pivot positions.   Can you find a vector such that is inconsistent?   For a general 3-dimensional vector , what can you say about the solution space of the equation ?      .   .  We see that   No  The solution space will form a line in , as in part b.      .  We consider the augmented matrix so that .  We see that   There is no vector such that is inconsistent because the augmented matrix cannot have a pivot position in the rightmost column.  The coefficient matrix will have exactly one column without a pivot position. Therefore, the solution space will form a line in , as in part b.   "
},
{
  "id": "sec-matrices-lin-combs-8-7",
  "level": "2",
  "url": "sec-matrices-lin-combs.html#sec-matrices-lin-combs-8-7",
  "type": "Exercise",
  "number": "18.2.6.7",
  "title": "",
  "body": " The operations that we perform in Gaussian elimination can be accomplished using matrix multiplication. This observation is the basis of an important technique that we will investigate in a subsequent chapter.  Let's consider the matrix .  Suppose that . Verify that is the matrix that results when the second row of is scaled by a factor of 7. What matrix would scale the third row by -3?  Suppose that . Verify that is the matrix that results from interchanging the first and second rows. What matrix would interchange the first and third rows?  Suppose that . Verify that is the matrix that results from multiplying the first row of by and adding it to the second row. What matrix would multiply the first row by 3 and add it to the third row?  When we performed Gaussian elimination, our first goal was to perform row operations that brought the matrix into a triangular form. For our matrix , find the row operations needed to find a row equivalent matrix in triangular form. By expressing these row operations in terms of matrix multiplication, find a matrix such that .       To scale the third row by , we would use the matrix .  To interchange the first and third rows, we would use .  To multiply the first row by 3 and add to the third row, we would use .   .     We compute that To scale the third row by , we would use the matrix .  We compute that To interchange the first and third rows, we would use .  We compute that To multiply the first row by 3 and add to the third row, we would use .  Keeping track of the operations we perform in Gaussian elimination, we define Then . So .   "
},
{
  "id": "sec-matrices-lin-combs-8-8",
  "level": "2",
  "url": "sec-matrices-lin-combs.html#sec-matrices-lin-combs-8-8",
  "type": "Exercise",
  "number": "18.2.6.8",
  "title": "",
  "body": " In this exercise, you will construct the inverse of a matrix, a subject that we will investigate more fully in the next chapter. Suppose that is the matrix: .  Find the vectors and such that the matrix satisfies .   In general, it is not true that . Check that it is true, however, for the specific and that appear in this problem.   Suppose that . What do you find when you evaluate ?  Suppose that we want to solve the equation . We know how to do this using Gaussian elimination; let's use our matrix to find a different way: . In other words, the solution to the equation is .  Consider the equation . Find the solution in two different ways, first using Gaussian elimination and then as , and verify that you have found the same result.           It is true, in this case, that .   .   .     We find and by solving the equations This gives   It is true, in this case, that .  We have seen that .  If , then is the solution to the equation .   "
},
{
  "id": "sec-matrices-lin-combs-8-9",
  "level": "2",
  "url": "sec-matrices-lin-combs.html#sec-matrices-lin-combs-8-9",
  "type": "Exercise",
  "number": "18.2.6.9",
  "title": "",
  "body": " Determine whether the following statements are true or false and provide a justification for your response.  If is defined, then the number of components of equals the number of rows of .  The solution space to the equation is equivalent to the solution space to the linear system whose augmented matrix is .  If a linear system of equations has 8 equations and 5 unknowns, then the shape of the matrix in the corresponding equation is .  If has a pivot position in every row, then every equation is consistent.  If is a matrix, then is inconsistent for some vector .     False  True  False  True  True     False, the number of components of equals the number of columns of .  True. This is a result of .  False. The shape of is .  True, because the augmented matrix cannot have a pivot position in the rightmost column.  True. Because there is not a pivot position in every row of , the augmented matrix will have a pivot position in the rightmost column for some vectors .   "
},
{
  "id": "sec-matrices-lin-combs-8-10",
  "level": "2",
  "url": "sec-matrices-lin-combs.html#sec-matrices-lin-combs-8-10",
  "type": "Exercise",
  "number": "18.2.6.10",
  "title": "",
  "body": " Suppose that is a matrix and that the equation has a unique solution for some vector .  What does this say about the pivot positions of the matrix ? Write the reduced row echelon form of .  Can you find another vector such that is inconsistent?  What can you say about the solution space to the equation ?  Suppose . Explain why every four-dimensional vector can be written as a linear combination of the vectors , , , and in exactly one way.       .  No   .  Every equation has exactly one solution.     Since the solution is unique, there must be a pivot position in every column of . This means that there are four pivot positions so there is a pivot position in every row. In other words, .  No, because has a pivot in every row.  There is only one solution, which is .  If is a four-dimensional vector, then has a unique solution since has a pivot position in every row and every column. This means that can be written as a linear combination of the columns of in exactly one way.   "
},
{
  "id": "sec-matrices-lin-combs-8-11",
  "level": "2",
  "url": "sec-matrices-lin-combs.html#sec-matrices-lin-combs-8-11",
  "type": "Exercise",
  "number": "18.2.6.11",
  "title": "",
  "body": " Define the matrix .  Describe the solution space to the homogeneous equation using a parametric description, if appropriate. What does this solution space represent geometrically?   Describe the solution space to the equation where . What does this solution space represent geometrically and how does it compare to the previous solution space?   We will now explain the relationship between the previous two solution spaces. Suppose that is a solution to the homogeneous equation; that is . Suppose also that is a solution to the equation ; that is, .  Use the Linearity Principle expressed in to explain why is a solution to the equation . You may do this by evaluating .  That is, if we find one solution to an equation , we may add any solution to the homogeneous equation to and still have a solution to the equation . In other words, the solution space to the equation is given by translating the solution space to the homogeneous equation by the vector .      .   .  Notice that .     We have the augmented matrix so that . The solution space is a line that passes through the origin.  Now we have so that . This is a line through parallel to . Notice that this line is parallel to the line in part a.  Notice that . This shows that if we add a vector from the solution space to to a vector from the solution space to , we obtain a vector in the solution space to .   "
},
{
  "id": "sec-matrices-lin-combs-8-12",
  "level": "2",
  "url": "sec-matrices-lin-combs.html#sec-matrices-lin-combs-8-12",
  "type": "Exercise",
  "number": "18.2.6.12",
  "title": "",
  "body": " Suppose that a city is starting a bicycle sharing program with bicycles at locations and . Bicycles that are rented at one location may be returned to either location at the end of the day. Over time, the city finds that 80% of bicycles rented at location are returned to with the other 20% returned to . Similarly, 50% of bicycles rented at location are returned to and 50% to .  To keep track of the bicycles, we form a vector where is the number of bicycles at location at the beginning of day and is the number of bicycles at . The information above tells us how to determine the distribution of bicycles the following day: Expressed in matrix-vector form, these expressions give where .   Let's check that this makes sense.  Suppose that there are 1000 bicycles at location and none at on day 1. This means we have . Find the number of bicycles at both locations on day 2 by evaluating .  Suppose that there are 1000 bicycles at location and none at on day 1. Form the vector and determine the number of bicycles at the two locations the next day by finding .     Suppose that one day there are 1050 bicycles at location and 450 at location . How many bicycles were there at each location the previous day?    Suppose that there are 500 bicycles at location and 500 at location on Monday. How many bicycles are there at the two locations on Tuesday? on Wednesday? on Thursday?     We see that   .   .     .   represents the distribution of bicycles on Tuesday, represents the distribution of bicycles on Wednesday, and on Thursday.     We see that   .   .    We solve the equation to obtain .  If represents the distribution of bicycles on Monday, then represents the distribution of bicycles on Tuesday, represents the distribution of bicycles on Wednesday, and on Thursday.   "
},
{
  "id": "sec-matrices-lin-combs-8-13",
  "level": "2",
  "url": "sec-matrices-lin-combs.html#sec-matrices-lin-combs-8-13",
  "type": "Exercise",
  "number": "18.2.6.13",
  "title": "",
  "body": " This problem is a continuation of the previous problem.  Let us define vectors . Show that .  Suppose that where and are scalars. Use the Linearity Principle expressed in to explain why .  Continuing in this way, explain why .   Suppose that there are initially 500 bicycles at location and 500 at location . Write the vector and find the scalars and such that .  Use the previous part of this problem to determine , and .  After a very long time, how are all the bicycles distributed?      and .  We have    and so on.  We find that .  Then   After a very long time, the second term becomes increasingly close to zero and so       and .  We have    and so on.  We find that .  Then   After a very long time, the second term becomes increasingly close to zero and so    "
},
{
  "id": "sec-span",
  "level": "1",
  "url": "sec-span.html",
  "type": "Section",
  "number": "18.3",
  "title": "The span of a set of vectors",
  "body": " The span of a set of vectors   Matrix multiplication allows us to rewrite a linear system in the form . Besides being a more compact way of expressing a linear system, this form allows us to think about linear systems geometrically since matrix multiplication is defined in terms of linear combinations of vectors.  We now return to our two fundamental questions, rephrased here in terms of matrix multiplication.   Existence: Is there a solution to the equation ?    Uniqueness: If there is a solution to the equation , is it unique?  In this section, we focus on the existence question and see how it leads to the concept of the span of a set of vectors.   The existence of solutions      If the equation is inconsistent, what can we say about the pivot positions of the augmented matrix ?    Consider the matrix  . If , is the equation consistent? If so, find a solution.    If , is the equation consistent? If so, find a solution.    Identify the pivot positions of .   For our two choices of the vector , one equation has a solution and the other does not. What feature of the pivot positions of the matrix tells us to expect this?       We know there must be a pivot position in the rightmost column of the augmented matrix.  We construct the augmented matrix which shows that the system is consistent. The solution space is described parametrically as   Now the augmented matrix is showing that the equation is inconsistent.  There are two pivot positions in , as shown.   Since there is a row of that does not have a pivot position, it is possible to augment by a vector so that we obtain a pivot position in the rightmost column of the augmented matrix. In this case, we have an inconsistent system.       The span of a set of vectors  In the preview activity, we considered a matrix and found that the equation has a solution for some vectors in and has no solution for others. We will introduce a concept called span that describes the vectors for which there is a solution.  We can write an matrix in terms of its columns . Remember that says that the equation is consistent if and only if we can express as a linear combination of .   span  The span of a set of vectors is the set of all linear combinations that can be formed from the vectors.  Alternatively, if , then the span of the vectors consists of all vectors for which the equation is consistent.     Considering the set of vectors and , we see that the vector is one vector in the span of the vectors and because it is a linear combination of and .  To determine whether the vector is in the span of and , we form the matrix and consider the equation . We have which shows that the equation is inconsistent. Therefore, is one vector that is not in the span of and .      Let's look at two examples to develop some intuition for the concept of span.  First, we will consider the set of vectors .   An interactive diagram for constructing linear combinations of the vectors and .      What vector is the linear combination of and with weights:    and ?    and ?    and ?      Can the vector be expressed as a linear combination of and ? Is the vector in the span of and ?   Can the vector be expressed as a linear combination of and ? Is the vector in the span of and ?   Describe the set of vectors in the span of and .   For what vectors does the equation have a solution?      We will now look at an example where .   An interactive diagram for constructing linear combinations of the vectors and .      What vector is the linear combination of and with weights:    and ?    and ?    and ?      Can the vector be expressed as a linear combination of and ? Is the vector in the span of and ?   Can the vector be expressed as a linear combination of and ? Is the vector in the span of and ?   Describe the set of vectors in the span of and .   For what vectors does the equation have a solution?         For the first set of vectors, we find:  We can form the linear combinations:  When and , the linear combination is .   When and , the linear combination is .   When and , the linear combination is .    Yes, we saw that there are at least two ways. For instance, when the weights are and . This means that is in the span of and .  No. No matter how we change the weights, the linear combination lies on the line through and . This means that is not in the span of and .  The span of and is the set of all vectors on the line through .  If the equation has a solution, must lie on the line defined by .    For the second set of vectors, we have:  We can form the linear combinations:   When and , the linear combination is .   When and , the linear combination is .   When and , the linear combination is .    Yes. Using the diagram, we see that . This means that is in the span of and .  Yes. Using the diagram, we see that . This means that is in the span of and .  Every two-dimensional vector is in the span of and .  The equation has a solution for every .      This activity aims to convey the geometric meaning of span. Remember that we can think of a linear combination of the two vectors and as a recipe for walking in the plane . We first move a prescribed amount in the direction of and then a prescribed amount in the direction of . The span consists of all the places we can walk to.    Let's consider the vectors and as shown in .   The vectors and and some linear combinations they create.      The figure shows us that is a linear combination of and . Indeed, we can verify this algebraically by constructing the linear system whose corresponding augmented matrix has the reduced row echelon form Because this system is consistent, we know that is in the span of and .  In fact, we can say more. Notice that the coefficient matrix has a pivot position in every row. This means that for any other vector , the augmented matrix corresponding to the equation cannot have a pivot position in its rightmost column: Therefore, the equation is consistent for every two-dimensional vector , which tells us that every two-dimensional vector is in the span of and . In this case, we say that the span of and is .  The intuitive meaning is that we can walk to any point in the plane by moving an appropriate distance in the and directions.      Now let's consider the vectors and as shown in .   The vectors and and some linear combinations they create.      From the figure, we expect that is not a linear combination of and . Once again, we can verify this algebraically by constructing the linear system The augmented matrix has the reduced row echelon form from which we see that the system is inconsistent. Therefore, is not in the span of and .  We should expect this behavior from the coefficient matrix Because the second row of the coefficient matrix does not have a pivot position, it is possible for a linear system to have a pivot position in its rightmost column:   If we notice that , we see that any linear combination of and , is actually a scalar multiple of . Therefore, the span of and is the line defined by the vector . Intuitively, this means that we can only walk to points on this line using these two vectors.     We will denote the span of the set of vectors by .   In , we saw that . However, for the vectors in , we saw that is simply a line.    Pivot positions and span  A set of vectors naturally defines a matrix whose columns are the given vectors. As we've seen, a vector is in precisely when the linear system is consistent.  The previous examples point to the fact that the span is related to the pivot positions of . While and develop this idea more fully, we will now examine the possibilities in .    In this activity, we will look at the span of sets of vectors in .   Suppose . Give a geometric description of and a rough sketch of and its span in .   A three-dimensional coordinate system for sketching and its span.        Now consider the two vectors . Sketch the vectors below. Then give a geometric description of and a rough sketch of the span in .   A coordinate system for sketching , , and .       Let's now look at this situation algebraically by writing write . Determine the conditions on , , and so that is in by considering the linear system or . Explain how this relates to your sketch of .   Consider the vectors    Is the vector in ?    Is the vector in ?   Give a geometric description of .    Consider the vectors . Form the matrix and find its reduced row echelon form. What does this tell you about ?   If the span of a set of vectors is , what can you say about the pivot positions of the matrix ?  What is the smallest number of vectors such that ?       is the line defined by .   is the -plane.   For the linear system to be consistent, we need , which means that the third coordinate of the vector must be 0 for to be in . In other words, must lie in the -plane.   We consider the two cases.  We have the augmented matrix which shows that the system is inconsistent. Therefore, is not in .  We have the augmented matrix which shows that the system is consistent. Therefore, is in .  The span is the plane in defined by and .   We have the reduced row echelon form Since there is a pivot position in every row, this says that every equation is consistent. The is therefore .  There must be a pivot position in every row.  If a set of vectors spans , its corresponding matrix must have a pivot position in every row. Because there can be at most one pivot position in a column, there must be at least three columns. Therefore, the smallest number of vectors that span is three.     The types of sets that appear as the span of a set of vectors in are relatively simple.   First, with a single nonzero vector, all linear combinations are simply scalar multiples of that vector so that the span of this vector is a line, as shown in .   The span of a single nonzero vector is a line.      Notice that the matrix formed by this vector has one pivot position. For example, .    The span of two vectors in that do not lie on the same line will be a plane, as seen in .   The span of these two vectors in is a plane.      For example, the vectors lead to the matrix with two pivot positions.   Finally, a set of three vectors, such as may form a matrix having three pivot positions one in every row. When this happens, no matter how we augment this matrix, it is impossible to obtain a pivot position in the rightmost column: Therefore, any linear system is consistent, which tells us that .    To summarize, we looked at the pivot positions in a matrix whose columns are the three-dimensional vectors . We found that with   one pivot position, the span was a line.    two pivot positions, the span was a plane.   three pivot positions, the span was .  Though we will return to these ideas later, for now take note of the fact that the span of a set of vectors in is a relatively simple, familiar geometric object.  The reasoning that led us to conclude that the span of a set of vectors is when the associated matrix has a pivot position in every row applies more generally.    Suppose we have vectors in . Then if and only if the matrix has a pivot position in every row.    This tells us something important about the number of vectors needed to span . Suppose we have vectors that span . The proposition tells us that the matrix has a pivot position in every row, such as in this reduced row echelon matrix. Since a matrix can have at most one pivot position in a column, there must be at least as many columns as there are rows, which implies that . For instance, if we have a set of vectors that span , there must be at least 632 vectors in the set.    A set of vectors whose span is contains at least vectors.    We have thought about a linear combination of a set of vectors as the result of walking a certain distance in the direction of , followed by walking a certain distance in the direction of , and so on. If , this means that we can walk to every point in using the directions . Intuitively, this proposition is telling us that we need at least directions to have the flexibility needed to reach every point in .   Terminology  Because span is a concept that is connected to a set of vectors, we say, The span of the set of vectors is .... While it may be tempting to say, The span of the matrix is ..., we should instead say The span of the columns of the matrix is ....      Summary  We defined the span of a set of vectors and developed some intuition for this concept through a series of examples.   The span of a set of vectors is the set of linear combinations of the vectors. We denote the span by .   A vector is in if and only if the linear system is consistent.   If the matrix has a pivot position in every row, then the span of these vectors is ; that is,   Any set of vectors that spans must have at least vectors.      In this exercise, we will consider the span of some sets of two- and three-dimensional vectors.   Consider the vectors .  Is in ?  Give a geometric description of .    Consider the vectors .  Is the vector in ?  Is the vector in ?   Is the vector in ?  Give a geometric description of .         .  For the following vectors,   is in .   is in .   is not in .   is a plane in .      The equation is consistent so is in . Since has a pivot position in every row, is .  Let's consider the following vectors.  The equation is consistent so is in .  The vector is in because .  The equation is not consistent so is not in .   is the plane defined by and .      Provide a justification for your response to the following questions.  Suppose you have a set of vectors . Can you guarantee that is in ?  Suppose that is an matrix. Can you guarantee that the equation is consistent?   What is ?      Yes  Yes   consists only of the vector .     Yes, because .  Yes, is a solution to the equation .   consists only of the vector .     For both parts of this exercise, give a geometric description of sets of the vectors and include a sketch.   For which vectors in is the equation consistent?   For which vectors in is the equation consistent?       must be a scalar multiple of .   can be any vector in .      so must be on the line defined by .   so can be any vector in .     Consider the following matrices: . Do the columns of span ? Do the columns of span ?    The columns of do not span . The columns of do.    The reduced row echelon form of is so the columns of do not span .  The reduced row echelon form of is so the columns of span .     Determine whether the following statements are true or false and provide a justification for your response. Throughout, we will assume that the matrix has columns ; that is, .  If the equation is consistent, then is in .   The equation is consistent.  If , , , and are vectors in , then their span is .  If is a linear combination of , then is in .  If is an matrix, then the span of the columns of is a set of vectors in .      True   True   False   True   False     True. If is a solution to , then the components of are weights whose linear combination is .  True, because is a solution.  False. The span could be a smaller set.  True. This is the definition of the span.  False. The span is a set of vectors in .     This exercise asks you to construct some matrices whose columns span a given set.  Construct a matrix whose columns span .  Construct a matrix whose columns span a plane in .  Construct a matrix whose columns span a line in .     We will choose matrices in reduced row echelon form.   .   .   .    We will choose matrices in reduced row echelon form.   .   .   .     Provide a justification for your response to the following questions.  Suppose that we have vectors in , , whose span is . Can every vector in be written as a linear combination of ?   Suppose that we have vectors in , , whose span is . Can every vector in be written uniquely as a linear combination of ?   Do the vectors span ?  Suppose that span . What can you guarantee about the value of ?  Can 17 vectors in span ?     Yes  No  Yes     No     Yes. Because the span is , every vector in can be written as a linear combination of the vectors.  No. The matrix must have a column without a pivot position. Therefore, any equation has infinitely many solutions.  Yes, because has a pivot position in every row.  There must be at least 438 vectors so .  No, because we need at least 20 vectors to span .     The following observation will be helpful in this exercise. If we want to find a solution to the equation , we could first find a solution to the equation and then find a solution to the equation .  Suppose that is a matrix whose columns span and is a matrix. In this case, we can form the product .  What is the shape of the product ?  Can you guarantee that the columns of span ?  If you know additionally that the span of the columns of is , can you guarantee that the columns of span ?         No  Yes      is a matrix.  No. Since the columns of span , then the equation has a solution for every vector . However, we may not be able to solve the equation if the columns of do not span .  Yes. If we are given a vector in , we can find a vector such that since the columns of span . Since the columns of span , we can find a vector such that . Then , which means that is the span of the columns of .     Suppose that is a matrix and that, for some vector , the equation has a unique solution.  What can you say about the pivot positions of ?  What can you say about the span of the columns of ?  If is some other vector in , what can you conclude about the equation ?  What can you about the solution space to the equation ?     There is a pivot position in each row and each column.  The span is .  There is a unique solution.  There is a unique solution      Since the solution is unique, each column of must have a pivot position. Since there are also 12 rows, each row must have a pivot position as well. This means that the row reduced echelon form of is the identity matrix.  Since there is a pivot position in every row, the span of the columns of is .  There is a unique solution to the equation because has a pivot position in every row and every column.  There is a unique solution .     Suppose that .  Is a linear combination of and ? If so, find weights such that .  Show that a linear combination can be rewritten as a linear combination of and .  Explain why .      Yes  We have   As seen in the last part of this problem, every linear combination of , , and can be rewritten as a linear combination of and .     Yes, because .  We have   As seen in the last part of this problem, every linear combination of , , and can be rewritten as a linear combination of and .     As defined in this section, the span of a set of vectors is generated by taking all possible linear combinations of those vectors. This exercise will demonstrate the fact that the span can also be realized as the solution space to a linear system.  We will consider the vectors    Is every vector in in ? If not, describe the span.    To describe as the solution space of a linear system, we will write . If is in , then the linear system corresponding to the augmented matrix must be consistent. This means that a pivot cannot occur in the rightmost column. Perform row operations to put this augmented matrix into a triangular form. Now identify an equation in , , and that tells us when there is no pivot in the rightmost column. The solution space to this equation describes .   In this example, the matrix formed by the vectors has two pivot positions. Suppose we were to consider another example in which this matrix had had only one pivot position. How would this have changed the linear system describing ?      is the plane defined by and .   .  There would be two equations involving the variables , , and .     Finding the reduced row echelon form shows that is a linear combination of and . Therefore, is the plane defined by and .  We see that If the equation is consistent, we must have . The solution space is the plane defined by and .  In that case, there would be two equations involving the variables , , and , and the solution space would be a line.     "
},
{
  "id": "sec-span-2-3",
  "level": "2",
  "url": "sec-span.html#sec-span-2-3",
  "type": "Preview Activity",
  "number": "18.3.1",
  "title": "The existence of solutions.",
  "body": " The existence of solutions      If the equation is inconsistent, what can we say about the pivot positions of the augmented matrix ?    Consider the matrix  . If , is the equation consistent? If so, find a solution.    If , is the equation consistent? If so, find a solution.    Identify the pivot positions of .   For our two choices of the vector , one equation has a solution and the other does not. What feature of the pivot positions of the matrix tells us to expect this?       We know there must be a pivot position in the rightmost column of the augmented matrix.  We construct the augmented matrix which shows that the system is consistent. The solution space is described parametrically as   Now the augmented matrix is showing that the equation is inconsistent.  There are two pivot positions in , as shown.   Since there is a row of that does not have a pivot position, it is possible to augment by a vector so that we obtain a pivot position in the rightmost column of the augmented matrix. In this case, we have an inconsistent system.    "
},
{
  "id": "sec-span-3-4",
  "level": "2",
  "url": "sec-span.html#sec-span-3-4",
  "type": "Definition",
  "number": "18.3.1",
  "title": "",
  "body": " span  The span of a set of vectors is the set of all linear combinations that can be formed from the vectors.  Alternatively, if , then the span of the vectors consists of all vectors for which the equation is consistent.  "
},
{
  "id": "sec-span-3-5",
  "level": "2",
  "url": "sec-span.html#sec-span-3-5",
  "type": "Example",
  "number": "18.3.2",
  "title": "",
  "body": "  Considering the set of vectors and , we see that the vector is one vector in the span of the vectors and because it is a linear combination of and .  To determine whether the vector is in the span of and , we form the matrix and consider the equation . We have which shows that the equation is inconsistent. Therefore, is one vector that is not in the span of and .   "
},
{
  "id": "activity-intro-span",
  "level": "2",
  "url": "sec-span.html#activity-intro-span",
  "type": "Activity",
  "number": "18.3.2",
  "title": "",
  "body": "  Let's look at two examples to develop some intuition for the concept of span.  First, we will consider the set of vectors .   An interactive diagram for constructing linear combinations of the vectors and .      What vector is the linear combination of and with weights:    and ?    and ?    and ?      Can the vector be expressed as a linear combination of and ? Is the vector in the span of and ?   Can the vector be expressed as a linear combination of and ? Is the vector in the span of and ?   Describe the set of vectors in the span of and .   For what vectors does the equation have a solution?      We will now look at an example where .   An interactive diagram for constructing linear combinations of the vectors and .      What vector is the linear combination of and with weights:    and ?    and ?    and ?      Can the vector be expressed as a linear combination of and ? Is the vector in the span of and ?   Can the vector be expressed as a linear combination of and ? Is the vector in the span of and ?   Describe the set of vectors in the span of and .   For what vectors does the equation have a solution?         For the first set of vectors, we find:  We can form the linear combinations:  When and , the linear combination is .   When and , the linear combination is .   When and , the linear combination is .    Yes, we saw that there are at least two ways. For instance, when the weights are and . This means that is in the span of and .  No. No matter how we change the weights, the linear combination lies on the line through and . This means that is not in the span of and .  The span of and is the set of all vectors on the line through .  If the equation has a solution, must lie on the line defined by .    For the second set of vectors, we have:  We can form the linear combinations:   When and , the linear combination is .   When and , the linear combination is .   When and , the linear combination is .    Yes. Using the diagram, we see that . This means that is in the span of and .  Yes. Using the diagram, we see that . This means that is in the span of and .  Every two-dimensional vector is in the span of and .  The equation has a solution for every .     "
},
{
  "id": "example-span-linear-indep",
  "level": "2",
  "url": "sec-span.html#example-span-linear-indep",
  "type": "Example",
  "number": "18.3.5",
  "title": "",
  "body": "  Let's consider the vectors and as shown in .   The vectors and and some linear combinations they create.      The figure shows us that is a linear combination of and . Indeed, we can verify this algebraically by constructing the linear system whose corresponding augmented matrix has the reduced row echelon form Because this system is consistent, we know that is in the span of and .  In fact, we can say more. Notice that the coefficient matrix has a pivot position in every row. This means that for any other vector , the augmented matrix corresponding to the equation cannot have a pivot position in its rightmost column: Therefore, the equation is consistent for every two-dimensional vector , which tells us that every two-dimensional vector is in the span of and . In this case, we say that the span of and is .  The intuitive meaning is that we can walk to any point in the plane by moving an appropriate distance in the and directions.   "
},
{
  "id": "example-span-linear-dep",
  "level": "2",
  "url": "sec-span.html#example-span-linear-dep",
  "type": "Example",
  "number": "18.3.7",
  "title": "",
  "body": "  Now let's consider the vectors and as shown in .   The vectors and and some linear combinations they create.      From the figure, we expect that is not a linear combination of and . Once again, we can verify this algebraically by constructing the linear system The augmented matrix has the reduced row echelon form from which we see that the system is inconsistent. Therefore, is not in the span of and .  We should expect this behavior from the coefficient matrix Because the second row of the coefficient matrix does not have a pivot position, it is possible for a linear system to have a pivot position in its rightmost column:   If we notice that , we see that any linear combination of and , is actually a scalar multiple of . Therefore, the span of and is the line defined by the vector . Intuitively, this means that we can only walk to points on this line using these two vectors.   "
},
{
  "id": "sec-span-3-10",
  "level": "2",
  "url": "sec-span.html#sec-span-3-10",
  "type": "Notation",
  "number": "18.3.9",
  "title": "",
  "body": " We will denote the span of the set of vectors by .  "
},
{
  "id": "activity-span-r3",
  "level": "2",
  "url": "sec-span.html#activity-span-r3",
  "type": "Activity",
  "number": "18.3.3",
  "title": "",
  "body": "  In this activity, we will look at the span of sets of vectors in .   Suppose . Give a geometric description of and a rough sketch of and its span in .   A three-dimensional coordinate system for sketching and its span.        Now consider the two vectors . Sketch the vectors below. Then give a geometric description of and a rough sketch of the span in .   A coordinate system for sketching , , and .       Let's now look at this situation algebraically by writing write . Determine the conditions on , , and so that is in by considering the linear system or . Explain how this relates to your sketch of .   Consider the vectors    Is the vector in ?    Is the vector in ?   Give a geometric description of .    Consider the vectors . Form the matrix and find its reduced row echelon form. What does this tell you about ?   If the span of a set of vectors is , what can you say about the pivot positions of the matrix ?  What is the smallest number of vectors such that ?       is the line defined by .   is the -plane.   For the linear system to be consistent, we need , which means that the third coordinate of the vector must be 0 for to be in . In other words, must lie in the -plane.   We consider the two cases.  We have the augmented matrix which shows that the system is inconsistent. Therefore, is not in .  We have the augmented matrix which shows that the system is consistent. Therefore, is in .  The span is the plane in defined by and .   We have the reduced row echelon form Since there is a pivot position in every row, this says that every equation is consistent. The is therefore .  There must be a pivot position in every row.  If a set of vectors spans , its corresponding matrix must have a pivot position in every row. Because there can be at most one pivot position in a column, there must be at least three columns. Therefore, the smallest number of vectors that span is three.    "
},
{
  "id": "fig-span-line",
  "level": "2",
  "url": "sec-span.html#fig-span-line",
  "type": "Figure",
  "number": "18.3.12",
  "title": "",
  "body": " The span of a single nonzero vector is a line.     "
},
{
  "id": "fig-span-plane",
  "level": "2",
  "url": "sec-span.html#fig-span-plane",
  "type": "Figure",
  "number": "18.3.13",
  "title": "",
  "body": " The span of these two vectors in is a plane.     "
},
{
  "id": "prop-pivot-row",
  "level": "2",
  "url": "sec-span.html#prop-pivot-row",
  "type": "Proposition",
  "number": "18.3.14",
  "title": "",
  "body": "  Suppose we have vectors in . Then if and only if the matrix has a pivot position in every row.   "
},
{
  "id": "prop-span-bound",
  "level": "2",
  "url": "sec-span.html#prop-span-bound",
  "type": "Proposition",
  "number": "18.3.15",
  "title": "",
  "body": "  A set of vectors whose span is contains at least vectors.   "
},
{
  "id": "sec-span-6-1",
  "level": "2",
  "url": "sec-span.html#sec-span-6-1",
  "type": "Exercise",
  "number": "18.3.4.1",
  "title": "",
  "body": " In this exercise, we will consider the span of some sets of two- and three-dimensional vectors.   Consider the vectors .  Is in ?  Give a geometric description of .    Consider the vectors .  Is the vector in ?  Is the vector in ?   Is the vector in ?  Give a geometric description of .         .  For the following vectors,   is in .   is in .   is not in .   is a plane in .      The equation is consistent so is in . Since has a pivot position in every row, is .  Let's consider the following vectors.  The equation is consistent so is in .  The vector is in because .  The equation is not consistent so is not in .   is the plane defined by and .    "
},
{
  "id": "sec-span-6-2",
  "level": "2",
  "url": "sec-span.html#sec-span-6-2",
  "type": "Exercise",
  "number": "18.3.4.2",
  "title": "",
  "body": " Provide a justification for your response to the following questions.  Suppose you have a set of vectors . Can you guarantee that is in ?  Suppose that is an matrix. Can you guarantee that the equation is consistent?   What is ?      Yes  Yes   consists only of the vector .     Yes, because .  Yes, is a solution to the equation .   consists only of the vector .   "
},
{
  "id": "sec-span-6-3",
  "level": "2",
  "url": "sec-span.html#sec-span-6-3",
  "type": "Exercise",
  "number": "18.3.4.3",
  "title": "",
  "body": " For both parts of this exercise, give a geometric description of sets of the vectors and include a sketch.   For which vectors in is the equation consistent?   For which vectors in is the equation consistent?       must be a scalar multiple of .   can be any vector in .      so must be on the line defined by .   so can be any vector in .   "
},
{
  "id": "sec-span-6-4",
  "level": "2",
  "url": "sec-span.html#sec-span-6-4",
  "type": "Exercise",
  "number": "18.3.4.4",
  "title": "",
  "body": " Consider the following matrices: . Do the columns of span ? Do the columns of span ?    The columns of do not span . The columns of do.    The reduced row echelon form of is so the columns of do not span .  The reduced row echelon form of is so the columns of span .   "
},
{
  "id": "sec-span-6-5",
  "level": "2",
  "url": "sec-span.html#sec-span-6-5",
  "type": "Exercise",
  "number": "18.3.4.5",
  "title": "",
  "body": " Determine whether the following statements are true or false and provide a justification for your response. Throughout, we will assume that the matrix has columns ; that is, .  If the equation is consistent, then is in .   The equation is consistent.  If , , , and are vectors in , then their span is .  If is a linear combination of , then is in .  If is an matrix, then the span of the columns of is a set of vectors in .      True   True   False   True   False     True. If is a solution to , then the components of are weights whose linear combination is .  True, because is a solution.  False. The span could be a smaller set.  True. This is the definition of the span.  False. The span is a set of vectors in .   "
},
{
  "id": "sec-span-6-6",
  "level": "2",
  "url": "sec-span.html#sec-span-6-6",
  "type": "Exercise",
  "number": "18.3.4.6",
  "title": "",
  "body": " This exercise asks you to construct some matrices whose columns span a given set.  Construct a matrix whose columns span .  Construct a matrix whose columns span a plane in .  Construct a matrix whose columns span a line in .     We will choose matrices in reduced row echelon form.   .   .   .    We will choose matrices in reduced row echelon form.   .   .   .   "
},
{
  "id": "sec-span-6-7",
  "level": "2",
  "url": "sec-span.html#sec-span-6-7",
  "type": "Exercise",
  "number": "18.3.4.7",
  "title": "",
  "body": " Provide a justification for your response to the following questions.  Suppose that we have vectors in , , whose span is . Can every vector in be written as a linear combination of ?   Suppose that we have vectors in , , whose span is . Can every vector in be written uniquely as a linear combination of ?   Do the vectors span ?  Suppose that span . What can you guarantee about the value of ?  Can 17 vectors in span ?     Yes  No  Yes     No     Yes. Because the span is , every vector in can be written as a linear combination of the vectors.  No. The matrix must have a column without a pivot position. Therefore, any equation has infinitely many solutions.  Yes, because has a pivot position in every row.  There must be at least 438 vectors so .  No, because we need at least 20 vectors to span .   "
},
{
  "id": "sec-span-6-8",
  "level": "2",
  "url": "sec-span.html#sec-span-6-8",
  "type": "Exercise",
  "number": "18.3.4.8",
  "title": "",
  "body": " The following observation will be helpful in this exercise. If we want to find a solution to the equation , we could first find a solution to the equation and then find a solution to the equation .  Suppose that is a matrix whose columns span and is a matrix. In this case, we can form the product .  What is the shape of the product ?  Can you guarantee that the columns of span ?  If you know additionally that the span of the columns of is , can you guarantee that the columns of span ?         No  Yes      is a matrix.  No. Since the columns of span , then the equation has a solution for every vector . However, we may not be able to solve the equation if the columns of do not span .  Yes. If we are given a vector in , we can find a vector such that since the columns of span . Since the columns of span , we can find a vector such that . Then , which means that is the span of the columns of .   "
},
{
  "id": "sec-span-6-9",
  "level": "2",
  "url": "sec-span.html#sec-span-6-9",
  "type": "Exercise",
  "number": "18.3.4.9",
  "title": "",
  "body": " Suppose that is a matrix and that, for some vector , the equation has a unique solution.  What can you say about the pivot positions of ?  What can you say about the span of the columns of ?  If is some other vector in , what can you conclude about the equation ?  What can you about the solution space to the equation ?     There is a pivot position in each row and each column.  The span is .  There is a unique solution.  There is a unique solution      Since the solution is unique, each column of must have a pivot position. Since there are also 12 rows, each row must have a pivot position as well. This means that the row reduced echelon form of is the identity matrix.  Since there is a pivot position in every row, the span of the columns of is .  There is a unique solution to the equation because has a pivot position in every row and every column.  There is a unique solution .   "
},
{
  "id": "sec-span-6-10",
  "level": "2",
  "url": "sec-span.html#sec-span-6-10",
  "type": "Exercise",
  "number": "18.3.4.10",
  "title": "",
  "body": " Suppose that .  Is a linear combination of and ? If so, find weights such that .  Show that a linear combination can be rewritten as a linear combination of and .  Explain why .      Yes  We have   As seen in the last part of this problem, every linear combination of , , and can be rewritten as a linear combination of and .     Yes, because .  We have   As seen in the last part of this problem, every linear combination of , , and can be rewritten as a linear combination of and .   "
},
{
  "id": "sec-span-6-11",
  "level": "2",
  "url": "sec-span.html#sec-span-6-11",
  "type": "Exercise",
  "number": "18.3.4.11",
  "title": "",
  "body": " As defined in this section, the span of a set of vectors is generated by taking all possible linear combinations of those vectors. This exercise will demonstrate the fact that the span can also be realized as the solution space to a linear system.  We will consider the vectors    Is every vector in in ? If not, describe the span.    To describe as the solution space of a linear system, we will write . If is in , then the linear system corresponding to the augmented matrix must be consistent. This means that a pivot cannot occur in the rightmost column. Perform row operations to put this augmented matrix into a triangular form. Now identify an equation in , , and that tells us when there is no pivot in the rightmost column. The solution space to this equation describes .   In this example, the matrix formed by the vectors has two pivot positions. Suppose we were to consider another example in which this matrix had had only one pivot position. How would this have changed the linear system describing ?      is the plane defined by and .   .  There would be two equations involving the variables , , and .     Finding the reduced row echelon form shows that is a linear combination of and . Therefore, is the plane defined by and .  We see that If the equation is consistent, we must have . The solution space is the plane defined by and .  In that case, there would be two equations involving the variables , , and , and the solution space would be a line.   "
},
{
  "id": "sec-linear-dep",
  "level": "1",
  "url": "sec-linear-dep.html",
  "type": "Section",
  "number": "18.4",
  "title": "Linear independence",
  "body": " Linear independence   In the previous section, questions about the existence of solutions of a linear system led to the concept of the span of a set of vectors. In particular, the span of a set of vectors is the set of vectors for which a solution to the linear system exists.  In this section, we turn to the uniqueness of solutions of a linear system, the second of our two fundamental questions . This will lead us to the concept of linear independence.    Let's begin by looking at some sets of vectors in . As we saw in the previous section, the span of a set of vectors in will be either a line, a plane, or itself.   Consider the following vectors in : . Describe the span of these vectors, , as a line, a plane, or .   Now consider the set of vectors: . Describe the span of these vectors, , as a line, a plane, or .   Show that the vector is a linear combination of and by finding weights such that .  Explain why any linear combination of , , and , can be written as a linear combination of and .  Explain why .         We will construct the matrix whose columns are , , and : Because there is a pivot in every row, tells us that .    Similarly, As there are two pivot positions, we see that is a plane in .    We see that which tells us that .    We have     Any linear combination of , , and is itself a linear combination of and .         Linear dependence  We have seen examples where the span of a set of three vectors in is and other examples where the span of three vectors is a plane. We would like to understand the difference between these two situations.    Let's consider the set of three vectors in : Forming the associated matrix gives Because there is a pivot position in every row, tells us that .      Now let's consider the set of three vectors: Forming the associated matrix gives Since the last row does not have a pivot position, we know that the span of these vectors is not but is instead a plane.  In fact, we can say more if we shift our perspective slightly and view this as an augmented matrix: In this way, we see that , which enables us to rewrite any linear combination of these three vectors:   In other words, any linear combination of , , and may be written as a linear combination using only the vectors and . Since the span of a set of vectors is simply the set of their linear combinations, this shows that As a result, adding the vector to the set of vectors and does not change the span.    Before exploring this type of behavior more generally, let's think about it from a geometric point of view. Suppose that we begin with the two vectors and in . The span of these two vectors is a plane in , as seen on the left of .   The span of the vectors , , and .       Because the vector is not a linear combination of and , it provides a direction to move that is independent of and . Adding this third vector therefore forms a set whose span is , as seen on the right of .  Similarly, the span of the vectors and in is also a plane. However, the third vector is a linear combination of and , which means that it already lies in the plane formed by and , as seen in . Since we can already move in this direction using just and , adding to the set does not change the span. As a result, it remains a plane.   The span of the vectors , , and .       What distinguishes these two examples is whether one of the vectors is a linear combination of the others, an observation that leads to the following definition.   linearly independent  linearly dependent   A set of vectors is called linearly dependent if one of the vectors is a linear combination of the others. Otherwise, the set of vectors is called linearly independent .    For the sake of completeness, we say that a set of vectors containing only one nonzero vector is linearly independent.    How to recognize linear dependence    We would like to develop a means to detect when a set of vectors is linearly dependent. This activity will point the way.  Suppose we have five vectors in that form the columns of a matrix having reduced row echelon form . Is it possible to write one of the vectors as a linear combination of the others? If so, show explicitly how one vector appears as a linear combination of some of the other vectors. Is this set of vectors linearly dependent or independent?  Suppose we have another set of three vectors in that form the columns of a matrix having reduced row echelon form . Is it possible to write one of these vectors , , as a linear combination of the others? If so, show explicitly how one vector appears as a linear combination of some of the other vectors. Is this set of vectors linearly dependent or independent?  By looking at the pivot positions, how can you determine whether the columns of a matrix are linearly dependent or independent?  If one vector in a set is the zero vector , can the set of vectors be linearly independent?  Suppose a set of vectors in has twelve vectors. Is it possible for this set to be linearly independent?      Let's focus on the first three vectors and view the matrix as an augmented one: This shows that so it is possible to write one of the vectors as a linear combination of the others. Therefore, the set is linearly dependent.  Applying the same reasoning as in the previous part, we see that we cannot write any of the vectors as a linear combination of the others. Therefore, the set is linearly independent.  The columns of a matrix are linearly independent exactly when there is a pivot position in every column of the matrix.  No, because we can write the zero vector as a linear combination of the other vectors: .  No, because the matrix formed by the vectors would have 12 columns and only 10 rows. There can be at most 10 pivot positions so there are at least two columns without pivot positions.     By now, we should expect that the pivot positions play an important role in determining whether the columns of a matrix are linearly dependent. For instance, suppose we have four vectors and their associated matrix Since the third column does not contain a pivot position, let's just focus on the first three columns and view them as an augmented matrix: This says that which tells us that the set of vectors is linearly dependent. Moreover, we see that   More generally, the same reasoning implies that a set of vectors is linearly dependent if the associated matrix has a column without a pivot position. Indeed, as illustrated here, a vector corresponding to a column without a pivot position can be expressed as a linear combination of the vectors whose columns do contain pivot positions.  Suppose instead that the matrix associated to a set of vectors has a pivot position in every column. Viewing this as an augmented matrix again, we see that the linear system is inconsistent since there is a pivot in the rightmost column, which means that cannot be expressed as a linear combination of the other vectors. Similarly, cannot be expressed as a linear combination of and . In fact, none of the vectors can be written as a linear combination of the others so this set of vectors is linearly independent.  The following proposition summarizes these findings.    The columns of a matrix are linearly independent if and only if every column contains a pivot position.    This condition imposes a constraint on how many vectors we can have in a linearly independent set. Here is an example of the reduced row echelon form of a matrix whose columns form a set of three linearly independent vectors in : Notice that there are at least as many rows as columns, which must be the case if every column is to have a pivot position.  More generally, if is a linearly independent set of vectors in , the associated matrix must have a pivot position in every column. Since every row contains at most one pivot position, the number of columns can be no greater than the number of rows. This means that the number of vectors in a linearly independent set can be no greater than the number of dimensions.    A linearly independent set of vectors in contains at most vectors.    This says, for instance, that any linearly independent set of vectors in can contain no more three vectors. We usually imagine three independent directions, such as up\/down, front\/back, left\/right, in our three-dimensional world. This proposition tells us that there can be no more independent directions.  The proposition above says that a set of vectors in that is linear independent has at most vectors. By comparison, says that a set of vectors whose span is has at least vectors.    Homogeneous equations  If is a matrix, we call the equation a homogeneous equation. As we'll see, the uniqueness of solutions to this equation reflects on the linear independence of the columns of .   Linear independence and homogeneous equations    Explain why the homogeneous equation is consistent no matter the matrix .  Consider the matrix whose columns we denote by , , and . Describe the solution space of the homogeneous equation using a parametric description, if appropriate.   Find a nonzero solution to the homogeneous equation and use it to find weights , , and such that .  Use the equation you found in the previous part to write one of the vectors as a linear combination of the others.  Are the vectors , , and linearly dependent or independent?      The vector is always a solution.  We have From the reduced row echelon form, we see that is a free variable and that we have The solution space is then written parametrically as   If we set , then we have the solution , which says that   We may rewrite this expression as , showing that is a linear combination of and .   The vectors , , and are linearly dependent, and we know this in two ways. We have seen how to express one vector as a linear combination of the others. Also, we have seen that the associated matrix has a column without a pivot position.      This activity shows how the solution space of the homogeneous equation indicates whether the columns of are linearly dependent or independent. First, we know that the equation always has at least one solution, the vector . Any other solution is a nonzero solution.    Let's consider the vectors and their associated matrix .  The homogeneous equation has the associated augmented matrix Therefore, has a column without a pivot position, which tells us that the vectors , , and are linearly dependent. However, we can also see this fact in another way.  The reduced row echelon matrix tells us that the homogeneous equation has a free variable so that there must be infinitely many solutions. In particular, we have so the solutions have the form   If we choose , then we obtain the nonzero solution to the homogeneous equation , which implies that In other words,   Because is a linear combination of and , we know that this set of vectors is linearly dependent.    As this example demonstrates, there are many ways we can view the question of linear independence, some of which are recorded in the following proposition.    For a matrix , the following statements are equivalent:   The columns of are linearly dependent.   One of the vectors in the set is a linear combination of the others.   The matrix has a column without a pivot position.   The homogeneous equation has infinitely many solutions and hence a nonzero solution.   There are weights , not all of which are zero, such that .        Summary  This section developed the concept of linear dependence of a set of vectors. More specifically, we saw that:  A set of vectors is linearly dependent if one of the vectors is a linear combination of the others.  A set of vectors is linearly independent if and only if the vectors form a matrix that has a pivot position in every column.  A set of linearly independent vectors in contains no more than vectors.  The columns of the matrix are linearly dependent if the homogeneous equation has a nonzero solution.  A set of vectors is linearly dependent if there are weights , not all of which are zero, such that .   At the beginning of the section, we said that this concept addressed the second of our two fundamental questions concerning the uniqueness of solutions to a linear system. It is worth comparing the results of this section with those of the previous one so that the parallels between them become clear.  As usual, we will write a matrix as a collection of vectors,    Span and Linear Independence      Span  Linear independence     A vector is in the span of a set of vectors if it is a linear combination of those vectors.    A set of vectors is linearly dependent if one of the vectors is a linear combination of the others.      A vector is in the span of if there exists a solution to .    The vectors are linearly independent if is the unique solution to .      The columns of an matrix span if the matrix has a pivot position in every row.    The columns of a matrix are linearly independent if the matrix has a pivot position in every column.      A set of vectors that span has at least vectors.    A set of linearly independent vectors in has at most vectors.         Consider the set of vectors .   Explain why this set of vectors is linearly dependent.   Write one of the vectors as a linear combination of the others.    Find weights , , , and , not all of which are zero, such that .   Suppose . Find a nonzero solution to the homogenous equation .     Four vectors in must always be linearly dependent.   .   , , , and         We view the vectors as the columns of a matrix so that Since there is a column without a pivot position, the vectors must be linearly dependent. We also expect this because four vectors in must always be linearly dependent.  If we view the first three columns as an augmented matrix, we have We see that .  From the solution to the previous part, we have , which says that , , , and is an appropriate choice of weights.  From the solution to the previous part, we see that is a solution of the homogeneous equation.     Consider the vectors .   Are these vectors linearly independent or linearly dependent?    Describe the .   Suppose that is a vector in . Explain why we can guarantee that may be written as a linear combination of , , and .   Suppose that is a vector in . In how many ways can be written as a linear combination of , , and ?     Linearly independent.   .  Because , every vector in can be written as a linear combination of , , and .  In exactly one way.     Viewing the vectors as the columns of a matrix, we have Since there is a pivot position in every column, the vectors are linearly independent.  Since the matrix has a pivot position in every row, we see that .  Because , every vector in can be written as a linear combination of , , and .  In exactly one way. Because the matrix has a pivot position in every column, there cannot be a free variable.     Respond to the following questions and provide a justification for your responses.   If the vectors and form a linearly dependent set, must one vector be a scalar multiple of the other?  Suppose that is a linearly independent set of vectors. What can you say about the linear independence or dependence of a subset of these vectors?  Suppose is a linearly independent set of vectors that form the columns of a matrix . If the equation is inconsistent, what can you say about the linear independence or dependence of the set of vectors ?     Yes  Any subset is linearly independent.  They form a linearly independent set.     Yes. If and one of the weights is not zero, we can rearrange this expression to solve for one of the vectors as a scalar multiple of the other. For instance, if , then .  Any subset of the vectors forms a linearly independent set. Because the original set is linearly independent, none of the vectors can be written as a linear combination of the others. This is still true when we look at a smaller set of the vectors.  The set of vectors will also be linearly independent. Since is inconsistent, we know that cannot be written as a linear combination of the other vectors. We also know that each of the vectors cannot be written as a linear combination of the others because is a linearly independent set.     Determine whether the following statements are true or false and provide a justification for your response.  If are linearly dependent, then one vector is a scalar multiple of one of the others.  If are vectors in , then the set of vectors is linearly dependent.  If are vectors in , then the set of vectors is linearly independent.  Suppose we have a set of vectors and that is a scalar multiple of . Then the set is linearly dependent.  Suppose that are linearly independent and form the columns of a matrix . If is consistent, then there is exactly one solution.     False  True  False  True  True     False. We only know that one vector can be written as a linear combination of the others.  True. If we put the vectors into a matrix, there are more columns than rows. Therefore, there must be a column without a pivot position so the vectors form a linearly dependent set.  False. They could form a linearly independent set, but we cannot guarantee it. We would have to look at the location of the pivot positions in the associated matrix.  True. In this case, can be written as a linear combination of the other vectors so the set is linearly dependent.  True. Since the vectors are linearly independent, has a pivot position in every column. Therefore, there is not a free variable in the description of the solution space to the equation . Therefore, the solution is unique.     Suppose we have a set of vectors in that satisfy the relationship: and suppose that is the matrix .  Find a nonzero solution to the equation .  Explain why the matrix has a column without a pivot position.  Write one of the vectors as a linear combination of the others.  Explain why the set of vectors is linearly dependent.        There are infinitely many solutions to the homogeneous equation .   .  One vector can be written as a linear combination of the others.     The vector is a solution.  There is a nonzero solution to the homogeneous equation . Since there is also the zero solution , there must be infinitely many solutions. This can only happen when there is a column of that does not have a pivot position.   .  Because one of the vectors can be written as a linear combination of the others, the set of vectors is linearly dependent.     Suppose that is a set of vectors in that form the columns of a matrix .  Suppose that the vectors span . What can you say about the number of vectors in this set?  Suppose instead that the vectors are linearly independent. What can you say about the number of vectors in this set?  Suppose that the vectors are both linearly independent and span . What can you say about the number of vectors in the set?  Assume that the vectors are both linearly independent and span . Given a vector in , what can you say about the solution space to the equation ?     There must be at least 27 vectors.  There must be at most 27 vectors.  There are exactly 27 vectors.  There is exactly one solution.     In this case, the matrix must have a pivot position in evey row. There must be at least 27 vectors for this to be possible.  In this case, the matrix must have a pivot position in every column. There must be at most 27 vectors for this to be possible.  There must be exactly 27 vectors.  There is exactly one solution to the equation because the matrix has a pivot position in every row and every column.     Given below are some descriptions of sets of vectors that form the columns of a matrix . For each description, give a possible reduced row echelon form for or indicate why there is no set of vectors satisfying the description by stating why the required reduced row echelon matrix cannot exist.  A set of 4 linearly independent vectors in .  A set of 4 linearly independent vectors in .  A set of 3 vectors whose span is .  A set of 5 linearly independent vectors in .  A set of 5 vectors whose span is .      .   .  This is not possible.  This is not possible.   .     There must be a pivot position in every column: .  There must be a pivot position in every row. .  This is not possible. The matrix has dimensions so there cannot be a pivot position in every row.  This is not possible. The matrix has dimensions so there cannot be a pivot position in every column.  There must be a pivot position in every row. .     When we explored matrix multiplication in , we saw that some properties that are true for real numbers are not true for matrices. This exercise will investigate that in some more depth.  Suppose that and are two matrices and that . If , what can you say about the linear independence of the columns of ?  Suppose that we have matrices , and such that . We have seen that we cannot generally conclude that . If we assume additionally that is a matrix whose columns are linearly independent, explain why . You may wish to begin by rewriting the equation as .      They are linearly dependent.  The columns of and are all equal.     Since , there is a column of , which we'll call , that is not zero. Since , we know that , which means that is a nonzero solution to the homogenous equation . Therefore, the columns of are linearly dependent.   Since the columns of are linearly independent, the only solution to the homogeneous equation is . Since that , every column of satisfies . Therefore, every column of is the zero vector, which implies that .     Suppose that is an unknown parameter and consider the set of vectors .  For what values of is the set of vectors linearly dependent?  For what values of does the set of vectors span ?      .   .     We construct the matrix If the vectors are linearly dependent, we cannot have a pivot position in the third column. This means that .  If the vectors span , there must be a pivot in the third row, which means that .     Given a set of linearly dependent vectors, we can eliminate some of the vectors to create a smaller, linearly independent set of vectors.  Suppose that is a linear combination of the vectors and . Explain why .  Consider the vectors . Write one of the vectors as a linear combination of the others. Find a set of three vectors whose span is the same as .  Are the three vectors you are left with linearly independent? If not, express one of the vectors as a linear combination of the others and find a set of two vectors whose span is the same as .  Give a geometric description of in as we did in .      Any linear combination of , , and can be rewritten as a linear combination of and .   .   .  It is a plane in .     If is a linear combination of and , then we can write . Then any linear combination of can be rewritten as   We have This shows that and so .  The remaining three vectors are linearly dependent because . We therefore have .  The vectors and are linearly independent so their span is a plane in .     "
},
{
  "id": "sec-linear-dep-2-3",
  "level": "2",
  "url": "sec-linear-dep.html#sec-linear-dep-2-3",
  "type": "Preview Activity",
  "number": "18.4.1",
  "title": "",
  "body": "  Let's begin by looking at some sets of vectors in . As we saw in the previous section, the span of a set of vectors in will be either a line, a plane, or itself.   Consider the following vectors in : . Describe the span of these vectors, , as a line, a plane, or .   Now consider the set of vectors: . Describe the span of these vectors, , as a line, a plane, or .   Show that the vector is a linear combination of and by finding weights such that .  Explain why any linear combination of , , and , can be written as a linear combination of and .  Explain why .         We will construct the matrix whose columns are , , and : Because there is a pivot in every row, tells us that .    Similarly, As there are two pivot positions, we see that is a plane in .    We see that which tells us that .    We have     Any linear combination of , , and is itself a linear combination of and .      "
},
{
  "id": "example-span-r3",
  "level": "2",
  "url": "sec-linear-dep.html#example-span-r3",
  "type": "Example",
  "number": "18.4.1",
  "title": "",
  "body": "  Let's consider the set of three vectors in : Forming the associated matrix gives Because there is a pivot position in every row, tells us that .   "
},
{
  "id": "example-span-plane",
  "level": "2",
  "url": "sec-linear-dep.html#example-span-plane",
  "type": "Example",
  "number": "18.4.2",
  "title": "",
  "body": "  Now let's consider the set of three vectors: Forming the associated matrix gives Since the last row does not have a pivot position, we know that the span of these vectors is not but is instead a plane.  In fact, we can say more if we shift our perspective slightly and view this as an augmented matrix: In this way, we see that , which enables us to rewrite any linear combination of these three vectors:   In other words, any linear combination of , , and may be written as a linear combination using only the vectors and . Since the span of a set of vectors is simply the set of their linear combinations, this shows that As a result, adding the vector to the set of vectors and does not change the span.   "
},
{
  "id": "figure-span-r3",
  "level": "2",
  "url": "sec-linear-dep.html#figure-span-r3",
  "type": "Figure",
  "number": "18.4.3",
  "title": "",
  "body": " The span of the vectors , , and .      "
},
{
  "id": "figure-span-plane",
  "level": "2",
  "url": "sec-linear-dep.html#figure-span-plane",
  "type": "Figure",
  "number": "18.4.4",
  "title": "",
  "body": " The span of the vectors , , and .      "
},
{
  "id": "sec-linear-dep-3-11",
  "level": "2",
  "url": "sec-linear-dep.html#sec-linear-dep-3-11",
  "type": "Definition",
  "number": "18.4.5",
  "title": "",
  "body": " linearly independent  linearly dependent   A set of vectors is called linearly dependent if one of the vectors is a linear combination of the others. Otherwise, the set of vectors is called linearly independent .   "
},
{
  "id": "sec-linear-dep-4-2",
  "level": "2",
  "url": "sec-linear-dep.html#sec-linear-dep-4-2",
  "type": "Activity",
  "number": "18.4.2",
  "title": "",
  "body": "  We would like to develop a means to detect when a set of vectors is linearly dependent. This activity will point the way.  Suppose we have five vectors in that form the columns of a matrix having reduced row echelon form . Is it possible to write one of the vectors as a linear combination of the others? If so, show explicitly how one vector appears as a linear combination of some of the other vectors. Is this set of vectors linearly dependent or independent?  Suppose we have another set of three vectors in that form the columns of a matrix having reduced row echelon form . Is it possible to write one of these vectors , , as a linear combination of the others? If so, show explicitly how one vector appears as a linear combination of some of the other vectors. Is this set of vectors linearly dependent or independent?  By looking at the pivot positions, how can you determine whether the columns of a matrix are linearly dependent or independent?  If one vector in a set is the zero vector , can the set of vectors be linearly independent?  Suppose a set of vectors in has twelve vectors. Is it possible for this set to be linearly independent?      Let's focus on the first three vectors and view the matrix as an augmented one: This shows that so it is possible to write one of the vectors as a linear combination of the others. Therefore, the set is linearly dependent.  Applying the same reasoning as in the previous part, we see that we cannot write any of the vectors as a linear combination of the others. Therefore, the set is linearly independent.  The columns of a matrix are linearly independent exactly when there is a pivot position in every column of the matrix.  No, because we can write the zero vector as a linear combination of the other vectors: .  No, because the matrix formed by the vectors would have 12 columns and only 10 rows. There can be at most 10 pivot positions so there are at least two columns without pivot positions.    "
},
{
  "id": "sec-linear-dep-4-7",
  "level": "2",
  "url": "sec-linear-dep.html#sec-linear-dep-4-7",
  "type": "Proposition",
  "number": "18.4.6",
  "title": "",
  "body": "  The columns of a matrix are linearly independent if and only if every column contains a pivot position.   "
},
{
  "id": "prop-linear-indep-bound",
  "level": "2",
  "url": "sec-linear-dep.html#prop-linear-indep-bound",
  "type": "Proposition",
  "number": "18.4.7",
  "title": "",
  "body": "  A linearly independent set of vectors in contains at most vectors.   "
},
{
  "id": "sec-linear-dep-5-3",
  "level": "2",
  "url": "sec-linear-dep.html#sec-linear-dep-5-3",
  "type": "Activity",
  "number": "18.4.3",
  "title": "Linear independence and homogeneous equations.",
  "body": " Linear independence and homogeneous equations    Explain why the homogeneous equation is consistent no matter the matrix .  Consider the matrix whose columns we denote by , , and . Describe the solution space of the homogeneous equation using a parametric description, if appropriate.   Find a nonzero solution to the homogeneous equation and use it to find weights , , and such that .  Use the equation you found in the previous part to write one of the vectors as a linear combination of the others.  Are the vectors , , and linearly dependent or independent?      The vector is always a solution.  We have From the reduced row echelon form, we see that is a free variable and that we have The solution space is then written parametrically as   If we set , then we have the solution , which says that   We may rewrite this expression as , showing that is a linear combination of and .   The vectors , , and are linearly dependent, and we know this in two ways. We have seen how to express one vector as a linear combination of the others. Also, we have seen that the associated matrix has a column without a pivot position.     "
},
{
  "id": "sec-linear-dep-5-5",
  "level": "2",
  "url": "sec-linear-dep.html#sec-linear-dep-5-5",
  "type": "Example",
  "number": "18.4.8",
  "title": "",
  "body": "  Let's consider the vectors and their associated matrix .  The homogeneous equation has the associated augmented matrix Therefore, has a column without a pivot position, which tells us that the vectors , , and are linearly dependent. However, we can also see this fact in another way.  The reduced row echelon matrix tells us that the homogeneous equation has a free variable so that there must be infinitely many solutions. In particular, we have so the solutions have the form   If we choose , then we obtain the nonzero solution to the homogeneous equation , which implies that In other words,   Because is a linear combination of and , we know that this set of vectors is linearly dependent.   "
},
{
  "id": "sec-linear-dep-5-7",
  "level": "2",
  "url": "sec-linear-dep.html#sec-linear-dep-5-7",
  "type": "Proposition",
  "number": "18.4.9",
  "title": "",
  "body": "  For a matrix , the following statements are equivalent:   The columns of are linearly dependent.   One of the vectors in the set is a linear combination of the others.   The matrix has a column without a pivot position.   The homogeneous equation has infinitely many solutions and hence a nonzero solution.   There are weights , not all of which are zero, such that .     "
},
{
  "id": "sec-linear-dep-6-5",
  "level": "2",
  "url": "sec-linear-dep.html#sec-linear-dep-6-5",
  "type": "Table",
  "number": "18.4.10",
  "title": "Span and Linear Independence",
  "body": " Span and Linear Independence      Span  Linear independence     A vector is in the span of a set of vectors if it is a linear combination of those vectors.    A set of vectors is linearly dependent if one of the vectors is a linear combination of the others.      A vector is in the span of if there exists a solution to .    The vectors are linearly independent if is the unique solution to .      The columns of an matrix span if the matrix has a pivot position in every row.    The columns of a matrix are linearly independent if the matrix has a pivot position in every column.      A set of vectors that span has at least vectors.    A set of linearly independent vectors in has at most vectors.     "
},
{
  "id": "sec-linear-dep-7-1",
  "level": "2",
  "url": "sec-linear-dep.html#sec-linear-dep-7-1",
  "type": "Exercise",
  "number": "18.4.5.1",
  "title": "",
  "body": " Consider the set of vectors .   Explain why this set of vectors is linearly dependent.   Write one of the vectors as a linear combination of the others.    Find weights , , , and , not all of which are zero, such that .   Suppose . Find a nonzero solution to the homogenous equation .     Four vectors in must always be linearly dependent.   .   , , , and         We view the vectors as the columns of a matrix so that Since there is a column without a pivot position, the vectors must be linearly dependent. We also expect this because four vectors in must always be linearly dependent.  If we view the first three columns as an augmented matrix, we have We see that .  From the solution to the previous part, we have , which says that , , , and is an appropriate choice of weights.  From the solution to the previous part, we see that is a solution of the homogeneous equation.   "
},
{
  "id": "sec-linear-dep-7-2",
  "level": "2",
  "url": "sec-linear-dep.html#sec-linear-dep-7-2",
  "type": "Exercise",
  "number": "18.4.5.2",
  "title": "",
  "body": " Consider the vectors .   Are these vectors linearly independent or linearly dependent?    Describe the .   Suppose that is a vector in . Explain why we can guarantee that may be written as a linear combination of , , and .   Suppose that is a vector in . In how many ways can be written as a linear combination of , , and ?     Linearly independent.   .  Because , every vector in can be written as a linear combination of , , and .  In exactly one way.     Viewing the vectors as the columns of a matrix, we have Since there is a pivot position in every column, the vectors are linearly independent.  Since the matrix has a pivot position in every row, we see that .  Because , every vector in can be written as a linear combination of , , and .  In exactly one way. Because the matrix has a pivot position in every column, there cannot be a free variable.   "
},
{
  "id": "sec-linear-dep-7-3",
  "level": "2",
  "url": "sec-linear-dep.html#sec-linear-dep-7-3",
  "type": "Exercise",
  "number": "18.4.5.3",
  "title": "",
  "body": " Respond to the following questions and provide a justification for your responses.   If the vectors and form a linearly dependent set, must one vector be a scalar multiple of the other?  Suppose that is a linearly independent set of vectors. What can you say about the linear independence or dependence of a subset of these vectors?  Suppose is a linearly independent set of vectors that form the columns of a matrix . If the equation is inconsistent, what can you say about the linear independence or dependence of the set of vectors ?     Yes  Any subset is linearly independent.  They form a linearly independent set.     Yes. If and one of the weights is not zero, we can rearrange this expression to solve for one of the vectors as a scalar multiple of the other. For instance, if , then .  Any subset of the vectors forms a linearly independent set. Because the original set is linearly independent, none of the vectors can be written as a linear combination of the others. This is still true when we look at a smaller set of the vectors.  The set of vectors will also be linearly independent. Since is inconsistent, we know that cannot be written as a linear combination of the other vectors. We also know that each of the vectors cannot be written as a linear combination of the others because is a linearly independent set.   "
},
{
  "id": "sec-linear-dep-7-4",
  "level": "2",
  "url": "sec-linear-dep.html#sec-linear-dep-7-4",
  "type": "Exercise",
  "number": "18.4.5.4",
  "title": "",
  "body": " Determine whether the following statements are true or false and provide a justification for your response.  If are linearly dependent, then one vector is a scalar multiple of one of the others.  If are vectors in , then the set of vectors is linearly dependent.  If are vectors in , then the set of vectors is linearly independent.  Suppose we have a set of vectors and that is a scalar multiple of . Then the set is linearly dependent.  Suppose that are linearly independent and form the columns of a matrix . If is consistent, then there is exactly one solution.     False  True  False  True  True     False. We only know that one vector can be written as a linear combination of the others.  True. If we put the vectors into a matrix, there are more columns than rows. Therefore, there must be a column without a pivot position so the vectors form a linearly dependent set.  False. They could form a linearly independent set, but we cannot guarantee it. We would have to look at the location of the pivot positions in the associated matrix.  True. In this case, can be written as a linear combination of the other vectors so the set is linearly dependent.  True. Since the vectors are linearly independent, has a pivot position in every column. Therefore, there is not a free variable in the description of the solution space to the equation . Therefore, the solution is unique.   "
},
{
  "id": "sec-linear-dep-7-5",
  "level": "2",
  "url": "sec-linear-dep.html#sec-linear-dep-7-5",
  "type": "Exercise",
  "number": "18.4.5.5",
  "title": "",
  "body": " Suppose we have a set of vectors in that satisfy the relationship: and suppose that is the matrix .  Find a nonzero solution to the equation .  Explain why the matrix has a column without a pivot position.  Write one of the vectors as a linear combination of the others.  Explain why the set of vectors is linearly dependent.        There are infinitely many solutions to the homogeneous equation .   .  One vector can be written as a linear combination of the others.     The vector is a solution.  There is a nonzero solution to the homogeneous equation . Since there is also the zero solution , there must be infinitely many solutions. This can only happen when there is a column of that does not have a pivot position.   .  Because one of the vectors can be written as a linear combination of the others, the set of vectors is linearly dependent.   "
},
{
  "id": "sec-linear-dep-7-6",
  "level": "2",
  "url": "sec-linear-dep.html#sec-linear-dep-7-6",
  "type": "Exercise",
  "number": "18.4.5.6",
  "title": "",
  "body": " Suppose that is a set of vectors in that form the columns of a matrix .  Suppose that the vectors span . What can you say about the number of vectors in this set?  Suppose instead that the vectors are linearly independent. What can you say about the number of vectors in this set?  Suppose that the vectors are both linearly independent and span . What can you say about the number of vectors in the set?  Assume that the vectors are both linearly independent and span . Given a vector in , what can you say about the solution space to the equation ?     There must be at least 27 vectors.  There must be at most 27 vectors.  There are exactly 27 vectors.  There is exactly one solution.     In this case, the matrix must have a pivot position in evey row. There must be at least 27 vectors for this to be possible.  In this case, the matrix must have a pivot position in every column. There must be at most 27 vectors for this to be possible.  There must be exactly 27 vectors.  There is exactly one solution to the equation because the matrix has a pivot position in every row and every column.   "
},
{
  "id": "sec-linear-dep-7-7",
  "level": "2",
  "url": "sec-linear-dep.html#sec-linear-dep-7-7",
  "type": "Exercise",
  "number": "18.4.5.7",
  "title": "",
  "body": " Given below are some descriptions of sets of vectors that form the columns of a matrix . For each description, give a possible reduced row echelon form for or indicate why there is no set of vectors satisfying the description by stating why the required reduced row echelon matrix cannot exist.  A set of 4 linearly independent vectors in .  A set of 4 linearly independent vectors in .  A set of 3 vectors whose span is .  A set of 5 linearly independent vectors in .  A set of 5 vectors whose span is .      .   .  This is not possible.  This is not possible.   .     There must be a pivot position in every column: .  There must be a pivot position in every row. .  This is not possible. The matrix has dimensions so there cannot be a pivot position in every row.  This is not possible. The matrix has dimensions so there cannot be a pivot position in every column.  There must be a pivot position in every row. .   "
},
{
  "id": "sec-linear-dep-7-8",
  "level": "2",
  "url": "sec-linear-dep.html#sec-linear-dep-7-8",
  "type": "Exercise",
  "number": "18.4.5.8",
  "title": "",
  "body": " When we explored matrix multiplication in , we saw that some properties that are true for real numbers are not true for matrices. This exercise will investigate that in some more depth.  Suppose that and are two matrices and that . If , what can you say about the linear independence of the columns of ?  Suppose that we have matrices , and such that . We have seen that we cannot generally conclude that . If we assume additionally that is a matrix whose columns are linearly independent, explain why . You may wish to begin by rewriting the equation as .      They are linearly dependent.  The columns of and are all equal.     Since , there is a column of , which we'll call , that is not zero. Since , we know that , which means that is a nonzero solution to the homogenous equation . Therefore, the columns of are linearly dependent.   Since the columns of are linearly independent, the only solution to the homogeneous equation is . Since that , every column of satisfies . Therefore, every column of is the zero vector, which implies that .   "
},
{
  "id": "sec-linear-dep-7-9",
  "level": "2",
  "url": "sec-linear-dep.html#sec-linear-dep-7-9",
  "type": "Exercise",
  "number": "18.4.5.9",
  "title": "",
  "body": " Suppose that is an unknown parameter and consider the set of vectors .  For what values of is the set of vectors linearly dependent?  For what values of does the set of vectors span ?      .   .     We construct the matrix If the vectors are linearly dependent, we cannot have a pivot position in the third column. This means that .  If the vectors span , there must be a pivot in the third row, which means that .   "
},
{
  "id": "sec-linear-dep-7-10",
  "level": "2",
  "url": "sec-linear-dep.html#sec-linear-dep-7-10",
  "type": "Exercise",
  "number": "18.4.5.10",
  "title": "",
  "body": " Given a set of linearly dependent vectors, we can eliminate some of the vectors to create a smaller, linearly independent set of vectors.  Suppose that is a linear combination of the vectors and . Explain why .  Consider the vectors . Write one of the vectors as a linear combination of the others. Find a set of three vectors whose span is the same as .  Are the three vectors you are left with linearly independent? If not, express one of the vectors as a linear combination of the others and find a set of two vectors whose span is the same as .  Give a geometric description of in as we did in .      Any linear combination of , , and can be rewritten as a linear combination of and .   .   .  It is a plane in .     If is a linear combination of and , then we can write . Then any linear combination of can be rewritten as   We have This shows that and so .  The remaining three vectors are linearly dependent because . We therefore have .  The vectors and are linearly independent so their span is a plane in .   "
},
{
  "id": "sec-linear-trans",
  "level": "1",
  "url": "sec-linear-trans.html",
  "type": "Section",
  "number": "18.5",
  "title": "Matrix transformations",
  "body": " Matrix transformations   The past few sections introduced us to matrix-vector multiplication as a means of thinking geometrically about the solutions to a linear system. In particular, we rewrote a linear system as a matrix equation and developed the concepts of span and linear independence in response to our two fundamental questions.   In this section, we will explore how matrix-vector multiplication defines certain types of functions, which we call matrix transformations , similar to those encountered in previous algebra courses. In particular, we will develop some algebraic tools for thinking about matrix transformations and look at some motivating examples. In the next section, we will see how matrix transformations describe important geometric operations and how they are used in computer animation.    We will begin by considering a more familiar situation; namely, the function , which takes a real number as an input and produces its square as its output.  What is the value of ?  Can we solve the equation ? If so, is the solution unique?  Can we solve the equation ? If so, is the solution unique?  Sketch a graph of the function in      Graph the function above.    We will now consider functions having the form . Draw a graph of the function on the left in .      Graphs of the function and .    Draw a graph of the function on the right of .  Remember that composing two functions means we use the output from one function as the input into the other; that is, . What function results from composing ?      We find .  If , then so there is not a unique solution.  There are no solutions to the equation .       The graph is shown on the left below.      The graph is shown on the right above.  Composing the functions, we find that . We see that the composition is a new linear function whose slope is obtained by multiplying the slopes of and .       Matrix transformations  In the preview activity, we considered familiar linear functions of a single variable, such as . We construct a function like this by choosing a number ; when given an input , the output is formed by multiplying by .  In this section, we will consider functions whose inputs are vectors and whose outputs are vectors defined through matrix-vector multiplication. That is, if is a matrix and is a vector, the function forms the product as its output. Such a function is called a matrix transformation.    matrix transformation  The matrix transformation associated to the matrix is the function that assigns to the vector the vector ; that is, .      The matrix defines a matrix transformation in the following way:   Notice that the input to is a two-dimensional vector and the output is a three-dimensional vector . As a shorthand, we will write to indicate that the inputs are two-dimensional vectors and the outputs are three-dimensional vectors.      Suppose we have a function that has the form We may write This shows that is a matrix transformation associated to the matrix       In this activity, we will look at some examples of matrix transformations.  To begin, suppose that is the matrix with associated matrix transformation .  What is ?  What is ?  What is ?  Is there a vector such that ?  Write as a two-dimensional vector.      Suppose that where .  What is the dimension of the vectors that are inputs for ?  What is the dimension of the vectors that are outputs?  If we describe this transformation as , what are the values of and and how do they relate to the shape of ?  Describe the vectors for which .   If is the matrix , what is in terms of the vectors and ? What about ?  Suppose that is a matrix and that . If , what is the matrix ?      If , then   .   .   .   .  We seek a vector such that . We can solve this equation to find the unique solution .    Now if the matrix has dimensions .   must be a four-dimensional vector.   must be a three-dimensional vector.  For this matrix, we have . In general, if is an matrix, .  If we solve the homogeneous equation , we find that .     , the first column of the matrix. Similarly, gives the second column of the matrix.  The matrix is      Let's discuss a few of the issues that appear in this activity. First, notice that the shape of the matrix and the dimension of the input vector must be compatible if the product is to be defined. In particular, if is an matrix, needs to be an -dimensional vector, and the resulting product will be an -dimensional vector. For the associated matrix transformation, we therefore write meaning takes vectors in as inputs and produces vectors in as outputs. For instance, if , then .  Second, we can often reconstruct the matrix if we only know some output values from its associated linear transformation by remembering that matrix-vector multiplication constructs linear combinations. For instance, if is an matrix , then . That is, we can find the first column of by evaluating . Similarly, the second column of is found by evaluating .  More generally, we will write the columns of the identity matrix as so that . This means that the column of is found by evaluating . We record this fact in the following proposition.    If is a matrix transformation given by , then the matrix has columns ; that is, .      Let's look at some examples and apply these observations.   To begin, suppose that is the matrix transformation that takes a two-dimensional vector as an input and outputs , the two-dimensional vector obtained by rotating counterclockwise by , as shown in .   The matrix transformation takes two-dimensional vectors on the left and rotates them by counterclockwise into the vectors on the right.       We will see in the next section that many geometric operations like this one can be performed by matrix transformations.   If we write , what are the values of and , and what is the shape of the associated matrix ?    Determine the matrix by applying .    If as shown on the left in , use your matrix to determine and verify that it agrees with that shown on the right of .    If , determine the vector obtained by rotating counterclockwise by .       Suppose that we work for a company that makes baked goods, including cakes, doughnuts, and eclairs. The company operates two bakeries, Bakery 1 and Bakery 2. In one hour of operation,  Bakery 1 produces 10 cakes, 50 doughnuts, and 30 eclairs.  Bakery 2 produces 20 cakes, 30 doughnuts, and 30 eclairs.  If Bakery 1 operates for hours and Bakery 2 for hours, we will use the vector to describe the operation of the two bakeries.  We would like to describe a matrix transformation where describes the number of hours the bakeries operate and describes the total number of cakes, doughnuts, and eclairs produced. That is, where is the number of cakes, is the number of doughnuts, and is the number of eclairs produced.   If , what are the values of and , and what is the shape of the associated matrix ?    We can determine the matrix using . For instance, will describe the number of cakes, doughnuts, and eclairs produced when Bakery 1 operates for one hour and Bakery 2 sits idle. What is this vector?    In the same way, determine . What is the matrix ?    If Bakery 1 operates for 120 hours and Bakery 2 for 180 hours, what is the total number of cakes, doughnuts, and eclairs produced?     Suppose that in one period of time, the company produces 5060 cakes, 14310 doughnuts, and 10470 eclairs. How long did each bakery operate?    Suppose that the company receives an order for a certain number of cakes, doughnuts, and eclairs. Can you guarantee that you can fill the order without having leftovers?                Since both the inputs and the outputs of are two-dimensional, it follows that and that is a matrix.    Since we have .    Multiplying , which agrees with the vector shown in the figure.     .          The shape of matrix is , and .     .     .     .    We solve the equation to obtain     No, you cannot guarantee this because the two columns of cannot span . If we view an order received as a three-dimensional vector , then a solution to the equation tells us how long to operate the two bakeries to produce this order. However, since is a matrix, it must have a row without a pivot position, which means that the equation will be inconsistent for some vectors .          In these examples, we glided over an important point: how do we know these functions can be expressed as matrix transformations? We will take up this question in detail in the next section and not worry about it for now.    Composing matrix transformations  It sometimes happens that we want to combine matrix transformations by performing one and then another. In the last activity, for instance, we considered the matrix transformation where is the result of rotating the two-dimensional vector by . Now suppose we are interested in rotating that vector twice; that is, we take a vector , rotate it by to obtain , and then rotate the result by again to obtain .  This process is called function composition and likely appeared in an earlier algebra course. For instance, if and , the composition of these functions obtained by first performing and then performing is denoted by   Composing matrix transformations is similar. Suppose that we have two matrix transformations, and . Their associated matrices will be denoted by and so that and . If we apply to a vector to obtain and then apply to the result, we have Notice that this implies that the composition is itself a matrix transformation and that the associated matrix is the product .   If and are matrix transformations with associated matrices and respectively, then the composition is also a matrix transformation whose associated matrix is the product .   Notice that the matrix transformations must be compatible if they are to be composed. In particular, the vector , an -dimensional vector, must be a suitable input vector for , which means that the inputs to must be -dimensional. In fact, this is the same condition we need to form the product of their associated matrices, namely, that the number of columns of is the same as the number of rows of .    We will explore the composition of matrix transformations by revisiting the matrix transformations from .   Let's begin with the matrix transformation that rotates a two-dimensional vector by to produce . We saw in the earlier activity that the associated matrix is . Suppose that we compose this matrix transformation with itself to obtain , which is the result of rotating by twice.   What is the matrix associated to the composition ?    What is the result of rotating twice?    Suppose that is the matrix transformation that rotates vectors by , as shown in .   The matrix transformation takes two-dimensional vectors on the left and rotates them by into the vectors on the right.       Use to find the matrix associated to and explain why it is the same matrix associated to .    Write the two-dimensional vector . How might this vector be expressed in terms of scalar multiplication and why does this make sense geometrically?       In the previous activity, we imagined a company that operates two bakeries. We found the matrix transformation where describes the number of cakes, doughnuts, and eclairs when Bakery1 runs for hours and Bakery 2 runs for hours. The associated matrix is .  Suppose now that  Each cake requires 4 cups of flour and and 2 cups of sugar.  Each doughnut requires 1 cup of flour and 1 cup of sugar.  Each eclair requires 1 cup of flour and 2 cups of sugar.  We will describe a matrix transformation where is a two-dimensional vector describing the number of cups of flour and sugar required to make cakes, doughnuts, and eclairs.   Use to write the matrix associated to the transformation .    If we make 1200 cakes, 2850 doughnuts, and 2250 eclairs, how many cups of flour and sugar are required?     Suppose that Bakery 1 operates for 75 hours and Bakery 2 operates for 53 hours. How many cakes, doughnuts, and eclairs are produced? How many cups of flour and sugar are required?    What is the meaning of the composition and what is its associated matrix?    In a certain time interval, both bakeries use a total of 5800 cups of flour and 5980 cups of sugar. How long have the two bakeries been operating?                The matrix is .     .    The matrix associated to is also since rotating by twice is the same as rotating once by .     , which makes sense because multiplying a vector by simply changes its direction.           .     .     and .     takes as input a vector that records the number of hours both bakeries operate and outputs a vector that tells us the total number of cups of flour and sugar used. The associated matrix is .    We want to find the vector for which . Solving this equation gives .            Discrete Dynamical Systems  In , we will give considerable attention to a specific type of matrix transformation, which is illustrated in the next activity.    Suppose we run a company that has two warehouses, which we will call and , and a fleet of 1000 delivery trucks. Every morning, a delivery truck goes out from one of the warehouses and returns in the evening to one of the warehouses. It is observed that  70% of the trucks that leave return to . The other 30% return to .  50% of the trucks that leave return to and 50% return to .   The distribution of trucks is represented by the vector when there are trucks at location and trucks at . If describes the distribution of trucks in the morning, then the matrix transformation will describe the distribution in the evening.   Suppose that all 1000 trucks begin the day at location and none at . How many trucks are at each location that evening? Using our vector representation, what is ?  So that we can find the matrix associated to , what does this tell us about ?  In the same way, suppose that all 1000 trucks begin the day at location and none at . How many trucks are at each location that evening? What is the result and what is ?  Find the matrix such that .  Suppose that there are 100 trucks at and 900 at in the morning. How many are there at the two locations in the evening?  Suppose that there are 550 trucks at and 450 at in the evening. How many trucks were there at the two locations that morning?  Suppose that all of the trucks are at location on Monday morning.  How many trucks are at each location Monday evening?  How many trucks are at each location Tuesday evening?  How many trucks are at each location Wednesday evening?   Suppose that is the matrix transformation that transforms the distribution of trucks one morning into the distribution of trucks in the morning one week (seven days) later. What is the matrix that defines the transformation ?      If 1000 trucks begin at , that evening we find that 70% of them are at with the remaining 30% at . Therefore, . Since , we see that .  In the same way, we see that so that .  The columns of are and so that .  Evaluate .  We solve to find .  We denote the distribution of trucks Monday morning by .  Monday evening, we have .  Tuesday evening, we have .  Wednesday evening, we have .    The matrix is .     As we will see later, this type of situation occurs frequently. We have a vector that describes the state of some system; in this case, describes the distribution of trucks between the two locations at a particular time. Then there is a matrix transformation that describes the state at some later time. We call the state vector and the transition function, as it describes the transition of the state vector from one time to the next. state vector  transition function   Beginning with an initial state , we would like to know how the state evolves over time. For instance, and so on.   discrete dynamical system We call this situation where the state of a system evolves from one time to the next according to the rule a discrete dynamical system . In , we will develop a theory that enables us to make long-term predictions about the evolution of the state vector.    Summary  This section introduced matrix transformations, functions that are defined by matrix-vector multiplication, such as for some matrix .  If is an matrix, then .  The columns of the matrix are given by evaluating the transformation on the vectors ; that is, .  The composition of matrix transformations corresponds to matrix multiplication.  A discrete dynamical system consists of a state vector along with a transition function that describes how the state vector evolves from one time to the next. Powers of the matrix determine the long-term behavior of the state vector.       Suppose that is the matrix transformation defined by the matrix and is the matrix transformation defined by where .  If , what are the values of and ? What values of and are appropriate for the transformation ?  Evaluate .  Evaluate .  Evaluate .  Find the matrix that defines the matrix transformation .      and .   .   .   .   .     Since is a matrix, we have . Since is a matrix, we have .   .   .   .   .     This problem concerns the identification of matrix transformations, about which more will be said in the next section.  Check that the following function is a matrix transformation by finding a matrix such that . .   Explain why is not a matrix transformation.       .  Because the first component has the term .     The matrix .  Because the first component has the term , there is no matrix such that .     Suppose that the matrix defines the matrix transformation .  Describe the vectors that satisfy .  Describe the vectors that satisfy .  Describe the vectors that satisfy .     .  There are no such vectors.   .     We have . The reduced row echelon form of is . Therefore, the solutions have the form .  We see that Since this is an inconsistent system, there are no vectors satisfying the equation.  In the same way, we have This shows that .     Suppose is a matrix transformation with where , , and are as shown in .      The vectors .     Sketch the vector .  What is the vector ?  Find all the vectors such that .      .   .   .      .   .  The matrix that defines is This shows that the solutions to are .     In and , we wrote matrix transformations in terms of the components of . This exercise makes use of that form.   Let's return to the example in concerning the company that operates two bakeries. We used a matrix transformation with input , which recorded the amount of time the two bakeries operated, and output , the number of cakes, doughnuts, and eclairs produced. The associated matrix is .   If , write the output as a three-dimensional vector in terms of and .    If Bakery 1 operates for hours and Bakery 2 for hours, how many cakes are produced?    Explain how you may have discovered this expression by considering the rates at which the two locations make cakes.       Suppose that a bicycle sharing program has two locations and . Bicycles are rented from some location in the morning and returned to a location in the evening. Suppose that   60% of bicycles that begin at in the morning are returned to in the evening while the other 40% are returned to .    30% of bicycles that begin at are returned to and the other 70% are returned to .      If is the number of bicycles at location and the number at in the morning, write an expression for the number of bicycles at in the evening.    Write an expression for the number of bicycles at in the evening.    Write an expression for , the vector that describs the distribution of bicycles in the evening.    Use this expression to identify the matrix associated to the matrix transformation .               We have .     .    Bakery 1 makes 10 cakes per hour and Bakery 2 makes 20.           .          .     .               We have     The number of cakes produced is .    This is consistent with the given information because Bakery 1 produces cakes at the rate of 10 cakes per hour so the number of cakes it produces is . In the same way, the number of cakes produced by Bakery 2 is giving a total of .          Of the bicycles that begin at , 60% of them end up at . Therefore is the number of bicycles that begin at and end at . Similarly, 70% of the bicycles that begin at end up at so this number is . Therefore, the total number of bicycles that end up at is     In the same way, the number of bicycles that end up at is     This gives the matrix     This expression leads to .          Determine whether the following statements are true or false and provide a justification for your response.  A matrix transformation is defined by where is a matrix.  If is a matrix transformation, then there are infinitely many vectors such that .  If is a matrix transformation, then it is possible that every equation has a solution for every vector .  If is a matrix transformation, then the equation always has a solution.     False  True  False  True     False. The dimensions of are .  True. The dimensions of are so there must be a column without a pivot position.  False. The dimensions of are so there must be a row without a pivot position.  True. The vector is always a solution.     Suppose that a company has three plants, called Plants 1, 2, and 3, that produce milk and yogurt . For every hour of operation,  Plant produces 20 units of milk and 15 units of yogurt.  Plant produces 30 units of milk and 5 units of yogurt.  Plant produces 0 units of milk and 40 units of yogurt.    Suppose that , , and record the amounts of time that the three plants are operated and that and record the amount of milk and yogurt produced. If we write and , find the matrix that defines the matrix transformation .  Furthermore, suppose that producing each unit of  milk requires 5 units of electricity and 8 units of labor.  yogurt requires 6 units of electricity and 10 units of labor.  If we write the vector to record the required amounts of electricity and labor , find the matrix that defines the matrix transformation .  If describes the amounts of time that the three plants are operated, how much milk and yogurt is produced? How much electricity and labor are required?  Find the matrix that describes the matrix transformation that gives the required amounts of electricity and labor when the each plants is operated an amount of time given by the vector .      .   .   and .   .     The first column of the matrix is . This shows us that the matrix is .  In the same way, the matrix is .   and .   .     Suppose that is a matrix transformation and that .  Find the vector .  Find the matrix that defines .  Find the vector .      .   .   .     We first find weights and such that by constructing the augmented matrix This shows that . Therefore,   In the same way, we find that This gives the matrix .  Then .     Suppose that two species and interact with one another and that we measure their populations every month. We record their populations in a state vector , where and are the populations of and , respectively. We observe that there is a matrix such that the matrix transformation is the transition function describing how the state vector evolves from month to month. We also observe that, at the beginning of July, the populations are described by the state vector .   What will the populations be at the beginning of August?  What were the populations at the beginning of June?  What will the populations be at the beginning of December?  What will the populations be at the beginning of July in the following year?       .   .   .   .    The initial state is the vector .  At the beginning of August, we have .  At the beginning of June, we have . Solving this equation gives .  At the beginning of December, we have .  At the beginning of July in the following year, we have .     Students in a school are sometimes absent due to an illness. Suppose that  95% of the students who attend school will attend school the next day.  50% of the students who are absent one day will be absent the next day.  We will record the number of present students and the number of absent students in a state vector and note that that state vector evolves from one day to the next according to the transition function . On Tuesday, the state vector is .   Suppose we initially have 1000 students who are present and none absent. Find .  Suppose we initially have 1000 students who are absent and none present. Find .  Use the results of parts a and b to find the matrix that defines the matrix transformation .  If on Tuesday, how are the students distributed on Wednesday?  How many students were present on Monday?  How many students are present on the following Tuesday?  What happens to the number of students who are present after a very long time?            .  There are 1665 students present and 135 absent.  1778  1636  It stabilizes around 1636 students present.      so that .   so that .  The matrix .   meaning that 1665 students are present and 135 are absent.  We want to solve the equation , which gives . There were 1778 students present on Monday.  The following Tuesday, the state will be described by the vector . This means that there were 1636 students present the following Tuesday.  If we compute for some large powers , we find that there are about 1636 students present every day after a very long time.     "
},
{
  "id": "sec-linear-trans-2-3",
  "level": "2",
  "url": "sec-linear-trans.html#sec-linear-trans-2-3",
  "type": "Preview Activity",
  "number": "18.5.1",
  "title": "",
  "body": "  We will begin by considering a more familiar situation; namely, the function , which takes a real number as an input and produces its square as its output.  What is the value of ?  Can we solve the equation ? If so, is the solution unique?  Can we solve the equation ? If so, is the solution unique?  Sketch a graph of the function in      Graph the function above.    We will now consider functions having the form . Draw a graph of the function on the left in .      Graphs of the function and .    Draw a graph of the function on the right of .  Remember that composing two functions means we use the output from one function as the input into the other; that is, . What function results from composing ?      We find .  If , then so there is not a unique solution.  There are no solutions to the equation .       The graph is shown on the left below.      The graph is shown on the right above.  Composing the functions, we find that . We see that the composition is a new linear function whose slope is obtained by multiplying the slopes of and .    "
},
{
  "id": "sec-linear-trans-3-4",
  "level": "2",
  "url": "sec-linear-trans.html#sec-linear-trans-3-4",
  "type": "Definition",
  "number": "18.5.3",
  "title": "",
  "body": "  matrix transformation  The matrix transformation associated to the matrix is the function that assigns to the vector the vector ; that is, .   "
},
{
  "id": "example-matrix-to-mt",
  "level": "2",
  "url": "sec-linear-trans.html#example-matrix-to-mt",
  "type": "Example",
  "number": "18.5.4",
  "title": "",
  "body": "  The matrix defines a matrix transformation in the following way:   Notice that the input to is a two-dimensional vector and the output is a three-dimensional vector . As a shorthand, we will write to indicate that the inputs are two-dimensional vectors and the outputs are three-dimensional vectors.   "
},
{
  "id": "example-mt-to-matrix",
  "level": "2",
  "url": "sec-linear-trans.html#example-mt-to-matrix",
  "type": "Example",
  "number": "18.5.5",
  "title": "",
  "body": "  Suppose we have a function that has the form We may write This shows that is a matrix transformation associated to the matrix    "
},
{
  "id": "sec-linear-trans-3-7",
  "level": "2",
  "url": "sec-linear-trans.html#sec-linear-trans-3-7",
  "type": "Activity",
  "number": "18.5.2",
  "title": "",
  "body": "  In this activity, we will look at some examples of matrix transformations.  To begin, suppose that is the matrix with associated matrix transformation .  What is ?  What is ?  What is ?  Is there a vector such that ?  Write as a two-dimensional vector.      Suppose that where .  What is the dimension of the vectors that are inputs for ?  What is the dimension of the vectors that are outputs?  If we describe this transformation as , what are the values of and and how do they relate to the shape of ?  Describe the vectors for which .   If is the matrix , what is in terms of the vectors and ? What about ?  Suppose that is a matrix and that . If , what is the matrix ?      If , then   .   .   .   .  We seek a vector such that . We can solve this equation to find the unique solution .    Now if the matrix has dimensions .   must be a four-dimensional vector.   must be a three-dimensional vector.  For this matrix, we have . In general, if is an matrix, .  If we solve the homogeneous equation , we find that .     , the first column of the matrix. Similarly, gives the second column of the matrix.  The matrix is     "
},
{
  "id": "prop-linear-trans-columns",
  "level": "2",
  "url": "sec-linear-trans.html#prop-linear-trans-columns",
  "type": "Proposition",
  "number": "18.5.6",
  "title": "",
  "body": "  If is a matrix transformation given by , then the matrix has columns ; that is, .   "
},
{
  "id": "activity-mt-intro",
  "level": "2",
  "url": "sec-linear-trans.html#activity-mt-intro",
  "type": "Activity",
  "number": "18.5.3",
  "title": "",
  "body": "  Let's look at some examples and apply these observations.   To begin, suppose that is the matrix transformation that takes a two-dimensional vector as an input and outputs , the two-dimensional vector obtained by rotating counterclockwise by , as shown in .   The matrix transformation takes two-dimensional vectors on the left and rotates them by counterclockwise into the vectors on the right.       We will see in the next section that many geometric operations like this one can be performed by matrix transformations.   If we write , what are the values of and , and what is the shape of the associated matrix ?    Determine the matrix by applying .    If as shown on the left in , use your matrix to determine and verify that it agrees with that shown on the right of .    If , determine the vector obtained by rotating counterclockwise by .       Suppose that we work for a company that makes baked goods, including cakes, doughnuts, and eclairs. The company operates two bakeries, Bakery 1 and Bakery 2. In one hour of operation,  Bakery 1 produces 10 cakes, 50 doughnuts, and 30 eclairs.  Bakery 2 produces 20 cakes, 30 doughnuts, and 30 eclairs.  If Bakery 1 operates for hours and Bakery 2 for hours, we will use the vector to describe the operation of the two bakeries.  We would like to describe a matrix transformation where describes the number of hours the bakeries operate and describes the total number of cakes, doughnuts, and eclairs produced. That is, where is the number of cakes, is the number of doughnuts, and is the number of eclairs produced.   If , what are the values of and , and what is the shape of the associated matrix ?    We can determine the matrix using . For instance, will describe the number of cakes, doughnuts, and eclairs produced when Bakery 1 operates for one hour and Bakery 2 sits idle. What is this vector?    In the same way, determine . What is the matrix ?    If Bakery 1 operates for 120 hours and Bakery 2 for 180 hours, what is the total number of cakes, doughnuts, and eclairs produced?     Suppose that in one period of time, the company produces 5060 cakes, 14310 doughnuts, and 10470 eclairs. How long did each bakery operate?    Suppose that the company receives an order for a certain number of cakes, doughnuts, and eclairs. Can you guarantee that you can fill the order without having leftovers?                Since both the inputs and the outputs of are two-dimensional, it follows that and that is a matrix.    Since we have .    Multiplying , which agrees with the vector shown in the figure.     .          The shape of matrix is , and .     .     .     .    We solve the equation to obtain     No, you cannot guarantee this because the two columns of cannot span . If we view an order received as a three-dimensional vector , then a solution to the equation tells us how long to operate the two bakeries to produce this order. However, since is a matrix, it must have a row without a pivot position, which means that the equation will be inconsistent for some vectors .         "
},
{
  "id": "sec-linear-trans-4-5",
  "level": "2",
  "url": "sec-linear-trans.html#sec-linear-trans-4-5",
  "type": "Proposition",
  "number": "18.5.8",
  "title": "",
  "body": " If and are matrix transformations with associated matrices and respectively, then the composition is also a matrix transformation whose associated matrix is the product .  "
},
{
  "id": "sec-linear-trans-4-7",
  "level": "2",
  "url": "sec-linear-trans.html#sec-linear-trans-4-7",
  "type": "Activity",
  "number": "18.5.4",
  "title": "",
  "body": "  We will explore the composition of matrix transformations by revisiting the matrix transformations from .   Let's begin with the matrix transformation that rotates a two-dimensional vector by to produce . We saw in the earlier activity that the associated matrix is . Suppose that we compose this matrix transformation with itself to obtain , which is the result of rotating by twice.   What is the matrix associated to the composition ?    What is the result of rotating twice?    Suppose that is the matrix transformation that rotates vectors by , as shown in .   The matrix transformation takes two-dimensional vectors on the left and rotates them by into the vectors on the right.       Use to find the matrix associated to and explain why it is the same matrix associated to .    Write the two-dimensional vector . How might this vector be expressed in terms of scalar multiplication and why does this make sense geometrically?       In the previous activity, we imagined a company that operates two bakeries. We found the matrix transformation where describes the number of cakes, doughnuts, and eclairs when Bakery1 runs for hours and Bakery 2 runs for hours. The associated matrix is .  Suppose now that  Each cake requires 4 cups of flour and and 2 cups of sugar.  Each doughnut requires 1 cup of flour and 1 cup of sugar.  Each eclair requires 1 cup of flour and 2 cups of sugar.  We will describe a matrix transformation where is a two-dimensional vector describing the number of cups of flour and sugar required to make cakes, doughnuts, and eclairs.   Use to write the matrix associated to the transformation .    If we make 1200 cakes, 2850 doughnuts, and 2250 eclairs, how many cups of flour and sugar are required?     Suppose that Bakery 1 operates for 75 hours and Bakery 2 operates for 53 hours. How many cakes, doughnuts, and eclairs are produced? How many cups of flour and sugar are required?    What is the meaning of the composition and what is its associated matrix?    In a certain time interval, both bakeries use a total of 5800 cups of flour and 5980 cups of sugar. How long have the two bakeries been operating?                The matrix is .     .    The matrix associated to is also since rotating by twice is the same as rotating once by .     , which makes sense because multiplying a vector by simply changes its direction.           .     .     and .     takes as input a vector that records the number of hours both bakeries operate and outputs a vector that tells us the total number of cups of flour and sugar used. The associated matrix is .    We want to find the vector for which . Solving this equation gives .         "
},
{
  "id": "subsec-dynamical-systems-3",
  "level": "2",
  "url": "sec-linear-trans.html#subsec-dynamical-systems-3",
  "type": "Activity",
  "number": "18.5.5",
  "title": "",
  "body": "  Suppose we run a company that has two warehouses, which we will call and , and a fleet of 1000 delivery trucks. Every morning, a delivery truck goes out from one of the warehouses and returns in the evening to one of the warehouses. It is observed that  70% of the trucks that leave return to . The other 30% return to .  50% of the trucks that leave return to and 50% return to .   The distribution of trucks is represented by the vector when there are trucks at location and trucks at . If describes the distribution of trucks in the morning, then the matrix transformation will describe the distribution in the evening.   Suppose that all 1000 trucks begin the day at location and none at . How many trucks are at each location that evening? Using our vector representation, what is ?  So that we can find the matrix associated to , what does this tell us about ?  In the same way, suppose that all 1000 trucks begin the day at location and none at . How many trucks are at each location that evening? What is the result and what is ?  Find the matrix such that .  Suppose that there are 100 trucks at and 900 at in the morning. How many are there at the two locations in the evening?  Suppose that there are 550 trucks at and 450 at in the evening. How many trucks were there at the two locations that morning?  Suppose that all of the trucks are at location on Monday morning.  How many trucks are at each location Monday evening?  How many trucks are at each location Tuesday evening?  How many trucks are at each location Wednesday evening?   Suppose that is the matrix transformation that transforms the distribution of trucks one morning into the distribution of trucks in the morning one week (seven days) later. What is the matrix that defines the transformation ?      If 1000 trucks begin at , that evening we find that 70% of them are at with the remaining 30% at . Therefore, . Since , we see that .  In the same way, we see that so that .  The columns of are and so that .  Evaluate .  We solve to find .  We denote the distribution of trucks Monday morning by .  Monday evening, we have .  Tuesday evening, we have .  Wednesday evening, we have .    The matrix is .    "
},
{
  "id": "sec-linear-trans-7-1",
  "level": "2",
  "url": "sec-linear-trans.html#sec-linear-trans-7-1",
  "type": "Exercise",
  "number": "18.5.5.1",
  "title": "",
  "body": " Suppose that is the matrix transformation defined by the matrix and is the matrix transformation defined by where .  If , what are the values of and ? What values of and are appropriate for the transformation ?  Evaluate .  Evaluate .  Evaluate .  Find the matrix that defines the matrix transformation .      and .   .   .   .   .     Since is a matrix, we have . Since is a matrix, we have .   .   .   .   .   "
},
{
  "id": "sec-linear-trans-7-2",
  "level": "2",
  "url": "sec-linear-trans.html#sec-linear-trans-7-2",
  "type": "Exercise",
  "number": "18.5.5.2",
  "title": "",
  "body": " This problem concerns the identification of matrix transformations, about which more will be said in the next section.  Check that the following function is a matrix transformation by finding a matrix such that . .   Explain why is not a matrix transformation.       .  Because the first component has the term .     The matrix .  Because the first component has the term , there is no matrix such that .   "
},
{
  "id": "sec-linear-trans-7-3",
  "level": "2",
  "url": "sec-linear-trans.html#sec-linear-trans-7-3",
  "type": "Exercise",
  "number": "18.5.5.3",
  "title": "",
  "body": " Suppose that the matrix defines the matrix transformation .  Describe the vectors that satisfy .  Describe the vectors that satisfy .  Describe the vectors that satisfy .     .  There are no such vectors.   .     We have . The reduced row echelon form of is . Therefore, the solutions have the form .  We see that Since this is an inconsistent system, there are no vectors satisfying the equation.  In the same way, we have This shows that .   "
},
{
  "id": "sec-linear-trans-7-4",
  "level": "2",
  "url": "sec-linear-trans.html#sec-linear-trans-7-4",
  "type": "Exercise",
  "number": "18.5.5.4",
  "title": "",
  "body": " Suppose is a matrix transformation with where , , and are as shown in .      The vectors .     Sketch the vector .  What is the vector ?  Find all the vectors such that .      .   .   .      .   .  The matrix that defines is This shows that the solutions to are .   "
},
{
  "id": "sec-linear-trans-7-5",
  "level": "2",
  "url": "sec-linear-trans.html#sec-linear-trans-7-5",
  "type": "Exercise",
  "number": "18.5.5.5",
  "title": "",
  "body": " In and , we wrote matrix transformations in terms of the components of . This exercise makes use of that form.   Let's return to the example in concerning the company that operates two bakeries. We used a matrix transformation with input , which recorded the amount of time the two bakeries operated, and output , the number of cakes, doughnuts, and eclairs produced. The associated matrix is .   If , write the output as a three-dimensional vector in terms of and .    If Bakery 1 operates for hours and Bakery 2 for hours, how many cakes are produced?    Explain how you may have discovered this expression by considering the rates at which the two locations make cakes.       Suppose that a bicycle sharing program has two locations and . Bicycles are rented from some location in the morning and returned to a location in the evening. Suppose that   60% of bicycles that begin at in the morning are returned to in the evening while the other 40% are returned to .    30% of bicycles that begin at are returned to and the other 70% are returned to .      If is the number of bicycles at location and the number at in the morning, write an expression for the number of bicycles at in the evening.    Write an expression for the number of bicycles at in the evening.    Write an expression for , the vector that describs the distribution of bicycles in the evening.    Use this expression to identify the matrix associated to the matrix transformation .               We have .     .    Bakery 1 makes 10 cakes per hour and Bakery 2 makes 20.           .          .     .               We have     The number of cakes produced is .    This is consistent with the given information because Bakery 1 produces cakes at the rate of 10 cakes per hour so the number of cakes it produces is . In the same way, the number of cakes produced by Bakery 2 is giving a total of .          Of the bicycles that begin at , 60% of them end up at . Therefore is the number of bicycles that begin at and end at . Similarly, 70% of the bicycles that begin at end up at so this number is . Therefore, the total number of bicycles that end up at is     In the same way, the number of bicycles that end up at is     This gives the matrix     This expression leads to .        "
},
{
  "id": "sec-linear-trans-7-6",
  "level": "2",
  "url": "sec-linear-trans.html#sec-linear-trans-7-6",
  "type": "Exercise",
  "number": "18.5.5.6",
  "title": "",
  "body": " Determine whether the following statements are true or false and provide a justification for your response.  A matrix transformation is defined by where is a matrix.  If is a matrix transformation, then there are infinitely many vectors such that .  If is a matrix transformation, then it is possible that every equation has a solution for every vector .  If is a matrix transformation, then the equation always has a solution.     False  True  False  True     False. The dimensions of are .  True. The dimensions of are so there must be a column without a pivot position.  False. The dimensions of are so there must be a row without a pivot position.  True. The vector is always a solution.   "
},
{
  "id": "sec-linear-trans-7-7",
  "level": "2",
  "url": "sec-linear-trans.html#sec-linear-trans-7-7",
  "type": "Exercise",
  "number": "18.5.5.7",
  "title": "",
  "body": " Suppose that a company has three plants, called Plants 1, 2, and 3, that produce milk and yogurt . For every hour of operation,  Plant produces 20 units of milk and 15 units of yogurt.  Plant produces 30 units of milk and 5 units of yogurt.  Plant produces 0 units of milk and 40 units of yogurt.    Suppose that , , and record the amounts of time that the three plants are operated and that and record the amount of milk and yogurt produced. If we write and , find the matrix that defines the matrix transformation .  Furthermore, suppose that producing each unit of  milk requires 5 units of electricity and 8 units of labor.  yogurt requires 6 units of electricity and 10 units of labor.  If we write the vector to record the required amounts of electricity and labor , find the matrix that defines the matrix transformation .  If describes the amounts of time that the three plants are operated, how much milk and yogurt is produced? How much electricity and labor are required?  Find the matrix that describes the matrix transformation that gives the required amounts of electricity and labor when the each plants is operated an amount of time given by the vector .      .   .   and .   .     The first column of the matrix is . This shows us that the matrix is .  In the same way, the matrix is .   and .   .   "
},
{
  "id": "sec-linear-trans-7-8",
  "level": "2",
  "url": "sec-linear-trans.html#sec-linear-trans-7-8",
  "type": "Exercise",
  "number": "18.5.5.8",
  "title": "",
  "body": " Suppose that is a matrix transformation and that .  Find the vector .  Find the matrix that defines .  Find the vector .      .   .   .     We first find weights and such that by constructing the augmented matrix This shows that . Therefore,   In the same way, we find that This gives the matrix .  Then .   "
},
{
  "id": "sec-linear-trans-7-9",
  "level": "2",
  "url": "sec-linear-trans.html#sec-linear-trans-7-9",
  "type": "Exercise",
  "number": "18.5.5.9",
  "title": "",
  "body": " Suppose that two species and interact with one another and that we measure their populations every month. We record their populations in a state vector , where and are the populations of and , respectively. We observe that there is a matrix such that the matrix transformation is the transition function describing how the state vector evolves from month to month. We also observe that, at the beginning of July, the populations are described by the state vector .   What will the populations be at the beginning of August?  What were the populations at the beginning of June?  What will the populations be at the beginning of December?  What will the populations be at the beginning of July in the following year?       .   .   .   .    The initial state is the vector .  At the beginning of August, we have .  At the beginning of June, we have . Solving this equation gives .  At the beginning of December, we have .  At the beginning of July in the following year, we have .   "
},
{
  "id": "sec-linear-trans-7-10",
  "level": "2",
  "url": "sec-linear-trans.html#sec-linear-trans-7-10",
  "type": "Exercise",
  "number": "18.5.5.10",
  "title": "",
  "body": " Students in a school are sometimes absent due to an illness. Suppose that  95% of the students who attend school will attend school the next day.  50% of the students who are absent one day will be absent the next day.  We will record the number of present students and the number of absent students in a state vector and note that that state vector evolves from one day to the next according to the transition function . On Tuesday, the state vector is .   Suppose we initially have 1000 students who are present and none absent. Find .  Suppose we initially have 1000 students who are absent and none present. Find .  Use the results of parts a and b to find the matrix that defines the matrix transformation .  If on Tuesday, how are the students distributed on Wednesday?  How many students were present on Monday?  How many students are present on the following Tuesday?  What happens to the number of students who are present after a very long time?            .  There are 1665 students present and 135 absent.  1778  1636  It stabilizes around 1636 students present.      so that .   so that .  The matrix .   meaning that 1665 students are present and 135 are absent.  We want to solve the equation , which gives . There were 1778 students present on Monday.  The following Tuesday, the state will be described by the vector . This means that there were 1636 students present the following Tuesday.  If we compute for some large powers , we find that there are about 1636 students present every day after a very long time.   "
},
{
  "id": "sec-transforms-geom",
  "level": "1",
  "url": "sec-transforms-geom.html",
  "type": "Section",
  "number": "18.6",
  "title": "The geometry of matrix transformations",
  "body": " The geometry of matrix transformations   Matrix transformations, which we explored in the last section, allow us to describe certain functions . In this section, we will demonstrate how matrix transformations provide a convenient way to describe geometric operations, such as rotations, reflections, and scalings. We will then explore how matrix transformations are used in computer animation.    We will describe the matrix transformation that reflects 2-dimensional vectors across the horizontal axis. For instance, illustrates how a vector is reflected onto the vector .      A vector and its reflection across the horizontal axis.    If , what is the vector ? Sketch the vectors and .  More generally, if , what is ?  Find the vectors and .  Use your results to write the matrix so that . Then verify that agrees with what you found in part b.  Describe the transformation that results from composing with itself; that is, what is the transformation ? Explain how matrix multiplication can be used to justify your response.       .   .   and .  We have the matrix . It follows that as expected.  If we reflect a vector twice in the horizontal axis, we obtain the original vector. The matrix for the transformation is simply .       The geometry of matrix transformations  We have now seen how a few geometric operations, such as rotations and reflections, can be described using matrix transformations. The following activity shows, more generally, that matrix transformations can perform a variety of important geometric operations.   Using matrix transformations to describe geometric operations    The matrix transformation transforms features shown on the left into features shown on the right.    For the following matrices , use the diagram to study the effect of the corresponding matrix transformation . For each transformation, describe the geometric effect the transformation has on the plane.   .   .   .   .   .   .   .   .       This transformation stretches by a factor of 2 in the horizontal direction.  This transformation stretches by a factor of 2 uniformly in all directions.  This is a clockwise rotation.  This transformation is called a shear ; it pushes vectors horizontally an amount equal to the vertical component.  This transformation reflects vectors in the vertical axis.  This transformation is called a projection ; it produces the shadow of the vector on the horizontal axis.  This transformation is called the identity ; it causes no change.  This transformation pushes vectors onto the line defined by the vector .     The previous activity presented some examples showing that matrix transformations can perform interesting geometric operations, such as rotations, scalings, and reflections. Before we go any further, we should explain why it is possible to represent these operations by matrix transformations. In fact, we ask more generally: what types of functions are represented as matrix transformations?  The linearity of matrix-vector multiplication provides the key to answering this question. Remember that if is a matrix, and vectors, and a scalar, then . This means that a matrix transformation satisfies the corresponding linearity property:  Linearity of Matrix Transformations       It turns out that, if satisfies these two linearity properties, then we can find a matrix such that . In fact, tells us how to form ; we simply write . We will now check that using the linearity of : .  The result is the following proposition.    The function is a matrix transformation where for some matrix if and only if . In this case, is the matrix whose columns are ; that is, .    Said simply, this proposition means says that if have a function and can verify the two linearity properties stated in the proposition, then we know that is a matrix transformation. Let's see how this works in practice.    We will consider the function that rotates a vector by in the counterclockwise direction to obtain as seen in .      The function rotates a vector counterclockwise by .   We first need to know that can be represented by a matrix transformation, which means, by , that we need to verify the linearity properties:   The next two figures illustrate why these properties hold. For instance, shows the relationship between and when is a scalar. In particular, scaling a vector and then rotating it is the same as rotating and then scaling it, which means that .      We see that the vector is a scalar multiple to so that .   Similarly, shows the relationship between , , and . Remember that the sum of two vectors is represented by the diagonal of the parallelogram defined by the two vectors. The rotation has the effect of rotating the parallelogram defined by and into the parallelogram defined by and , explaining why .      We see that the vector is the sum of and so that .   Having verified these two properties, we now know that the function that rotates vectors by is a matrix transformation. We may therefore write it as where is the matrix . The columns of this matrix, and , are shown on the right of .      The matrix transformation rotates and by .   Notice that forms an isosceles right triangle, as shown in . Since the length of is 1, the length of , the hypotenuse of the triangle, is also 1, and by Pythagoras' theorem, the lengths of its legs are .      The vector has length 1 and is the hypotenuse of a right isosceles triangle.   This leads to . In the same way, we find that so that the matrix is . You may wish to check this using the interactive diagram in the previous activity using the approximation .    In this example, we found that , a function describing a rotation in the plane, was in fact a matrix transformation by checking that The same kind of thinking applies more generally to show that rotations, reflections, and scalings are matrix transformations. Similarly, we could revisit the functions in and verify that they are matrix transformations.    In this activity, we seek to describe various matrix transformations by finding the matrix that gives the desired transformation. All of the transformations that we study here have the form .  Find the matrix of the transformation that has no effect on vectors; that is, .  Find the matrix of the transformation that reflects vectors in across the line .  What is the result of composing the reflection you found in the previous part with itself; that is, what is the effect of reflecting across the line and then reflecting across this line again? Provide a geometric explanation for your result as well as an algebraic one obtained by multiplying matrices.  Find the matrix that rotates vectors counterclockwise in the plane by .  Compare the result of rotating by and then reflecting in the line to the result of first reflecting in and then rotating .  Find the matrix that results from composing a rotation with itself four times; that is, if is the matrix transformation that rotates vectors by , find the matrix for . Explain why your result makes sense geometrically.  Explain why the matrix that rotates vectors counterclockwise by an angle is .     We use the fact that the columns of the requested matrices have the form .   .   .  The composition of this reflection with itself is described by multiplying the matrix by itself. This produces the matrix , which we just saw is the matrix for the identity transformation. This means that reflecting a vector in the line twice produces the original vector.   .  If we first rotate and then reflect, we obtain the matrix transformation defined by which is the matrix for reflecting in the horizontal axis.  If we first reflect and then rotate, we obtain the matrix which is the matrix for reflecting in the vertical axis.  Composing four times corresponds to raising the matrix to the fourth power, which gives us the identity matrix .  If we consider the effect of rotating the vector by an angle , we obtain the vector .       Matrix transformations and computer animation  Linear algebra plays a significant role in computer animation. We will now illustrate how matrix transformations and some of the ideas we have developed in this section are used by computer animators to create the illusion of motion in their characters.   shows a test character used by Pixar animators. On the left is the original definition of the character; on the right, we see that the character has been moved into a different pose. To make it appear that the character is moving, animators create a sequence of frames in which the character's pose is modified slightly from one frame to the next often using matrix transformations.       Computer animators define a character and create motion by drawing it in a sequence of poses. copyright Disney\/Pixar     Of course, realistic characters will be drawn in three-dimensions. To keep things a little more simple, however, we will look at this two-dimensional character and devise matrix transformations that move them into different poses.    Of course, the first thing we may wish to do is simply move them to a different position in the plane, such as that shown in . Motions like this are called translations .      Translating our character to a new position in the plane.   This presents a problem because a matrix transformation has the property that . This means that a matrix transformation cannot move the origin of the coordinate plane. To address this restriction, animators use homogeneous coordinates , which are formed by placing the two-dimensional coordinate plane inside as the plane , as shown in .      Include the two-dimensional coordinate plane in as the plane so that we can translate the character.   As a result, rather than describing points in the plane as vectors , we describe them as three-dimensional vectors . As we see in the next activity, this allows us to translate our character in the plane.    In this activity, we will use homogeneous coordinates and matrix transformations to move our character into a variety of poses.   Since we regard our character as living in , we will consider matrix transformations defined by matrices . Verify that such a matrix transformation transforms points in the plane into points in the same plane; that is, verify that . Express the coordinates of the resulting point and in terms of the coordinates of the original point and .   An interactive diagram that allows us to move the character using homogeneous coordinates.     Find the matrix transformation that translates our character to a new position in the plane, as shown in        Translating to a new position.    As originally drawn, our character is waving with one of their hands. In one of the movie's scenes, we would like them to wave with their other hand, as shown in . Find the matrix transformation that moves them into this pose.       Waving with the other hand.    Later, our character performs a cartwheel by moving through the sequence of poses shown in . Find the matrix transformations that create these poses.             Performing a cartwheel.    Next, we would like to find the transformations that zoom in on our character's face, as shown in . To do this, you should think about composing matrix transformations. This can be accomplished in the diagram by using the Compose button, which makes the current pose, displayed on the right, the new beginning pose, displayed on the left. What is the matrix transformation that moves the character from the original pose, shown in the upper left, to the final pose, shown in the lower right?             Zooming in on our characters' face.    We would also like to create our character's shadow, shown in the sequence of poses in . Find the sequence of matrix transformations that achieves this. In particular, find the matrix transformation that takes our character from their original pose to their shadow in the lower right.             Casting a shadow.    Write a final scene to the movie and describe how to construct a sequence of matrix transformations that create your scene.        which shows that   Notice that the entries and are responsible for the translation. Therefore, we need the matrix transformation defined by .  We would like to reflect in the vertical axis so we use the matrix .  The character is successively rotated by using the matrix .  We first translate the character down two units using the matrix . Then we zoom in by uniformly stretching by a factor of using the matrix . The net effect is the transformation described by the matrix   The shadow is first created using the shear . Then the vertical scale is compressed using the matrix .       Summary  This section explored how geometric operations are performed by matrix transformations.  A function is a matrix transformation if and only if these properties are satisfied:   Geometric operations, such as rotations, reflections, and scalings, can be represented as matrix transformations.  Composing geometric operations corresponds to matrix multiplication.  Computer animators use homogeneous coordinates and matrix transformations to create the illusion of motion.       For each of the following geometric operations in the plane, find a matrix that defines the matrix transformation performing the operation.  Rotates vectors by .  Reflects vectors across the vertical axis.  Reflects vectors across the line .  Rotates vectors counterclockwise by .  First rotates vectors counterclockwise by and then reflects in the line .      .   .   .   .   .    We create the following matrices as .   .   .   .   .   .     This exercise investigates the composition of reflections in the plane.  Find the result of first reflecting across the line and then . What familiar operation is the cumulative effect of this composition?  What happens if you compose the operations in the opposite order; that is, what happens if you first reflect across and then ? What familiar operation results?  What familiar geometric operation results if you first reflect across the line and then ?  What familiar geometric operation results if you first rotate by and then reflect across the line ?   It is a general fact that the composition of two reflections results in a rotation through twice the angle from the first line of reflection to the second. We will investigate this more generally in     This is the same as a counterclockwise rotation.  This is the same as a clockwise rotation.  This is the same as a rotation.  This is the same as a reflection in the horizontal axis.      . This is the same as a counterclockwise rotation.   . This is the same as a clockwise rotation.   . This is the same as a rotation.   . This is the same as a reflection in the horizontal axis.     Shown below in are the vectors , , and in .      The vectors , , and in .     Imagine that the thumb of your right hand points in the direction of . A positive rotation about the axis corresponds to a rotation in the direction in which your fingers point. Find the matrix definining the matrix transformation that rotates vectors by around the -axis.  In the same way, find the matrix that rotates vectors by around the -axis.  Find the matrix that rotates vectors by around the -axis.  What is the cumulative effect of rotating by about the -axis, followed by a rotation about the -axis, followed by a rotation about the -axis.         .   .   .   .    We construct the matrices as .   .   .   .   , which is a rotation about the -axis.     If a matrix transformation performs a geometric operation, we would like to find a matrix transformation that undoes that operation.  Suppose that is the matrix transformation that rotates vectors by . Find a matrix transformation that undoes the rotation; that is, takes back into so that . Think geometrically about what the transformation should be and then verify it algebraically.  We say that is the inverse of and we will write it as .  Verify algebraically that the reflection across the line is its own inverse; that is, .  The matrix transformation defined by the matrix is called a shear . Find the inverse of .  Describe the geometric effect of the matrix transformation defined by and then find its inverse.       .  The square of the matrix is the identity.   .   .     The transformation should rotate vectors by . The matrix is therefore .  To check , we compute .  The matrix defining the reflection is . Multiplying this by itself gives the identity.  We should apply a shear in the opposite direction: .  We will undo the stretch in the horizontal and vertical directions with the matrix .     We have seen that the matrix performs a rotation through an angle about the origin. Suppose instead that we would like to rotate by about the point . Using homogeneous coordinates, we will develop a matrix that performs this operation.  Our strategy is to  begin with a vector whose tail is at the point ,  translate the vector so that its tail is at the origin,  rotate by , and  translate the vector so that its tail is back at .  This is shown in .      A sequence of matrix transformations that, when read right to left and top to bottom, rotate a vector about the point .   Remember that, when working with homogeneous coordinates, we consider matrices of the form .  The first operation is a translation by . Find the matrix that performs this translation.  The second operation is a rotation about the origin. Find the matrix that performs this rotation.  The third operation is a translation by . Find the matrix that performs this translation.  Use these matrices to find the matrix that performs a rotation about .  Use your matrix to determine where the point ends up if rotated by about the .      .   .   .     The point is rotated to .      .   .   .     If we call this matrix , we compute . Therefore, the point is rotated to .     Consider the matrix transformation that assigns to a vector the closest vector on horizontal axis as illustrated in . This transformation is called the projection onto the horizontal axis. You may imagine as the shadow cast by from a flashlight far up on the positive -axis.      Projection onto the -axis.    Find the matrix that defines this matrix transformation .  Find the matrix that defines projection on the vertical axis.  What is the result of composing the projection onto the horizontal axis with the projection onto the vertical axis?  Find the matrix that defines projection onto the line .       .   .   .   .      .   .   . The result of composing the two projections sends every vector to .   .     This exercise concerns the matrix transformations defined by matrices of the form . Let's begin by looking at two special types of these matrices.  First, consider the matrix where and so that . Describe the geometric effect of this matrix. More generally, suppose we have , where is a positive number. What is the geometric effect of on vectors in the plane?  Suppose now that and so that . What is the geometric effect of on vectors in the plane? More generally, suppose we have . What is the geometric effect of on vectors in the plane?  In general, the composition of matrix transformation depends on the order in which we compose them. For these transformations, however, it is not the case. Check this by verifying that .  Let's now look at the general case where . We will draw the vector in the plane and express it using polar coordinates and as shown in .      A vector may be expressed in polar coordinates.   We then have . Show that the matrix .  Using this description, describe the geometric effect on vectors in the plane of the matrix transformation defined by .  Suppose we have a matrix transformation defined by a matrix and another transformation defined by where . Describe the geometric effect of the composition in terms of the , , , and .  The matrices of this form give a model for the complex numbers and will play an important role in .    It stretches vectors by .  This matrix rotates vectors by an angle .  No matter which order we multiply the matrices, we find that their product is   This follows from the previous part.  If we use polar coordinates to write , multiplying a vector by will stretch the vector by and rotate it by .  If then has the effect of stretching a vector by and rotating it by .     The matrix stretches vectors by . It has the same effect as scalar multiplication by .  This matrix rotates vectors by an angle .  No matter which order we multiply the matrices, we find that their product is   This follows from the previous part.  If we use polar coordinates to write , multiplying a vector by will stretch the vector by and rotate it by .  If then has the effect of stretching a vector by and rotating it by .     We saw earlier that the rotation in the plane through an angle is given by the matrix: . We would like to find a similar expression for the matrix that represents the reflection across , the line passing through the origin and making an angle of with the positive -axis, as shown in .      The reflection across the line .    To do this, notice that this reflection can be obtained by composing three separate transformations as shown in . Beginning with the vector , we apply the transformation to rotate by and obtain . Next, we apply , a reflection in the horizontal axis, followed by , a rotation by . We see that is the same as the reflection of in the original line .      Reflection in the line as a composition of three transformations.   Using this decomposition, show that the reflection in the line is described by the matrix . You will need to remember the trigonometric identities: .   Now that we have a matrix that describes the reflection in the line , show that the composition of the reflection in the horizontal axis followed by the reflection in is a counterclockwise rotation by an angle . We saw some examples of this earlier in .     We have   Compute that      We have   Compute that      "
},
{
  "id": "sec-transforms-geom-2-2",
  "level": "2",
  "url": "sec-transforms-geom.html#sec-transforms-geom-2-2",
  "type": "Preview Activity",
  "number": "18.6.1",
  "title": "",
  "body": "  We will describe the matrix transformation that reflects 2-dimensional vectors across the horizontal axis. For instance, illustrates how a vector is reflected onto the vector .      A vector and its reflection across the horizontal axis.    If , what is the vector ? Sketch the vectors and .  More generally, if , what is ?  Find the vectors and .  Use your results to write the matrix so that . Then verify that agrees with what you found in part b.  Describe the transformation that results from composing with itself; that is, what is the transformation ? Explain how matrix multiplication can be used to justify your response.       .   .   and .  We have the matrix . It follows that as expected.  If we reflect a vector twice in the horizontal axis, we obtain the original vector. The matrix for the transformation is simply .    "
},
{
  "id": "activity-linear-trans-geom",
  "level": "2",
  "url": "sec-transforms-geom.html#activity-linear-trans-geom",
  "type": "Activity",
  "number": "18.6.2",
  "title": "Using matrix transformations to describe geometric operations.",
  "body": " Using matrix transformations to describe geometric operations    The matrix transformation transforms features shown on the left into features shown on the right.    For the following matrices , use the diagram to study the effect of the corresponding matrix transformation . For each transformation, describe the geometric effect the transformation has on the plane.   .   .   .   .   .   .   .   .       This transformation stretches by a factor of 2 in the horizontal direction.  This transformation stretches by a factor of 2 uniformly in all directions.  This is a clockwise rotation.  This transformation is called a shear ; it pushes vectors horizontally an amount equal to the vertical component.  This transformation reflects vectors in the vertical axis.  This transformation is called a projection ; it produces the shadow of the vector on the horizontal axis.  This transformation is called the identity ; it causes no change.  This transformation pushes vectors onto the line defined by the vector .    "
},
{
  "id": "prop-linear-trans-characterization",
  "level": "2",
  "url": "sec-transforms-geom.html#prop-linear-trans-characterization",
  "type": "Proposition",
  "number": "18.6.3",
  "title": "",
  "body": "  The function is a matrix transformation where for some matrix if and only if . In this case, is the matrix whose columns are ; that is, .   "
},
{
  "id": "sec-transforms-geom-3-10",
  "level": "2",
  "url": "sec-transforms-geom.html#sec-transforms-geom-3-10",
  "type": "Example",
  "number": "18.6.4",
  "title": "",
  "body": "  We will consider the function that rotates a vector by in the counterclockwise direction to obtain as seen in .      The function rotates a vector counterclockwise by .   We first need to know that can be represented by a matrix transformation, which means, by , that we need to verify the linearity properties:   The next two figures illustrate why these properties hold. For instance, shows the relationship between and when is a scalar. In particular, scaling a vector and then rotating it is the same as rotating and then scaling it, which means that .      We see that the vector is a scalar multiple to so that .   Similarly, shows the relationship between , , and . Remember that the sum of two vectors is represented by the diagonal of the parallelogram defined by the two vectors. The rotation has the effect of rotating the parallelogram defined by and into the parallelogram defined by and , explaining why .      We see that the vector is the sum of and so that .   Having verified these two properties, we now know that the function that rotates vectors by is a matrix transformation. We may therefore write it as where is the matrix . The columns of this matrix, and , are shown on the right of .      The matrix transformation rotates and by .   Notice that forms an isosceles right triangle, as shown in . Since the length of is 1, the length of , the hypotenuse of the triangle, is also 1, and by Pythagoras' theorem, the lengths of its legs are .      The vector has length 1 and is the hypotenuse of a right isosceles triangle.   This leads to . In the same way, we find that so that the matrix is . You may wish to check this using the interactive diagram in the previous activity using the approximation .   "
},
{
  "id": "sec-transforms-geom-3-12",
  "level": "2",
  "url": "sec-transforms-geom.html#sec-transforms-geom-3-12",
  "type": "Activity",
  "number": "18.6.3",
  "title": "",
  "body": "  In this activity, we seek to describe various matrix transformations by finding the matrix that gives the desired transformation. All of the transformations that we study here have the form .  Find the matrix of the transformation that has no effect on vectors; that is, .  Find the matrix of the transformation that reflects vectors in across the line .  What is the result of composing the reflection you found in the previous part with itself; that is, what is the effect of reflecting across the line and then reflecting across this line again? Provide a geometric explanation for your result as well as an algebraic one obtained by multiplying matrices.  Find the matrix that rotates vectors counterclockwise in the plane by .  Compare the result of rotating by and then reflecting in the line to the result of first reflecting in and then rotating .  Find the matrix that results from composing a rotation with itself four times; that is, if is the matrix transformation that rotates vectors by , find the matrix for . Explain why your result makes sense geometrically.  Explain why the matrix that rotates vectors counterclockwise by an angle is .     We use the fact that the columns of the requested matrices have the form .   .   .  The composition of this reflection with itself is described by multiplying the matrix by itself. This produces the matrix , which we just saw is the matrix for the identity transformation. This means that reflecting a vector in the line twice produces the original vector.   .  If we first rotate and then reflect, we obtain the matrix transformation defined by which is the matrix for reflecting in the horizontal axis.  If we first reflect and then rotate, we obtain the matrix which is the matrix for reflecting in the vertical axis.  Composing four times corresponds to raising the matrix to the fourth power, which gives us the identity matrix .  If we consider the effect of rotating the vector by an angle , we obtain the vector .    "
},
{
  "id": "fig-blob-man",
  "level": "2",
  "url": "sec-transforms-geom.html#fig-blob-man",
  "type": "Figure",
  "number": "18.6.10",
  "title": "",
  "body": "     Computer animators define a character and create motion by drawing it in a sequence of poses. copyright Disney\/Pixar  "
},
{
  "id": "fig-animate-translate",
  "level": "2",
  "url": "sec-transforms-geom.html#fig-animate-translate",
  "type": "Figure",
  "number": "18.6.11",
  "title": "",
  "body": "    Translating our character to a new position in the plane.  "
},
{
  "id": "fig-animate-homogeneous",
  "level": "2",
  "url": "sec-transforms-geom.html#fig-animate-homogeneous",
  "type": "Figure",
  "number": "18.6.12",
  "title": "",
  "body": "    Include the two-dimensional coordinate plane in as the plane so that we can translate the character.  "
},
{
  "id": "sec-transforms-geom-4-11",
  "level": "2",
  "url": "sec-transforms-geom.html#sec-transforms-geom-4-11",
  "type": "Activity",
  "number": "18.6.4",
  "title": "",
  "body": "  In this activity, we will use homogeneous coordinates and matrix transformations to move our character into a variety of poses.   Since we regard our character as living in , we will consider matrix transformations defined by matrices . Verify that such a matrix transformation transforms points in the plane into points in the same plane; that is, verify that . Express the coordinates of the resulting point and in terms of the coordinates of the original point and .   An interactive diagram that allows us to move the character using homogeneous coordinates.     Find the matrix transformation that translates our character to a new position in the plane, as shown in        Translating to a new position.    As originally drawn, our character is waving with one of their hands. In one of the movie's scenes, we would like them to wave with their other hand, as shown in . Find the matrix transformation that moves them into this pose.       Waving with the other hand.    Later, our character performs a cartwheel by moving through the sequence of poses shown in . Find the matrix transformations that create these poses.             Performing a cartwheel.    Next, we would like to find the transformations that zoom in on our character's face, as shown in . To do this, you should think about composing matrix transformations. This can be accomplished in the diagram by using the Compose button, which makes the current pose, displayed on the right, the new beginning pose, displayed on the left. What is the matrix transformation that moves the character from the original pose, shown in the upper left, to the final pose, shown in the lower right?             Zooming in on our characters' face.    We would also like to create our character's shadow, shown in the sequence of poses in . Find the sequence of matrix transformations that achieves this. In particular, find the matrix transformation that takes our character from their original pose to their shadow in the lower right.             Casting a shadow.    Write a final scene to the movie and describe how to construct a sequence of matrix transformations that create your scene.        which shows that   Notice that the entries and are responsible for the translation. Therefore, we need the matrix transformation defined by .  We would like to reflect in the vertical axis so we use the matrix .  The character is successively rotated by using the matrix .  We first translate the character down two units using the matrix . Then we zoom in by uniformly stretching by a factor of using the matrix . The net effect is the transformation described by the matrix   The shadow is first created using the shear . Then the vertical scale is compressed using the matrix .    "
},
{
  "id": "sec-transforms-geom-6-1",
  "level": "2",
  "url": "sec-transforms-geom.html#sec-transforms-geom-6-1",
  "type": "Exercise",
  "number": "18.6.4.1",
  "title": "",
  "body": " For each of the following geometric operations in the plane, find a matrix that defines the matrix transformation performing the operation.  Rotates vectors by .  Reflects vectors across the vertical axis.  Reflects vectors across the line .  Rotates vectors counterclockwise by .  First rotates vectors counterclockwise by and then reflects in the line .      .   .   .   .   .    We create the following matrices as .   .   .   .   .   .   "
},
{
  "id": "ex-compose-reflections",
  "level": "2",
  "url": "sec-transforms-geom.html#ex-compose-reflections",
  "type": "Exercise",
  "number": "18.6.4.2",
  "title": "",
  "body": " This exercise investigates the composition of reflections in the plane.  Find the result of first reflecting across the line and then . What familiar operation is the cumulative effect of this composition?  What happens if you compose the operations in the opposite order; that is, what happens if you first reflect across and then ? What familiar operation results?  What familiar geometric operation results if you first reflect across the line and then ?  What familiar geometric operation results if you first rotate by and then reflect across the line ?   It is a general fact that the composition of two reflections results in a rotation through twice the angle from the first line of reflection to the second. We will investigate this more generally in     This is the same as a counterclockwise rotation.  This is the same as a clockwise rotation.  This is the same as a rotation.  This is the same as a reflection in the horizontal axis.      . This is the same as a counterclockwise rotation.   . This is the same as a clockwise rotation.   . This is the same as a rotation.   . This is the same as a reflection in the horizontal axis.   "
},
{
  "id": "sec-transforms-geom-6-3",
  "level": "2",
  "url": "sec-transforms-geom.html#sec-transforms-geom-6-3",
  "type": "Exercise",
  "number": "18.6.4.3",
  "title": "",
  "body": " Shown below in are the vectors , , and in .      The vectors , , and in .     Imagine that the thumb of your right hand points in the direction of . A positive rotation about the axis corresponds to a rotation in the direction in which your fingers point. Find the matrix definining the matrix transformation that rotates vectors by around the -axis.  In the same way, find the matrix that rotates vectors by around the -axis.  Find the matrix that rotates vectors by around the -axis.  What is the cumulative effect of rotating by about the -axis, followed by a rotation about the -axis, followed by a rotation about the -axis.         .   .   .   .    We construct the matrices as .   .   .   .   , which is a rotation about the -axis.   "
},
{
  "id": "sec-transforms-geom-6-4",
  "level": "2",
  "url": "sec-transforms-geom.html#sec-transforms-geom-6-4",
  "type": "Exercise",
  "number": "18.6.4.4",
  "title": "",
  "body": " If a matrix transformation performs a geometric operation, we would like to find a matrix transformation that undoes that operation.  Suppose that is the matrix transformation that rotates vectors by . Find a matrix transformation that undoes the rotation; that is, takes back into so that . Think geometrically about what the transformation should be and then verify it algebraically.  We say that is the inverse of and we will write it as .  Verify algebraically that the reflection across the line is its own inverse; that is, .  The matrix transformation defined by the matrix is called a shear . Find the inverse of .  Describe the geometric effect of the matrix transformation defined by and then find its inverse.       .  The square of the matrix is the identity.   .   .     The transformation should rotate vectors by . The matrix is therefore .  To check , we compute .  The matrix defining the reflection is . Multiplying this by itself gives the identity.  We should apply a shear in the opposite direction: .  We will undo the stretch in the horizontal and vertical directions with the matrix .   "
},
{
  "id": "sec-transforms-geom-6-5",
  "level": "2",
  "url": "sec-transforms-geom.html#sec-transforms-geom-6-5",
  "type": "Exercise",
  "number": "18.6.4.5",
  "title": "",
  "body": " We have seen that the matrix performs a rotation through an angle about the origin. Suppose instead that we would like to rotate by about the point . Using homogeneous coordinates, we will develop a matrix that performs this operation.  Our strategy is to  begin with a vector whose tail is at the point ,  translate the vector so that its tail is at the origin,  rotate by , and  translate the vector so that its tail is back at .  This is shown in .      A sequence of matrix transformations that, when read right to left and top to bottom, rotate a vector about the point .   Remember that, when working with homogeneous coordinates, we consider matrices of the form .  The first operation is a translation by . Find the matrix that performs this translation.  The second operation is a rotation about the origin. Find the matrix that performs this rotation.  The third operation is a translation by . Find the matrix that performs this translation.  Use these matrices to find the matrix that performs a rotation about .  Use your matrix to determine where the point ends up if rotated by about the .      .   .   .     The point is rotated to .      .   .   .     If we call this matrix , we compute . Therefore, the point is rotated to .   "
},
{
  "id": "sec-transforms-geom-6-6",
  "level": "2",
  "url": "sec-transforms-geom.html#sec-transforms-geom-6-6",
  "type": "Exercise",
  "number": "18.6.4.6",
  "title": "",
  "body": " Consider the matrix transformation that assigns to a vector the closest vector on horizontal axis as illustrated in . This transformation is called the projection onto the horizontal axis. You may imagine as the shadow cast by from a flashlight far up on the positive -axis.      Projection onto the -axis.    Find the matrix that defines this matrix transformation .  Find the matrix that defines projection on the vertical axis.  What is the result of composing the projection onto the horizontal axis with the projection onto the vertical axis?  Find the matrix that defines projection onto the line .       .   .   .   .      .   .   . The result of composing the two projections sends every vector to .   .   "
},
{
  "id": "sec-transforms-geom-6-7",
  "level": "2",
  "url": "sec-transforms-geom.html#sec-transforms-geom-6-7",
  "type": "Exercise",
  "number": "18.6.4.7",
  "title": "",
  "body": " This exercise concerns the matrix transformations defined by matrices of the form . Let's begin by looking at two special types of these matrices.  First, consider the matrix where and so that . Describe the geometric effect of this matrix. More generally, suppose we have , where is a positive number. What is the geometric effect of on vectors in the plane?  Suppose now that and so that . What is the geometric effect of on vectors in the plane? More generally, suppose we have . What is the geometric effect of on vectors in the plane?  In general, the composition of matrix transformation depends on the order in which we compose them. For these transformations, however, it is not the case. Check this by verifying that .  Let's now look at the general case where . We will draw the vector in the plane and express it using polar coordinates and as shown in .      A vector may be expressed in polar coordinates.   We then have . Show that the matrix .  Using this description, describe the geometric effect on vectors in the plane of the matrix transformation defined by .  Suppose we have a matrix transformation defined by a matrix and another transformation defined by where . Describe the geometric effect of the composition in terms of the , , , and .  The matrices of this form give a model for the complex numbers and will play an important role in .    It stretches vectors by .  This matrix rotates vectors by an angle .  No matter which order we multiply the matrices, we find that their product is   This follows from the previous part.  If we use polar coordinates to write , multiplying a vector by will stretch the vector by and rotate it by .  If then has the effect of stretching a vector by and rotating it by .     The matrix stretches vectors by . It has the same effect as scalar multiplication by .  This matrix rotates vectors by an angle .  No matter which order we multiply the matrices, we find that their product is   This follows from the previous part.  If we use polar coordinates to write , multiplying a vector by will stretch the vector by and rotate it by .  If then has the effect of stretching a vector by and rotating it by .   "
},
{
  "id": "ex-reflection-compose-general",
  "level": "2",
  "url": "sec-transforms-geom.html#ex-reflection-compose-general",
  "type": "Exercise",
  "number": "18.6.4.8",
  "title": "",
  "body": " We saw earlier that the rotation in the plane through an angle is given by the matrix: . We would like to find a similar expression for the matrix that represents the reflection across , the line passing through the origin and making an angle of with the positive -axis, as shown in .      The reflection across the line .    To do this, notice that this reflection can be obtained by composing three separate transformations as shown in . Beginning with the vector , we apply the transformation to rotate by and obtain . Next, we apply , a reflection in the horizontal axis, followed by , a rotation by . We see that is the same as the reflection of in the original line .      Reflection in the line as a composition of three transformations.   Using this decomposition, show that the reflection in the line is described by the matrix . You will need to remember the trigonometric identities: .   Now that we have a matrix that describes the reflection in the line , show that the composition of the reflection in the horizontal axis followed by the reflection in is a counterclockwise rotation by an angle . We saw some examples of this earlier in .     We have   Compute that      We have   Compute that    "
},
{
  "id": "sec-matrix-inverse",
  "level": "1",
  "url": "sec-matrix-inverse.html",
  "type": "Section",
  "number": "19.1",
  "title": "Invertibility",
  "body": " Invertibility   Up to this point, we have used the Gaussian elimination algorithm to find solutions to linear systems. We now investigate another way to find solutions to the equation when the matrix has the same number of rows and columns. To get started, let's look at some familiar examples.      Explain how you would solve the equation using multiplication rather than division.  Find the matrix that rotates vectors counterclockwise by .  Find the matrix that rotates vectors clockwise by .  What do you expect the product to be? Explain the reasoning behind your expectation and then compute to verify it.  Solve the equation using Gaussian elimination.   Explain why your solution may also be found by computing .       Dividing by is the same as multiplying by , the multiplicative inverse of . We have   As we have seen a few times, the matrix is   Here, the matrix is   We should expect that since the effect of rotating by clockwise followed by rotating counterclockwise is to leave a vector unchanged. We can verify this by performing the matrix multiplication.  We have so the solution is .  The equation is asking us to find the vector that becomes after being rotated by . If we rotate by in the opposite direction, it will have this property. That is, if , then        Invertible matrices  The preview activity began with a familiar type of equation, , and asked for a strategy to solve it. One possible response is to divide both sides by 3. Instead, let's rephrase this as multiplying by , the multiplicative inverse of 3.  Now that we are interested in solving equations of the form , we might try to find a similar approach. Is there a matrix that plays the role of the multiplicative inverse of ? Of course, the real number does not have a multiplicative inverse so we probably shouldn't expect every matrix to have a multiplicative inverse. We will see, however, that many do.    invertible  matrix, inverse  An matrix is called invertible if there is a matrix such that , where is the identity matrix. The matrix is called the inverse of and denoted .     matrix, square Notice that we only define invertibility for matrices that have the same number of rows and columns in which case we say that the matrix is square .    Suppose that is the matrix that rotates two-dimensional vectors counterclockwise by and that rotates vectors by . We have We can check that which shows that is invertible and that .  Notice that if we multiply the matrices in the opposite order, we find that , which says that is also invertible and that . In other words, and are inverses of each other.      This activity demonstrates a procedure for finding the inverse of a matrix .   Suppose that . To find an inverse , we write its columns as and require that In other words, we can find the columns of by solving the equations Solve these equations to find and . Then write the matrix and verify that . This is enough for us to conclude that is the inverse of .     Find the product and explain why we now know that is invertible and .     What happens when you try to find the inverse of ?    We now develop a condition that must be satisfied by an invertible matrix. Suppose that is an invertible matrix with inverse and suppose that is any -dimensional vector. Since , we have This says that the equation is consistent and that is a solution.  Since we know that is consistent for any vector , what does this say about the span of the columns of ?    Since is a square matrix, what does this say about the pivot positions of ? What is the reduced row echelon form of ?    In this activity, we have studied the matrices Find the reduced row echelon form of each and explain how those forms enable us to conclude that one matrix is invertible and the other is not.        Solving the two equations for and gives . We can verify that, as we expect, .  We find that , which is the condition that tells us that is invertible.  Seeking the first column of , we see that the equation is not consistent. This means that is not invertible.  Since the equation is consistent for every , we know that the span of the columns of is .  Because the span of the columns of is , there is a pivot position in every row. Since is square, there is also a pivot position in every column. This means that the reduced row echelon form of must be the identity matrix .  We see that which shows that is invertible and is not.       We can reformulate this procedure for finding the inverse of a matrix. For the sake of convenience, suppose that is a invertible matrix with inverse . Rather than solving the equations separately, we can solve them at the same time by augmenting by both vectors and and finding the reduced row echelon form.  For example, if , we form This shows that the matrix is the inverse of .  In other words, beginning with , we augment by the identify and find the reduced row echelon form to determine :     In fact, this reformulation will always work. Suppose that is an invertible matrix with inverse . Suppose furthermore that is any -dimensional vector and consider the equation . We know that is a solution because     If is an invertible matrix with inverse , then any equation is consistent and is a solution. In other words, the solution to is .    Notice that this is similar to saying that the solution to is , as we saw in the preview activity.  Now since is consistent for every vector , the columns of must span so there is a pivot position in every row. Since is also square, this means that the reduced row echelon form of is the identity matrix.    The matrix is invertible if and only if the reduced row echelon form of is the identity matrix: . In addition, we can find the inverse by augmenting by the identity and finding the reduced row echelon form:     You may have noticed that says that the solution to the equation is . Indeed, we know that this equation has a unique solution because has a pivot position in every column.  It is important to remember that the product of two matrices depends on the order in which they are multiplied. That is, if and are matrices, then it sometimes happens that . However, something fortunate happens when we consider invertibility. It turns out that if is an matrix and that , then it is also true that . We have verified this in a few examples so far, and explains why it always happens. This leads to the following proposition.    If is a invertible matrix with inverse , then , which tells us that is invertible with inverse . In other words,       Solving equations with an inverse  If is an invertible matrix, then shows us how to use to solve equations involving . In particular, the solution to is .    We'll begin by considering the square matrix    Describe the solution space to the equation by augmenting and finding the reduced row echelon form.     Using , explain why is invertible and find its inverse.    Now use the inverse to solve the equation and verify that your result agrees with what you found in part a.    If you have defined a matrix B in Sage, you can find it's inverse as B.inverse() or B^-1 . Use Sage to find the inverse of the matrix and use it to solve the equation .     If and are the two matrices defined in this activity, find their product and verify that it is invertible.    Compute the products and . Which one agrees with ?    Explain your finding by considering the product and using associativity to regroup the products so that the middle two terms are multiplied first.        Constructing the augmented matrix, we see that which says that there is a unique solution .    Our work in part a shows that from which we conclude that is invertible. To find the inverse, which says that     We see that .    Sage tells us that .    Sage helps us see that , which tells us that is invertible.   We find that .   We see that       The next proposition summarizes much of what we have found about invertible matrices.   Properties of invertible matrices     An matrix is invertible if and only if .  If is invertible, then the solution to the equation is given by .  We can find by finding the reduced row echelon form of ; namely, .  If and are two invertible matrices, then their product is also invertible and .      There is a simple formula for finding the inverse of a matrix: , which can be easily checked. The condition that be invertible is, in this case, reduced to the condition that . We will understand this condition better once we have explored determinants in . There is a similar formula for the inverse of a matrix, but there is not a good reason to write it here.    Triangular matrices and Gaussian elimination  With some of the ideas we've developed, we can recast the Gaussian elimination algorithm in terms of matrix multiplication and invertibility. This will be especially helpful later when we consider determinants and LU factorizations. Triangular matrices will play an important role.    lower triangular matrix  upper triangular matrix  We say that a matrix is lower triangular if all its entries above the diagonal are zero. Similarly, is upper triangular if all the entries below the diagonal are zero.    For example, the matrix below is a lower triangular matrix while is an upper triangular one. .  We can develop a simple test to determine whether an lower triangular matrix is invertible. Let's use Gaussian elimination to find the reduced row echelon form of the lower triangular matrix Because the entries on the diagonal are nonzero, we find a pivot position in every row, which tells us that the matrix is invertible.  If, however, there is a zero entry on the diagonal, the matrix cannot be invertible. Considering the matrix below, we see that having a zero on the diagonal leads to a row without a pivot position.     An triangular matrix is invertible if and only if the entries on the diagonal are all nonzero.     Gaussian elimination and matrix multiplication   This activity explores how the row operations of scaling, interchange, and replacement can be performed using matrix multiplication.  As an example, we consider the matrix and apply a replacement operation that multiplies the first row by and adds it to the second row. Rather than performing this operation in the usual way, we construct a new matrix by applying the desired replacement operation to the identity matrix. To illustrate, we begin with the identity matrix and form a new matrix by multiplying the first row by and adding it to the second row to obtain   Show that the product is the result of applying the replacement operation to .   Explain why is invertible and find its inverse .  Describe the relationship between and and use the connection to replacement operations to explain why it holds.  Other row operations can be performed using a similar procedure. For instance, suppose we want to scale the second row of by . Find a matrix so that is the same as that obtained from the scaling operation. Why is invertible and what is ?   Finally, suppose we want to interchange the first and third rows of . Find a matrix , usually called a permutation matrix that performs this operation. What is ?  The original matrix is seen to be row equivalent to the upper triangular matrix by performing three replacement operations on : Find the matrices , , and that perform these row replacement operations so that .  Explain why the matrix product is invertible and use this fact to write . What is the matrix that you find? Why do you think we denote it by ?        Performing the matrix multiplication, we find that   We know that is invertible because it is a lower triangular matrix whose diagonal entries are all 1. We find that , which can be verified.  But we can see this in another way as well. The replacement operation is reversible; that is, multiplying the first row by and adding it to the second row can be undone by multiplying the first row by and adding it to the second row.  We find that This makes sense because scaling a row by can be undone by scaling the same row by .  We find that Moreover, because we can undo the interchange operation by repeating it.  Continuing with the Gaussian elimination algorithm, we have , as above, we then have .  Each of the matrices , , and is invertible so their product will be as well. Since , we have . Moreover, gives . Notice that this matrix is lower triangular so we call it .      matrix, elementary The following are examples of matrices, known as elementary matrices , that perform the row operations on a matrix having three rows.  Replacement  Multiplying the second row by 3 and adding it to the third row is performed by We often use to describe these matrices because they are lower triangular.  Scaling  Multiplying the third row by 2 is performed by   Interchange  Interchanging the first two rows is performed by       Suppose we have For the forward substitution phase of Gaussian elimination, we perform a sequence of three replacement operations. The first replacement operation multiplies the first row by and adds the result to the second row. We can perform this operation by multiplying by the lower triangular matrix where   The next two replacement operations are performed by the matrices so that   Notice that the inverse of has the simple form: . This says that if we want to undo the operation of multiplying the first row by and adding to the second row, we should multiply the first row by and add it to the second row. That is the effect of .  Notice that we now have , which gives where is the lower triangular matrix This way of writing as the product of a lower and an upper triangular matrix is known as an factorization of , and its usefulness will be explored in .      Summary  In this section, we found conditions guaranteeing that a matrix has an inverse. When these conditions hold, we also found an algorithm for finding the inverse.  A square matrix is invertible if there is a matrix , known as the inverse of , such that . We usually write .  The matrix is invertible if and only if it is row equivalent to , the identity matrix.  If a matrix is invertible, we can use Gaussian elimination to find its inverse: .  If a matrix is invertible, then the solution to the equation is .  The row operations of replacement, scaling, and interchange can be performed by multiplying by elementary matrices.       Consider the matrix .   Explain why has an inverse.  Find the inverse of by augmenting by the identity to form .  Use your inverse to solve the equation .             .     We see that , the identity matrix, which implies that has an inverse.  We have which says that   We compute that .     In this exercise, we will consider matrices as defining matrix transformations.  Write the matrix that performs a rotation. What geometric operation undoes this rotation? Find the matrix that perform this operation and verify that it is .  Write the matrix that performs a rotation. Verify that so that , and explain geometrically why this is the case.  Find three more matrices that satisfy .      and .   .         . To undo the rotation, we will perform a clockwise rotation, which is defined by the matrix .   . We see that because rotating by is its own inverse.  We can do this by constructing matrices that define reflections, such as      Inverses for certain types of matrices can be found in a relatively straightforward fashion.   The matrix is called diagonal since the only nonzero entries are on the diagonal of the matrix.     Find by augmenting by the identity and finding its reduced row echelon form.    Under what conditions is a diagonal matrix invertible?    Explain why the inverse of a diagonal matrix is also diagonal and explain the relationship between the diagonal entries in and .       Consider the lower triangular matrix .   Find by augmenting by the identity and finding its reduced row echelon form.    Explain why the inverse of a lower triangular matrix is also lower triangular.                .    When the entries on the diagonal are all nonzero.    Consider the steps performed in row reducing augmented by .           .    Consider the steps in row reducing augmented by .                .    When the entries on the diagonal are all nonzero.    Because the process of row reducing augmented by can be performed using only scalings. Then the diagonal entries of and are reciprocals of one another.           .     is always lower triangular because the only row operations needed to row reduce augmented by are replacements in which a multiple of one row is added to a row underneath it.          Our definition of an invertible matrix requires that be a square matrix. Let's examine what happens when is not square. For instance, suppose that .  Verify that . In this case, we say that is a left inverse of .   If has a left inverse , we can still use it to find solutions to linear equations. If we know there is a solution to the equation , we can multiply both sides of the equation by to find .  Suppose you know there is a solution to the equation . Use the left inverse to find and verify that it is a solution.  Now consider the matrix and verify that is also a left inverse of . This shows that the matrix may have more than one left inverse.       .      .     We compute that .  We find , which is indeed a solution to the equation .  In the same way, we compute .     If a matrix is invertible, there is a sequence of row operations that transforms into the identity matrix . We have seen that every row operation can be performed by matrix multiplication. If the step in the Gaussian elimination process is performed by multiplying by , then we have , which means that . For each of the following matrices, find a sequence of row operations that transforms the matrix to the identity . Write the matrices that perform the steps and use them to find .   .   .   .      .   .   .     If then so that .  We use three replacement operations Therefore, so .  We use a scaling followed by replacement operations: This gives so that .     Suppose that is an matrix.  Suppose that is invertible with inverse . This means that . Explain why must be invertible with inverse .  Suppose that is invertible with inverse . Explain why is invertible. What is in terms of and ?     If has inverse , then . This means that so is invertible with inverse .  In the same way, we have , which shows that .     If has inverse , then . This means that so is invertible with inverse .  In the same way, we have , which shows that .     Determine whether the following statements are true or false and explain your reasoning.  If is invertible, then the columns of are linearly independent.  If is a square matrix whose diagonal entries are all nonzero, then is invertible.  If is an invertible matrix, then span of the columns of is .  If is invertible, then there is a nonzero solution to the homogeneous equation .  If is an matrix and the equation has a solution for every vector , then is invertible.      True  False  True  False  True     True. If is invertible, then it has a pivot position in every column, which implies that the columns are linearly independent.  False. We only know this if is a triangular matrix. For instance, the matrix is not invertible.  True. If is invertible, then it has a pivot position in every row, which implies that the columns span .  False. Since there is a pivot position in every column, the homogeneous equation has only the zero solution .  True. In this case, the columns of span so there must be a pivot position in every row. Because is a square matrix, it must be row equivalent to the identity matrix .     Provide a justification for your response to the following questions.  Suppose that is a square matrix with two identical columns. Can be invertible?  Suppose that is a square matrix with two identical rows. Can be invertible?  Suppose that is an invertible matrix and that . Can you conclude that ?  Suppose that is an invertible matrix. What can you say about the span of the columns of ?  Suppose that is an invertible matrix and that is row equivalent to . Can you guarantee that is invertible?      No  No  Yes  The span is .  Yes     No. If has two identical columns, then the columns are not linearly independent. This means there is a column without a pivot position and so is not row equivalent to the identity matrix .  No. If we perform a replacement operation that multiplies one of the equal rows by and adds it to the other equal row, we obtain a matrix having a row whose entries are all zero. This says that the reduced row echelon form has such a row as well. Therefore, there is a row that does not contain a pivot position so the reduced row echelon form cannot be the identity .  Yes. If we multiply both sides of by on the left, then we see that .  The inverse is also invertible since is its inverse. Therefore, the span of the columns of is .  Yes. Since is row equivalent to the identity matrix , the matrix is as well. Therefore, is invertible.     Suppose that we start with the matrix , perform the following sequence of row operations:  Multiply row 1 by -2 and add to row 2.  Multiply row 1 by 4 and add to row 3.  Scale row 2 by .  Multiply row 2 by -1 and add to row 3,  and arrive at the upper triangular matrix    Write the matrices , , , and that perform the four row operations.  Find the matrix .  We then have . Now that we have the matrix , find the original matrix .      We have    .   .     We have   We have .  We write .     We say that two square matrices and are similar if there is an invertible matrix such that .   If and are similar, explain why and are similar as well. In particular, if , explain why .    If and are similar and is invertible, explain why is also invertible.    If and are similar and both are invertible, explain why and are similar.    If is similar to and is similar to , explain why is similar to . To begin, you may wish to assume that and .          .     .     .     .          .     .     .     .       Suppose that and are two matrices and that is invertible. We would like to explain why both and are invertible.   We first explain why is invertible.   Since is invertible, explain why any solution to the homogeneous equation is .    Use this fact to explain why any solution to must be .    Explain why must be invertible.       Now we explain why is invertible.   Since is invertible, explain why the equation is consistent for every vector .    Using the fact that is consistent for every , explain why every equation is consistent.    Explain why must be invertible.                .    If , then .    We now know that .          The span of the columns of is .    A solution to produces a solution to .    The span of the columns of is .               Since is invertible, , which says that there is pivot position in every column. Therefore, is the only solution to the equation .    If is a solution to the equation , then , which says that .    Since the only solution to the homogeneous equation is the zero solution, we know that so is invertible.          Since is invertible, the span of the columns of is , which says that every equation is consistent.    If is the solution to the equation , then satisfies , which means that is consistent.    We now know that the span of the columns of is , which tells us that is invertible.          We defined an matrix to be invertible if there is a matrix such that . In this exercise, we will explain why it is also true that , which is the statement of . This means that, if , then .  Suppose that is an -dimensional vector. Since , explain why and use this to explain why the only vector for which is .   Explain why this implies that must be invertible. We will call the inverse so that .  Beginning with , explain why and why this tells us that .      If , then .   is invertible because .  Multiply on the left by and the right by and regroup the matrix multiplications.     If , then .  Since the only solution to the homogeneous equation is , we know that the columns of are linearly independent and so has a pivot position in every column. Since is a square matrix, this tells us that so that is invertible.  If we multiply on the left by and the right by , then we have       "
},
{
  "id": "sec-matrix-inverse-2-2",
  "level": "2",
  "url": "sec-matrix-inverse.html#sec-matrix-inverse-2-2",
  "type": "Preview Activity",
  "number": "19.1.1",
  "title": "",
  "body": "    Explain how you would solve the equation using multiplication rather than division.  Find the matrix that rotates vectors counterclockwise by .  Find the matrix that rotates vectors clockwise by .  What do you expect the product to be? Explain the reasoning behind your expectation and then compute to verify it.  Solve the equation using Gaussian elimination.   Explain why your solution may also be found by computing .       Dividing by is the same as multiplying by , the multiplicative inverse of . We have   As we have seen a few times, the matrix is   Here, the matrix is   We should expect that since the effect of rotating by clockwise followed by rotating counterclockwise is to leave a vector unchanged. We can verify this by performing the matrix multiplication.  We have so the solution is .  The equation is asking us to find the vector that becomes after being rotated by . If we rotate by in the opposite direction, it will have this property. That is, if , then     "
},
{
  "id": "sec-matrix-inverse-3-4",
  "level": "2",
  "url": "sec-matrix-inverse.html#sec-matrix-inverse-3-4",
  "type": "Definition",
  "number": "19.1.1",
  "title": "",
  "body": "  invertible  matrix, inverse  An matrix is called invertible if there is a matrix such that , where is the identity matrix. The matrix is called the inverse of and denoted .   "
},
{
  "id": "sec-matrix-inverse-3-6",
  "level": "2",
  "url": "sec-matrix-inverse.html#sec-matrix-inverse-3-6",
  "type": "Example",
  "number": "19.1.2",
  "title": "",
  "body": "  Suppose that is the matrix that rotates two-dimensional vectors counterclockwise by and that rotates vectors by . We have We can check that which shows that is invertible and that .  Notice that if we multiply the matrices in the opposite order, we find that , which says that is also invertible and that . In other words, and are inverses of each other.   "
},
{
  "id": "sec-matrix-inverse-3-7",
  "level": "2",
  "url": "sec-matrix-inverse.html#sec-matrix-inverse-3-7",
  "type": "Activity",
  "number": "19.1.2",
  "title": "",
  "body": "  This activity demonstrates a procedure for finding the inverse of a matrix .   Suppose that . To find an inverse , we write its columns as and require that In other words, we can find the columns of by solving the equations Solve these equations to find and . Then write the matrix and verify that . This is enough for us to conclude that is the inverse of .     Find the product and explain why we now know that is invertible and .     What happens when you try to find the inverse of ?    We now develop a condition that must be satisfied by an invertible matrix. Suppose that is an invertible matrix with inverse and suppose that is any -dimensional vector. Since , we have This says that the equation is consistent and that is a solution.  Since we know that is consistent for any vector , what does this say about the span of the columns of ?    Since is a square matrix, what does this say about the pivot positions of ? What is the reduced row echelon form of ?    In this activity, we have studied the matrices Find the reduced row echelon form of each and explain how those forms enable us to conclude that one matrix is invertible and the other is not.        Solving the two equations for and gives . We can verify that, as we expect, .  We find that , which is the condition that tells us that is invertible.  Seeking the first column of , we see that the equation is not consistent. This means that is not invertible.  Since the equation is consistent for every , we know that the span of the columns of is .  Because the span of the columns of is , there is a pivot position in every row. Since is square, there is also a pivot position in every column. This means that the reduced row echelon form of must be the identity matrix .  We see that which shows that is invertible and is not.    "
},
{
  "id": "example-inverse-augment-I",
  "level": "2",
  "url": "sec-matrix-inverse.html#example-inverse-augment-I",
  "type": "Example",
  "number": "19.1.3",
  "title": "",
  "body": "  We can reformulate this procedure for finding the inverse of a matrix. For the sake of convenience, suppose that is a invertible matrix with inverse . Rather than solving the equations separately, we can solve them at the same time by augmenting by both vectors and and finding the reduced row echelon form.  For example, if , we form This shows that the matrix is the inverse of .  In other words, beginning with , we augment by the identify and find the reduced row echelon form to determine :    "
},
{
  "id": "proposition-inverse-solve",
  "level": "2",
  "url": "sec-matrix-inverse.html#proposition-inverse-solve",
  "type": "Proposition",
  "number": "19.1.4",
  "title": "",
  "body": "  If is an invertible matrix with inverse , then any equation is consistent and is a solution. In other words, the solution to is .   "
},
{
  "id": "proposition-invertible-rref",
  "level": "2",
  "url": "sec-matrix-inverse.html#proposition-invertible-rref",
  "type": "Proposition",
  "number": "19.1.5",
  "title": "",
  "body": "  The matrix is invertible if and only if the reduced row echelon form of is the identity matrix: . In addition, we can find the inverse by augmenting by the identity and finding the reduced row echelon form:    "
},
{
  "id": "proposition-inverse-inverse",
  "level": "2",
  "url": "sec-matrix-inverse.html#proposition-inverse-inverse",
  "type": "Proposition",
  "number": "19.1.6",
  "title": "",
  "body": "  If is a invertible matrix with inverse , then , which tells us that is invertible with inverse . In other words,    "
},
{
  "id": "sec-matrix-inverse-4-3",
  "level": "2",
  "url": "sec-matrix-inverse.html#sec-matrix-inverse-4-3",
  "type": "Activity",
  "number": "19.1.3",
  "title": "",
  "body": "  We'll begin by considering the square matrix    Describe the solution space to the equation by augmenting and finding the reduced row echelon form.     Using , explain why is invertible and find its inverse.    Now use the inverse to solve the equation and verify that your result agrees with what you found in part a.    If you have defined a matrix B in Sage, you can find it's inverse as B.inverse() or B^-1 . Use Sage to find the inverse of the matrix and use it to solve the equation .     If and are the two matrices defined in this activity, find their product and verify that it is invertible.    Compute the products and . Which one agrees with ?    Explain your finding by considering the product and using associativity to regroup the products so that the middle two terms are multiplied first.        Constructing the augmented matrix, we see that which says that there is a unique solution .    Our work in part a shows that from which we conclude that is invertible. To find the inverse, which says that     We see that .    Sage tells us that .    Sage helps us see that , which tells us that is invertible.   We find that .   We see that      "
},
{
  "id": "proposition-invertible-properties",
  "level": "2",
  "url": "sec-matrix-inverse.html#proposition-invertible-properties",
  "type": "Proposition",
  "number": "19.1.7",
  "title": "Properties of invertible matrices.",
  "body": " Properties of invertible matrices     An matrix is invertible if and only if .  If is invertible, then the solution to the equation is given by .  We can find by finding the reduced row echelon form of ; namely, .  If and are two invertible matrices, then their product is also invertible and .     "
},
{
  "id": "subsec-triangular-invertible-3",
  "level": "2",
  "url": "sec-matrix-inverse.html#subsec-triangular-invertible-3",
  "type": "Definition",
  "number": "19.1.8",
  "title": "",
  "body": "  lower triangular matrix  upper triangular matrix  We say that a matrix is lower triangular if all its entries above the diagonal are zero. Similarly, is upper triangular if all the entries below the diagonal are zero.   "
},
{
  "id": "proposition-triangular-invertibility",
  "level": "2",
  "url": "sec-matrix-inverse.html#proposition-triangular-invertibility",
  "type": "Proposition",
  "number": "19.1.9",
  "title": "",
  "body": "  An triangular matrix is invertible if and only if the entries on the diagonal are all nonzero.   "
},
{
  "id": "subsec-triangular-invertible-8",
  "level": "2",
  "url": "sec-matrix-inverse.html#subsec-triangular-invertible-8",
  "type": "Activity",
  "number": "19.1.4",
  "title": "Gaussian elimination and matrix multiplication.",
  "body": " Gaussian elimination and matrix multiplication   This activity explores how the row operations of scaling, interchange, and replacement can be performed using matrix multiplication.  As an example, we consider the matrix and apply a replacement operation that multiplies the first row by and adds it to the second row. Rather than performing this operation in the usual way, we construct a new matrix by applying the desired replacement operation to the identity matrix. To illustrate, we begin with the identity matrix and form a new matrix by multiplying the first row by and adding it to the second row to obtain   Show that the product is the result of applying the replacement operation to .   Explain why is invertible and find its inverse .  Describe the relationship between and and use the connection to replacement operations to explain why it holds.  Other row operations can be performed using a similar procedure. For instance, suppose we want to scale the second row of by . Find a matrix so that is the same as that obtained from the scaling operation. Why is invertible and what is ?   Finally, suppose we want to interchange the first and third rows of . Find a matrix , usually called a permutation matrix that performs this operation. What is ?  The original matrix is seen to be row equivalent to the upper triangular matrix by performing three replacement operations on : Find the matrices , , and that perform these row replacement operations so that .  Explain why the matrix product is invertible and use this fact to write . What is the matrix that you find? Why do you think we denote it by ?        Performing the matrix multiplication, we find that   We know that is invertible because it is a lower triangular matrix whose diagonal entries are all 1. We find that , which can be verified.  But we can see this in another way as well. The replacement operation is reversible; that is, multiplying the first row by and adding it to the second row can be undone by multiplying the first row by and adding it to the second row.  We find that This makes sense because scaling a row by can be undone by scaling the same row by .  We find that Moreover, because we can undo the interchange operation by repeating it.  Continuing with the Gaussian elimination algorithm, we have , as above, we then have .  Each of the matrices , , and is invertible so their product will be as well. Since , we have . Moreover, gives . Notice that this matrix is lower triangular so we call it .    "
},
{
  "id": "subsec-triangular-invertible-10",
  "level": "2",
  "url": "sec-matrix-inverse.html#subsec-triangular-invertible-10",
  "type": "Example",
  "number": "19.1.10",
  "title": "",
  "body": "  Suppose we have For the forward substitution phase of Gaussian elimination, we perform a sequence of three replacement operations. The first replacement operation multiplies the first row by and adds the result to the second row. We can perform this operation by multiplying by the lower triangular matrix where   The next two replacement operations are performed by the matrices so that   Notice that the inverse of has the simple form: . This says that if we want to undo the operation of multiplying the first row by and adding to the second row, we should multiply the first row by and add it to the second row. That is the effect of .  Notice that we now have , which gives where is the lower triangular matrix This way of writing as the product of a lower and an upper triangular matrix is known as an factorization of , and its usefulness will be explored in .   "
},
{
  "id": "sec-matrix-inverse-7-1",
  "level": "2",
  "url": "sec-matrix-inverse.html#sec-matrix-inverse-7-1",
  "type": "Exercise",
  "number": "19.1.5.1",
  "title": "",
  "body": " Consider the matrix .   Explain why has an inverse.  Find the inverse of by augmenting by the identity to form .  Use your inverse to solve the equation .             .     We see that , the identity matrix, which implies that has an inverse.  We have which says that   We compute that .   "
},
{
  "id": "sec-matrix-inverse-7-2",
  "level": "2",
  "url": "sec-matrix-inverse.html#sec-matrix-inverse-7-2",
  "type": "Exercise",
  "number": "19.1.5.2",
  "title": "",
  "body": " In this exercise, we will consider matrices as defining matrix transformations.  Write the matrix that performs a rotation. What geometric operation undoes this rotation? Find the matrix that perform this operation and verify that it is .  Write the matrix that performs a rotation. Verify that so that , and explain geometrically why this is the case.  Find three more matrices that satisfy .      and .   .         . To undo the rotation, we will perform a clockwise rotation, which is defined by the matrix .   . We see that because rotating by is its own inverse.  We can do this by constructing matrices that define reflections, such as    "
},
{
  "id": "sec-matrix-inverse-7-3",
  "level": "2",
  "url": "sec-matrix-inverse.html#sec-matrix-inverse-7-3",
  "type": "Exercise",
  "number": "19.1.5.3",
  "title": "",
  "body": " Inverses for certain types of matrices can be found in a relatively straightforward fashion.   The matrix is called diagonal since the only nonzero entries are on the diagonal of the matrix.     Find by augmenting by the identity and finding its reduced row echelon form.    Under what conditions is a diagonal matrix invertible?    Explain why the inverse of a diagonal matrix is also diagonal and explain the relationship between the diagonal entries in and .       Consider the lower triangular matrix .   Find by augmenting by the identity and finding its reduced row echelon form.    Explain why the inverse of a lower triangular matrix is also lower triangular.                .    When the entries on the diagonal are all nonzero.    Consider the steps performed in row reducing augmented by .           .    Consider the steps in row reducing augmented by .                .    When the entries on the diagonal are all nonzero.    Because the process of row reducing augmented by can be performed using only scalings. Then the diagonal entries of and are reciprocals of one another.           .     is always lower triangular because the only row operations needed to row reduce augmented by are replacements in which a multiple of one row is added to a row underneath it.        "
},
{
  "id": "sec-matrix-inverse-7-4",
  "level": "2",
  "url": "sec-matrix-inverse.html#sec-matrix-inverse-7-4",
  "type": "Exercise",
  "number": "19.1.5.4",
  "title": "",
  "body": " Our definition of an invertible matrix requires that be a square matrix. Let's examine what happens when is not square. For instance, suppose that .  Verify that . In this case, we say that is a left inverse of .   If has a left inverse , we can still use it to find solutions to linear equations. If we know there is a solution to the equation , we can multiply both sides of the equation by to find .  Suppose you know there is a solution to the equation . Use the left inverse to find and verify that it is a solution.  Now consider the matrix and verify that is also a left inverse of . This shows that the matrix may have more than one left inverse.       .      .     We compute that .  We find , which is indeed a solution to the equation .  In the same way, we compute .   "
},
{
  "id": "sec-matrix-inverse-7-5",
  "level": "2",
  "url": "sec-matrix-inverse.html#sec-matrix-inverse-7-5",
  "type": "Exercise",
  "number": "19.1.5.5",
  "title": "",
  "body": " If a matrix is invertible, there is a sequence of row operations that transforms into the identity matrix . We have seen that every row operation can be performed by matrix multiplication. If the step in the Gaussian elimination process is performed by multiplying by , then we have , which means that . For each of the following matrices, find a sequence of row operations that transforms the matrix to the identity . Write the matrices that perform the steps and use them to find .   .   .   .      .   .   .     If then so that .  We use three replacement operations Therefore, so .  We use a scaling followed by replacement operations: This gives so that .   "
},
{
  "id": "sec-matrix-inverse-7-6",
  "level": "2",
  "url": "sec-matrix-inverse.html#sec-matrix-inverse-7-6",
  "type": "Exercise",
  "number": "19.1.5.6",
  "title": "",
  "body": " Suppose that is an matrix.  Suppose that is invertible with inverse . This means that . Explain why must be invertible with inverse .  Suppose that is invertible with inverse . Explain why is invertible. What is in terms of and ?     If has inverse , then . This means that so is invertible with inverse .  In the same way, we have , which shows that .     If has inverse , then . This means that so is invertible with inverse .  In the same way, we have , which shows that .   "
},
{
  "id": "sec-matrix-inverse-7-7",
  "level": "2",
  "url": "sec-matrix-inverse.html#sec-matrix-inverse-7-7",
  "type": "Exercise",
  "number": "19.1.5.7",
  "title": "",
  "body": " Determine whether the following statements are true or false and explain your reasoning.  If is invertible, then the columns of are linearly independent.  If is a square matrix whose diagonal entries are all nonzero, then is invertible.  If is an invertible matrix, then span of the columns of is .  If is invertible, then there is a nonzero solution to the homogeneous equation .  If is an matrix and the equation has a solution for every vector , then is invertible.      True  False  True  False  True     True. If is invertible, then it has a pivot position in every column, which implies that the columns are linearly independent.  False. We only know this if is a triangular matrix. For instance, the matrix is not invertible.  True. If is invertible, then it has a pivot position in every row, which implies that the columns span .  False. Since there is a pivot position in every column, the homogeneous equation has only the zero solution .  True. In this case, the columns of span so there must be a pivot position in every row. Because is a square matrix, it must be row equivalent to the identity matrix .   "
},
{
  "id": "sec-matrix-inverse-7-8",
  "level": "2",
  "url": "sec-matrix-inverse.html#sec-matrix-inverse-7-8",
  "type": "Exercise",
  "number": "19.1.5.8",
  "title": "",
  "body": " Provide a justification for your response to the following questions.  Suppose that is a square matrix with two identical columns. Can be invertible?  Suppose that is a square matrix with two identical rows. Can be invertible?  Suppose that is an invertible matrix and that . Can you conclude that ?  Suppose that is an invertible matrix. What can you say about the span of the columns of ?  Suppose that is an invertible matrix and that is row equivalent to . Can you guarantee that is invertible?      No  No  Yes  The span is .  Yes     No. If has two identical columns, then the columns are not linearly independent. This means there is a column without a pivot position and so is not row equivalent to the identity matrix .  No. If we perform a replacement operation that multiplies one of the equal rows by and adds it to the other equal row, we obtain a matrix having a row whose entries are all zero. This says that the reduced row echelon form has such a row as well. Therefore, there is a row that does not contain a pivot position so the reduced row echelon form cannot be the identity .  Yes. If we multiply both sides of by on the left, then we see that .  The inverse is also invertible since is its inverse. Therefore, the span of the columns of is .  Yes. Since is row equivalent to the identity matrix , the matrix is as well. Therefore, is invertible.   "
},
{
  "id": "sec-matrix-inverse-7-9",
  "level": "2",
  "url": "sec-matrix-inverse.html#sec-matrix-inverse-7-9",
  "type": "Exercise",
  "number": "19.1.5.9",
  "title": "",
  "body": " Suppose that we start with the matrix , perform the following sequence of row operations:  Multiply row 1 by -2 and add to row 2.  Multiply row 1 by 4 and add to row 3.  Scale row 2 by .  Multiply row 2 by -1 and add to row 3,  and arrive at the upper triangular matrix    Write the matrices , , , and that perform the four row operations.  Find the matrix .  We then have . Now that we have the matrix , find the original matrix .      We have    .   .     We have   We have .  We write .   "
},
{
  "id": "sec-matrix-inverse-7-10",
  "level": "2",
  "url": "sec-matrix-inverse.html#sec-matrix-inverse-7-10",
  "type": "Exercise",
  "number": "19.1.5.10",
  "title": "",
  "body": " We say that two square matrices and are similar if there is an invertible matrix such that .   If and are similar, explain why and are similar as well. In particular, if , explain why .    If and are similar and is invertible, explain why is also invertible.    If and are similar and both are invertible, explain why and are similar.    If is similar to and is similar to , explain why is similar to . To begin, you may wish to assume that and .          .     .     .     .          .     .     .     .     "
},
{
  "id": "sec-matrix-inverse-7-11",
  "level": "2",
  "url": "sec-matrix-inverse.html#sec-matrix-inverse-7-11",
  "type": "Exercise",
  "number": "19.1.5.11",
  "title": "",
  "body": " Suppose that and are two matrices and that is invertible. We would like to explain why both and are invertible.   We first explain why is invertible.   Since is invertible, explain why any solution to the homogeneous equation is .    Use this fact to explain why any solution to must be .    Explain why must be invertible.       Now we explain why is invertible.   Since is invertible, explain why the equation is consistent for every vector .    Using the fact that is consistent for every , explain why every equation is consistent.    Explain why must be invertible.                .    If , then .    We now know that .          The span of the columns of is .    A solution to produces a solution to .    The span of the columns of is .               Since is invertible, , which says that there is pivot position in every column. Therefore, is the only solution to the equation .    If is a solution to the equation , then , which says that .    Since the only solution to the homogeneous equation is the zero solution, we know that so is invertible.          Since is invertible, the span of the columns of is , which says that every equation is consistent.    If is the solution to the equation , then satisfies , which means that is consistent.    We now know that the span of the columns of is , which tells us that is invertible.        "
},
{
  "id": "ex-right-inverse",
  "level": "2",
  "url": "sec-matrix-inverse.html#ex-right-inverse",
  "type": "Exercise",
  "number": "19.1.5.12",
  "title": "",
  "body": " We defined an matrix to be invertible if there is a matrix such that . In this exercise, we will explain why it is also true that , which is the statement of . This means that, if , then .  Suppose that is an -dimensional vector. Since , explain why and use this to explain why the only vector for which is .   Explain why this implies that must be invertible. We will call the inverse so that .  Beginning with , explain why and why this tells us that .      If , then .   is invertible because .  Multiply on the left by and the right by and regroup the matrix multiplications.     If , then .  Since the only solution to the homogeneous equation is , we know that the columns of are linearly independent and so has a pivot position in every column. Since is a square matrix, this tells us that so that is invertible.  If we multiply on the left by and the right by , then we have     "
},
{
  "id": "sec-bases",
  "level": "1",
  "url": "sec-bases.html",
  "type": "Section",
  "number": "19.2",
  "title": "Bases and coordinate systems",
  "body": " Bases and coordinate systems   Standard Cartesian coordinates are commonly used to describe points in the plane. If we mention the point , we know that we arrive at this point from the origin by moving four units to the right and three units up.  Sometimes, however, it is more natural to work in a different coordinate system. Suppose that you live in the city whose map is shown in and that you would like to give a guest directions for getting from your house to the store. You would probably say something like, \"Go four blocks up Maple. Then turn left on Main for three blocks.\" The grid of streets in the city gives a more natural coordinate system than standard north-south, east-west coordinates.      A city map.   In this section, we will develop the concept of a basis through which we will create new coordinate systems in . We will see that the right choice of a coordinate system provides a more natural way to approach some problems.    Consider the vectors in , which are shown in .      Linear combinations of and .     Indicate the linear combination on the figure.  Express the vector as a linear combination of and .  Find the linear combination .  Express the vector as a linear combination of and .  Explain why every vector in can be written as a linear combination of and in exactly one way.      We can see graphically, or we can compute, that .  Again, we graphically see that .  Since the linear combination extends beyond the figure, we compute that .  We need to find the solution to the linear system , which is .  The matrix has a pivot position in every row and every column.      In the preview activity, we worked with a set of two vectors in and found that we could express any vector in in two different ways: in the usual way where the components of the vector describe horizontal and vertical changes, and in a new way as a linear combination of and . We could also translate between these two descriptions. This example illustrates the central idea of this section.    Bases  In the preview activity, we created a new coordinate system for using linear combinations of a set of two vectors. More generally, the following definition will guide us.    basis  A set of vectors in is called a basis for if the set of vectors spans and is linearly independent.      We will look at some examples of bases in this activity.  In the preview activity, we worked with the set of vectors in : . Explain why these vectors form a basis for .  Consider the set of vectors in  and determine whether they form a basis for .   Do the vectors form a basis for ?  Explain why the vectors form a basis for .  If a set of vectors forms a basis for , what can you guarantee about the pivot positions of the matrix ?  If the set of vectors is a basis for , how many vectors must be in the set?       The matrix is row equivalent to the identity matrix so it has a pivot position in every row. The span of the columns is therefore . There is also a pivot position in every column, which means that the columns are linearly independent.  We note that Since there is a pivot position in every row, the span of the vectors is . Since there is a pivot position in every column, the vectors are linearly independent. Consequently, this set of vectors forms a basis for .  The matrix whose columns are the vectors , , , and has dimensions . Therefore, there cannot be a pivot position in every column, which tells us that the columns cannot be linearly independent. Therefore, the set of vectors do not form a basis for .  Putting these vectors into a matrix produces the identity matrix, which has a pivot position in every row and every column. Therefore, the span of the vectors is , and they are linearly independent.  There must be a pivot position in every row and every column.  A basis for must have vectors. Because the associated matrix must have a pivot position in every row and every column, there must be the same number of columns as there are rows. Since the vectors are -dimensional, there must be 10 vectors.     We can develop a test to determine if a set of vectors forms a basis for by considering the matrix . To be a basis, this set of vectors must span and be linearly independent.  We know that the span of the set of vectors is if and only if has a pivot position in every row. We also know that the set of vectors is linearly independent if and only if has a pivot position in every column. This means that a set of vectors forms a basis if and only if has a pivot in every row and every column. Therefore, must be row equivalent to the identity matrix : .  In addition to helping identify bases, this fact tells us something important about the number of vectors in a basis. Since the matrix has a pivot position in every row and every column, it must have the same number of rows as columns. Therefore, the number of vectors in a basis for must be . For example, a basis for must have exactly 23 vectors.    A set of vectors forms a basis for if and only if the matrix This means there must be vectors in a basis for .      Notice that the vectors form the columns of the identity matrix, which implies that this set forms a basis for . More generally, the set of vectors forms a basis for , which we call the standard basis for . basis, standard       Coordinate systems  A basis for forms a coordinate system for , as we will describe. Rather than continuing to write a list of vectors, we will find it convenient to denote a basis using a single symbol, such as     In this section's preview activity, we considered the vectors , which form a basis for .   In the standard coordinate system, the point is found by moving 2 units to the right and 3 units down. We would like to define a new coordinate system where we interpret to mean we move two times along and 3 times along . As we see in the figure, doing so leaves us at the point , expressed in the usual coordinate system.    We have seen that . The coordinates of the vector in the new coordinate system are the weights that we use to create as a linear combination of and .  Since we now have two descriptions of the vector , we need some notation to keep track of which coordinate system we are using. Because , we will write . More generally, will denote the coordinates of in the basis ; that is, is the vector of weights such that .  For example, if the coordinates of in the basis are , then and we conclude that . This demonstrates how we can translate coordinates in the basis into standard coordinates.  Suppose we know the expression of a vector in standard coordinates. How can we find its coordinates in the basis ? For instance, suppose and that we would like to find . We can write which means that or This linear system for the weights defines an augmented matrix . which means that .    This example illustrates how a basis in provides a new coordinate system for and shows how we may translate between this coordinate system and the standard one.  More generally, suppose that is a basis for . We know that the span of the vectors is , which implies that any vector in can be written as a linear combination of the vectors. In addition, we know that the vectors are linearly independent, which means that we can write as a linear combination of the vectors in exactly one way. Therefore, we have where the weights are unique. In this case, we write the coordinate description of in the basis as .    Let's begin with the basis of where .  If the coordinates of in the basis are , what is the vector ?  If , find the coordinates of in the basis ; that is, find .  Find a matrix such that, for any vector , we have . Explain why this matrix is invertible.  Using what you found in the previous part, find a matrix such that, for any vector , we have . What is the relationship between the two matrices and ? Explain why this relationship holds.  Suppose we consider the standard basis . What is the relationship between and ?  Suppose we also consider the basis . Find a matrix that converts coordinates in the basis into coordinates in the basis ; that is, . You may wish to think about converting coordinates from the basis into the standard coordinate system and then into the basis .      We know that .  We solve the linear system to find .  If , we have This matrix , whose columns are the vectors and , has a pivot position in every row and every column because the vectors form a basis. It is, therefore, row equivalent to the identity matrix and hence invertible.  Since we have , we also have .  We have   If we define to be the matrix whose columns are and , then Therefore,      This activity demonstrates how we can efficiently convert between coordinate systems defined by different bases. Let's consider a basis and a vector . We know that If we use to denote the matrix whose columns are the basis vectors, then we find that where . This means that the matrix converts coordinates in the basis into standard coordinates.  Since the columns of are the basis vectors , we know that , and is therefore invertible. Since we have , we must also have .    If is a basis and the matrix whose columns are the basis vectors, then     If we have another basis , we find, in the same way, that for the conversion between coordinates in the basis into standard coordinates. We then have . Therefore, is the matrix that converts -coordinates into -coordinates.    Examples of bases  We will now look at some examples of bases that illustrate how it can be useful to study a problem using a different coordinate system.    Let's consider the basis of : It is relatively straightforward to convert a vector's representation in this basis into to the standard basis using the matrix whose columns are the basis vectors: For example, suppose that the vector is described in the coordinate system defined by the basis as . We then have .  Consider now the vector . If we would like to express in the coordinate system defined by , then we compute .      Suppose we work for a company that records its quarterly revenue, in millions of dollars, as:   A company's quarterly revenue    Quarter  Revenue    1  10.3   2  13.1   3  7.5   4  8.2      Rather than using a table to record the data, we could display it in a graph or write it as a vector in : .    Let's consider a new basis for using vectors . We may view these basis elements graphically, as in       A representation of the basis elements of .   To convert our revenue vectors into the coordinates given by , we form the matrices: In particular, if the revenue vector is , then Notice that the first component of is the average of the components of .  For our particular revenue vector , we have This means that our revenue vector is . We will think about what these terms mean by adding them together one at a time.    The first term, gives us the average revenue over the year.     The average revenue for the first two quarters is 11.7, which is 1.925 million dollars above the yearly average. Similarly, the average revenue for the last two quarters is 1.925 million dollars below the yearly average. This is recorded by the second term      Finally, the first quarter's revenue is 1.400 million dollars below the average over the first two quarters and the second quarter's revenue is 1.400 million dollars above that average. This, and the corresponding data for the last two quarters, is captured by the last two terms:      If we write , we see that the coefficient measures the average revenue over the year, measures the deviation from the annual average in the first and second halves of the year, and measures how the revenue in the first and second quarter differs from the average in the first half of the year. In this way, the coefficients provide a view of the revenue over different time scales, from an annual summary to a finer view of quarterly behavior.  This basis is sometimes called a Haar wavelet basis, and the change of basis is known as a Haar wavelet transform. In the next section, we will see how this basis provides a useful way to store digital images.     Edge detection   An important problem in the field of computer vision is to detect edges in a digital photograph, as is shown in . Edge detection algorithms are useful when, say, we want a robot to locate an object in its field of view. Graphic designers also use these algorithms to create artistic effects.       A canyon wall in Capitol Reef National Park and the result of an edge detection algorithm.   We will consider a very simple version of an edge detection algorithm to give a sense of how this works. Rather than considering a two-dimensional photograph, we will think about a one-dimensional row of pixels in a photograph. The grayscale values of a pixel measure the brightness of a pixel; a grayscale value of 0 corresponds to black, and a value of 255 corresponds to white.  Suppose, for simplicity, that the grayscale values for a row of six pixels are represented by a vector in :    .    We can easily see that there is a jump in brightness between pixels 4 and 5, but how can we detect it computationally? We will introduce a new basis for with vectors: .  Construct the matrix that relates the standard coordinate system with the coordinates in the basis .  Determine the matrix that converts the representation of in standard coordinates into the coordinate system defined by .   Suppose the vectors are expressed in general terms as . Using the relationship , determine an expression for the coefficient in terms of . What does measure in terms of the grayscale values of the pixels? What does measure in terms of the grayscale values of the pixels?  Now for the specific vector , determine the representation of in the -coordinate system.  Explain how the coefficients in determine the location of the jump in brightness in the grayscale values represented by the vector .    Readers who are familiar with calculus may recognize that this change of basis converts a vector into , the set of changes in . This process is similar to differentiation in calculus. Similarly, the process of converting into the vector adds together the changes in a process similar to integration. As a result, this change of basis represents a linear algebraic version of the Fundamental Theorem of Calculus.     We form the matrix   We find that   We see that so measures the change in brightness between one pixel and its neighbor. Similarly, , which measures another change in brightness.  We compute that   Most of the coefficients that measure changes are relatively small in absolute value. The coefficient , however, which measures the change in brightness between the fourth and fifth pixel, has a large absolute value. This tells us that there is a large change in brightness between the fourth and fifth pixel, which points to an edge in the image.       Summary  We defined a basis to be a set of vectors that is linearly independent and whose span is .  A set of vectors forms a basis for if and only if the matrix . This means there must be vectors in a basis for .  If forms a basis for , then any vector in can be written as a linear combination of the vectors in exactly one way.  We used the basis to define a coordinate system in which , the coordinates of in the basis , are defined by .   Forming the matrix whose columns are the basis vectors, we can convert between coordinate systems: .       Shown in are two vectors and in the plane .      Vectors and in .    Explain why is a basis for .  Using , indicate the vectors such that            Using , find the representation if   .   .   .   Find if .     The vectors are linearly independent and span .  The grid on the figure indicates that   .   .   .    The grid on the figure indicates that   .   .   .     .     The vectors are linearly independent and span . We can see this by forming the matrix   The grid on the figure indicates that   .   .   .    The grid on the figure indicates that   .   .   .    We form the matrix so that . We then need to solve the equation , which gives .     Consider vectors and let and .  Explain why and are both bases of .  If , find and .   If , find and .  If , find and .  Find a matrix such that .     The sets of the vectors are both linearly independent and span .   and .   and .   and .   .     In both cases, we see that so that both sets of vectors are linearly independent and span .  We solve and to find and .  We have . We then solve to find .  In the same way, we find and .  We have , which shows that .     Consider the following vectors in : .  Explain why forms a basis for .  Explain how to convert , the representation of a vector in the coordinates defined by , into , its representation in the standard coordinate system.  Explain how to convert the vector into , its representation in the coordinate system defined by .  If , find .  If , find .     The vectors are linearly independent and span .  We have .  We have .   .   .     Form the matrix which shows that the vectors are linearly independent and span .  We have .  We have where   We find .  We find .     Consider the following vectors in : .  Do these vectors form a basis for ? Explain your thinking.   Find a subset of these vectors that forms a basis of .  Suppose you have a set of vectors in such that . Find a subset of the vectors that forms a basis for .     No, because a basis for must contain exactly three vectors.   , , and .   , , , and .     Looking at the reduced row echelon form, we find This shows that the vectors are not linearly independent since there is not a pivot position in every column. Therefore, the set of vectors does not form a basis for . Of course, we also know this because a set of vectors for must contain exactly three vectors.  From the reduced row echelon form, we see that the set of vectors spans because there is a pivot position in every row. We also see that and . This means that , , and will span and therefore form a basis.  In the same way, we see that , , , and for a basis for .     This exercise involves a simple Fourier transform, which will play an important role in the next section.  Suppose that we have the vectors .  Explain why is a basis for . Notice that you may enter into Sage as cos(pi\/6) .   If , find .  Find the matrices and . If and , explain why is the average of , , and .      The vectors are linearly independent and span .   .  Since we have , we have .     By forming the matrix and finding its reduced row echelon form, we see that the vectors are linearly independent and span . They therefore form a basis of .  We solve to find .  We have Since we have , we have .     Determine whether the following statements are true or false and provide a justification for your response.  If the columns of a matrix form a basis for , then is invertible.  There must be 125 vectors in a basis for .  If is a basis of , then every vector in can be expressed as a linear combination of basis vectors.  The coordinates are the weights that form as a linear combination of basis vectors.  If the basis vectors form the columns of the matrix , then .      True  True  True  True  False     True. If the columns of form a basis, then has a pivot position in every row and every column. Therefore, the reduced row echelon form of is the identity matrix, which implies that is invertible.  True. The number of vectors in a basis of must be .  True. If is a basis, then the vectors in span , which means that every vector in can be written as a linear combination of the vectors in .  True. This is the definition of .  False. The relationship is .     Provide a justification for your response to each of the following questions.  Suppose you have linearly independent vectors in . Can you guarantee that they form a basis of ?  If is an invertible matrix, do the columns necessarily form a basis of ?  Suppose we have an invertible matrix , and we perform a sequence of row operations on to form a matrix . Can you guarantee that the columns of form a basis for ?  Suppose you have a set of 10 vectors in and that every vector in can be written as a linear combination of these vectors. Can you guarantee that this set of vectors is a basis for ?     Yes  Yes  Yes  Yes     Yes. A matrix formed from linearly independent vectors in will have a pivot position in every column. Since the matrix has the same number of rows and columns, there must also be a pivot position in every row. This means that the vectors span and therefore form a basis.  Yes. An invertible matrix is row equivalent to the identity matrix, which means that the columns are linearly independent and span . This implies that the columns form a basis of .  Yes. The matrix is row equivalent to the identity matrix so must be as well. This means that the columns of form a basis for .  Yes. The span of the set of vectors is , which says that the associated matrix is square and has a pivot position in every row. Therefore, it must have a pivot position in every column, which means that the set of vectors forms a basis for .     Crystallographers find it convenient to use coordinate systems that are adapted to the specific geometry of a crystal. As a two-dimensional example, consider a layer of graphite in which carbon atoms are arranged in regular hexagons to form the crystalline structure shown in .      A layer of carbon atoms in a graphite crystal.   The origin of the coordinate system is at the carbon atom labeled by 0 . It is convenient to choose the basis defined by the vectors and and the coordinate system it defines.   Locate the points for which   ,   ,   .    Find the coordinates for all the carbon atoms in the hexagon whose lower left vertex is labeled 0 .  What are the coordinates of the center of that hexagon, which is labeled C ?  How do the coordinates of the atoms in the hexagon whose lower left corner is labeled 1 compare to the coordinates in the hexagon whose lower left corner is labeled \"0\"?  Does the point whose coordinates are correspond to a carbon atom or the center of a hexagon?     The points are indicated in the figure.      .   .  The coordinates differ by   It is the center of a hexagon.      The points are indicated in .   The points requested in part a of this exercise.       Moving counterclockwise around the hexagon, the coordinates are    .  We obtain the coordinates for the hexagon with the vertex labeled 1 by adding the coordinate expression of the point 1 , which is to those of the original hexagon.  It is the center of a hexagon. Adding or subtracting to the coordinates translates one hexagon to another. This means that can be translated to , which is the center of a hexagon.     Suppose that and .  Explain why is a basis for .  Find and .  Use what you found in the previous part of this problem to find and .   If , find .  Find a matrix such that .  You should find that the matrix is a very simple matrix, which means that this basis is well suited to study the effect of multiplication by . This observation is the central idea of the next chapter.    The vectors are linearly independent and span .   and .   and .   .   .     The vectors are linearly independent and span .  We compute that and .   and .  We know that , which means that . Therefore, .  If , then and . Therefore, , which says that .     "
},
{
  "id": "fig-city-map",
  "level": "2",
  "url": "sec-bases.html#fig-city-map",
  "type": "Figure",
  "number": "19.2.1",
  "title": "",
  "body": "    A city map.  "
},
{
  "id": "sec-bases-2-5",
  "level": "2",
  "url": "sec-bases.html#sec-bases-2-5",
  "type": "Preview Activity",
  "number": "19.2.1",
  "title": "",
  "body": "  Consider the vectors in , which are shown in .      Linear combinations of and .     Indicate the linear combination on the figure.  Express the vector as a linear combination of and .  Find the linear combination .  Express the vector as a linear combination of and .  Explain why every vector in can be written as a linear combination of and in exactly one way.      We can see graphically, or we can compute, that .  Again, we graphically see that .  Since the linear combination extends beyond the figure, we compute that .  We need to find the solution to the linear system , which is .  The matrix has a pivot position in every row and every column.     "
},
{
  "id": "sec-bases-3-3",
  "level": "2",
  "url": "sec-bases.html#sec-bases-3-3",
  "type": "Definition",
  "number": "19.2.3",
  "title": "",
  "body": "  basis  A set of vectors in is called a basis for if the set of vectors spans and is linearly independent.   "
},
{
  "id": "sec-bases-3-4",
  "level": "2",
  "url": "sec-bases.html#sec-bases-3-4",
  "type": "Activity",
  "number": "19.2.2",
  "title": "",
  "body": "  We will look at some examples of bases in this activity.  In the preview activity, we worked with the set of vectors in : . Explain why these vectors form a basis for .  Consider the set of vectors in  and determine whether they form a basis for .   Do the vectors form a basis for ?  Explain why the vectors form a basis for .  If a set of vectors forms a basis for , what can you guarantee about the pivot positions of the matrix ?  If the set of vectors is a basis for , how many vectors must be in the set?       The matrix is row equivalent to the identity matrix so it has a pivot position in every row. The span of the columns is therefore . There is also a pivot position in every column, which means that the columns are linearly independent.  We note that Since there is a pivot position in every row, the span of the vectors is . Since there is a pivot position in every column, the vectors are linearly independent. Consequently, this set of vectors forms a basis for .  The matrix whose columns are the vectors , , , and has dimensions . Therefore, there cannot be a pivot position in every column, which tells us that the columns cannot be linearly independent. Therefore, the set of vectors do not form a basis for .  Putting these vectors into a matrix produces the identity matrix, which has a pivot position in every row and every column. Therefore, the span of the vectors is , and they are linearly independent.  There must be a pivot position in every row and every column.  A basis for must have vectors. Because the associated matrix must have a pivot position in every row and every column, there must be the same number of columns as there are rows. Since the vectors are -dimensional, there must be 10 vectors.    "
},
{
  "id": "sec-bases-3-8",
  "level": "2",
  "url": "sec-bases.html#sec-bases-3-8",
  "type": "Proposition",
  "number": "19.2.4",
  "title": "",
  "body": "  A set of vectors forms a basis for if and only if the matrix This means there must be vectors in a basis for .   "
},
{
  "id": "sec-bases-3-9",
  "level": "2",
  "url": "sec-bases.html#sec-bases-3-9",
  "type": "Example",
  "number": "19.2.5",
  "title": "",
  "body": "  Notice that the vectors form the columns of the identity matrix, which implies that this set forms a basis for . More generally, the set of vectors forms a basis for , which we call the standard basis for . basis, standard    "
},
{
  "id": "sec-bases-4-3",
  "level": "2",
  "url": "sec-bases.html#sec-bases-4-3",
  "type": "Example",
  "number": "19.2.6",
  "title": "",
  "body": "  In this section's preview activity, we considered the vectors , which form a basis for .   In the standard coordinate system, the point is found by moving 2 units to the right and 3 units down. We would like to define a new coordinate system where we interpret to mean we move two times along and 3 times along . As we see in the figure, doing so leaves us at the point , expressed in the usual coordinate system.    We have seen that . The coordinates of the vector in the new coordinate system are the weights that we use to create as a linear combination of and .  Since we now have two descriptions of the vector , we need some notation to keep track of which coordinate system we are using. Because , we will write . More generally, will denote the coordinates of in the basis ; that is, is the vector of weights such that .  For example, if the coordinates of in the basis are , then and we conclude that . This demonstrates how we can translate coordinates in the basis into standard coordinates.  Suppose we know the expression of a vector in standard coordinates. How can we find its coordinates in the basis ? For instance, suppose and that we would like to find . We can write which means that or This linear system for the weights defines an augmented matrix . which means that .   "
},
{
  "id": "sec-bases-4-6",
  "level": "2",
  "url": "sec-bases.html#sec-bases-4-6",
  "type": "Activity",
  "number": "19.2.3",
  "title": "",
  "body": "  Let's begin with the basis of where .  If the coordinates of in the basis are , what is the vector ?  If , find the coordinates of in the basis ; that is, find .  Find a matrix such that, for any vector , we have . Explain why this matrix is invertible.  Using what you found in the previous part, find a matrix such that, for any vector , we have . What is the relationship between the two matrices and ? Explain why this relationship holds.  Suppose we consider the standard basis . What is the relationship between and ?  Suppose we also consider the basis . Find a matrix that converts coordinates in the basis into coordinates in the basis ; that is, . You may wish to think about converting coordinates from the basis into the standard coordinate system and then into the basis .      We know that .  We solve the linear system to find .  If , we have This matrix , whose columns are the vectors and , has a pivot position in every row and every column because the vectors form a basis. It is, therefore, row equivalent to the identity matrix and hence invertible.  Since we have , we also have .  We have   If we define to be the matrix whose columns are and , then Therefore,     "
},
{
  "id": "proposition-coordinate-transform",
  "level": "2",
  "url": "sec-bases.html#proposition-coordinate-transform",
  "type": "Proposition",
  "number": "19.2.7",
  "title": "",
  "body": "  If is a basis and the matrix whose columns are the basis vectors, then    "
},
{
  "id": "sec-bases-5-3",
  "level": "2",
  "url": "sec-bases.html#sec-bases-5-3",
  "type": "Example",
  "number": "19.2.8",
  "title": "",
  "body": "  Let's consider the basis of : It is relatively straightforward to convert a vector's representation in this basis into to the standard basis using the matrix whose columns are the basis vectors: For example, suppose that the vector is described in the coordinate system defined by the basis as . We then have .  Consider now the vector . If we would like to express in the coordinate system defined by , then we compute .   "
},
{
  "id": "example-wavelet-basis",
  "level": "2",
  "url": "sec-bases.html#example-wavelet-basis",
  "type": "Example",
  "number": "19.2.9",
  "title": "",
  "body": "  Suppose we work for a company that records its quarterly revenue, in millions of dollars, as:   A company's quarterly revenue    Quarter  Revenue    1  10.3   2  13.1   3  7.5   4  8.2      Rather than using a table to record the data, we could display it in a graph or write it as a vector in : .    Let's consider a new basis for using vectors . We may view these basis elements graphically, as in       A representation of the basis elements of .   To convert our revenue vectors into the coordinates given by , we form the matrices: In particular, if the revenue vector is , then Notice that the first component of is the average of the components of .  For our particular revenue vector , we have This means that our revenue vector is . We will think about what these terms mean by adding them together one at a time.    The first term, gives us the average revenue over the year.     The average revenue for the first two quarters is 11.7, which is 1.925 million dollars above the yearly average. Similarly, the average revenue for the last two quarters is 1.925 million dollars below the yearly average. This is recorded by the second term      Finally, the first quarter's revenue is 1.400 million dollars below the average over the first two quarters and the second quarter's revenue is 1.400 million dollars above that average. This, and the corresponding data for the last two quarters, is captured by the last two terms:      If we write , we see that the coefficient measures the average revenue over the year, measures the deviation from the annual average in the first and second halves of the year, and measures how the revenue in the first and second quarter differs from the average in the first half of the year. In this way, the coefficients provide a view of the revenue over different time scales, from an annual summary to a finer view of quarterly behavior.  This basis is sometimes called a Haar wavelet basis, and the change of basis is known as a Haar wavelet transform. In the next section, we will see how this basis provides a useful way to store digital images.   "
},
{
  "id": "sec-bases-5-5",
  "level": "2",
  "url": "sec-bases.html#sec-bases-5-5",
  "type": "Activity",
  "number": "19.2.4",
  "title": "Edge detection.",
  "body": " Edge detection   An important problem in the field of computer vision is to detect edges in a digital photograph, as is shown in . Edge detection algorithms are useful when, say, we want a robot to locate an object in its field of view. Graphic designers also use these algorithms to create artistic effects.       A canyon wall in Capitol Reef National Park and the result of an edge detection algorithm.   We will consider a very simple version of an edge detection algorithm to give a sense of how this works. Rather than considering a two-dimensional photograph, we will think about a one-dimensional row of pixels in a photograph. The grayscale values of a pixel measure the brightness of a pixel; a grayscale value of 0 corresponds to black, and a value of 255 corresponds to white.  Suppose, for simplicity, that the grayscale values for a row of six pixels are represented by a vector in :    .    We can easily see that there is a jump in brightness between pixels 4 and 5, but how can we detect it computationally? We will introduce a new basis for with vectors: .  Construct the matrix that relates the standard coordinate system with the coordinates in the basis .  Determine the matrix that converts the representation of in standard coordinates into the coordinate system defined by .   Suppose the vectors are expressed in general terms as . Using the relationship , determine an expression for the coefficient in terms of . What does measure in terms of the grayscale values of the pixels? What does measure in terms of the grayscale values of the pixels?  Now for the specific vector , determine the representation of in the -coordinate system.  Explain how the coefficients in determine the location of the jump in brightness in the grayscale values represented by the vector .    Readers who are familiar with calculus may recognize that this change of basis converts a vector into , the set of changes in . This process is similar to differentiation in calculus. Similarly, the process of converting into the vector adds together the changes in a process similar to integration. As a result, this change of basis represents a linear algebraic version of the Fundamental Theorem of Calculus.     We form the matrix   We find that   We see that so measures the change in brightness between one pixel and its neighbor. Similarly, , which measures another change in brightness.  We compute that   Most of the coefficients that measure changes are relatively small in absolute value. The coefficient , however, which measures the change in brightness between the fourth and fifth pixel, has a large absolute value. This tells us that there is a large change in brightness between the fourth and fifth pixel, which points to an edge in the image.    "
},
{
  "id": "sec-bases-7-1",
  "level": "2",
  "url": "sec-bases.html#sec-bases-7-1",
  "type": "Exercise",
  "number": "19.2.5.1",
  "title": "",
  "body": " Shown in are two vectors and in the plane .      Vectors and in .    Explain why is a basis for .  Using , indicate the vectors such that            Using , find the representation if   .   .   .   Find if .     The vectors are linearly independent and span .  The grid on the figure indicates that   .   .   .    The grid on the figure indicates that   .   .   .     .     The vectors are linearly independent and span . We can see this by forming the matrix   The grid on the figure indicates that   .   .   .    The grid on the figure indicates that   .   .   .    We form the matrix so that . We then need to solve the equation , which gives .   "
},
{
  "id": "sec-bases-7-2",
  "level": "2",
  "url": "sec-bases.html#sec-bases-7-2",
  "type": "Exercise",
  "number": "19.2.5.2",
  "title": "",
  "body": " Consider vectors and let and .  Explain why and are both bases of .  If , find and .   If , find and .  If , find and .  Find a matrix such that .     The sets of the vectors are both linearly independent and span .   and .   and .   and .   .     In both cases, we see that so that both sets of vectors are linearly independent and span .  We solve and to find and .  We have . We then solve to find .  In the same way, we find and .  We have , which shows that .   "
},
{
  "id": "sec-bases-7-3",
  "level": "2",
  "url": "sec-bases.html#sec-bases-7-3",
  "type": "Exercise",
  "number": "19.2.5.3",
  "title": "",
  "body": " Consider the following vectors in : .  Explain why forms a basis for .  Explain how to convert , the representation of a vector in the coordinates defined by , into , its representation in the standard coordinate system.  Explain how to convert the vector into , its representation in the coordinate system defined by .  If , find .  If , find .     The vectors are linearly independent and span .  We have .  We have .   .   .     Form the matrix which shows that the vectors are linearly independent and span .  We have .  We have where   We find .  We find .   "
},
{
  "id": "sec-bases-7-4",
  "level": "2",
  "url": "sec-bases.html#sec-bases-7-4",
  "type": "Exercise",
  "number": "19.2.5.4",
  "title": "",
  "body": " Consider the following vectors in : .  Do these vectors form a basis for ? Explain your thinking.   Find a subset of these vectors that forms a basis of .  Suppose you have a set of vectors in such that . Find a subset of the vectors that forms a basis for .     No, because a basis for must contain exactly three vectors.   , , and .   , , , and .     Looking at the reduced row echelon form, we find This shows that the vectors are not linearly independent since there is not a pivot position in every column. Therefore, the set of vectors does not form a basis for . Of course, we also know this because a set of vectors for must contain exactly three vectors.  From the reduced row echelon form, we see that the set of vectors spans because there is a pivot position in every row. We also see that and . This means that , , and will span and therefore form a basis.  In the same way, we see that , , , and for a basis for .   "
},
{
  "id": "sec-bases-7-5",
  "level": "2",
  "url": "sec-bases.html#sec-bases-7-5",
  "type": "Exercise",
  "number": "19.2.5.5",
  "title": "",
  "body": " This exercise involves a simple Fourier transform, which will play an important role in the next section.  Suppose that we have the vectors .  Explain why is a basis for . Notice that you may enter into Sage as cos(pi\/6) .   If , find .  Find the matrices and . If and , explain why is the average of , , and .      The vectors are linearly independent and span .   .  Since we have , we have .     By forming the matrix and finding its reduced row echelon form, we see that the vectors are linearly independent and span . They therefore form a basis of .  We solve to find .  We have Since we have , we have .   "
},
{
  "id": "sec-bases-7-6",
  "level": "2",
  "url": "sec-bases.html#sec-bases-7-6",
  "type": "Exercise",
  "number": "19.2.5.6",
  "title": "",
  "body": " Determine whether the following statements are true or false and provide a justification for your response.  If the columns of a matrix form a basis for , then is invertible.  There must be 125 vectors in a basis for .  If is a basis of , then every vector in can be expressed as a linear combination of basis vectors.  The coordinates are the weights that form as a linear combination of basis vectors.  If the basis vectors form the columns of the matrix , then .      True  True  True  True  False     True. If the columns of form a basis, then has a pivot position in every row and every column. Therefore, the reduced row echelon form of is the identity matrix, which implies that is invertible.  True. The number of vectors in a basis of must be .  True. If is a basis, then the vectors in span , which means that every vector in can be written as a linear combination of the vectors in .  True. This is the definition of .  False. The relationship is .   "
},
{
  "id": "sec-bases-7-7",
  "level": "2",
  "url": "sec-bases.html#sec-bases-7-7",
  "type": "Exercise",
  "number": "19.2.5.7",
  "title": "",
  "body": " Provide a justification for your response to each of the following questions.  Suppose you have linearly independent vectors in . Can you guarantee that they form a basis of ?  If is an invertible matrix, do the columns necessarily form a basis of ?  Suppose we have an invertible matrix , and we perform a sequence of row operations on to form a matrix . Can you guarantee that the columns of form a basis for ?  Suppose you have a set of 10 vectors in and that every vector in can be written as a linear combination of these vectors. Can you guarantee that this set of vectors is a basis for ?     Yes  Yes  Yes  Yes     Yes. A matrix formed from linearly independent vectors in will have a pivot position in every column. Since the matrix has the same number of rows and columns, there must also be a pivot position in every row. This means that the vectors span and therefore form a basis.  Yes. An invertible matrix is row equivalent to the identity matrix, which means that the columns are linearly independent and span . This implies that the columns form a basis of .  Yes. The matrix is row equivalent to the identity matrix so must be as well. This means that the columns of form a basis for .  Yes. The span of the set of vectors is , which says that the associated matrix is square and has a pivot position in every row. Therefore, it must have a pivot position in every column, which means that the set of vectors forms a basis for .   "
},
{
  "id": "sec-bases-7-8",
  "level": "2",
  "url": "sec-bases.html#sec-bases-7-8",
  "type": "Exercise",
  "number": "19.2.5.8",
  "title": "",
  "body": " Crystallographers find it convenient to use coordinate systems that are adapted to the specific geometry of a crystal. As a two-dimensional example, consider a layer of graphite in which carbon atoms are arranged in regular hexagons to form the crystalline structure shown in .      A layer of carbon atoms in a graphite crystal.   The origin of the coordinate system is at the carbon atom labeled by 0 . It is convenient to choose the basis defined by the vectors and and the coordinate system it defines.   Locate the points for which   ,   ,   .    Find the coordinates for all the carbon atoms in the hexagon whose lower left vertex is labeled 0 .  What are the coordinates of the center of that hexagon, which is labeled C ?  How do the coordinates of the atoms in the hexagon whose lower left corner is labeled 1 compare to the coordinates in the hexagon whose lower left corner is labeled \"0\"?  Does the point whose coordinates are correspond to a carbon atom or the center of a hexagon?     The points are indicated in the figure.      .   .  The coordinates differ by   It is the center of a hexagon.      The points are indicated in .   The points requested in part a of this exercise.       Moving counterclockwise around the hexagon, the coordinates are    .  We obtain the coordinates for the hexagon with the vertex labeled 1 by adding the coordinate expression of the point 1 , which is to those of the original hexagon.  It is the center of a hexagon. Adding or subtracting to the coordinates translates one hexagon to another. This means that can be translated to , which is the center of a hexagon.   "
},
{
  "id": "sec-bases-7-9",
  "level": "2",
  "url": "sec-bases.html#sec-bases-7-9",
  "type": "Exercise",
  "number": "19.2.5.9",
  "title": "",
  "body": " Suppose that and .  Explain why is a basis for .  Find and .  Use what you found in the previous part of this problem to find and .   If , find .  Find a matrix such that .  You should find that the matrix is a very simple matrix, which means that this basis is well suited to study the effect of multiplication by . This observation is the central idea of the next chapter.    The vectors are linearly independent and span .   and .   and .   .   .     The vectors are linearly independent and span .  We compute that and .   and .  We know that , which means that . Therefore, .  If , then and . Therefore, , which says that .   "
},
{
  "id": "sec-jpeg",
  "level": "1",
  "url": "sec-jpeg.html",
  "type": "Section",
  "number": "19.3",
  "title": "Image compression",
  "body": " Image compression   Digital images, such as the photographs taken on your phone, are displayed as a rectangular array of pixels. For example, the photograph in is 1440 pixels wide and 1468 pixels high. If we were to zoom in on the photograph, we would be able to see individual pixels, such as those shown on the right.       An image stored as a array of pixels along with a close-up of a smaller array.   A lot of data is required to display this image. A quantity of digital data is frequently measured in bytes, where one byte is the amount of storage needed to record an integer between 0 and 255. As we will see shortly, each pixel requires three bytes to record that pixel's color. This means the amount of data required to display this image is bytes or about 6.3 megabytes.  Of course, we would like to store this image on a phone or computer and perhaps transmit it through our data plan to share it with others. If possible, we would like to find a way to represent this image using a smaller amount of data so that we don't run out of memory on our phone and quickly exhaust our data plan.  As we will see in this section, the JPEG compression algorithm provides a means for doing just that. This image, when stored in the JPEG format, requires only 467,359 bytes of data, which is about 7% of the 6.3 megabytes required to display the image. That is, when we display this image, we are reconstructing it from only 7% of the original data. This isn't too surprising since there is quite a bit of redundancy in the image; the left half of the image is almost uniformly blue. The JPEG algorithm detects this redundancy by representing the data using bases that are well-suited to the task.    Since we will be using various bases and the coordinate systems they define, let's review how to translate between coordinate systems.  Suppose that we have a basis for . Explain what we mean by the representation of a vector in the coordinate system defined by .  If we are given the representation , how can we recover the vector ?  If we are given the vector , how can we find ?  Suppose that is a basis for . If , find the vector .   If , find .      The components of the vector are the weights that express as a linear combination of the basis vectors; that is, if .  If we form the matrix , then .  As before, .  We find .  We find .       Color models  A color is represented digitally by a vector in . There are different ways in which we can represent colors, however, depending on whether a computer or a human will be processing the color. We will describe two of these representations, called color models , and demonstrate how they are used in the JPEG compression algorithm.  Digital displays typically create colors by blending together various amounts of red, green, and blue. We can therefore describe a color by putting its constituent amounts of red, green, and blue into a vector . The quantities , , and are each stored with one byte of information so they are integers between 0 and 255. This is called the color model.  color model   We define a basis where to define a new coordinate system with coordinates we denote , , and : . luminance  chrominance The coordinate is called luminance while and are called blue and red chrominance , respectively. In this coordinate system, luminance will vary from 0 to 255, while the chrominances vary between -127.5 and 127.5. This is known as the color model. (To be completely accurate, we should add 127.5 to the chrominance values so that they lie between 0 and 255, but we won't worry about that here.)  color model     This activity investigates these two color models, which we view as coordinate systems for describing colors.    First, we will explore the color model.   The color model.     What happens when , (pushed all the way to the left), and is allowed to vary?  What happens when , , and is allowed to vary?  How can you create black in this color model?  How can you create white?     Next, we will explore the color model.   The color model.     What happens when and (kept in the center) and is allowed to vary?  What happens when (pushed to the left), (kept in the center), and is allowed to increase between 0 and 127.5?  What happens when , , and is allowed to increase between 0 and 127.5?  How can you create black in this color model?  How can you create white?    Verify that is a basis for .    Find the matrix that converts from coordinates into coordinates. Then find the matrix that converts from coordinates back into coordinates.   Find the coordinates for the following colors and check, using the diagrams above, that the two representations agree.  Pure red is .  Pure blue is .  Pure white is .  Pure black is .    Find the coordinates for the following colors and check, using the diagrams above, that the two representations agree.   .   .   .    Write an expression for  The luminance as it depends on , , and .  The blue chrominance as it depends on , , and .  The red chrominance as it depends on , , and .   Explain how these quantities can be roughly interpreted by stating that  the luminance represents the brightness of the color.  the blue chrominance measures the amount of blue in the color.  the red chrominance measures the amount of red in the color.         Working with the color model, we find that  we produce red with varying degrees of brightness.  we produce blue with varying degrees of brightness.   .   .    Working with the color model, we find that  we produce gray with varying degrees of brightness.  we produce blue with varying degrees of brightness.  we produce red with varying degrees of brightness.   .   with .    If we row reduce the matrix whose columns are the vectors in , we obtain the identity matrix, which means that the vectors are linearly independent and span .  The matrices are and   To convert from to , we multiply by so that                To convert from to , we multiply by so that             We have The expression for is a weighted average of the , , and values. The expression for takes half the amount of and subtracts the amounts of red and green. Likewise, the expression for takes half the amount of and subtracts the amounts of green and blue.     These two color models provide us with two ways to represent colors, each of which is useful in a certain context. Digital displays, such as those in phones and computer monitors, create colors by combining various amounts of red, green, and blue. The model is therefore most relevant in digital applications.  By contrast, the color model was created based on research into human vision and aims to concentrate the most visually important data into a single coordinate, the luminance, to which our eyes are most sensitive. Of course, any basis of must have three vectors so we need two more coordinates, blue and red chrominance, if we want to represent all colors.  To see this explicitly, shown in is the original image and the image as rendered with only the luminance. That is, on the right, the color of each pixel is represented by only one byte, which is the luminance. This image essentially looks like a grayscale version of the original image with all its visual detail. In fact, before digital television became the standard, television signals were broadcast using the color model. When a signal was displayed on a black-and-white television, the luminance was displayed and the two chrominance values simply ignored.       The original image rendered with only the luminance values.   For comparison, shown in are the corresponding images created using only the blue chrominance and the red chrominance. Notice that the amount of visual detail is considerably less in these images.       The original image rendered, on the left, with only blue chrominance and, on the right, with only red chrominance.   The aim of the JPEG compression algorithm is to represent an image using the smallest amount of data possible. By converting from the color model to the color model, we are concentrating the most visually important data into the luminance values. This is helpful because we can safely ignore some of the data in the chrominance values since that data is not as visually important.    The JPEG compression algorithm  The key to representing the image using a smaller amount of data is to detect redundancies in the data. To begin, we first break the image, which is composed of pixels, into small blocks of pixels. For example, we will consider the block of pixels outlined in green in the original image, shown on the left of . The image on the right zooms in on the block.       An block of pixels outlined in green in the original image on the left. We see the same block on a smaller scale on the right.   Notice that this block, as seen in the original image, is very small. If we were to change some of the colors in this block slightly, our eyes would probably not notice.    Here we see a close-up of the block. The important point here is that the colors do not change too much over this block. In fact, we expect this to be true for most of the blocks. There will, of course, be some blocks that contain dramatic changes, such as where the sky and rock intersect, but they will be the exception.    The block under consideration.   Following our earlier work, we will change the representation of colors from the color model to the model. This separates the colors into luminance and chrominance values that we will consider separately. In , we see the luminance values of this block. Again, notice how these values do not vary significantly over the block.       The luminance values in this block.   Our strategy in the compression algorithm is to perform a change of basis to take advantage of the fact that the luminance values do not change significantly over the block. Rather than recording the luminance of each of the pixels, this change of basis will allow us to record the average luminance along with some information about how the individual colors vary from the average.  Let's look at the first column of luminance values, which is a vector in : . We will perform a change of basis and describe this vector by the average of the luminance values and information about variations from the average.   Discrete Fourier Transform The JPEG compression algorithm uses the Discrete Fourier Transform , which is defined using the basis whose basis vectors are   On first glance, this probably looks intimidating, but we can make sense of it by looking at these vectors graphically. Shown in are four of these basis vectors. Notice that is constantly 1, varies relatively slowly, varies a little more rapidly, and varies quite rapidly. The main thing to notice is that: the basis vectors vary at different rates with the first vectors varying relatively slowly and the later vectors varying more rapidly.      Four of the basis vectors , , , and .    These vectors form the basis for . Remember that is the vector of luminance values in the first column as seen on the right. We will write in the new coordinates . The coordinates are called the Fourier coefficients of the vector .      We will explore the influence that the Fourier coefficients have on the vector .   To begin, we'll look at the Fourier coefficient .   The effect of the Fourier coefficient on the vector .    Describe the effect that has on the vector . Would you describe the components in as constant, slowly varying, or rapidly varying?    By comparison, let's see how the Fourier coefficient influences .   The effect of the Fourier coefficient on the vector .    Describe the effect that has on the vector . Would you describe the components in as constant, slowly varying, or rapidly varying?    Let's now investigate how the Fourier coefficient influences the vector .   The effect of the Fourier coefficient on the vector .    Describe the effect that has on the vector . Would you describe the components in as constant, slowly varying, or rapidly varying?   If the components of vary relatively slowly, what would you expect to be true of the Fourier coefficients ?  The Sage cell below will construct the vector , which is denoted P , and its inverse , which is denoted Pinv . Evaluate this Sage cell and notice that it prints the matrix . Now look at the form of and explain why is the average of the luminance values in the vector .  The Sage cell below defines the vector , which is the vector of luminance values in the first column, as seen in . Use the cell below to find the vector of Fourier coefficients . If you have evaluated the cell above, you will still be able to refer to P and Pinv in this cell. Write the Fourier coefficients and discuss the relative sizes of the coefficients.  Let's see what happens when we simply ignore the coefficients and . Form a new vector of Fourier coefficients by rounding the coefficients to the nearest integer and setting and to zero. This is an approximation to , the vector of Fourier coefficients. Use the approximation to to form an approximation of the vector . How much does your approximation differ from the actual vector ?  When we ignore the Fourier coefficients corresponding to rapidly varying basis elements, we see that the vector that we reconstruct is very close to the original one. In fact, the luminance values in the approximation differ by at most one or two from the actual luminance values. Our eyes are not sensitive enough to detect this difference.  So far, we have concentrated on only one column in our block of luminance values. Let's now consider all of the columns. The following Sage cell defines a matrix called luminance , which is the matrix of luminance values. Find the matrix whose columns are the Fourier coefficients of the columns of luminance values.   Notice that the first row of this matrix consists of the Fourier coefficient for each of the columns. Just as we saw before, the entries in this row do not change significantly as we move across the row. In the Sage cell below, write these entries in the vector and find the corresponding Fourier coefficients.        Changing has the effect of adding a constant to the components of .  The coefficient introduces a slow variation into the components of .  The coefficient introduces a rapid variation into the components of .  The coefficients with larger values of will be small.  We have From the top row of this matrix, we see that .  We find that Notice that the largest Fourier coefficient is and that the coefficients , , , and are relatively small. This reflects the fact that the luminance does not vary rapidly.  Notice that showing that the approximation is quite good.  We have   We see that the Fourier coefficients of these Fourier coefficients is . Once again, we see that is the largest Fourier coefficient and the coefficients corresponding to rapid variations are small.     Up to this point, we have been working with the luminance values in one block of our image. We formed the Fourier coefficients for each of the columns of this block. Once we notice that the Fourier coefficients across a row are relatively constant, it seems reasonable to find the Fourier coefficients of the rows of the matrix of Fourier coefficients. Doing so leads to the matrix .  If we were to look inside a JPEG image file, we would see lots of matrices like this. For each block, there would be three matrices of Fourier coefficients of the rows of Fourier coefficients, one matrix for each of the luminance, blue chrominance, and red chrominance values. However, we store these Fourier coefficients as integers inside the JPEG file so we need to round off the coefficients to the nearest integer, as shown here: .  There are many zeroes in this matrix, and we can save space in a JPEG image file by only recording the nonzero Fourier coefficients.  In fact, when a JPEG file is created, there is a quality parameter that can be set, such as that shown in . When the quality parameter is high, we will store many of the Fourier coefficients; when it is low, we will ignore more of them.      When creating a JPEG file, we choose a value of the quality parameter.   To see how this works, suppose the quality setting is relatively high. After rounding off the Fourier coefficients, we will set all of the coefficients whose absolute value is less than 2 to zero, which creates the matrix: Notice that there are 12 nonzero Fourier coefficients, out of 64, that we need to record. Consequently, we only save of the data.  If instead, the quality setting is relatively low, we set all of the Fourier coefficients whose absolute value is less than 4 to zero, creating the matrix: . Notice that there are only 5 nonzero Fourier coefficients that we need to record now, meaning we save only of the data. This will result in a smaller JPEG file describing the image.  With a lower quality setting, we have thrown away more information about the Fourier coefficients so the image will not be reconstructed as accurately. To see this, we can reconstruct the luminance values from the Fourier coefficients by converting back into the standard coordinate system. Rather than showing the luminance values themselves, we will show the difference in the original luminance values and the reconstructed luminance values. When the quality setting was high and we stored 12 Fourier coefficients, we find this difference to be . When the quality setting is lower and we store only 5 Fourier coefficients, the difference is .  This demonstrates the trade off. With a high quality setting, we require more storage to save more of the data, but the reconstructed image is closer to the original. With the lower quality setting, we require less storage, but the reconstructed image differs more from the original.  If we remember that the visual information stored by the blue and red chrominance values is not as important as that contained in the luminance values, we feel safer in discarding more of the Fourier coefficients for the chrominance values resulting in an even greater savings.  Shown in is the original image compared to a version stored with a very low quality setting. If you look carefully, you can individual blocks.       The original image and the result of storing the image with a low quality setting.   This discussion of the JPEG compression algorithm is meant to explore the ideas that underlie its construction and demonstrate the importance of a choice of basis and its accompanying coordinate system. There are a few details, most notably about the rounding of the Fourier coefficients, that are not strictly accurate. The actual implementation is a little more complicated, but the presentation here conveys the spirit of the algorithm.  The JPEG compression algorithm allows us to store image files using only a fraction of the data. Similar ideas are used to efficiently store digital music and video files.    Summary  This section has explored how appropriate changes in bases help us reconstruct an image using only a fraction of its data. This is known as image compression.  There are several ways of representing colors, all of which use vectors in . We explored the color model, which is appropriate in digital applications, and the model, in which the most important visual information is conveyed by the component, known as luminance.  We also explored a change of basis called the Discrete Fourier Transform. In the coordinate system that results, the first coefficient measures the average of the components of a vector. Other coefficients measure variations in the components away from the average.  We put both of these ideas to use in demonstrating the JPEG compression algorithm. An image is broken into blocks, and the colors into luminance, blue chrominance, and red chrominance. Applying the Discrete Fourier Transform allows us to reconstruct a good approximation of the image using only a fraction of the original data.       Consider the vector .  In the Sage cell below is a copy of the change of basis matrices that define the Fourier transform. Find the Fourier coefficients of .   We will now form the vector , which is an approximation of by rounding all the Fourier coefficients of to the nearest integer to obtain . Now find the vector and compare this approximation to . What is the error in this approximation?  Repeat the last part of this problem, but set the rounded Fourier coefficients to zero if they have an absolute value less than five. Use it to create a second approximation of . What is the error in this approximation?  Compare the number of nonzero Fourier coefficients that you have in the two approximations and compare the accuracy of the approximations. Using a few sentences, discuss the comparisons that you find.       .        In the first approximation, the reconstructed vector agrees with the original vector . In the second approximation, there are only two nonzero Fourier coefficients, and we see that the approximating vector differs from the original vector. In the second case, however, we are using only one quarter of the Fourier coefficients. Though the entries in the vector differ from those in the original vector , the difference is probably not visually noticeable since the possible values range from 0 to 255.      .  We round the Fourier coefficients to the vector and find the approximation , which compares to the original vector . In this case, the approximation is the same as the original vector .  Now we found the Fourier coefficients to and find the approximation , which compares to the original vector . Now we see that the approximating vector differs from the original vector , though not significantly.  In the first approximation, the reconstructed vector agrees with the original vector . In the second approximation, there are only two nonzero Fourier coefficients, and we see that the approximating vector differs from the original vector. In the second case, however, we are using only one quarter of the Fourier coefficients. Though the entries in the vector differ from those in the original vector , the difference is probably not visually noticeable since the possible values range from 0 to 255.     There are several steps to the JPEG compression algorithm. The following questions examine the motivation behind some of them.  What is the overall goal of the JPEG compression algorithm?  Why do we convert colors from the the color model to the model?  Why do we decompose the image into a collection of arrays of pixels?  What role does the Discrete Fourier Transform play in the JPEG compression algorithm?  Why is the information conveyed by the rapid-variation Fourier coefficients, generally speaking, less important than the slow-variation coefficients?     The goal is to represent the image in a format that requires less storage but still allows us to reconstruct the image with most of the visual information preserved.  In the color model, the most significant visual information is stored in the channel. Therefore, we should store most of this information. The information in the and channels is not as visually significant so we can store a smaller amount of that data without noticing the effect.  As seen in an image, these blocks are relatively small, which means that the approximations we reconstruct are probably not noticeably different from the original. Also, the blocks are small enough so that the image does not typically change a lot over the block. At the same time, the blocks are large enough to allow us to save storage by throwing away some of the Fourier coefficients.  The Discrete Fourier Transform represents the data in a block as its average and variations from the average. Since the blocks are relatively small, there will typically be small variations away from the average. Therefore, we can often ignore those variations without visually detecting the approximation.  Our eyes do not detect individual pixels so we do not detect differences from one pixel to the next. This means that we do not typically notice rapid variations.     The goal is to represent the image in a format that requires less storage but still allows us to reconstruct the image with most of the visual information preserved.  In the color model, the most significant visual information is stored in the channel. Therefore, we should store most of this information. The information in the and channels is not as visually significant so we can store a smaller amount of that data without noticing the effect.  As seen in an image, these blocks are relatively small, which means that the approximations we reconstruct are probably not noticeably different from the original. Also, the blocks are small enough so that the image does not typically change a lot over the block. At the same time, the blocks are large enough to allow us to save storage by throwing away some of the Fourier coefficients.  The Discrete Fourier Transform represents the data in a block as its average and variations from the average. Since the blocks are relatively small, there will typically be small variations away from the average. Therefore, we can often ignore those variations without visually detecting the approximation.  Our eyes do not detect individual pixels so we do not detect differences from one pixel to the next. This means that we do not typically notice rapid variations.     The Fourier transform that we used in this section is often called the Discrete Fourier Cosine Transform because it is defined using a basis consisting of cosine functions. There is also a Fourier Sine Transform defined using a basis consisting of sine functions. For instance, in , the basis vectors of are We can think of these vectors graphically, as shown in .      The vectors that form the basis .    The Sage cell below defines the matrix S whose columns are the vectors in the basis as well as the matrix C whose columns form the basis used in the Fourier Cosine Transform.   In the block of luminance values we considered in this section, the first column begins with the four entries 176, 181, 165, and 139, as seen in . These form the vector . Find both and .  Write a sentence or two comparing the values for the Fourier Sine coefficients and the Fourier Cosine coefficients .  Suppose now that . Find the Fourier Sine coefficients and the Fourier Cosine coefficients .  Write a few sentences explaining why we use the Fourier Cosine Transform in the JPEG compression algorithm rather than the Fourier Sine Transform.      and .  The higher Fourier Cosine coefficients are smaller than their counterparts in the Fourier Sine transform. This means that we can ignore these coefficients in the Fourier Cosine transform without losing too much information.   and .  Generally speaking, we can safely ignore more of the coefficients in the Fourier Cosine transform. Most of the important information is channeled into the first coefficient, which records the average of the components of the vector .     We find that   The higher Fourier Cosine coefficients are smaller than their counterparts in the Fourier Sine transform. This means that we can ignore these coefficients in the Fourier Cosine transform without losing too much information.  We find that   Generally speaking, we can safely ignore more of the coefficients in the Fourier Cosine transform. Most of the important information is channeled into the first coefficient, which records the average of the components of the vector .     In , we looked at a basis for that we called the Haar wavelet basis. The basis vectors are , which may be understood graphically as in . We will denote this basis by .      The Haar wavelet basis represented graphically.   The change of coordinates from a vector in to is called the Haar wavelet transform and we write . The coefficients are called wavelet coefficients.  Let's work with the block of luminance values in the upper left corner of our larger block: .    The following Sage cell defines the matrix W whose columns are the basis vectors in . If is the first column of luminance values in the block above, find the wavelet coefficients .   Notice that gives the average value of the components of and describes how the averages of the first two and last two components differ from the overall average. The coefficients and describe small-scale variations between the first two components and last two components, respectively.  If we set the last wavelet coefficients and , we obtain the wavelet coefficients for a vector that approximates . Find the vector and compare it to the original vector .  What impact does the fact that and have on the form of the vector ? Explain how setting these coefficients to zero ignores the behavior of on a small scale.  In the JPEG compression algorithm, we looked at the Fourier coefficients of all the columns of luminance values and then performed a Fourier transform on the rows. The Sage cell below will perform the same operation using the wavelet transform; that is, it will first find the wavelet coefficients of each of the columns and then perform the wavelet transform on the rows. You only need to evaluate the cell to find the wavelet coefficients obtained in this way.   Now set all the wavelet coefficients equal to zero except those in the upper left block and use them to define the matrix coeffs in the Sage cell below. This has the effect of ignoring all of the small-scale differences. Evaluating this cell will recover the approximate luminance values.    Explain how the wavelet transform and this approximation can be used to create a lower resolution version of the image.   This kind of wavelet transform is the basis of the JPEG 2000 compression algorithm, which is an alternative to the usual JPEG algorithm.     .   .  The first two components are equal as are the last two components.  We find the wavelet coefficients   We find the approximate luminance values to be   The luminance values are constant on blocks. It is as if the image is represented with pixels that are twice as wide and twice as tall.     We find that .  We have the approximation and .  The first two components are equal as are the last two components. Since we see no difference in these components, we have lost the information that differentiates the components on a small scale.  We find the wavelet coefficients   We find the approximate luminance values to be   The luminance values are constant on blocks. It is as if the image is represented with pixels that are twice as wide and twice as tall.     In this section, we looked at the and color models. In this exercise, we will look at the color model where is the hue, is the saturation, and is the value of the color. All three quantities vary between 0 and 255.   The color model.     If you leave and at some fixed values, what happens when you change the value of ?  Increase the value to 255, which is on the far right. Describe what happens when you vary the saturation using a fixed hue and value .  Describe what happens when and are fixed and varies.  How can you create white in this color model?  How can you create black in this color model?  Find an approximate range of hues that correspond to blue.  Find an approximate range of hues that correspond to green.   The color model concentrates the most important visual information in the luminance coordinate, which roughly measures the brightness of the color. The other two coordinates describe the hue of the color. By contrast, the color model concentrates all the information about the hue in the coordinate.  This is useful in computer vision applications. For instance, if we want a robot to detect a blue ball in its field of vision, we can specify a range of hue values to search for. If the lighting changes in the room, the saturation and value may change, but the hue will not. This increases the likelihood that the robot will still detect the blue ball across a wide range of lighting conditions.    The color changes but not the brightness.  The color appears more washed out with a low saturation and appears more vibrant with a high saturation.  The brightness changes.  Set the saturation and the value .  Set .   .   .     The color changes but not the brightness.  The color appears more washed out with a low saturation and appears more vibrant with a high saturation.  The brightness changes.  Set the saturation and the value .  Set .   .   .     "
},
{
  "id": "fig-jpeg-orig",
  "level": "2",
  "url": "sec-jpeg.html#fig-jpeg-orig",
  "type": "Figure",
  "number": "19.3.1",
  "title": "",
  "body": "     An image stored as a array of pixels along with a close-up of a smaller array.  "
},
{
  "id": "sec-jpeg-2-6",
  "level": "2",
  "url": "sec-jpeg.html#sec-jpeg-2-6",
  "type": "Preview Activity",
  "number": "19.3.1",
  "title": "",
  "body": "  Since we will be using various bases and the coordinate systems they define, let's review how to translate between coordinate systems.  Suppose that we have a basis for . Explain what we mean by the representation of a vector in the coordinate system defined by .  If we are given the representation , how can we recover the vector ?  If we are given the vector , how can we find ?  Suppose that is a basis for . If , find the vector .   If , find .      The components of the vector are the weights that express as a linear combination of the basis vectors; that is, if .  If we form the matrix , then .  As before, .  We find .  We find .    "
},
{
  "id": "sec-jpeg-3-5",
  "level": "2",
  "url": "sec-jpeg.html#sec-jpeg-3-5",
  "type": "Activity",
  "number": "19.3.2",
  "title": "",
  "body": "  This activity investigates these two color models, which we view as coordinate systems for describing colors.    First, we will explore the color model.   The color model.     What happens when , (pushed all the way to the left), and is allowed to vary?  What happens when , , and is allowed to vary?  How can you create black in this color model?  How can you create white?     Next, we will explore the color model.   The color model.     What happens when and (kept in the center) and is allowed to vary?  What happens when (pushed to the left), (kept in the center), and is allowed to increase between 0 and 127.5?  What happens when , , and is allowed to increase between 0 and 127.5?  How can you create black in this color model?  How can you create white?    Verify that is a basis for .    Find the matrix that converts from coordinates into coordinates. Then find the matrix that converts from coordinates back into coordinates.   Find the coordinates for the following colors and check, using the diagrams above, that the two representations agree.  Pure red is .  Pure blue is .  Pure white is .  Pure black is .    Find the coordinates for the following colors and check, using the diagrams above, that the two representations agree.   .   .   .    Write an expression for  The luminance as it depends on , , and .  The blue chrominance as it depends on , , and .  The red chrominance as it depends on , , and .   Explain how these quantities can be roughly interpreted by stating that  the luminance represents the brightness of the color.  the blue chrominance measures the amount of blue in the color.  the red chrominance measures the amount of red in the color.         Working with the color model, we find that  we produce red with varying degrees of brightness.  we produce blue with varying degrees of brightness.   .   .    Working with the color model, we find that  we produce gray with varying degrees of brightness.  we produce blue with varying degrees of brightness.  we produce red with varying degrees of brightness.   .   with .    If we row reduce the matrix whose columns are the vectors in , we obtain the identity matrix, which means that the vectors are linearly independent and span .  The matrices are and   To convert from to , we multiply by so that                To convert from to , we multiply by so that             We have The expression for is a weighted average of the , , and values. The expression for takes half the amount of and subtracts the amounts of red and green. Likewise, the expression for takes half the amount of and subtracts the amounts of green and blue.    "
},
{
  "id": "fig-jpeg-luminance",
  "level": "2",
  "url": "sec-jpeg.html#fig-jpeg-luminance",
  "type": "Figure",
  "number": "19.3.4",
  "title": "",
  "body": "     The original image rendered with only the luminance values.  "
},
{
  "id": "fig-jpeg-chrominance",
  "level": "2",
  "url": "sec-jpeg.html#fig-jpeg-chrominance",
  "type": "Figure",
  "number": "19.3.5",
  "title": "",
  "body": "     The original image rendered, on the left, with only blue chrominance and, on the right, with only red chrominance.  "
},
{
  "id": "fig-jpeg-block",
  "level": "2",
  "url": "sec-jpeg.html#fig-jpeg-block",
  "type": "Figure",
  "number": "19.3.6",
  "title": "",
  "body": "     An block of pixels outlined in green in the original image on the left. We see the same block on a smaller scale on the right.  "
},
{
  "id": "fig-jpeg-block-zoom",
  "level": "2",
  "url": "sec-jpeg.html#fig-jpeg-block-zoom",
  "type": "Figure",
  "number": "19.3.7",
  "title": "",
  "body": "  Here we see a close-up of the block. The important point here is that the colors do not change too much over this block. In fact, we expect this to be true for most of the blocks. There will, of course, be some blocks that contain dramatic changes, such as where the sky and rock intersect, but they will be the exception.    The block under consideration.  "
},
{
  "id": "fig-jpeg-block-luminance",
  "level": "2",
  "url": "sec-jpeg.html#fig-jpeg-block-luminance",
  "type": "Figure",
  "number": "19.3.8",
  "title": "",
  "body": "     The luminance values in this block.  "
},
{
  "id": "fig-jpeg-fourier-basis",
  "level": "2",
  "url": "sec-jpeg.html#fig-jpeg-fourier-basis",
  "type": "Figure",
  "number": "19.3.9",
  "title": "",
  "body": "    Four of the basis vectors , , , and .  "
},
{
  "id": "sec-jpeg-4-14",
  "level": "2",
  "url": "sec-jpeg.html#sec-jpeg-4-14",
  "type": "Activity",
  "number": "19.3.3",
  "title": "",
  "body": "  We will explore the influence that the Fourier coefficients have on the vector .   To begin, we'll look at the Fourier coefficient .   The effect of the Fourier coefficient on the vector .    Describe the effect that has on the vector . Would you describe the components in as constant, slowly varying, or rapidly varying?    By comparison, let's see how the Fourier coefficient influences .   The effect of the Fourier coefficient on the vector .    Describe the effect that has on the vector . Would you describe the components in as constant, slowly varying, or rapidly varying?    Let's now investigate how the Fourier coefficient influences the vector .   The effect of the Fourier coefficient on the vector .    Describe the effect that has on the vector . Would you describe the components in as constant, slowly varying, or rapidly varying?   If the components of vary relatively slowly, what would you expect to be true of the Fourier coefficients ?  The Sage cell below will construct the vector , which is denoted P , and its inverse , which is denoted Pinv . Evaluate this Sage cell and notice that it prints the matrix . Now look at the form of and explain why is the average of the luminance values in the vector .  The Sage cell below defines the vector , which is the vector of luminance values in the first column, as seen in . Use the cell below to find the vector of Fourier coefficients . If you have evaluated the cell above, you will still be able to refer to P and Pinv in this cell. Write the Fourier coefficients and discuss the relative sizes of the coefficients.  Let's see what happens when we simply ignore the coefficients and . Form a new vector of Fourier coefficients by rounding the coefficients to the nearest integer and setting and to zero. This is an approximation to , the vector of Fourier coefficients. Use the approximation to to form an approximation of the vector . How much does your approximation differ from the actual vector ?  When we ignore the Fourier coefficients corresponding to rapidly varying basis elements, we see that the vector that we reconstruct is very close to the original one. In fact, the luminance values in the approximation differ by at most one or two from the actual luminance values. Our eyes are not sensitive enough to detect this difference.  So far, we have concentrated on only one column in our block of luminance values. Let's now consider all of the columns. The following Sage cell defines a matrix called luminance , which is the matrix of luminance values. Find the matrix whose columns are the Fourier coefficients of the columns of luminance values.   Notice that the first row of this matrix consists of the Fourier coefficient for each of the columns. Just as we saw before, the entries in this row do not change significantly as we move across the row. In the Sage cell below, write these entries in the vector and find the corresponding Fourier coefficients.        Changing has the effect of adding a constant to the components of .  The coefficient introduces a slow variation into the components of .  The coefficient introduces a rapid variation into the components of .  The coefficients with larger values of will be small.  We have From the top row of this matrix, we see that .  We find that Notice that the largest Fourier coefficient is and that the coefficients , , , and are relatively small. This reflects the fact that the luminance does not vary rapidly.  Notice that showing that the approximation is quite good.  We have   We see that the Fourier coefficients of these Fourier coefficients is . Once again, we see that is the largest Fourier coefficient and the coefficients corresponding to rapid variations are small.    "
},
{
  "id": "fig-jpeg-quality",
  "level": "2",
  "url": "sec-jpeg.html#fig-jpeg-quality",
  "type": "Figure",
  "number": "19.3.13",
  "title": "",
  "body": "    When creating a JPEG file, we choose a value of the quality parameter.  "
},
{
  "id": "fig-jpeg-image-low",
  "level": "2",
  "url": "sec-jpeg.html#fig-jpeg-image-low",
  "type": "Figure",
  "number": "19.3.14",
  "title": "",
  "body": "     The original image and the result of storing the image with a low quality setting.  "
},
{
  "id": "sec-jpeg-6-1",
  "level": "2",
  "url": "sec-jpeg.html#sec-jpeg-6-1",
  "type": "Exercise",
  "number": "19.3.4.1",
  "title": "",
  "body": " Consider the vector .  In the Sage cell below is a copy of the change of basis matrices that define the Fourier transform. Find the Fourier coefficients of .   We will now form the vector , which is an approximation of by rounding all the Fourier coefficients of to the nearest integer to obtain . Now find the vector and compare this approximation to . What is the error in this approximation?  Repeat the last part of this problem, but set the rounded Fourier coefficients to zero if they have an absolute value less than five. Use it to create a second approximation of . What is the error in this approximation?  Compare the number of nonzero Fourier coefficients that you have in the two approximations and compare the accuracy of the approximations. Using a few sentences, discuss the comparisons that you find.       .        In the first approximation, the reconstructed vector agrees with the original vector . In the second approximation, there are only two nonzero Fourier coefficients, and we see that the approximating vector differs from the original vector. In the second case, however, we are using only one quarter of the Fourier coefficients. Though the entries in the vector differ from those in the original vector , the difference is probably not visually noticeable since the possible values range from 0 to 255.      .  We round the Fourier coefficients to the vector and find the approximation , which compares to the original vector . In this case, the approximation is the same as the original vector .  Now we found the Fourier coefficients to and find the approximation , which compares to the original vector . Now we see that the approximating vector differs from the original vector , though not significantly.  In the first approximation, the reconstructed vector agrees with the original vector . In the second approximation, there are only two nonzero Fourier coefficients, and we see that the approximating vector differs from the original vector. In the second case, however, we are using only one quarter of the Fourier coefficients. Though the entries in the vector differ from those in the original vector , the difference is probably not visually noticeable since the possible values range from 0 to 255.   "
},
{
  "id": "sec-jpeg-6-2",
  "level": "2",
  "url": "sec-jpeg.html#sec-jpeg-6-2",
  "type": "Exercise",
  "number": "19.3.4.2",
  "title": "",
  "body": " There are several steps to the JPEG compression algorithm. The following questions examine the motivation behind some of them.  What is the overall goal of the JPEG compression algorithm?  Why do we convert colors from the the color model to the model?  Why do we decompose the image into a collection of arrays of pixels?  What role does the Discrete Fourier Transform play in the JPEG compression algorithm?  Why is the information conveyed by the rapid-variation Fourier coefficients, generally speaking, less important than the slow-variation coefficients?     The goal is to represent the image in a format that requires less storage but still allows us to reconstruct the image with most of the visual information preserved.  In the color model, the most significant visual information is stored in the channel. Therefore, we should store most of this information. The information in the and channels is not as visually significant so we can store a smaller amount of that data without noticing the effect.  As seen in an image, these blocks are relatively small, which means that the approximations we reconstruct are probably not noticeably different from the original. Also, the blocks are small enough so that the image does not typically change a lot over the block. At the same time, the blocks are large enough to allow us to save storage by throwing away some of the Fourier coefficients.  The Discrete Fourier Transform represents the data in a block as its average and variations from the average. Since the blocks are relatively small, there will typically be small variations away from the average. Therefore, we can often ignore those variations without visually detecting the approximation.  Our eyes do not detect individual pixels so we do not detect differences from one pixel to the next. This means that we do not typically notice rapid variations.     The goal is to represent the image in a format that requires less storage but still allows us to reconstruct the image with most of the visual information preserved.  In the color model, the most significant visual information is stored in the channel. Therefore, we should store most of this information. The information in the and channels is not as visually significant so we can store a smaller amount of that data without noticing the effect.  As seen in an image, these blocks are relatively small, which means that the approximations we reconstruct are probably not noticeably different from the original. Also, the blocks are small enough so that the image does not typically change a lot over the block. At the same time, the blocks are large enough to allow us to save storage by throwing away some of the Fourier coefficients.  The Discrete Fourier Transform represents the data in a block as its average and variations from the average. Since the blocks are relatively small, there will typically be small variations away from the average. Therefore, we can often ignore those variations without visually detecting the approximation.  Our eyes do not detect individual pixels so we do not detect differences from one pixel to the next. This means that we do not typically notice rapid variations.   "
},
{
  "id": "sec-jpeg-6-3",
  "level": "2",
  "url": "sec-jpeg.html#sec-jpeg-6-3",
  "type": "Exercise",
  "number": "19.3.4.3",
  "title": "",
  "body": " The Fourier transform that we used in this section is often called the Discrete Fourier Cosine Transform because it is defined using a basis consisting of cosine functions. There is also a Fourier Sine Transform defined using a basis consisting of sine functions. For instance, in , the basis vectors of are We can think of these vectors graphically, as shown in .      The vectors that form the basis .    The Sage cell below defines the matrix S whose columns are the vectors in the basis as well as the matrix C whose columns form the basis used in the Fourier Cosine Transform.   In the block of luminance values we considered in this section, the first column begins with the four entries 176, 181, 165, and 139, as seen in . These form the vector . Find both and .  Write a sentence or two comparing the values for the Fourier Sine coefficients and the Fourier Cosine coefficients .  Suppose now that . Find the Fourier Sine coefficients and the Fourier Cosine coefficients .  Write a few sentences explaining why we use the Fourier Cosine Transform in the JPEG compression algorithm rather than the Fourier Sine Transform.      and .  The higher Fourier Cosine coefficients are smaller than their counterparts in the Fourier Sine transform. This means that we can ignore these coefficients in the Fourier Cosine transform without losing too much information.   and .  Generally speaking, we can safely ignore more of the coefficients in the Fourier Cosine transform. Most of the important information is channeled into the first coefficient, which records the average of the components of the vector .     We find that   The higher Fourier Cosine coefficients are smaller than their counterparts in the Fourier Sine transform. This means that we can ignore these coefficients in the Fourier Cosine transform without losing too much information.  We find that   Generally speaking, we can safely ignore more of the coefficients in the Fourier Cosine transform. Most of the important information is channeled into the first coefficient, which records the average of the components of the vector .   "
},
{
  "id": "sec-jpeg-6-4",
  "level": "2",
  "url": "sec-jpeg.html#sec-jpeg-6-4",
  "type": "Exercise",
  "number": "19.3.4.4",
  "title": "",
  "body": " In , we looked at a basis for that we called the Haar wavelet basis. The basis vectors are , which may be understood graphically as in . We will denote this basis by .      The Haar wavelet basis represented graphically.   The change of coordinates from a vector in to is called the Haar wavelet transform and we write . The coefficients are called wavelet coefficients.  Let's work with the block of luminance values in the upper left corner of our larger block: .    The following Sage cell defines the matrix W whose columns are the basis vectors in . If is the first column of luminance values in the block above, find the wavelet coefficients .   Notice that gives the average value of the components of and describes how the averages of the first two and last two components differ from the overall average. The coefficients and describe small-scale variations between the first two components and last two components, respectively.  If we set the last wavelet coefficients and , we obtain the wavelet coefficients for a vector that approximates . Find the vector and compare it to the original vector .  What impact does the fact that and have on the form of the vector ? Explain how setting these coefficients to zero ignores the behavior of on a small scale.  In the JPEG compression algorithm, we looked at the Fourier coefficients of all the columns of luminance values and then performed a Fourier transform on the rows. The Sage cell below will perform the same operation using the wavelet transform; that is, it will first find the wavelet coefficients of each of the columns and then perform the wavelet transform on the rows. You only need to evaluate the cell to find the wavelet coefficients obtained in this way.   Now set all the wavelet coefficients equal to zero except those in the upper left block and use them to define the matrix coeffs in the Sage cell below. This has the effect of ignoring all of the small-scale differences. Evaluating this cell will recover the approximate luminance values.    Explain how the wavelet transform and this approximation can be used to create a lower resolution version of the image.   This kind of wavelet transform is the basis of the JPEG 2000 compression algorithm, which is an alternative to the usual JPEG algorithm.     .   .  The first two components are equal as are the last two components.  We find the wavelet coefficients   We find the approximate luminance values to be   The luminance values are constant on blocks. It is as if the image is represented with pixels that are twice as wide and twice as tall.     We find that .  We have the approximation and .  The first two components are equal as are the last two components. Since we see no difference in these components, we have lost the information that differentiates the components on a small scale.  We find the wavelet coefficients   We find the approximate luminance values to be   The luminance values are constant on blocks. It is as if the image is represented with pixels that are twice as wide and twice as tall.   "
},
{
  "id": "sec-jpeg-6-5",
  "level": "2",
  "url": "sec-jpeg.html#sec-jpeg-6-5",
  "type": "Exercise",
  "number": "19.3.4.5",
  "title": "",
  "body": " In this section, we looked at the and color models. In this exercise, we will look at the color model where is the hue, is the saturation, and is the value of the color. All three quantities vary between 0 and 255.   The color model.     If you leave and at some fixed values, what happens when you change the value of ?  Increase the value to 255, which is on the far right. Describe what happens when you vary the saturation using a fixed hue and value .  Describe what happens when and are fixed and varies.  How can you create white in this color model?  How can you create black in this color model?  Find an approximate range of hues that correspond to blue.  Find an approximate range of hues that correspond to green.   The color model concentrates the most important visual information in the luminance coordinate, which roughly measures the brightness of the color. The other two coordinates describe the hue of the color. By contrast, the color model concentrates all the information about the hue in the coordinate.  This is useful in computer vision applications. For instance, if we want a robot to detect a blue ball in its field of vision, we can specify a range of hue values to search for. If the lighting changes in the room, the saturation and value may change, but the hue will not. This increases the likelihood that the robot will still detect the blue ball across a wide range of lighting conditions.    The color changes but not the brightness.  The color appears more washed out with a low saturation and appears more vibrant with a high saturation.  The brightness changes.  Set the saturation and the value .  Set .   .   .     The color changes but not the brightness.  The color appears more washed out with a low saturation and appears more vibrant with a high saturation.  The brightness changes.  Set the saturation and the value .  Set .   .   .   "
},
{
  "id": "sec-determinants",
  "level": "1",
  "url": "sec-determinants.html",
  "type": "Section",
  "number": "19.4",
  "title": "Determinants",
  "body": " Determinants   As invertibility plays a central role in this chapter, we need a criterion that tells us when a matrix is invertible. We already know that a square matrix is invertible if and only if it is row equivalent to the identity matrix. In this section, we will develop a second, numerical criterion that tells us when a square matrix is invertible.  To begin, let's consider a matrix whose columns are vectors and . We have frequently drawn the vectors and studied the linear combinations they form using a figure such as .      Linear combinations of two vectors and form a collection of congruent parallelograms.   Notice how the linear combinations form a set of congruent parallelograms in the plane. In this section, we will use the area of these parallelograms to define a numerical quantity called the determinant that tells us whether the matrix is invertible.   To recall, the area of parallelogram is found by multiplying the length of one side by the perpendicular distance to its parallel side. Using the notation in the figure, the area of the parallelogram is .      We will explore the area formula in this preview activity.  Find the area of the following parallelograms.    1.   2.   3.     4.   5.            Explain why the area of the parallelogram formed by the vectors and is the same as that formed by and .         We find the following areas.  A square has area 1.  A rectangle has area 6.  The square has side length giving an area of 2.  If we consider the horizontal length as the base, we see that so that the area is 4.  In the same way, we can consider both the base and height to be 2 so that the area is 4.    If we consider the base to be the length of , then the height, which is the perpendicular distance to its parallel side, is the same in both parallelograms.       Determinants of matrices  We will begin by defining the determinant of a matrix . First, however, we need to define the orientation of an ordered pair of vectors. As shown in , an ordered pair of vectors and is called positively oriented if the angle, measured in the counterclockwise direction, from to is less than ; we say the pair is negatively oriented if it is more than .      The vectors on the left are positively oriented while the ones on the right are negatively oriented.     determinant  Suppose a matrix has columns and . If the pair of vectors is positively oriented, then the determinant of , denoted , is the area of the parallelogram formed by and . If the pair is negatively oriented, then is -1 times the area of the parallelogram.      Consider the determinant of the identity matrix . As seen on the left of , the vectors and form a positively oriented pair. Since the parallelogram they form is a square, we have       The determinant , as seen on the left. On the right, we see that where is the matrix whose columns are shown.   Now consider the matrix . As seen on the right of , the vectors and form a negatively oriented pair. The parallelogram they define is a rectangle so we have .      In this activity, we will find the determinant of some simple matrices and discover some important properties of determinants.   The geometric meaning of the determinant of a matrix.      Use the diagram to find the determinant of the matrix . Along with , what does this lead you to believe is generally true about the determinant of a diagonal matrix?  Use the diagram to find the determinant of the matrix . What is the geometric effect of the matrix transformation defined by this matrix?  Use the diagram to find the determinant of the matrix . More generally, what do you notice about the determinant of any matrix of the form ? What does this say about the determinant of an upper triangular matrix?  Use the diagram to find the determinant of any matrix of the form . What does this say about the determinant of a lower triangular matrix?  Use the diagram to find the determinant of the matrix . In general, what is the determinant of a matrix whose columns are linearly dependent?  Consider the matrices . Use the diagram to find the determinants of , , and . What does this suggest is generally true about the relationship of to and ?      The determinant is because the vectors are negatively oriented and the rectangle has sides of length and . The determinant of a diagonal matrix seems to be the product of the diagonal entries.  The matrix transformation is a reflection over the line and we see that the determinant is .  The determinant will continue to be for any value of . This illustrates the fact that the determinant of an upper triangular matrix equals the product of its diagonal entries.  The same reasoning tells us that this determinant is and, in fact, the determinant of a lower triangular matrix equals the product of its diagonal entries.  The determinant of this matrix is because the parallelogram formed by the vector has no area. This suggests that the determinant of a matrix whose columns are linearly dependent is .  We find that , , and . This suggests that .     Later in this section, we will learn an algebraic technique for computing determinants. In the meantime, we will simply note that we can define determinants for matrices by measuring the volume of a box defined by the columns of the matrix, even if this box resides in for some very large .   For example, the columns of a matrix will form a parallelpiped, like the one shown here, and there is a means by which we can classify sets of such vectors as either positively or negatively oriented. Therefore, we can define the determinant in terms of the volume of the parallelpiped, but we will not worry about the details here.    Though the previous activity deals with determinants of matrices, it illustrates some important properties of determinants that are true more generally.  If is a triangular matrix, then equals the product of the entries on the diagonal. For example, , since the two parallelograms in have equal area.       The determinant of a triangular matrix equals the product of its diagonal entries.    We also saw that because the columns form a negatively oriented pair. You may remember from that a matrix such as this is obtained by interchanging two rows of the identity matrix.  The determinant satisfies a multiplicative property, which says that Rather than simply thinking of the determinant as the area of a parallelogram, we may also think of it as a factor by which areas are scaled under the matrix transformation defined by the matrix. Applying the matrix transformation defined by will scale area by . If we then compose with the matrix transformation defined by , area will scale a second time by the factor . The net effect is that the matrix transformation defined by scales area by so that .      The determinant satisfies these properties:   The determinant of a triangular matrix equals the product of its diagonal entries.    If is obtained by interchanging two rows of the identity matrix, then .     .         Determinants and invertibility  Perhaps the most important property of determinants also appeared in the previous activity. We saw that when the columns of the matrix are linearly dependent, the parallelogram formed by those vectors folds down onto a line. For instance, if , then the resulting parallelogram, as shown in , has zero area, which means that .   When the columns of are linearly dependent, we find that .      The condition that the columns of are linearly dependent is precisely the same as the condition that is not invertible. This leads us to believe that is not invertible if and only if its determinant is zero. The following proposition expresses this thought.    The matrix is invertible if and only if .    To understand this proposition more fully, let's remember that the matrix is invertible if and only if it is row equivalent to the identity matrix . We will therefore consider how the determinant changes when we perform row operations on a matrix. Along the way, we will discover an effective means to compute the determinant.  In , we saw how to describe the three row operations, scaling, interchange, and replacement, using matrix multiplication. If we perform a row operation on the matrix to obtain the matrix , we would like to relate and . To do so, remember that   Scalings are performed by multiplying a matrix by a diagonal matrix, such as which has the effect of multiplying the second row of by to obtain . Since is diagonal, we know that its determinant is the product of its diagonal entries so that . This means that and therefore In general, if we scale a row of by , we have .  Interchanges are performed by matrices such as which has the effect of interchanging the first and second rows of . As we saw in , . Therefore, when , we have In other words, when we perform an interchange.  Row replacement operations are performed by matrices such as which multiplies the first row by and adds the result to the third row. Since this is a lower triangular matrix, we know that the determinant is the product of the diagonal entries, which says that . This means that when , we have . In other words, a row replacement does not change the determinant.     The effect of row operations on the determinant      If is obtained from by scaling a row by , then .    If is obtained from by interchanging two rows, then .    If is obtained from by performing a row replacement operation, then .         We will investigate the connection between the determinant of a matrix and its invertibility using Gaussian elimination.  Consider the two upper triangular matrices Remembering , which of the matrices and are invertible? What are the determinants and ?  Explain why an upper triangular matrix is invertible if and only if its determinant is not zero.  Let's now consider the matrix and begin the Gaussian elimination process with a row replacement operation . What is the relationship between and ?  Next we perform another row replacement operation: . What is the relationship between and ?  Finally, we perform an interchange: to arrive at an upper triangular matrix . What is the relationship between and ?  Since is upper triangular, we can compute its determinant, which allows us to find . What is ? Is invertible?  Now consider the matrix Perform a sequence of row operations to find an upper triangular matrix that is row equivalent to . Use this to determine and whether is invertible.  Suppose we apply a sequence of row operations on a matrix to obtain . Explain why if and only if .  Explain why an matrix is invertible if and only if .      The matrix is invertible because we see there is a pivot position in every row and column. The matrix , however, is not invertible because there is not a pivot position in the third row. Also, and .  The determinant of an upper triangular matrix equals the product of its diagonal entries. Consequently, if the determinant of an upper triangular matrix is not zero, then each of its diagonal entries must be nonzero. In this case, there is a pivot position in every row and every column so that the matrix is invertible.  Row replacement operations do not change the determinant so .  In the same way, .  Interchanges change the sign of the determinant so .  The determinant since it is the product of the diagonal entries of . This means that . We see that is invertible because , which has a pivot position in every row and every column, is invertible.  Beginning with a row replacement operation, we arrive at . We next scale the second row by to obtain . Another row replacement operation gives . Putting these operations together, we see that . In this case, is not invertible because , which has a row without a pivot position, is not invertible.  Performing one of the three row operations either leaves the determinant unchanged (row replacement), changes its sign (interchange), or multiplies it by a nonzero number (scaling). Therefore, if we begin with a matrix whose determinant is not zero, the determinant remains nonzero after any row operation is applied.  If we apply a sequence of row operations to to find a row equivalent matrix that is upper triangular, we know that if and only if . We also know that is invertible if and only if is invertible. Putting these facts together, we conclude that if and only if is invertible.     As seen in this activity, row operations can be used to compute the determinant of a matrix. More specifically, applying the forward substitution phase of Gaussian elimination to the matrix leads us to an upper triangular matrix so that .  We know that is invertible when all of its diagonal entries are nonzero. We also know that under the same condition. This tells us is invertible if and only if .  Now if , we also have since applying a sequence of row operations to only multiplies the determinant by a nonzero number. It then follows that is invertible so . Therefore, we also know that and so must also be invertible.  This explains and so we know that is invertible if and only if .  Finally, notice that if is invertible, we have , which tells us that Therefore, .    If is an invertible matrix, then .      Cofactor expansions  We now have a technique for computing the determinant of a matrix using row operations. There is another way to compute determinants, using what are called cofactor expansions , that will be important for us in the next chapter. We will describe this method here.  To begin, the determinant of a matrix is . With a little bit of work, it can be shown that this number is the same as the signed area of the parallelogram we introduced earlier.  Using a cofactor expansion to find the determinant of a more general matrix is a little more work so we will demonstrate it with an example.    We illustrate how to use a cofactor expansion to find the determinant of where   To begin, we choose one row or column. It doesn't matter which we choose because the result will be the same in any case. Here, we choose the second row .  The determinant will be found by creating a sum of terms, one for each entry in the row we have chosen. For each entry in the row, we form its term by multiplying   where and are the row and column numbers, respectively, of the entry,  the entry itself, and  the determinant of the entries left over when we have crossed out the row and column containing the entry.     Since we are computing the determinant of this matrix using the second row, the entry in the first column of this row is . Let's see how to form the term from this entry.  The term itself is , and the matrix that is left over when we cross out the second row and first column is whose determinant is . Since this entry is in the second row and first column, the term we construct is .  Putting this together, we find the determinant to be . Notice that this agrees with the determinant that we found for this matrix using row operations in the last activity.      We will explore cofactor expansions through some examples.  Using a cofactor expansion, show that the determinant of the following matrix . Remember that you can choose any row or column to create the expansion, but the choice of a particular row or column may simplify the computation.  Use a cofactor expansion to find the determinant of . Explain how the cofactor expansion technique shows that the determinant of a triangular matrix is equal to the product of its diagonal entries.  Use a cofactor expansion to determine whether the following vectors form a basis of : .  Sage will compute the determinant of a matrix A with the command A.det() . Use Sage to find the determinant of the matrix .       We will using a cofactor expanion along the first row so that   Expanding along the first row gives   We form the matrix whose columns are the three given vectors. Expanding along either the second row or third column to take advantage of the zero in the entry, we see that , which means that is not invertible. Therefore, the vectors do not form a basis for .  Sage tells us that .       Summary  In this section, we associated a numerical quantity, the determinant, to a square matrix and showed how it tells us whether the matrix is invertible.  The determinant of a matrix has a geometric interpretation. In particular, when , the determinant is the signed area of the parallelogram formed by the two columns of the matrix.  The determinant satisfies many properties. For instance, and the determinant of a triangular matrix is equal to the product of its diagonal entries.  These properties helped us compute the determinant of a matrix using row operations. This also led to the important observation that the determinant of a matrix is nonzero if and only if the matrix is invertible.  Finally, we learned how to compute the determinant of a matrix using cofactor expansions, which will be a valuable tool for us in the next chapter.   We have seen three ways to compute the determinant: by interpreting the determinant as a signed area or volume; by applying appropriate row operations; and by using a cofactor expansion. It's worth spending a moment to think about the relative merits of these approaches.  The geometric definition of the determinant tells us that the determinant is measuring a natural geometric quantity, an insight that does not easily come through the other two approaches. The intuition we gain by thinking about the determinant geometrically makes it seem reasonable that the determinant should be zero for matrices that are not invertible: if the columns are linearly dependent, the vectors cannot create a positive volume.  Approaching the determinant through row operations provides an effective means of computing the determinant. In fact, this is what most computer programs do behind the scenes when they compute a determinant. This approach is also a useful theoretical tool for explaining why the determinant tells us whether a matrix is invertible.  The cofactor expansion method will be useful to us in the next chapter when we look at eigenvalues and eigenvectors. It is not, however, a practical way to compute a determinant. To see why, consider the fact that the determinant of a matrix, written as , requires us to compute two terms, and . To compute the determinant of a matrix, we need to compute three determinants, which involves terms. For a matrix, we need to compute four determinants, which produces terms. Continuing in this way, we see that the cofactor expansion of a matrix would involve terms.  By contrast, we have seen that the number of steps required to perform Gaussian elimination on an matrix is proportional to . When , we have , which points to the fact that finding the determinant using Gaussian elimination is considerably less work.     Consider the matrices .  Find the determinants of and using row operations.   Now find the determinants of and using cofactor expansions to verify your results     and .   We find that and .    This exercise concerns rotations and reflections in .  Suppose that is the matrix that performs a counterclockwise rotation in . Draw a typical picture of the vectors that form the columns of and use the geometric definition of the determinant to determine .  Suppose that is the matrix that performs a reflection in a line passing through the origin. Draw a typical picture of the columns of and use the geometric definition of the determinant to determine .  As we saw in , the matrices have the form . Compute the determinants of and and verify that they agree with what you found in the earlier parts of this exercise.      .   .     The vectors and that form the columns of are found by rotating the standard basis vectors and . Consequently, they are positively oriented and form a square. This says that .  The vectors and that form the columns of are found by reflecting the standard basis vectors and . Consequently, they are negatively oriented and form a square. This says that .  We see that and .     In the next chapter, we will say that matrices and are similar if there is a matrix such that .  Suppose that and are matrices and that there is a matrix such that . Explain why .  Suppose that is a matrix and that there is a matrix such that . Find .       because .   .      .   since .     Consider the matrix where is a parameter.  Find an expression for in terms of the parameter .  Use your expression for to determine the values of for which the vectors are linearly independent.          .     We see that .  The vectors are linearly independent when the matrix is invertible, which means that . Therefore, the vectors are linearly independent when .     Determine whether the following statements are true or false and explain your response.  If we have a square matrix and multiply the first row by and add it to the third row to obtain , then .  If we interchange two rows of a matrix, then the determinant is unchanged.  If we scale a row of the matrix by to obtain , then .  If and are row equivalent and , then also.  If is row equivalent to the identity matrix, then .      False  False  True  True  False     False. This is a row replacement operation, which leaves the determinant unchanged.  False. Applying an interchange operation changes the sign of the determinant.  True. Scaling a row of by multiplies the determinant by .  True. Row operations either leave the determinant unchanged, change its sign, or multiply it by a nonzero number. Therefore, if and and are related through a sequence of row operations, then .  False. It is true that , but a sequence of row operations that cause and to be row equivalent may multiply the determinant by a nonzero number or change its sign. We do know, however, that and so is invertible.     Suppose that and are matrices such that and . Find the following determinants:   .   .   .   .   .      .   .   .   .   .     Multiplying the entire matrix by scales each row by . Therefore, .  We have .  We know that .  Multiplying the matrix by scales each row by so we have .   .     Suppose that and are matrices.  If and are both invertible, use determinants to explain why is invertible.  If is invertible, use determinants to explain why both and are invertible.     If and are invertible, then and . Therefore, , which shows that is invertible.  If is invertible, then . This implies that and , from which we conclude that both and are invertible.     If and are invertible, then and . Therefore, , which shows that is invertible.  If is invertible, then . This implies that and , from which we conclude that both and are invertible.     Provide a justification for your responses to the following questions.  If every entry in one row of a matrix is zero, what can you say about the determinant?  If two rows of a square matrix are identical, what can you say about the determinant?  If two columns of a square matrix are identical, what can you say about the determinant?  If one column of a matrix is a linear combination of the others, what can you say about the determinant?     In all four cases, the determinant must be zero.    The determinant must be zero. This is because the matrix cannot be invertible since there is a row without a pivot. Also, applying a cofactor expansion along that row will produce zero for the determinant.  The determinant is zero again. If we multiply one of the rows by and add it to the other row, we obtain a row whose entries are all zero. This operation does not change the determinant, but we know that the determinant of the new matrix is zero by the previous part of this problem.  The determinant must be zero. This is because the columns are not linearly independent, which means that the matrix cannot be invertible. Hence, the determinant is zero.  The determinant must be zero. This is because the columns are not linearly independent, which means that the matrix cannot be invertible. Hence, the determinant is zero.     Consider the matrix .  Assuming that , rewrite the equation in terms of , , and .  Explain why and , the first two columns of , satisfy the equation you found in the previous part.  Explain why the solution space of this equation is the plane spanned by and .      .  If we replace the third column by either or , we obtain a matrix whose columns are linearly dependent.  If for a vector , then is a linear combination of and .     Using a cofactor expansion, we have .  If we replace the third column by either or , we obtain a matrix whose columns are linearly dependent. Therefore, the determinant of the matrix must be zero.  If for some vector , then the columns of must be linear dependent. Therefore, is a linear combination of and and lies in the plane that is their span.     In this section, we studied the effect of row operations on the matrix . In this exercise, we will study the effect of analogous column operations.  Suppose that is the matrix . Also consider elementary matrices .  Explain why the matrix is obtained from by replacing the first column by . We call this a column replacement operation. Explain why column replacement operations do not change the determinant.  Explain why the matrix is obtained from by multiplying the second column by . Explain the effect that scaling a column has on the determinant of a matrix.  Explain why the matrix is obtained from by interchanging the first and third columns. What is the effect of this operation on the determinant?  Use column operations to compute the determinant of .      Column replacements do not change the determinant because .  Scaling a column by multiplies the determinant by because .  Interchanges change the sign of the determinant because .   .     The columns of are found by multiplying the columns of by . If we remember that a vector multiplied by a matrix forms a linear combination of the columns of , we see that the first column of is and that the other two columns are unchanged.  Since , we have , which says that this operation does not change the determinant.  Multiplying by multiplies the determinant by since .  Multiplying by changes the sign of the determinant since .  If we interchange the first and third columns of , we have and . Now we will multiply the first column by and add to the second column to obtain and . Let's interchange the second and third columns so that so that and then multiply the second column by and add to the third column so that This gives .     Consider the matrices . Use row operations to find the determinants of these matrices.    , , and .   It takes three interchanges to see that , which implies that .  Two interchanges show us that and hence .  Two interchanges produce a diagonal matrix so that .    Consider the matrices   Use row (and\/or column) operations to find the determinants of these matrices.  Write the and matrices that follow in this pattern and state their determinants based on what you have seen.      , , , and .  If and are the and matrices, respectively, we expect that and .      , , , and .  If and are the and matrices, respectively, we expect that and .     The following matrix is called a Vandermond matrix: .  Use row operations to explain why .  Explain why is invertible if and only if , , and are all distinct real numbers.  There is a natural way to generalize this to a matrix with parameters , , , and . Write this matrix and state its determinant based on your previous work.    This matrix appeared in when we were found a polynomial that passed through a given set of points.    Perform a sequence of row operations to form an upper triangular matrix.  If , , and are distinct, then .  The determinant is .     If , , and are not distinct, we see that because two of the rows are identical. Therefore, we apply the following row operations, assuming , , and are distinct: Because of the scalings by and , we have .  If , , and are distinct, then , which implies that is invertible.  In the case, we have      "
},
{
  "id": "fig-intro-dets",
  "level": "2",
  "url": "sec-determinants.html#fig-intro-dets",
  "type": "Figure",
  "number": "19.4.1",
  "title": "",
  "body": "    Linear combinations of two vectors and form a collection of congruent parallelograms.  "
},
{
  "id": "sec-determinants-2-6",
  "level": "2",
  "url": "sec-determinants.html#sec-determinants-2-6",
  "type": "Preview Activity",
  "number": "19.4.1",
  "title": "",
  "body": "  We will explore the area formula in this preview activity.  Find the area of the following parallelograms.    1.   2.   3.     4.   5.            Explain why the area of the parallelogram formed by the vectors and is the same as that formed by and .         We find the following areas.  A square has area 1.  A rectangle has area 6.  The square has side length giving an area of 2.  If we consider the horizontal length as the base, we see that so that the area is 4.  In the same way, we can consider both the base and height to be 2 so that the area is 4.    If we consider the base to be the length of , then the height, which is the perpendicular distance to its parallel side, is the same in both parallelograms.    "
},
{
  "id": "fig-det-orientation",
  "level": "2",
  "url": "sec-determinants.html#fig-det-orientation",
  "type": "Figure",
  "number": "19.4.2",
  "title": "",
  "body": "    The vectors on the left are positively oriented while the ones on the right are negatively oriented.  "
},
{
  "id": "sec-determinants-3-4",
  "level": "2",
  "url": "sec-determinants.html#sec-determinants-3-4",
  "type": "Definition",
  "number": "19.4.3",
  "title": "",
  "body": "  determinant  Suppose a matrix has columns and . If the pair of vectors is positively oriented, then the determinant of , denoted , is the area of the parallelogram formed by and . If the pair is negatively oriented, then is -1 times the area of the parallelogram.   "
},
{
  "id": "example-det-identity",
  "level": "2",
  "url": "sec-determinants.html#example-det-identity",
  "type": "Example",
  "number": "19.4.4",
  "title": "",
  "body": "  Consider the determinant of the identity matrix . As seen on the left of , the vectors and form a positively oriented pair. Since the parallelogram they form is a square, we have       The determinant , as seen on the left. On the right, we see that where is the matrix whose columns are shown.   Now consider the matrix . As seen on the right of , the vectors and form a negatively oriented pair. The parallelogram they define is a rectangle so we have .   "
},
{
  "id": "sec-determinants-3-6",
  "level": "2",
  "url": "sec-determinants.html#sec-determinants-3-6",
  "type": "Activity",
  "number": "19.4.2",
  "title": "",
  "body": "  In this activity, we will find the determinant of some simple matrices and discover some important properties of determinants.   The geometric meaning of the determinant of a matrix.      Use the diagram to find the determinant of the matrix . Along with , what does this lead you to believe is generally true about the determinant of a diagonal matrix?  Use the diagram to find the determinant of the matrix . What is the geometric effect of the matrix transformation defined by this matrix?  Use the diagram to find the determinant of the matrix . More generally, what do you notice about the determinant of any matrix of the form ? What does this say about the determinant of an upper triangular matrix?  Use the diagram to find the determinant of any matrix of the form . What does this say about the determinant of a lower triangular matrix?  Use the diagram to find the determinant of the matrix . In general, what is the determinant of a matrix whose columns are linearly dependent?  Consider the matrices . Use the diagram to find the determinants of , , and . What does this suggest is generally true about the relationship of to and ?      The determinant is because the vectors are negatively oriented and the rectangle has sides of length and . The determinant of a diagonal matrix seems to be the product of the diagonal entries.  The matrix transformation is a reflection over the line and we see that the determinant is .  The determinant will continue to be for any value of . This illustrates the fact that the determinant of an upper triangular matrix equals the product of its diagonal entries.  The same reasoning tells us that this determinant is and, in fact, the determinant of a lower triangular matrix equals the product of its diagonal entries.  The determinant of this matrix is because the parallelogram formed by the vector has no area. This suggests that the determinant of a matrix whose columns are linearly dependent is .  We find that , , and . This suggests that .    "
},
{
  "id": "fig-parallelogram-f",
  "level": "2",
  "url": "sec-determinants.html#fig-parallelogram-f",
  "type": "Figure",
  "number": "19.4.7",
  "title": "",
  "body": "     The determinant of a triangular matrix equals the product of its diagonal entries.  "
},
{
  "id": "proposition-det-properties",
  "level": "2",
  "url": "sec-determinants.html#proposition-det-properties",
  "type": "Proposition",
  "number": "19.4.8",
  "title": "",
  "body": "  The determinant satisfies these properties:   The determinant of a triangular matrix equals the product of its diagonal entries.    If is obtained by interchanging two rows of the identity matrix, then .     .      "
},
{
  "id": "figure-linear-dep-det",
  "level": "2",
  "url": "sec-determinants.html#figure-linear-dep-det",
  "type": "Figure",
  "number": "19.4.9",
  "title": "",
  "body": " When the columns of are linearly dependent, we find that .     "
},
{
  "id": "prop-invertible-det",
  "level": "2",
  "url": "sec-determinants.html#prop-invertible-det",
  "type": "Proposition",
  "number": "19.4.10",
  "title": "",
  "body": "  The matrix is invertible if and only if .   "
},
{
  "id": "proposition-det-row-operations",
  "level": "2",
  "url": "sec-determinants.html#proposition-det-row-operations",
  "type": "Proposition",
  "number": "19.4.11",
  "title": "The effect of row operations on the determinant.",
  "body": " The effect of row operations on the determinant      If is obtained from by scaling a row by , then .    If is obtained from by interchanging two rows, then .    If is obtained from by performing a row replacement operation, then .      "
},
{
  "id": "sec-determinants-4-9",
  "level": "2",
  "url": "sec-determinants.html#sec-determinants-4-9",
  "type": "Activity",
  "number": "19.4.3",
  "title": "",
  "body": "  We will investigate the connection between the determinant of a matrix and its invertibility using Gaussian elimination.  Consider the two upper triangular matrices Remembering , which of the matrices and are invertible? What are the determinants and ?  Explain why an upper triangular matrix is invertible if and only if its determinant is not zero.  Let's now consider the matrix and begin the Gaussian elimination process with a row replacement operation . What is the relationship between and ?  Next we perform another row replacement operation: . What is the relationship between and ?  Finally, we perform an interchange: to arrive at an upper triangular matrix . What is the relationship between and ?  Since is upper triangular, we can compute its determinant, which allows us to find . What is ? Is invertible?  Now consider the matrix Perform a sequence of row operations to find an upper triangular matrix that is row equivalent to . Use this to determine and whether is invertible.  Suppose we apply a sequence of row operations on a matrix to obtain . Explain why if and only if .  Explain why an matrix is invertible if and only if .      The matrix is invertible because we see there is a pivot position in every row and column. The matrix , however, is not invertible because there is not a pivot position in the third row. Also, and .  The determinant of an upper triangular matrix equals the product of its diagonal entries. Consequently, if the determinant of an upper triangular matrix is not zero, then each of its diagonal entries must be nonzero. In this case, there is a pivot position in every row and every column so that the matrix is invertible.  Row replacement operations do not change the determinant so .  In the same way, .  Interchanges change the sign of the determinant so .  The determinant since it is the product of the diagonal entries of . This means that . We see that is invertible because , which has a pivot position in every row and every column, is invertible.  Beginning with a row replacement operation, we arrive at . We next scale the second row by to obtain . Another row replacement operation gives . Putting these operations together, we see that . In this case, is not invertible because , which has a row without a pivot position, is not invertible.  Performing one of the three row operations either leaves the determinant unchanged (row replacement), changes its sign (interchange), or multiplies it by a nonzero number (scaling). Therefore, if we begin with a matrix whose determinant is not zero, the determinant remains nonzero after any row operation is applied.  If we apply a sequence of row operations to to find a row equivalent matrix that is upper triangular, we know that if and only if . We also know that is invertible if and only if is invertible. Putting these facts together, we conclude that if and only if is invertible.    "
},
{
  "id": "sec-determinants-4-15",
  "level": "2",
  "url": "sec-determinants.html#sec-determinants-4-15",
  "type": "Proposition",
  "number": "19.4.12",
  "title": "",
  "body": "  If is an invertible matrix, then .   "
},
{
  "id": "sec-determinants-5-5",
  "level": "2",
  "url": "sec-determinants.html#sec-determinants-5-5",
  "type": "Example",
  "number": "19.4.13",
  "title": "",
  "body": "  We illustrate how to use a cofactor expansion to find the determinant of where   To begin, we choose one row or column. It doesn't matter which we choose because the result will be the same in any case. Here, we choose the second row .  The determinant will be found by creating a sum of terms, one for each entry in the row we have chosen. For each entry in the row, we form its term by multiplying   where and are the row and column numbers, respectively, of the entry,  the entry itself, and  the determinant of the entries left over when we have crossed out the row and column containing the entry.     Since we are computing the determinant of this matrix using the second row, the entry in the first column of this row is . Let's see how to form the term from this entry.  The term itself is , and the matrix that is left over when we cross out the second row and first column is whose determinant is . Since this entry is in the second row and first column, the term we construct is .  Putting this together, we find the determinant to be . Notice that this agrees with the determinant that we found for this matrix using row operations in the last activity.   "
},
{
  "id": "sec-determinants-5-6",
  "level": "2",
  "url": "sec-determinants.html#sec-determinants-5-6",
  "type": "Activity",
  "number": "19.4.4",
  "title": "",
  "body": "  We will explore cofactor expansions through some examples.  Using a cofactor expansion, show that the determinant of the following matrix . Remember that you can choose any row or column to create the expansion, but the choice of a particular row or column may simplify the computation.  Use a cofactor expansion to find the determinant of . Explain how the cofactor expansion technique shows that the determinant of a triangular matrix is equal to the product of its diagonal entries.  Use a cofactor expansion to determine whether the following vectors form a basis of : .  Sage will compute the determinant of a matrix A with the command A.det() . Use Sage to find the determinant of the matrix .       We will using a cofactor expanion along the first row so that   Expanding along the first row gives   We form the matrix whose columns are the three given vectors. Expanding along either the second row or third column to take advantage of the zero in the entry, we see that , which means that is not invertible. Therefore, the vectors do not form a basis for .  Sage tells us that .    "
},
{
  "id": "sec-determinants-7-1",
  "level": "2",
  "url": "sec-determinants.html#sec-determinants-7-1",
  "type": "Exercise",
  "number": "19.4.5.1",
  "title": "",
  "body": " Consider the matrices .  Find the determinants of and using row operations.   Now find the determinants of and using cofactor expansions to verify your results     and .   We find that and .  "
},
{
  "id": "sec-determinants-7-2",
  "level": "2",
  "url": "sec-determinants.html#sec-determinants-7-2",
  "type": "Exercise",
  "number": "19.4.5.2",
  "title": "",
  "body": " This exercise concerns rotations and reflections in .  Suppose that is the matrix that performs a counterclockwise rotation in . Draw a typical picture of the vectors that form the columns of and use the geometric definition of the determinant to determine .  Suppose that is the matrix that performs a reflection in a line passing through the origin. Draw a typical picture of the columns of and use the geometric definition of the determinant to determine .  As we saw in , the matrices have the form . Compute the determinants of and and verify that they agree with what you found in the earlier parts of this exercise.      .   .     The vectors and that form the columns of are found by rotating the standard basis vectors and . Consequently, they are positively oriented and form a square. This says that .  The vectors and that form the columns of are found by reflecting the standard basis vectors and . Consequently, they are negatively oriented and form a square. This says that .  We see that and .   "
},
{
  "id": "sec-determinants-7-3",
  "level": "2",
  "url": "sec-determinants.html#sec-determinants-7-3",
  "type": "Exercise",
  "number": "19.4.5.3",
  "title": "",
  "body": " In the next chapter, we will say that matrices and are similar if there is a matrix such that .  Suppose that and are matrices and that there is a matrix such that . Explain why .  Suppose that is a matrix and that there is a matrix such that . Find .       because .   .      .   since .   "
},
{
  "id": "sec-determinants-7-4",
  "level": "2",
  "url": "sec-determinants.html#sec-determinants-7-4",
  "type": "Exercise",
  "number": "19.4.5.4",
  "title": "",
  "body": " Consider the matrix where is a parameter.  Find an expression for in terms of the parameter .  Use your expression for to determine the values of for which the vectors are linearly independent.          .     We see that .  The vectors are linearly independent when the matrix is invertible, which means that . Therefore, the vectors are linearly independent when .   "
},
{
  "id": "sec-determinants-7-5",
  "level": "2",
  "url": "sec-determinants.html#sec-determinants-7-5",
  "type": "Exercise",
  "number": "19.4.5.5",
  "title": "",
  "body": " Determine whether the following statements are true or false and explain your response.  If we have a square matrix and multiply the first row by and add it to the third row to obtain , then .  If we interchange two rows of a matrix, then the determinant is unchanged.  If we scale a row of the matrix by to obtain , then .  If and are row equivalent and , then also.  If is row equivalent to the identity matrix, then .      False  False  True  True  False     False. This is a row replacement operation, which leaves the determinant unchanged.  False. Applying an interchange operation changes the sign of the determinant.  True. Scaling a row of by multiplies the determinant by .  True. Row operations either leave the determinant unchanged, change its sign, or multiply it by a nonzero number. Therefore, if and and are related through a sequence of row operations, then .  False. It is true that , but a sequence of row operations that cause and to be row equivalent may multiply the determinant by a nonzero number or change its sign. We do know, however, that and so is invertible.   "
},
{
  "id": "sec-determinants-7-6",
  "level": "2",
  "url": "sec-determinants.html#sec-determinants-7-6",
  "type": "Exercise",
  "number": "19.4.5.6",
  "title": "",
  "body": " Suppose that and are matrices such that and . Find the following determinants:   .   .   .   .   .      .   .   .   .   .     Multiplying the entire matrix by scales each row by . Therefore, .  We have .  We know that .  Multiplying the matrix by scales each row by so we have .   .   "
},
{
  "id": "sec-determinants-7-7",
  "level": "2",
  "url": "sec-determinants.html#sec-determinants-7-7",
  "type": "Exercise",
  "number": "19.4.5.7",
  "title": "",
  "body": " Suppose that and are matrices.  If and are both invertible, use determinants to explain why is invertible.  If is invertible, use determinants to explain why both and are invertible.     If and are invertible, then and . Therefore, , which shows that is invertible.  If is invertible, then . This implies that and , from which we conclude that both and are invertible.     If and are invertible, then and . Therefore, , which shows that is invertible.  If is invertible, then . This implies that and , from which we conclude that both and are invertible.   "
},
{
  "id": "sec-determinants-7-8",
  "level": "2",
  "url": "sec-determinants.html#sec-determinants-7-8",
  "type": "Exercise",
  "number": "19.4.5.8",
  "title": "",
  "body": " Provide a justification for your responses to the following questions.  If every entry in one row of a matrix is zero, what can you say about the determinant?  If two rows of a square matrix are identical, what can you say about the determinant?  If two columns of a square matrix are identical, what can you say about the determinant?  If one column of a matrix is a linear combination of the others, what can you say about the determinant?     In all four cases, the determinant must be zero.    The determinant must be zero. This is because the matrix cannot be invertible since there is a row without a pivot. Also, applying a cofactor expansion along that row will produce zero for the determinant.  The determinant is zero again. If we multiply one of the rows by and add it to the other row, we obtain a row whose entries are all zero. This operation does not change the determinant, but we know that the determinant of the new matrix is zero by the previous part of this problem.  The determinant must be zero. This is because the columns are not linearly independent, which means that the matrix cannot be invertible. Hence, the determinant is zero.  The determinant must be zero. This is because the columns are not linearly independent, which means that the matrix cannot be invertible. Hence, the determinant is zero.   "
},
{
  "id": "sec-determinants-7-9",
  "level": "2",
  "url": "sec-determinants.html#sec-determinants-7-9",
  "type": "Exercise",
  "number": "19.4.5.9",
  "title": "",
  "body": " Consider the matrix .  Assuming that , rewrite the equation in terms of , , and .  Explain why and , the first two columns of , satisfy the equation you found in the previous part.  Explain why the solution space of this equation is the plane spanned by and .      .  If we replace the third column by either or , we obtain a matrix whose columns are linearly dependent.  If for a vector , then is a linear combination of and .     Using a cofactor expansion, we have .  If we replace the third column by either or , we obtain a matrix whose columns are linearly dependent. Therefore, the determinant of the matrix must be zero.  If for some vector , then the columns of must be linear dependent. Therefore, is a linear combination of and and lies in the plane that is their span.   "
},
{
  "id": "sec-determinants-7-10",
  "level": "2",
  "url": "sec-determinants.html#sec-determinants-7-10",
  "type": "Exercise",
  "number": "19.4.5.10",
  "title": "",
  "body": " In this section, we studied the effect of row operations on the matrix . In this exercise, we will study the effect of analogous column operations.  Suppose that is the matrix . Also consider elementary matrices .  Explain why the matrix is obtained from by replacing the first column by . We call this a column replacement operation. Explain why column replacement operations do not change the determinant.  Explain why the matrix is obtained from by multiplying the second column by . Explain the effect that scaling a column has on the determinant of a matrix.  Explain why the matrix is obtained from by interchanging the first and third columns. What is the effect of this operation on the determinant?  Use column operations to compute the determinant of .      Column replacements do not change the determinant because .  Scaling a column by multiplies the determinant by because .  Interchanges change the sign of the determinant because .   .     The columns of are found by multiplying the columns of by . If we remember that a vector multiplied by a matrix forms a linear combination of the columns of , we see that the first column of is and that the other two columns are unchanged.  Since , we have , which says that this operation does not change the determinant.  Multiplying by multiplies the determinant by since .  Multiplying by changes the sign of the determinant since .  If we interchange the first and third columns of , we have and . Now we will multiply the first column by and add to the second column to obtain and . Let's interchange the second and third columns so that so that and then multiply the second column by and add to the third column so that This gives .   "
},
{
  "id": "sec-determinants-7-11",
  "level": "2",
  "url": "sec-determinants.html#sec-determinants-7-11",
  "type": "Exercise",
  "number": "19.4.5.11",
  "title": "",
  "body": " Consider the matrices . Use row operations to find the determinants of these matrices.    , , and .   It takes three interchanges to see that , which implies that .  Two interchanges show us that and hence .  Two interchanges produce a diagonal matrix so that .  "
},
{
  "id": "sec-determinants-7-12",
  "level": "2",
  "url": "sec-determinants.html#sec-determinants-7-12",
  "type": "Exercise",
  "number": "19.4.5.12",
  "title": "",
  "body": " Consider the matrices   Use row (and\/or column) operations to find the determinants of these matrices.  Write the and matrices that follow in this pattern and state their determinants based on what you have seen.      , , , and .  If and are the and matrices, respectively, we expect that and .      , , , and .  If and are the and matrices, respectively, we expect that and .   "
},
{
  "id": "sec-determinants-7-13",
  "level": "2",
  "url": "sec-determinants.html#sec-determinants-7-13",
  "type": "Exercise",
  "number": "19.4.5.13",
  "title": "",
  "body": " The following matrix is called a Vandermond matrix: .  Use row operations to explain why .  Explain why is invertible if and only if , , and are all distinct real numbers.  There is a natural way to generalize this to a matrix with parameters , , , and . Write this matrix and state its determinant based on your previous work.    This matrix appeared in when we were found a polynomial that passed through a given set of points.    Perform a sequence of row operations to form an upper triangular matrix.  If , , and are distinct, then .  The determinant is .     If , , and are not distinct, we see that because two of the rows are identical. Therefore, we apply the following row operations, assuming , , and are distinct: Because of the scalings by and , we have .  If , , and are distinct, then , which implies that is invertible.  In the case, we have    "
},
{
  "id": "sec-subspaces",
  "level": "1",
  "url": "sec-subspaces.html",
  "type": "Section",
  "number": "19.5",
  "title": "Subspaces",
  "body": " Subspaces   In this chapter, we have been looking at bases for , sets of vectors that are linearly independent and span . Frequently, however, we focus on only a subset of . In particular, if we are given an matrix , we have been interested in both the span of the columns of and the solution space to the homogeneous equation . In this section, we will expand the concept of basis to describe sets like these.    Let's consider the following matrix and its reduced row echelon form. .  Are the columns of linearly independent? Is the span of the columns ?  Give a parametric description of the solution space to the homogeneous equation .  Explain how this parametric description produces two vectors and whose span is the solution space to the equation .  What can you say about the linear independence of the set of vectors and ?  Let's denote the columns of as , , , and . Explain why and can be written as linear combinations of and .  Explain why and are linearly independent and       The columns of are not linearly independent since there is not a pivot position in every column. Also, the span of the columns is not because there is not a pivot position in every row.  From the reduced row echelon form, we see that the homogeneous equation leads to the equations which leads to the parametric description   We see that every vector in the solution space is a linear combination of the vectors and .  This pair of vectors is linearly independent because one is not a scalar multiple of the other.  From the reduced row echelon form of , we see that and .  We see that and are linearly independent from the reduced row echelon form of . Moreover, we know that and can be written as linear combinations of and . Therefore, any linear combination of , , , and can be written as a linear combination of and alone.       Subspaces  Our goal is to develop a common framework for describing subsets like the span of the columns of a matrix and the solution space to a homogeneous equation. That leads us to the following definition.   subspace   A subspace of is a subset of that is the span of a set of vectors.    Since we have explored the concept of span in some detail, this definition just gives us a new word to describe something familiar. Let's look at some examples.   Subspaces of   In and the following discussion, we looked at subspaces in without explicitly using that language. Let's recall some of those examples.     Suppose we have a single nonzero vector . The span of is a subspace, which we'll write as . As we have seen, the span of a single vector consists of all scalar multiples of that vector, and these form a line passing through the origin.        If instead we have two linearly independent vectors and , the subspace is a plane passing through the origin.      Consider the three vectors , , and . Since we know that every 3-dimensional vector can be written as a linear combination, we have .    One more subspace worth mentioning is . Since any linear combination of the zero vector is itself the zero vector, this subspace consists of a single vector, .     In fact, any subspace of is one of these types: the origin, a line, a plane, or all of .     We will look at some sets of vectors and the subspaces they form.   If is a set of vectors in , explain why can be expressed as a linear combination of these vectors. Use this fact to explain why the zero vector belongs to any subspace in .    Explain why the line on the left of is not a subspace of and why the line on the right is.   Two lines in , one of which is a subspace and one of which is not.         Consider the vectors and describe the subspace of .     Consider the vectors    Write as a linear combination of and .    Explain why .    Describe the subspace of .       Suppose that , , , and are four vectors in and that Give a description of the subspace of .          If we choose all the weights , then the linear combination This means that is the subspace .    The line on the left cannot be a subspace of since it does not contain the zero vector. The line on the right is a subspace because it can be represented as the span of any nonzero vector on the line.    The matrix whose columns are the given vectors has a pivot in every row. Therefore, the span of these vectors is and so .       We see that .    Any linear combination     The subspace is a plane in .       Since we can write and , then which is a plane in .       As the activity shows, it is possible to represent some subspaces as the span of more than one set of vectors. We are particularly interested in representing a subspace as the span of a linearly independent set of vectors.    dimension  A basis for a subspace of is a set of vectors in that are linearly independent and whose span is . We say that the dimension of the subspace , denoted , is the number of vectors in any basis.     A subspace of   Suppose we have the 4-dimensional vectors , , and that define the subspace of . Suppose also that From the reduced row echelon form of the matrix, we see that . Therefore, any linear combination of , , and can be rewritten as a linear combination of and . This tells us that   Furthermore, the reduced row echelon form of the matrix shows that and are linearly independent. Therefore, is a basis for , which means that is a two-dimensional subspace of .   Subspaces of are either   0-dimensional, consisting of the single vector ,    a 1-dimensional line,    a 2-dimensional plane, or    the 3-dimensional subspace .   There is no 4-dimensional subspace of because there is no linearly independent set of four vectors in .  There are two important subspaces associated to any matrix, each of which springs from one of our two fundamental questions, as we will now see.    The column space of   The first subspace associated to a matrix that we'll consider is its column space.    column space  If is an matrix, we call the span of its columns the column space of and denote it as .    Notice that the columns of are vectors in , which means that any linear combination of the columns is also in . Since the column space is described as the span of a set of vectors, we see that is a subspace of .    We will explore some column spaces in this activity.  Consider the matrix Since is the span of the columns, we have Explain why can be written as a linear combination of and and why .   Explain why the vectors and form a basis for and why is a 2-dimensional subspace of and therefore a plane.  Now consider the matrix and its reduced row echelon form: Explain why is a 1-dimensional subspace of and is therefore a line.  For a general matrix , what is the relationship between the dimension and the number of pivot positions in ?  How does the location of the pivot positions indicate a basis for ?  If is an invertible matrix, what can you say about the column space ?   Suppose that is an matrix and that . If is an 8-dimensional vector, what can you say about the equation ?       We have which shows that the vectors are not linearly independent and, in fact, that . As we've seen several times, this means that any linear combination of , , and can be written as a linear combination of and alone and hence that   The reduced row echelon form of shows that and are linearly independent. We also know that the span of these two vectors is . Therefore, they form a basis for .  Denoting the columns of as , the reduced row echelon form shows that , , and . Therefore, any linear combination of , , , and can be written as a linear combination of alone. This means that forms a basis for , which is then the line consisting of all scalar multiples of . p  The number of vectors in a basis of equals the number of pivot positions. Therefore, equals the number of pivot positions in .  As the examples in this activity illustrate, the columns of that contain pivot positions form a basis for .  If is invertible, then it has a pivot position in every row, which means that the span of the columns is . Therefore, .  Since , we know that every 8-dimensional vector is in . This means that is in the span of the columns of so the equation must be consistent.      Consider the matrix and its reduced row echelon form: and denote the columns of as .  It is certainly true that by the definition of the column space. However, the reduced row echelon form of the matrix shows us that the vectors are not linearly independent so do not form a basis for .  From the reduced row echelon form, however, we can see that . This means that any linear combination of can be written as a linear combination of just and . Therefore, we see that .  Moreover, the reduced row echelon form shows that and are linearly independent, which implies that they form a basis for . This means that is a 2-dimensional subspace of , which is a plane in , having basis .   In general, a column without a pivot position can be written as a linear combination of the columns that have pivot positions. This means that a basis for will always be given by the columns of having pivot positions. This leads us to the following definition and proposition.   rank  matrix, rank   The rank of a matrix is the number of pivot positions in and is denoted by .      If is an matrix, then is a subspace of whose dimension equals . The columns of that contain pivot positions form a basis for .    For example, the rank of the matrix in is two because there are two pivot positions. A basis for is given by the first two columns of since those columns have pivot positions.  As a note of caution, we determine the pivot positions by looking at the reduced row echelon form of . However, we form a basis of from the columns of rather than the columns of the reduced row echelon matrix.    The null space of   The second subspace associated to a matrix is its null space.    null space  If is an matrix, we call the subset of vectors in satisfying the null space of and denote it by .    Remember that a subspace is a subset that can be represented as the span of a set of vectors. The column space of , which is simply the span of the columns of , fits this definition. It may not be immediately clear how the null space of , which is the solution space of the equation , does, but we will see that is a subspace of .    We will explore some null spaces in this activity and see why satisfies the definition of a subspace.  Consider the matrix and give a parametric description of the solution space to the equation . In other words, give a parametric description of .    This parametric description shows that the vectors satisfying the equation can be written as a linear combination of a set of vectors. In other words, this description shows why is the span of a set of vectors and is therefore a subspace. Identify a set of vectors whose span is .    Use this set of vectors to find a basis for and state the dimension of .   The null space is a subspace of for which value of ?  Now consider the matrix whose reduced row echelon form is given by Give a parametric description of .   The parametric description gives a set of vectors that span . Explain why this set of vectors is linearly independent and hence forms a basis. What is the dimension of ?    For a general matrix , how does the number of pivot positions indicate the dimension of ?    Suppose that the columns of a matrix are linearly independent. What can you say about ?       We have which leads to the parametric description of the solution space to the homogeneous equation:   The parametric description shows that every solution to the equation is a linear combination of and .   The vectors and are linearly independent so they form a basis for . Therefore, is 2-dimensional.   The vectors in are 4-dimensional so is a subspace of .  A parametric description of the null space is . We can check that the vectors are linearly independent so they form a basis for . This means that is 3-dimensional.  The number of vectors in a basis of the null space equals the number of free variables that appear in the equation , which is the number of columns that do not have pivot positions. This says that equals the number of columns of minus the number of pivot positions.  If the columns are linearly independent, then the homogeneous equation has only the zero solution . Therefore, .      Consider the matrix along with its reduced row echelon form:   To find a parametric description of the solution space to , imagine that we augment both and its reduced row echelon form by a column of zeroes, which leads to the equations Notice that , , and are free variables so we rewrite these equations as In vector form, we have   This expression says that any vector satisfying is a linear combination of the vectors It is straightforward to check that these vectors are linearly independent, which means that , , and form a basis for , a 3-dimensional subspace of .   As illustrated in this example, the dimension of is equal to the number of free variables in the equation , which equals the number of columns of without pivot positions or the number of columns of minus the number of pivot positions.    If is an matrix, then is a subspace of whose dimension is     Combining and shows that    If is an matrix, then       Summary  Once again, we find ourselves revisiting our two fundamental questions concerning the existence and uniqueness of solutions to linear systems. The column space contains all the vectors for which the equation is consistent. The null space is the solution space to the equation , which reflects on the uniqueness of solutions to this and other equations.    A subspace of is a subset of that can be represented as the span of a set of vectors. A basis of is a linearly independent set of vectors whose span is .  If is an matrix, the column space is the span of the columns of and forms a subspace of .  A basis for is found from the columns of that have pivot positions. The dimension is therefore .  The null space is the solution space to the homogeneous equation and is a subspace of .  A basis for is found through a parametric description of the solution space of , and we have that .       Suppose that and its reduced row echelon form are .  The null space is a subspace of for what ? The column space is a subspace of for what ?  What are the dimensions and ?  Find a basis for the column space .  Find a basis for the null space .       is a subspace of and is a subspace of .   and .            is a subspace of and is a subspace of .  Because there are three pivot positions, we see that . Therefore, and .  A basis for is given by the columns of that contain pivot positions. Therefore, a basis is   We can write a parametric description for the solution space to the homogeneous equation as Therefore, a basis for is      Suppose that .  Is the vector in ?  Is the vector in ?  Is the vector in ?  Is the vector in ?  Is the vector in ?      Yes  No  No  Yes  No     Yes. This vector is a column of so it may be written as a linear combination of the columns of .  No. Vectors in must be three-dimensional.  No. Vectors in must be four-dimensional.  Yes, because this vector, when multiplied by , gives .  No, because this vector, when multiplied by , does not give .     Determine whether the following statements are true or false and provide a justification for your response. Unless otherwise stated, assume that is an matrix.  If is a matrix, then is a subspace of .  If , then the columns of are linearly independent.  If , then is invertible.  If has a pivot position in every column, then .  If and , then is invertible.      False  True  False  False  True     False. is a subspace of .  True. In this case, the only solution to the homogeneous equation is the zero solution . This means that every column has a pivot position so the columns are linearly independent.  False. The matrix is not necessarily a square matrix.  False. If has a pivot position in every column, then .  True. Since , we know that has a pivot position in every row. Since , we know that has a pivot position in every column. Therefore, must be a square matrix and invertible.     Explain why the following statements are true.  If is invertible, then .  If is invertible, then .  If , then .      If , then if is invertible.  The column space consists of all the vectors for which is consistent. We can rewrite this condition as , which means that is consistent exactly when is.  If , then and have the same reduced row echelon form, which determines .     If , then if is invertible.  The column space consists of all the vectors for which is consistent. We can rewrite this condition as , which means that is consistent exactly when is.  If , then and have the same reduced row echelon form, which determines .     For each of the following conditions, construct a matrix having the given properties.   .   .   .   .      .   .   .   .      .   .   .   .     Suppose that is a matrix.  Is it possible that ?  If , what can you say about ?  If , what can you say about ?   If , what can you say about ?  If , what can you say about ?      No      is a plane in    is a line in    .     No. There are more columns than rows so there must be at least one column without a pivot position.  Remember that we have . Therefore, we have in this case, which implies that .  Here, so is a plane in .  Here, so is a line in .  Here, so .     Suppose we have the vectors and that is a matrix such that and .  What are the dimensions of ?  Find such a matrix .              Since and are three-dimensional vectors, must have three rows. Since and are 4-dimensional, must have four columns. Alternatively, we know that and . Therefore, the number of columns is . Hence, is a matrix.  We can use and as the first two columns of . We also know that and . If we call the other two columns and , then implies that so that . Since , we have , which says that . This gives      Suppose that is an matrix and that .  What can you conclude about ?  What can you conclude about ?      and .   We know that is invertible so and .    Suppose that is a matrix and there is an invertible matrix such that .  What can you conclude about ?  What can you conclude about ?      and .   We know that so is invertible. Therefore, and .    In this section, we saw that the solution space to the homogeneous equation is a subspace of for some . In this exercise, we will investigate whether the solution space to another equation can form a subspace.  Let's consider the matrix .  Find a parametric description of the solution space to the homogeneous equation .   Graph the solution space to the homogeneous equation to the right.     Find a parametric description of the solution space to the equation and graph it above.  Is the solution space to the equation a subspace of ?  Find a parametric description of the solution space to the equation and graph it above.  What can you say about all the solution spaces to equations of the form when is a vector in ?  Suppose that the solution space to the equation forms a subspace. Explain why it must be true that .        The solution space forms a line through the origin.     No   . The graph of the solution spaces is shown below.     They are all parallel to the solution space to the homogeneous equation.   must be in the solution space.     We have which shows that describes the solution space to the homogeneous equation.  The solution space forms a line through the origin.  The solution space has the parametric description , which is a line parallel to the solution space to the homogeneous equation.  This cannot form a subspace since it does not contain the vector .  The solution space has the parametric description , which is a line parallel to the solution space to the homogeneous equation. The graph of the solution spaces is shown below.     They are all parallel to the solution space to the homogeneous equation.  If the solution space is a subspace, then must be in the solution space. This means that .     "
},
{
  "id": "sec-subspaces-2-2",
  "level": "2",
  "url": "sec-subspaces.html#sec-subspaces-2-2",
  "type": "Preview Activity",
  "number": "19.5.1",
  "title": "",
  "body": "  Let's consider the following matrix and its reduced row echelon form. .  Are the columns of linearly independent? Is the span of the columns ?  Give a parametric description of the solution space to the homogeneous equation .  Explain how this parametric description produces two vectors and whose span is the solution space to the equation .  What can you say about the linear independence of the set of vectors and ?  Let's denote the columns of as , , , and . Explain why and can be written as linear combinations of and .  Explain why and are linearly independent and       The columns of are not linearly independent since there is not a pivot position in every column. Also, the span of the columns is not because there is not a pivot position in every row.  From the reduced row echelon form, we see that the homogeneous equation leads to the equations which leads to the parametric description   We see that every vector in the solution space is a linear combination of the vectors and .  This pair of vectors is linearly independent because one is not a scalar multiple of the other.  From the reduced row echelon form of , we see that and .  We see that and are linearly independent from the reduced row echelon form of . Moreover, we know that and can be written as linear combinations of and . Therefore, any linear combination of , , , and can be written as a linear combination of and alone.    "
},
{
  "id": "sec-subspaces-3-3",
  "level": "2",
  "url": "sec-subspaces.html#sec-subspaces-3-3",
  "type": "Definition",
  "number": "19.5.1",
  "title": "",
  "body": " subspace   A subspace of is a subset of that is the span of a set of vectors.   "
},
{
  "id": "sec-subspaces-3-5",
  "level": "2",
  "url": "sec-subspaces.html#sec-subspaces-3-5",
  "type": "Example",
  "number": "19.5.2",
  "title": "Subspaces of <span xmlns:pf=\"https:\/\/prefigure.org\" class=\"process-math\">\\(\\real^3\\)<\/span>.",
  "body": " Subspaces of   In and the following discussion, we looked at subspaces in without explicitly using that language. Let's recall some of those examples.     Suppose we have a single nonzero vector . The span of is a subspace, which we'll write as . As we have seen, the span of a single vector consists of all scalar multiples of that vector, and these form a line passing through the origin.        If instead we have two linearly independent vectors and , the subspace is a plane passing through the origin.      Consider the three vectors , , and . Since we know that every 3-dimensional vector can be written as a linear combination, we have .    One more subspace worth mentioning is . Since any linear combination of the zero vector is itself the zero vector, this subspace consists of a single vector, .     In fact, any subspace of is one of these types: the origin, a line, a plane, or all of .  "
},
{
  "id": "sec-subspaces-3-6",
  "level": "2",
  "url": "sec-subspaces.html#sec-subspaces-3-6",
  "type": "Activity",
  "number": "19.5.2",
  "title": "",
  "body": "  We will look at some sets of vectors and the subspaces they form.   If is a set of vectors in , explain why can be expressed as a linear combination of these vectors. Use this fact to explain why the zero vector belongs to any subspace in .    Explain why the line on the left of is not a subspace of and why the line on the right is.   Two lines in , one of which is a subspace and one of which is not.         Consider the vectors and describe the subspace of .     Consider the vectors    Write as a linear combination of and .    Explain why .    Describe the subspace of .       Suppose that , , , and are four vectors in and that Give a description of the subspace of .          If we choose all the weights , then the linear combination This means that is the subspace .    The line on the left cannot be a subspace of since it does not contain the zero vector. The line on the right is a subspace because it can be represented as the span of any nonzero vector on the line.    The matrix whose columns are the given vectors has a pivot in every row. Therefore, the span of these vectors is and so .       We see that .    Any linear combination     The subspace is a plane in .       Since we can write and , then which is a plane in .      "
},
{
  "id": "sec-subspaces-3-8",
  "level": "2",
  "url": "sec-subspaces.html#sec-subspaces-3-8",
  "type": "Definition",
  "number": "19.5.4",
  "title": "",
  "body": "  dimension  A basis for a subspace of is a set of vectors in that are linearly independent and whose span is . We say that the dimension of the subspace , denoted , is the number of vectors in any basis.   "
},
{
  "id": "sec-subspaces-3-9",
  "level": "2",
  "url": "sec-subspaces.html#sec-subspaces-3-9",
  "type": "Example",
  "number": "19.5.5",
  "title": "A subspace of <span xmlns:pf=\"https:\/\/prefigure.org\" class=\"process-math\">\\(\\real^4\\)<\/span>.",
  "body": " A subspace of   Suppose we have the 4-dimensional vectors , , and that define the subspace of . Suppose also that From the reduced row echelon form of the matrix, we see that . Therefore, any linear combination of , , and can be rewritten as a linear combination of and . This tells us that   Furthermore, the reduced row echelon form of the matrix shows that and are linearly independent. Therefore, is a basis for , which means that is a two-dimensional subspace of .  "
},
{
  "id": "sec-subspaces-4-3",
  "level": "2",
  "url": "sec-subspaces.html#sec-subspaces-4-3",
  "type": "Definition",
  "number": "19.5.6",
  "title": "",
  "body": "  column space  If is an matrix, we call the span of its columns the column space of and denote it as .   "
},
{
  "id": "sec-subspaces-4-5",
  "level": "2",
  "url": "sec-subspaces.html#sec-subspaces-4-5",
  "type": "Activity",
  "number": "19.5.3",
  "title": "",
  "body": "  We will explore some column spaces in this activity.  Consider the matrix Since is the span of the columns, we have Explain why can be written as a linear combination of and and why .   Explain why the vectors and form a basis for and why is a 2-dimensional subspace of and therefore a plane.  Now consider the matrix and its reduced row echelon form: Explain why is a 1-dimensional subspace of and is therefore a line.  For a general matrix , what is the relationship between the dimension and the number of pivot positions in ?  How does the location of the pivot positions indicate a basis for ?  If is an invertible matrix, what can you say about the column space ?   Suppose that is an matrix and that . If is an 8-dimensional vector, what can you say about the equation ?       We have which shows that the vectors are not linearly independent and, in fact, that . As we've seen several times, this means that any linear combination of , , and can be written as a linear combination of and alone and hence that   The reduced row echelon form of shows that and are linearly independent. We also know that the span of these two vectors is . Therefore, they form a basis for .  Denoting the columns of as , the reduced row echelon form shows that , , and . Therefore, any linear combination of , , , and can be written as a linear combination of alone. This means that forms a basis for , which is then the line consisting of all scalar multiples of . p  The number of vectors in a basis of equals the number of pivot positions. Therefore, equals the number of pivot positions in .  As the examples in this activity illustrate, the columns of that contain pivot positions form a basis for .  If is invertible, then it has a pivot position in every row, which means that the span of the columns is . Therefore, .  Since , we know that every 8-dimensional vector is in . This means that is in the span of the columns of so the equation must be consistent.    "
},
{
  "id": "example-col-basis",
  "level": "2",
  "url": "sec-subspaces.html#example-col-basis",
  "type": "Example",
  "number": "19.5.7",
  "title": "",
  "body": " Consider the matrix and its reduced row echelon form: and denote the columns of as .  It is certainly true that by the definition of the column space. However, the reduced row echelon form of the matrix shows us that the vectors are not linearly independent so do not form a basis for .  From the reduced row echelon form, however, we can see that . This means that any linear combination of can be written as a linear combination of just and . Therefore, we see that .  Moreover, the reduced row echelon form shows that and are linearly independent, which implies that they form a basis for . This means that is a 2-dimensional subspace of , which is a plane in , having basis .  "
},
{
  "id": "sec-subspaces-4-8",
  "level": "2",
  "url": "sec-subspaces.html#sec-subspaces-4-8",
  "type": "Definition",
  "number": "19.5.8",
  "title": "",
  "body": " rank  matrix, rank   The rank of a matrix is the number of pivot positions in and is denoted by .   "
},
{
  "id": "proposition-col-dim",
  "level": "2",
  "url": "sec-subspaces.html#proposition-col-dim",
  "type": "Proposition",
  "number": "19.5.9",
  "title": "",
  "body": "  If is an matrix, then is a subspace of whose dimension equals . The columns of that contain pivot positions form a basis for .   "
},
{
  "id": "sec-subspaces-5-3",
  "level": "2",
  "url": "sec-subspaces.html#sec-subspaces-5-3",
  "type": "Definition",
  "number": "19.5.10",
  "title": "",
  "body": "  null space  If is an matrix, we call the subset of vectors in satisfying the null space of and denote it by .   "
},
{
  "id": "sec-subspaces-5-5",
  "level": "2",
  "url": "sec-subspaces.html#sec-subspaces-5-5",
  "type": "Activity",
  "number": "19.5.4",
  "title": "",
  "body": "  We will explore some null spaces in this activity and see why satisfies the definition of a subspace.  Consider the matrix and give a parametric description of the solution space to the equation . In other words, give a parametric description of .    This parametric description shows that the vectors satisfying the equation can be written as a linear combination of a set of vectors. In other words, this description shows why is the span of a set of vectors and is therefore a subspace. Identify a set of vectors whose span is .    Use this set of vectors to find a basis for and state the dimension of .   The null space is a subspace of for which value of ?  Now consider the matrix whose reduced row echelon form is given by Give a parametric description of .   The parametric description gives a set of vectors that span . Explain why this set of vectors is linearly independent and hence forms a basis. What is the dimension of ?    For a general matrix , how does the number of pivot positions indicate the dimension of ?    Suppose that the columns of a matrix are linearly independent. What can you say about ?       We have which leads to the parametric description of the solution space to the homogeneous equation:   The parametric description shows that every solution to the equation is a linear combination of and .   The vectors and are linearly independent so they form a basis for . Therefore, is 2-dimensional.   The vectors in are 4-dimensional so is a subspace of .  A parametric description of the null space is . We can check that the vectors are linearly independent so they form a basis for . This means that is 3-dimensional.  The number of vectors in a basis of the null space equals the number of free variables that appear in the equation , which is the number of columns that do not have pivot positions. This says that equals the number of columns of minus the number of pivot positions.  If the columns are linearly independent, then the homogeneous equation has only the zero solution . Therefore, .    "
},
{
  "id": "example-null-intro",
  "level": "2",
  "url": "sec-subspaces.html#example-null-intro",
  "type": "Example",
  "number": "19.5.11",
  "title": "",
  "body": " Consider the matrix along with its reduced row echelon form:   To find a parametric description of the solution space to , imagine that we augment both and its reduced row echelon form by a column of zeroes, which leads to the equations Notice that , , and are free variables so we rewrite these equations as In vector form, we have   This expression says that any vector satisfying is a linear combination of the vectors It is straightforward to check that these vectors are linearly independent, which means that , , and form a basis for , a 3-dimensional subspace of .  "
},
{
  "id": "proposition-nul-dim",
  "level": "2",
  "url": "sec-subspaces.html#proposition-nul-dim",
  "type": "Proposition",
  "number": "19.5.12",
  "title": "",
  "body": "  If is an matrix, then is a subspace of whose dimension is    "
},
{
  "id": "sec-subspaces-5-10",
  "level": "2",
  "url": "sec-subspaces.html#sec-subspaces-5-10",
  "type": "Proposition",
  "number": "19.5.13",
  "title": "",
  "body": "  If is an matrix, then    "
},
{
  "id": "sec-subspaces-7-1",
  "level": "2",
  "url": "sec-subspaces.html#sec-subspaces-7-1",
  "type": "Exercise",
  "number": "19.5.5.1",
  "title": "",
  "body": " Suppose that and its reduced row echelon form are .  The null space is a subspace of for what ? The column space is a subspace of for what ?  What are the dimensions and ?  Find a basis for the column space .  Find a basis for the null space .       is a subspace of and is a subspace of .   and .            is a subspace of and is a subspace of .  Because there are three pivot positions, we see that . Therefore, and .  A basis for is given by the columns of that contain pivot positions. Therefore, a basis is   We can write a parametric description for the solution space to the homogeneous equation as Therefore, a basis for is    "
},
{
  "id": "sec-subspaces-7-2",
  "level": "2",
  "url": "sec-subspaces.html#sec-subspaces-7-2",
  "type": "Exercise",
  "number": "19.5.5.2",
  "title": "",
  "body": " Suppose that .  Is the vector in ?  Is the vector in ?  Is the vector in ?  Is the vector in ?  Is the vector in ?      Yes  No  No  Yes  No     Yes. This vector is a column of so it may be written as a linear combination of the columns of .  No. Vectors in must be three-dimensional.  No. Vectors in must be four-dimensional.  Yes, because this vector, when multiplied by , gives .  No, because this vector, when multiplied by , does not give .   "
},
{
  "id": "sec-subspaces-7-3",
  "level": "2",
  "url": "sec-subspaces.html#sec-subspaces-7-3",
  "type": "Exercise",
  "number": "19.5.5.3",
  "title": "",
  "body": " Determine whether the following statements are true or false and provide a justification for your response. Unless otherwise stated, assume that is an matrix.  If is a matrix, then is a subspace of .  If , then the columns of are linearly independent.  If , then is invertible.  If has a pivot position in every column, then .  If and , then is invertible.      False  True  False  False  True     False. is a subspace of .  True. In this case, the only solution to the homogeneous equation is the zero solution . This means that every column has a pivot position so the columns are linearly independent.  False. The matrix is not necessarily a square matrix.  False. If has a pivot position in every column, then .  True. Since , we know that has a pivot position in every row. Since , we know that has a pivot position in every column. Therefore, must be a square matrix and invertible.   "
},
{
  "id": "sec-subspaces-7-4",
  "level": "2",
  "url": "sec-subspaces.html#sec-subspaces-7-4",
  "type": "Exercise",
  "number": "19.5.5.4",
  "title": "",
  "body": " Explain why the following statements are true.  If is invertible, then .  If is invertible, then .  If , then .      If , then if is invertible.  The column space consists of all the vectors for which is consistent. We can rewrite this condition as , which means that is consistent exactly when is.  If , then and have the same reduced row echelon form, which determines .     If , then if is invertible.  The column space consists of all the vectors for which is consistent. We can rewrite this condition as , which means that is consistent exactly when is.  If , then and have the same reduced row echelon form, which determines .   "
},
{
  "id": "sec-subspaces-7-5",
  "level": "2",
  "url": "sec-subspaces.html#sec-subspaces-7-5",
  "type": "Exercise",
  "number": "19.5.5.5",
  "title": "",
  "body": " For each of the following conditions, construct a matrix having the given properties.   .   .   .   .      .   .   .   .      .   .   .   .   "
},
{
  "id": "sec-subspaces-7-6",
  "level": "2",
  "url": "sec-subspaces.html#sec-subspaces-7-6",
  "type": "Exercise",
  "number": "19.5.5.6",
  "title": "",
  "body": " Suppose that is a matrix.  Is it possible that ?  If , what can you say about ?  If , what can you say about ?   If , what can you say about ?  If , what can you say about ?      No      is a plane in    is a line in    .     No. There are more columns than rows so there must be at least one column without a pivot position.  Remember that we have . Therefore, we have in this case, which implies that .  Here, so is a plane in .  Here, so is a line in .  Here, so .   "
},
{
  "id": "sec-subspaces-7-7",
  "level": "2",
  "url": "sec-subspaces.html#sec-subspaces-7-7",
  "type": "Exercise",
  "number": "19.5.5.7",
  "title": "",
  "body": " Suppose we have the vectors and that is a matrix such that and .  What are the dimensions of ?  Find such a matrix .              Since and are three-dimensional vectors, must have three rows. Since and are 4-dimensional, must have four columns. Alternatively, we know that and . Therefore, the number of columns is . Hence, is a matrix.  We can use and as the first two columns of . We also know that and . If we call the other two columns and , then implies that so that . Since , we have , which says that . This gives    "
},
{
  "id": "sec-subspaces-7-8",
  "level": "2",
  "url": "sec-subspaces.html#sec-subspaces-7-8",
  "type": "Exercise",
  "number": "19.5.5.8",
  "title": "",
  "body": " Suppose that is an matrix and that .  What can you conclude about ?  What can you conclude about ?      and .   We know that is invertible so and .  "
},
{
  "id": "sec-subspaces-7-9",
  "level": "2",
  "url": "sec-subspaces.html#sec-subspaces-7-9",
  "type": "Exercise",
  "number": "19.5.5.9",
  "title": "",
  "body": " Suppose that is a matrix and there is an invertible matrix such that .  What can you conclude about ?  What can you conclude about ?      and .   We know that so is invertible. Therefore, and .  "
},
{
  "id": "sec-subspaces-7-10",
  "level": "2",
  "url": "sec-subspaces.html#sec-subspaces-7-10",
  "type": "Exercise",
  "number": "19.5.5.10",
  "title": "",
  "body": " In this section, we saw that the solution space to the homogeneous equation is a subspace of for some . In this exercise, we will investigate whether the solution space to another equation can form a subspace.  Let's consider the matrix .  Find a parametric description of the solution space to the homogeneous equation .   Graph the solution space to the homogeneous equation to the right.     Find a parametric description of the solution space to the equation and graph it above.  Is the solution space to the equation a subspace of ?  Find a parametric description of the solution space to the equation and graph it above.  What can you say about all the solution spaces to equations of the form when is a vector in ?  Suppose that the solution space to the equation forms a subspace. Explain why it must be true that .        The solution space forms a line through the origin.     No   . The graph of the solution spaces is shown below.     They are all parallel to the solution space to the homogeneous equation.   must be in the solution space.     We have which shows that describes the solution space to the homogeneous equation.  The solution space forms a line through the origin.  The solution space has the parametric description , which is a line parallel to the solution space to the homogeneous equation.  This cannot form a subspace since it does not contain the vector .  The solution space has the parametric description , which is a line parallel to the solution space to the homogeneous equation. The graph of the solution spaces is shown below.     They are all parallel to the solution space to the homogeneous equation.  If the solution space is a subspace, then must be in the solution space. This means that .   "
},
{
  "id": "sec-eigen-intro",
  "level": "1",
  "url": "sec-eigen-intro.html",
  "type": "Section",
  "number": "20.1",
  "title": "An introduction to eigenvalues and eigenvectors",
  "body": " An introduction to eigenvalues and eigenvectors   This section introduces the concept of eigenvalues and eigenvectors and offers an example that motivates our interest in them. The point here is to develop an intuitive understanding of eigenvalues and eigenvectors and explain how they can be used to simplify some problems that we have previously encountered. In the rest of this chapter, we will develop this concept into a richer theory and illustrate its use with more meaningful examples.    Before we introduce the definition of eigenvectors and eigenvalues, it will be helpful to remember some ideas we have seen previously.    Suppose that is the vector shown in the figure. Sketch the vector and the vector .     State the geometric effect that scalar multiplication has on the vector . Then sketch all the vectors of the form where is a scalar.  State the geometric effect of the matrix transformation defined by .  Suppose that is a matrix and that and are vectors such that . Use the linearity of matrix multiplication to express the following vectors in terms of and .   .   .   .   .   .   .        The vectors are as shown.     Scalar multiplication has the effect of stretching and possibly flipping along the line defined by .  This matrix transformation stretches vectors by a factor of in the horizontal direction and flips vectors vertically.  Applying linearity, we see that   .   .   .   .   .   .         A few examples  We will now introduce the definition of eigenvalues and eigenvectors and then look at a few simple examples.    eigenvalue  eigenvector  Given a square matrix , we say that a nonzero vector is an eigenvector of if there is a scalar such that . The scalar is called the eigenvalue associated to the eigenvector .    At first glance, there is a lot going on in this definition so let's look at an example.    Consider the matrix and the vector . We find that . In other words, , which says that is an eigenvector of the matrix with associated eigenvalue .  Similarly, if , we find that . Here again, we have showing that is an eigenvector of with associated eigenvalue .      This definition has an important geometric interpretation that we will investigate here.  Suppose that is a nonzero vector and that is a scalar. What is the geometric relationship between and ?  Let's now consider the eigenvector condition: . Here we have two vectors, and . If , what is the geometric relationship between and ?    A geometric interpretation of the eigenvalue-eigenvector condition .    Choose the matrix . Move the vector so that the eigenvector condition holds. What is the eigenvector and what is the associated eigenvalue?  By algebraically computing , verify that the eigenvector condition holds for the vector that you found.  If you multiply the eigenvector that you found by , do you still have an eigenvector? If so, what is the associated eigenvalue?  Are you able to find another eigenvector that is not a scalar multiple of the first one that you found? If so, what is the eigenvector and what is the associated eigenvalue?  Now consider the matrix . Use the diagram to describe any eigenvectors and associated eigenvalues.  Finally, consider the matrix . Use the diagram to describe any eigenvectors and associated eigenvalues. What geometric transformation does this matrix perform on vectors? How does this explain the presence of any eigenvectors?       The vectors and lie on the same line.  The vectors and lie on the same line.  There are many possibilities, but we see that is an eigenvector with associated eigenvalue .  If we perform the matrix multiplication, we see that .  Yes, is still an eigenvector with associated eigenvalue .  We see that is an eigenvector with associated eigenvalue .  The only eigenvectors that appear are scalar multiples of with associated eigenvalue .  There are no eigenvectors. The matrix transformation rotates vectors by so it is not possible for and to lie on the same line.     Let's consider the ideas we saw in the activity in some more depth. To be an eigenvector of , the vector must satisfy for some scalar . This means that and are scalar multiples of each other so they must lie on the same line.  Consider now the matrix . On the left of , we see that is not an eigenvector of since the vectors and do not lie on the same line. On the right, however, we see that is an eigenvector. In fact, is obtained from by stretching by a factor of . Therefore, is an eigenvector of with eigenvalue .       On the left, the vector is not an eigenvector. On the right, the vector is an eigenvector with eigenvalue .   It is not difficult to see that any multiple of is also an eigenvector of with eigenvalue . Indeed, we will see later that all the eigenvectors associated to a given eigenvalue form a subspace of .  In , we see that is also an eigenvector with eigenvalue .      Here we see another eigenvector with eigenvalue .   The interactive diagram we used in the activity is meant to convey the fact that the eigenvectors of a matrix are special vectors. Most of the time, the vectors and appear visually unrelated. For certain vectors, however, and line up with one another. Something important is going on when that happens so we call attention to these vectors by calling them eigenvectors. For these vectors, the operation of multiplying by reduces to the much simpler operation of scalar multiplying by . The reason eigenvectors are important is because it is extremely convenient to be able to replace matrix multiplication by scalar multiplication.    The usefulness of eigenvalues and eigenvectors  In the next section, we will introduce an algebraic technique for finding the eigenvalues and eigenvectors of a matrix. Before doing that, however, we would like to discuss why eigenvalues and eigenvectors are so useful.  Let's continue looking at the example . We have seen that is an eigenvector with eigenvalue and is an eigenvector with eigenvalue . This means that and . By the linearity of matrix multiplication, we can determine what happens when we multiply a linear combination of and by : .   For instance, if we consider the vector , we find that as seen in the figure.    In other words, multiplying by has the effect of stretching a vector in the direction by a factor of and flipping in direction.  We can draw an analogy with the more familiar example of the diagonal matrix . As we have seen, the matrix transformation defined by combines a horizontal stretching by a factor of 3 with a reflection across the horizontal axis, as is illustrated in .      The diagonal matrix stretches vectors horizontally by a factor of and flips vectors vertically.   The matrix has a similar effect when viewed in the basis defined by the eigenvectors and , as seen in .      The matrix has the same geometric effect as the diagonal matrix when expressed in the coordinate system defined by the basis of eigenvectors.   In a sense that will be made precise later, having a set of eigenvectors of that forms a basis of enables us to think of as being equivalent to a diagonal matrix . Of course, as the other examples in the previous activity show, it may not always be possible to form a basis from the eigenvectors of a matrix. For example, the only eigenvectors of the matrix , which represents a shear, have the form . In this example, we are not able to create a basis for consisting of eigenvectors of the matrix. This is also true for the matrix , which represents a rotation.    Let's consider an example that illustrates how we can put these ideas to use.  Suppose that we work for a car rental company that has two locations, and . When a customer rents a car at one location, they have the option to return it to either location at the end of the day. After doing some market research, we determine:  80% of the cars rented at location are returned to and 20% are returned to .  40% of the cars rented at location are returned to and 60% are returned to .    Suppose that there are 1000 cars at location and no cars at location on Monday morning. How many cars are there are locations and at the end of the day on Monday?  How many are at locations and at end of the day on Tuesday?  If we let and be the number of cars at locations and , respectively, at the end of day , we then have We can write the vector to reflect the number of cars at the two locations at the end of day , which says that or where .  Suppose that . Compute and to demonstrate that and are eigenvectors of . What are the associated eigenvalues and ?  We said that 1000 cars are initially at location and none at location . This means that the initial vector describing the number of cars is . Write as a linear combination of and .  Remember that and are eigenvectors of . Use the linearity of matrix multiplication to write the vector , describing the number of cars at the two locations at the end of the first day, as a linear combination of and .  Write the vector as a linear combination of and . Then write the next few vectors as linear combinations of and :   .   .   .   .    What will happen to the number of cars at the two locations after a very long time? Explain how writing as a linear combination of eigenvectors helps you determine the long-term behavior.     The solution to this activity is given in the text below.    This activity is important and motivates much of our work with eigenvalues and eigenvectors so it's worth reviewing to make sure we have a clear understanding of the concepts.  First, we compute This shows that is an eigenvector of with eigenvalue and is an eigenvector of with eigenvalue .  By the linearity of matrix matrix multiplication, we have . Therefore, we will write the vector describing the initial distribution of cars as a linear combination of and ; that is, . To do, we form the augmented matrix and row reduce: . Therefore, .  To determine the distribution of cars on subsequent days, we will repeatedly multiply by . We find that .  In particular, this shows us that . Taking notice of the pattern, we may write . Multiplying a number by is the same as taking 20% of that number. As each day goes by, the second term is multiplied by so the coefficient of in the expression for will eventually become extremely small. We therefore see that the distribution of cars will stabilize at .  Notice how our understanding of the eigenvectors of the matrix allows us to replace matrix multiplication with the simpler operation of scalar multiplication. As a result, we can look far into the future without having to repeatedly perform matrix multiplication.  Furthermore, notice how this example relies on the fact that we can express the initial vector as a linear combination of eigenvectors. For this reason, we would like, when given an matrix, to be able to create a basis of that consists of its eigenvectors. We will frequently return to this question in later sections.    If is an matrix, can we form a basis of consisting of eigenvectors of ?      Summary  We defined an eigenvector of a square matrix to be a nonzero vector such that for some scalar , which is called the eigenvalue associated to .  If is an eigenvector, then matrix multiplication by reduces to the simpler operation of scalar multiplication by .  Scalar multiples of an eigenvector are also eigenvectors. In fact, we will see that the eigenvectors associated to an eigenvalue form a subspace.  If we can form a basis for consisting of eigenvectors of , then is, in some sense, equivalent to a diagonal matrix.  Rewriting a vector as a linear combination of eigenvectors of simplifies the process of repeatedly multiplying by .       Consider the matrix and vectors .  Show that and are eigenvectors of and find their associated eigenvalues.    Express the vector as a linear combination of and .  Use this expression to compute , , and as a linear combination of eigenvectors.     We find that and so the associated eigenvalues are and .   .  We find      We find that and so the associated eigenvalues are and .  Setting up an augmented matrix and row reducing shows us that .  We then have      Consider the matrix and vectors   Show that the vectors , , and are eigenvectors of and find their associated eigenvalues.    Express the vector as a linear combination of the eigenvectors.  Use this expression to compute , , and as a linear combination of eigenvectors.     We see that , , and . The associated eigenvalues are , , and .   .  We find      We see that , , and . The associated eigenvalues are , , and .  After forming an augmented matrix, we find .  We then have      Suppose that is an matrix.  Explain why is an eigenvalue of if and only if there is a nonzero solution to the homogeneous equation .  Explain why is not invertible if and only if is an eigenvalue.  If is an eigenvector of having associated eigenvalue , explain why is also an eigenvector of with associated eigenvalue .  If is invertible and is eigenvector of having associated eigenvalue , explain why is also an eigenvector of with associated eigenvalue .  The matrix has eigenvectors and and associated eigenvalues and . What are some eigenvectors and associated eigenvalues for ?      If is an eigenvalue, then there is a nonzero vector such that .  If is an eigenvalue of , then there is a nonzero solution to the homogeneous equation .  If , we can multiply both sides by and to obtain .  Notice that , which means that is an eigenvector with associated eigenvalue .  The vectors and are eigenvectors of with associated eigenvalues and .     If is an eigenvalue, then there is a nonzero vector such that . This means that an associated eigenvector is a nonzero solution to the homogeneous equation .  If is an eigenvalue of , then there is a nonzero solution to the homogeneous equation , which happens exactly when is not invertible.  If is an eigenvector of with associated eigenvalue , then . Therefore, , which means that is an eigenvector with associated eigenvalue .  If , we can multiply both sides by and to obtain . This shows that is an eigenvector of with associated eigenvalue .  The vectors and are eigenvectors of with associated eigenvalues and .     Suppose that is a matrix with eigenvectors and and eigenvalues and as shown in .   The vectors and are eigenvectors of .      Sketch the vectors , , and .                For the following matrices, find the eigenvectors and associated eigenvalues by thinking geometrically about the corresponding matrix transformation.   .   .  What are the eigenvectors and associated eigenvalues of the identity matrix?  What are the eigenvectors and associated eigenvalues of a diagonal matrix with distinct diagonal entries?     Every two-dimensional vector is an eigenvector with associated eigenvalue .  We have eigenvectors with associated eigenvalue and with .  Every vector is an eigenvector of the identity matrix with associated eigenvalue .  The standard basis vectors are eigenvectors and the associated eigenvalues are the corresponding diagonal entries.     The corresponding matrix transformation stretches every two-dimensional vector by a factor of . Therefore, every two-dimensional vector is an eigenvector with associated eigenvalue .  The corresponding matrix transformation stretches vectors horizontally by a factor of and reflects them while stretching by a factor of vertically. We have eigenvectors with associated eigenvalue and with .  For any vector , we have . Therefore, every vector is an eigenvector of the identity matrix with associated eigenvalue .  The standard basis vectors are eigenvectors and the associated eigenvalues are the corresponding diagonal entries.     Suppose that is a matrix having eigenvectors and associated eigenvalues and .   If , find the vector .    Find the vectors and .    What is the matrix ?          .     and .     .         We have . Therefore, .    We have and so     From the results of the previous part, we have .       Determine whether the following statements are true or false and provide a justification for your response.  The eigenvalues of a diagonal matrix are equal to the entries on the diagonal.  If , then as well.  Every vector is an eigenvector of the identity matrix.  If is an eigenvalue of , then is invertible.  For every matrix , it is possible to find a basis of consisting of eigenvectors of .     True  False  True  False  False     True. The associated eigenvectors are the standard basis vectors .  False. .  True, because .  False. If is an eigenvalue of , then there is a nonzero solution to the homogeneous equation so is not invertible.  False. We saw the example , which represents a rotation and has no eigenvectors.     Suppose that is an matrix.   Assuming that is an eigenvector of whose associated eigenvector is nonzero, explain why is in .    Assuming that is an eigenvector of whose associated eigenvector is zero, explain why is in .    Consider the two special matrices below and find their eigenvectors and associated eigenvalues.      Because .  Because .  For the matrix , with associated eigenvalue , and and with associated eigenvalue .  For , with associated eigenvalue and and with associated eigenvalue .      In this case, we have or , which says that the equation is consistent.    With this assumption, , which means that is a solution to the homogeneous equation .    The column space of is spanned by and we notice that . Therefore, is an eigenvector with associated eigenvalue .  We also know that that the rank of this matrix is 1 so is two-dimensional. A basis for the null space is and so these vectors are eigenvectors with associated eigenvalue . In the same way, is an eigenvector of with associated eigenvalue and and are eigenvectors with associated eigenvalue .     For each of the following matrix transformations, describe the eigenvalues and eigenvectors of the corresponding matrix .  A reflection in in the line .  A rotation in .  A rotation in about the -axis.  A rotation in about the -axis.       with associated eigenvalue and with associated eigenvalue .  Every two-dimensional vector is an eigenvector with associated eigenvalue .   with associated eigenvalue . and with associated eigenvalue .   with associated eigenvalue .      A vector lying along the line of reflection is unchanged so , which shows that is an eigenvector with associated eigenvalue . At the same time, so is an eigenvector with associated eigenvalue .  Every vector satisfies so every two-dimensional vector is an eigenvector with associated eigenvalue .  Vectors along the -axis are unchanged so is an eigenvector with associated eigenvalue . Vectors in the -plane are multiplied by so and are eigenvectors with associated eigenvalue .  The vector is an eigenvector with associated eigenvalue . There are no other eigenvectors that are not scalar multiples of this one.      Suppose we have two species, and , where species preys on . Their populations, in millions, in year are denoted by and and satisfy . We will keep track of the populations in year using the vector so that .  Show that and are eigenvectors of and find their associated eigenvalues.  Suppose that the initial populations are described by the vector . Express as a linear combination of and .  Find the populations after one year, two years, and three years by writing the vectors , , and as linear combinations of and .  What is the general form for ?  After a very long time, what is the ratio of to ?      with associated eigenvalue and with associated eigenvalue .   .  We have   In general, .  The ratio of to is 1:3.     We can compute and . This means that is an eigenvector with associated eigenvalue and is an eigenvector with associated eigenvalue .  Setting up an augmented matrix and row reducing shows that .  We have   In general, .  After a long time, becomes large so that becomes very close to zero. This means that . So and . This means the ratio of to is 1:3.     "
},
{
  "id": "sec-eigen-intro-2-2",
  "level": "2",
  "url": "sec-eigen-intro.html#sec-eigen-intro-2-2",
  "type": "Preview Activity",
  "number": "20.1.1",
  "title": "",
  "body": "  Before we introduce the definition of eigenvectors and eigenvalues, it will be helpful to remember some ideas we have seen previously.    Suppose that is the vector shown in the figure. Sketch the vector and the vector .     State the geometric effect that scalar multiplication has on the vector . Then sketch all the vectors of the form where is a scalar.  State the geometric effect of the matrix transformation defined by .  Suppose that is a matrix and that and are vectors such that . Use the linearity of matrix multiplication to express the following vectors in terms of and .   .   .   .   .   .   .        The vectors are as shown.     Scalar multiplication has the effect of stretching and possibly flipping along the line defined by .  This matrix transformation stretches vectors by a factor of in the horizontal direction and flips vectors vertically.  Applying linearity, we see that   .   .   .   .   .   .      "
},
{
  "id": "sec-eigen-intro-3-3",
  "level": "2",
  "url": "sec-eigen-intro.html#sec-eigen-intro-3-3",
  "type": "Definition",
  "number": "20.1.1",
  "title": "",
  "body": "  eigenvalue  eigenvector  Given a square matrix , we say that a nonzero vector is an eigenvector of if there is a scalar such that . The scalar is called the eigenvalue associated to the eigenvector .   "
},
{
  "id": "sec-eigen-intro-3-5",
  "level": "2",
  "url": "sec-eigen-intro.html#sec-eigen-intro-3-5",
  "type": "Example",
  "number": "20.1.2",
  "title": "",
  "body": "  Consider the matrix and the vector . We find that . In other words, , which says that is an eigenvector of the matrix with associated eigenvalue .  Similarly, if , we find that . Here again, we have showing that is an eigenvector of with associated eigenvalue .   "
},
{
  "id": "activity-eigen-geom",
  "level": "2",
  "url": "sec-eigen-intro.html#activity-eigen-geom",
  "type": "Activity",
  "number": "20.1.2",
  "title": "",
  "body": "  This definition has an important geometric interpretation that we will investigate here.  Suppose that is a nonzero vector and that is a scalar. What is the geometric relationship between and ?  Let's now consider the eigenvector condition: . Here we have two vectors, and . If , what is the geometric relationship between and ?    A geometric interpretation of the eigenvalue-eigenvector condition .    Choose the matrix . Move the vector so that the eigenvector condition holds. What is the eigenvector and what is the associated eigenvalue?  By algebraically computing , verify that the eigenvector condition holds for the vector that you found.  If you multiply the eigenvector that you found by , do you still have an eigenvector? If so, what is the associated eigenvalue?  Are you able to find another eigenvector that is not a scalar multiple of the first one that you found? If so, what is the eigenvector and what is the associated eigenvalue?  Now consider the matrix . Use the diagram to describe any eigenvectors and associated eigenvalues.  Finally, consider the matrix . Use the diagram to describe any eigenvectors and associated eigenvalues. What geometric transformation does this matrix perform on vectors? How does this explain the presence of any eigenvectors?       The vectors and lie on the same line.  The vectors and lie on the same line.  There are many possibilities, but we see that is an eigenvector with associated eigenvalue .  If we perform the matrix multiplication, we see that .  Yes, is still an eigenvector with associated eigenvalue .  We see that is an eigenvector with associated eigenvalue .  The only eigenvectors that appear are scalar multiples of with associated eigenvalue .  There are no eigenvectors. The matrix transformation rotates vectors by so it is not possible for and to lie on the same line.    "
},
{
  "id": "fig-eigen-intro",
  "level": "2",
  "url": "sec-eigen-intro.html#fig-eigen-intro",
  "type": "Figure",
  "number": "20.1.4",
  "title": "",
  "body": "     On the left, the vector is not an eigenvector. On the right, the vector is an eigenvector with eigenvalue .  "
},
{
  "id": "fig-eigen-intro-2",
  "level": "2",
  "url": "sec-eigen-intro.html#fig-eigen-intro-2",
  "type": "Figure",
  "number": "20.1.5",
  "title": "",
  "body": "    Here we see another eigenvector with eigenvalue .  "
},
{
  "id": "fig-eigen-intro-diagonal",
  "level": "2",
  "url": "sec-eigen-intro.html#fig-eigen-intro-diagonal",
  "type": "Figure",
  "number": "20.1.6",
  "title": "",
  "body": "    The diagonal matrix stretches vectors horizontally by a factor of and flips vectors vertically.  "
},
{
  "id": "fig-eigen-intro-A",
  "level": "2",
  "url": "sec-eigen-intro.html#fig-eigen-intro-A",
  "type": "Figure",
  "number": "20.1.7",
  "title": "",
  "body": "    The matrix has the same geometric effect as the diagonal matrix when expressed in the coordinate system defined by the basis of eigenvectors.  "
},
{
  "id": "activity-eigen-intro",
  "level": "2",
  "url": "sec-eigen-intro.html#activity-eigen-intro",
  "type": "Activity",
  "number": "20.1.3",
  "title": "",
  "body": "  Let's consider an example that illustrates how we can put these ideas to use.  Suppose that we work for a car rental company that has two locations, and . When a customer rents a car at one location, they have the option to return it to either location at the end of the day. After doing some market research, we determine:  80% of the cars rented at location are returned to and 20% are returned to .  40% of the cars rented at location are returned to and 60% are returned to .    Suppose that there are 1000 cars at location and no cars at location on Monday morning. How many cars are there are locations and at the end of the day on Monday?  How many are at locations and at end of the day on Tuesday?  If we let and be the number of cars at locations and , respectively, at the end of day , we then have We can write the vector to reflect the number of cars at the two locations at the end of day , which says that or where .  Suppose that . Compute and to demonstrate that and are eigenvectors of . What are the associated eigenvalues and ?  We said that 1000 cars are initially at location and none at location . This means that the initial vector describing the number of cars is . Write as a linear combination of and .  Remember that and are eigenvectors of . Use the linearity of matrix multiplication to write the vector , describing the number of cars at the two locations at the end of the first day, as a linear combination of and .  Write the vector as a linear combination of and . Then write the next few vectors as linear combinations of and :   .   .   .   .    What will happen to the number of cars at the two locations after a very long time? Explain how writing as a linear combination of eigenvectors helps you determine the long-term behavior.     The solution to this activity is given in the text below.   "
},
{
  "id": "question-eigen-basis",
  "level": "2",
  "url": "sec-eigen-intro.html#question-eigen-basis",
  "type": "Question",
  "number": "20.1.8",
  "title": "",
  "body": "  If is an matrix, can we form a basis of consisting of eigenvectors of ?   "
},
{
  "id": "sec-eigen-intro-6-1",
  "level": "2",
  "url": "sec-eigen-intro.html#sec-eigen-intro-6-1",
  "type": "Exercise",
  "number": "20.1.4.1",
  "title": "",
  "body": " Consider the matrix and vectors .  Show that and are eigenvectors of and find their associated eigenvalues.    Express the vector as a linear combination of and .  Use this expression to compute , , and as a linear combination of eigenvectors.     We find that and so the associated eigenvalues are and .   .  We find      We find that and so the associated eigenvalues are and .  Setting up an augmented matrix and row reducing shows us that .  We then have    "
},
{
  "id": "sec-eigen-intro-6-2",
  "level": "2",
  "url": "sec-eigen-intro.html#sec-eigen-intro-6-2",
  "type": "Exercise",
  "number": "20.1.4.2",
  "title": "",
  "body": " Consider the matrix and vectors   Show that the vectors , , and are eigenvectors of and find their associated eigenvalues.    Express the vector as a linear combination of the eigenvectors.  Use this expression to compute , , and as a linear combination of eigenvectors.     We see that , , and . The associated eigenvalues are , , and .   .  We find      We see that , , and . The associated eigenvalues are , , and .  After forming an augmented matrix, we find .  We then have    "
},
{
  "id": "sec-eigen-intro-6-3",
  "level": "2",
  "url": "sec-eigen-intro.html#sec-eigen-intro-6-3",
  "type": "Exercise",
  "number": "20.1.4.3",
  "title": "",
  "body": " Suppose that is an matrix.  Explain why is an eigenvalue of if and only if there is a nonzero solution to the homogeneous equation .  Explain why is not invertible if and only if is an eigenvalue.  If is an eigenvector of having associated eigenvalue , explain why is also an eigenvector of with associated eigenvalue .  If is invertible and is eigenvector of having associated eigenvalue , explain why is also an eigenvector of with associated eigenvalue .  The matrix has eigenvectors and and associated eigenvalues and . What are some eigenvectors and associated eigenvalues for ?      If is an eigenvalue, then there is a nonzero vector such that .  If is an eigenvalue of , then there is a nonzero solution to the homogeneous equation .  If , we can multiply both sides by and to obtain .  Notice that , which means that is an eigenvector with associated eigenvalue .  The vectors and are eigenvectors of with associated eigenvalues and .     If is an eigenvalue, then there is a nonzero vector such that . This means that an associated eigenvector is a nonzero solution to the homogeneous equation .  If is an eigenvalue of , then there is a nonzero solution to the homogeneous equation , which happens exactly when is not invertible.  If is an eigenvector of with associated eigenvalue , then . Therefore, , which means that is an eigenvector with associated eigenvalue .  If , we can multiply both sides by and to obtain . This shows that is an eigenvector of with associated eigenvalue .  The vectors and are eigenvectors of with associated eigenvalues and .   "
},
{
  "id": "sec-eigen-intro-6-4",
  "level": "2",
  "url": "sec-eigen-intro.html#sec-eigen-intro-6-4",
  "type": "Exercise",
  "number": "20.1.4.4",
  "title": "",
  "body": " Suppose that is a matrix with eigenvectors and and eigenvalues and as shown in .   The vectors and are eigenvectors of .      Sketch the vectors , , and .              "
},
{
  "id": "sec-eigen-intro-6-5",
  "level": "2",
  "url": "sec-eigen-intro.html#sec-eigen-intro-6-5",
  "type": "Exercise",
  "number": "20.1.4.5",
  "title": "",
  "body": " For the following matrices, find the eigenvectors and associated eigenvalues by thinking geometrically about the corresponding matrix transformation.   .   .  What are the eigenvectors and associated eigenvalues of the identity matrix?  What are the eigenvectors and associated eigenvalues of a diagonal matrix with distinct diagonal entries?     Every two-dimensional vector is an eigenvector with associated eigenvalue .  We have eigenvectors with associated eigenvalue and with .  Every vector is an eigenvector of the identity matrix with associated eigenvalue .  The standard basis vectors are eigenvectors and the associated eigenvalues are the corresponding diagonal entries.     The corresponding matrix transformation stretches every two-dimensional vector by a factor of . Therefore, every two-dimensional vector is an eigenvector with associated eigenvalue .  The corresponding matrix transformation stretches vectors horizontally by a factor of and reflects them while stretching by a factor of vertically. We have eigenvectors with associated eigenvalue and with .  For any vector , we have . Therefore, every vector is an eigenvector of the identity matrix with associated eigenvalue .  The standard basis vectors are eigenvectors and the associated eigenvalues are the corresponding diagonal entries.   "
},
{
  "id": "sec-eigen-intro-6-6",
  "level": "2",
  "url": "sec-eigen-intro.html#sec-eigen-intro-6-6",
  "type": "Exercise",
  "number": "20.1.4.6",
  "title": "",
  "body": " Suppose that is a matrix having eigenvectors and associated eigenvalues and .   If , find the vector .    Find the vectors and .    What is the matrix ?          .     and .     .         We have . Therefore, .    We have and so     From the results of the previous part, we have .     "
},
{
  "id": "sec-eigen-intro-6-7",
  "level": "2",
  "url": "sec-eigen-intro.html#sec-eigen-intro-6-7",
  "type": "Exercise",
  "number": "20.1.4.7",
  "title": "",
  "body": " Determine whether the following statements are true or false and provide a justification for your response.  The eigenvalues of a diagonal matrix are equal to the entries on the diagonal.  If , then as well.  Every vector is an eigenvector of the identity matrix.  If is an eigenvalue of , then is invertible.  For every matrix , it is possible to find a basis of consisting of eigenvectors of .     True  False  True  False  False     True. The associated eigenvectors are the standard basis vectors .  False. .  True, because .  False. If is an eigenvalue of , then there is a nonzero solution to the homogeneous equation so is not invertible.  False. We saw the example , which represents a rotation and has no eigenvectors.   "
},
{
  "id": "sec-eigen-intro-6-8",
  "level": "2",
  "url": "sec-eigen-intro.html#sec-eigen-intro-6-8",
  "type": "Exercise",
  "number": "20.1.4.8",
  "title": "",
  "body": " Suppose that is an matrix.   Assuming that is an eigenvector of whose associated eigenvector is nonzero, explain why is in .    Assuming that is an eigenvector of whose associated eigenvector is zero, explain why is in .    Consider the two special matrices below and find their eigenvectors and associated eigenvalues.      Because .  Because .  For the matrix , with associated eigenvalue , and and with associated eigenvalue .  For , with associated eigenvalue and and with associated eigenvalue .      In this case, we have or , which says that the equation is consistent.    With this assumption, , which means that is a solution to the homogeneous equation .    The column space of is spanned by and we notice that . Therefore, is an eigenvector with associated eigenvalue .  We also know that that the rank of this matrix is 1 so is two-dimensional. A basis for the null space is and so these vectors are eigenvectors with associated eigenvalue . In the same way, is an eigenvector of with associated eigenvalue and and are eigenvectors with associated eigenvalue .   "
},
{
  "id": "sec-eigen-intro-6-9",
  "level": "2",
  "url": "sec-eigen-intro.html#sec-eigen-intro-6-9",
  "type": "Exercise",
  "number": "20.1.4.9",
  "title": "",
  "body": " For each of the following matrix transformations, describe the eigenvalues and eigenvectors of the corresponding matrix .  A reflection in in the line .  A rotation in .  A rotation in about the -axis.  A rotation in about the -axis.       with associated eigenvalue and with associated eigenvalue .  Every two-dimensional vector is an eigenvector with associated eigenvalue .   with associated eigenvalue . and with associated eigenvalue .   with associated eigenvalue .      A vector lying along the line of reflection is unchanged so , which shows that is an eigenvector with associated eigenvalue . At the same time, so is an eigenvector with associated eigenvalue .  Every vector satisfies so every two-dimensional vector is an eigenvector with associated eigenvalue .  Vectors along the -axis are unchanged so is an eigenvector with associated eigenvalue . Vectors in the -plane are multiplied by so and are eigenvectors with associated eigenvalue .  The vector is an eigenvector with associated eigenvalue . There are no other eigenvectors that are not scalar multiples of this one.    "
},
{
  "id": "sec-eigen-intro-6-10",
  "level": "2",
  "url": "sec-eigen-intro.html#sec-eigen-intro-6-10",
  "type": "Exercise",
  "number": "20.1.4.10",
  "title": "",
  "body": " Suppose we have two species, and , where species preys on . Their populations, in millions, in year are denoted by and and satisfy . We will keep track of the populations in year using the vector so that .  Show that and are eigenvectors of and find their associated eigenvalues.  Suppose that the initial populations are described by the vector . Express as a linear combination of and .  Find the populations after one year, two years, and three years by writing the vectors , , and as linear combinations of and .  What is the general form for ?  After a very long time, what is the ratio of to ?      with associated eigenvalue and with associated eigenvalue .   .  We have   In general, .  The ratio of to is 1:3.     We can compute and . This means that is an eigenvector with associated eigenvalue and is an eigenvector with associated eigenvalue .  Setting up an augmented matrix and row reducing shows that .  We have   In general, .  After a long time, becomes large so that becomes very close to zero. This means that . So and . This means the ratio of to is 1:3.   "
},
{
  "id": "sec-eigen-find",
  "level": "1",
  "url": "sec-eigen-find.html",
  "type": "Section",
  "number": "20.2",
  "title": "Finding eigenvalues and eigenvectors",
  "body": " Finding eigenvalues and eigenvectors   The last section introduced eigenvalues and eigenvectors, presented the underlying geometric intuition behind their definition, and demonstrated their use in understanding the long-term behavior of certain systems. We will now develop a more algebraic understanding of eigenvalues and eigenvectors. In particular, we will find an algebraic method for determining the eigenvalues and eigenvectors of a square matrix.    Let's begin by reviewing some important ideas that we have seen previously.  Suppose that is a square matrix and that the nonzero vector is a solution to the homogeneous equation . What can we conclude about the invertibility of ?  How does the determinant tell us if there is a nonzero solution to the homogeneous equation ?  Suppose that . Find the determinant . What does this tell us about the solution space to the homogeneous equation ?  Find a basis for .  What is the relationship between the rank of a matrix and the dimension of its null space?      The matrix cannot have a pivot position in every column so it is not invertible.  If there is a nonzero solution to the homogeneous equation , then is not invertible so .  We find that so there is a nonzero solution to the homogeneous equation.  The reduced row echelon form of is so the solution space to the homogeneous equation may be described parametrically as . A basis for is therefore .  If is an matrix, then .       The characteristic polynomial  We will first see that the eigenvalues of a square matrix appear as the roots of a particular polynomial. To begin, notice that we originally defined an eigenvector as a nonzero vector that satisfies the equation . We will rewrite this as In other words, an eigenvector is a solution of the homogeneous equation . This puts us in the familiar territory explored in the next activity.    The eigenvalues of a square matrix are defined by the condition that there be a nonzero solution to the homogeneous equation .  If there is a nonzero solution to the homogeneous equation , what can we conclude about the invertibility of the matrix ?  If there is a nonzero solution to the homogeneous equation , what can we conclude about the determinant ?  Let's consider the matrix from which we construct . Find the determinant . What kind of equation do you obtain when we set this determinant to zero to obtain ?  Use the determinant you found in the previous part to find the eigenvalues by solving the equation . We considered this matrix in so we should find the same eigenvalues for that we found by reasoning geometrically there.  Consider the matrix and find its eigenvalues by solving the equation .  Consider the matrix and find its eigenvalues by solving the equation .  Find the eigenvalues of the triangular matrix . What is generally true about the eigenvalues of a triangular matrix?         The matrix cannot be invertible.    It must be the case that .    We find that .     so we find eigenvalues and .    For this matrix, we have so there is one eigenvalue, .     so there are complex eigenvalues, and .    Because the determinant of a triangular matrix equals the product of its diagonal entries, The eigenvalues are equal to the entries on the diagonal.       This activity demonstrates a technique that enables us to find the eigenvalues of a square matrix . Since an eigenvalue is a scalar for which the equation has a nonzero solution, it must be the case that is not invertible. Therefore, its determinant is zero. This gives us the equation whose solutions are the eigenvalues of . This equation is called the characteristic equation of . characteristic equation    If we write the characteristic equation for the matrix , we see that This shows us that the eigenvalues are and .    characteristic polynomial In general, the expression is a polynomial in , which is called the characteristic polynomial of . If is an matrix, the degree of the characteristic polynomial is . For instance, if is a matrix, then is a quadratic polynomial; if is a matrix, then is a cubic polynomial.  The matrix in has a characteristic polynomial with two real and distinct roots. This will not always be the case, as demonstrated in the next two examples.   Consider the matrix , whose characteristic equation is In this case, the characteristic polynomial has one real root, which means that this matrix has a single real eigenvalue, .    To find the eigenvalues of a triangular matrix, we remember that the determinant of a triangular matrix is the product of the entries on the diagonal. For instance, the following triangular matrix has the characteristic equation showing that the eigenvalues are the diagonal entries .     Finding eigenvectors  Now that we can find the eigenvalues of a square matrix by solving the characteristic equation , we will turn to the question of finding the eigenvectors associated to an eigenvalue . The key, as before, is to note that an eigenvector is a nonzero solution to the homogeneous equation . In other words, the eigenvectors associated to an eigenvalue form the null space .  This shows that the eigenvectors associated to an eigenvalue form a subspace of . We will denote the subspace of eigenvectors of a matrix associated to the eigenvalue by and note that . We say that is the eigenspace of associated to the eigenvalue . eigenspace     In this activity, we will find the eigenvectors of a matrix as the null space of the matrix .  Let's begin with the matrix . We have seen that is an eigenvalue. Form the matrix and find a basis for the eigenspace . What is the dimension of this eigenspace? For each of the basis vectors , verify that .  We also saw that is an eigenvalue. Form the matrix and find a basis for the eigenspace . What is the dimension of this eigenspace? For each of the basis vectors , verify that .  Is it possible to form a basis of consisting of eigenvectors of ?  Now consider the matrix . Write the characteristic equation for and use it to find the eigenvalues of . For each eigenvalue, find a basis for its eigenspace . Is it possible to form a basis of consisting of eigenvectors of ?  Next, consider the matrix . Write the characteristic equation for and use it to find the eigenvalues of . For each eigenvalue, find a basis for its eigenspace . Is it possible to form a basis of consisting of eigenvectors of ?  Finally, find the eigenvalues and eigenvectors of the diagonal matrix . Explain your result by considering the geometric effect of the matrix transformation defined by .       We have The null space is one-dimensional with basis .  We have The null space is one-dimensional with basis .  We can form a basis for consisting of eigenvectors of by taking .  The characteristic equation is , which means that there is a single eigenvalue . This eigenspace is two-dimensional with basis . In this case, we can form a basis for consisting of eigenvectors of .  The characteristic equation is so there is again a single eigenvalue . In this case, the eigenspace is one-dimensional with basis vector . It is not possible to form a basis for consisting of eigenvectors.  We have eigenvectors with associated eigenvector and with associated eigenvector .     Once we find an eigenvalue of a matrix , describing the associated eigenspace amounts to the familiar task of describing the null space .   Revisiting the matrix from , we recall that we found eigenvalues and .  Considering the eigenvalue , we have Since the eigenvectors are the solutions of the equation , we see that they are determined by the single equation or . Therefore the eigenvectors in have the form In other words, is a one-dimensional subspace of with basis vector or basis vector . In the same way, we find that a basis for the eigenspace is .  We note that, for this matrix, it is possible to construct a basis of consisting of eigenvectors, namely,     Consider the matrix whose characteristic equation is   There is a single eigenvalue , and we find that Therefore, the eigenspace is one-dimensional with a basis vector .    If , then which implies that there is a single eigenvalue . We find that which says that every two-dimensional vector satisfies . Therefore, every vector is an eigenvector and so . This eigenspace is two-dimensional.  We can see this in another way. The matrix transformation defined by rotates vectors by , which says that for every vector . In other words, every two-dimensional vector is an eigenvector with associated eigenvalue .   These last two examples illustrate two types of behavior when there is a single eigenvalue. In one case, we are able to construct a basis of using eigenvectors; in the other, we are not. We will explore this behavior more in the next subsection.   A check on our work  When finding eigenvalues and their associated eigenvectors in this way, we first find eigenvalues by solving the characteristic equation. If is a solution to the characteristic equation, then is not invertible and, consequently, must contain a row without a pivot position.  This serves as a check on our work. If we row reduce and find the identity matrix, then we have made an error either in solving the characteristic equation or in finding .     The characteristic polynomial and the dimension of eigenspaces  Given a square matrix , we saw in the previous section the value of being able to express any vector in as a linear combination of eigenvectors of . For this reason, asks when we can construct a basis of consisting of eigenvectors. We will explore this question more fully now.  As we saw above, the eigenvalues of are the solutions of the characteristic equation . The examples we have considered demonstrate some different types of behavior. For instance, we have seen the characteristic equations    , which has real and distinct roots,     , which has repeated roots, and     , which has complex roots.     If is an matrix, then the characteristic polynomial is a degree polynomial, and this means that it has roots. Therefore, the characteristic equation can be written as giving eigenvalues . As we have seen, some of the eigenvalues may be complex. Moreover, some of the eigenvalues may appear in this list more than once. However, we can always write the characteristic equation in the form The number of times that appears as a factor in the characteristic polynomial, is called the multiplicity of the eigenvalue . multiplicity     We have seen that the matrix has the characteristic equation . This matrix has a single eigenvalue , which has multiplicity .      If a matrix has the characteristic equation , then that matrix has four eigenvalues: having multiplicity 2; having multiplicity 1; having multiplicity 7; and having multiplicity 2. The degree of the characteristic polynomial is the sum of the multiplicities so this matrix must be a matrix.    The multiplicities of the eigenvalues are important because they influence the dimension of the eigenspaces. We know that the dimension of an eigenspace must be at least one; the following proposition also tells us the dimension of an eigenspace can be no larger than the multiplicity of its associated eigenvalue.    If is a real eigenvalue of the matrix with multiplicity , then .      The diagonal matrix has the characteristic equation . There is a single eigenvalue having multiplicity , and we saw earlier that .      The matrix has the characteristic equation . This tells us that there is a single eigenvalue having multiplicity . In contrast with the previous example, we have .      We saw earlier that the matrix has the characteristic equation . There are three eigenvalues each having multiplicity . By the proposition, we are guaranteed that the dimension of each eigenspace is ; that is, . It turns out that this is enough to guarantee that there is a basis of consisting of eigenvectors.      If a matrix has the characteristic equation , we know there are four eigenvalues . Without more information, all we can say about the dimensions of the eigenspaces is We can guarantee that , but we cannot be more specific about the dimensions of the other eigenspaces.    Fortunately, if we have an matrix, it frequently happens that the characteristic equation has the form where there are distinct real eigenvalues, each of which has multiplicity . In this case, the dimension of each of the eigenspaces . With a little work, it can be seen that choosing a basis vector for each of the eigenspaces produces a basis for . We therefore have the following proposition.    If is an matrix having distinct real eigenvalues, then there is a basis of consisting of eigenvectors of .    This proposition provides one answer to our . The next activity explores this question further.      Identify the eigenvalues, and their multiplicities, of an matrix whose characteristic polynomial is . What can you conclude about the dimensions of the eigenspaces? What is the shape of the matrix? Do you have enough information to guarantee that there is a basis of consisting of eigenvectors?  Find the eigenvalues of and state their multiplicities. Can you find a basis of consisting of eigenvectors of this matrix?  Consider the matrix whose characteristic equation is .  Identify the eigenvalues and their multiplicities.  For each eigenvalue , find a basis of the eigenspace and state its dimension.  Is there a basis of consisting of eigenvectors of ?   Now consider the matrix whose characteristic equation is also .  Identify the eigenvalues and their multiplicities.  For each eigenvalue , find a basis of the eigenspace and state its dimension.  Is there a basis of consisting of eigenvectors of ?   Consider the matrix whose characteristic equation is .  Identify the eigenvalues and their multiplicities.  For each eigenvalue , find a basis of the eigenspace and state its dimension.  Is there a basis of consisting of eigenvectors of ?        There are three eigenvalues, has multiplicity , has multiplicity , and has multiplicity . We know that We can guarantee that , but we can say nothing further about the other two eigenspaces.  The dimension of the matrix is since the degree of the characteristic polynomial is . We cannot guarantee that we can form a basis for consisting of eigenvectors, however.  There is one eigenvalue having multiplicity two. Because the eigenspace is one-dimensional, however, we cannot find a basis for consisting of eigenvectors of .  For the matrix ,  We have eigenvalues with multiplicity and with multiplicity .  The eigenspace is two-dimensional with basis . The eigenspace is one-dimensional with basis vector .  We are able to form a basis for consisting of eigenvectors of .    For the matrix ,  We have eigenvalues with multiplicity and with multiplicity .  The eigenspace is one-dimensional with basis vector . The eigenspace is also one-dimensional with basis vector .  It is not possible to form a basis for consisting of eigenvectors of .    For this matrix,  There are three eigenvalues , , and , each having multiplicity .  A basis vector for the eigenspace is . A basis vector for the eigenspace is . A basis vector for the eigenspace is .  We can form a basis for consisting of eigenvectors of .         Using Sage to find eigenvalues and eigenvectors  We can use Sage to find the characteristic polynomial, eigenvalues, and eigenvectors of a matrix. As we will see, however, some care is required when dealing with matrices whose entries include decimals.    We will use Sage to find the eigenvalues and eigenvectors of a matrix. Let's begin with the matrix .  We can find the characteristic polynomial of by writing A.charpoly('lambda') . Notice that we have to give Sage a variable in which to write the polynomial; here, we use lambda though x works just as well.   The factored form of the characteristic polynomial may be more useful since it will tell us the eigenvalues and their multiplicities. The factored characteristic polynomial is found with A.fcp('lambda') .    If we only want the eigenvalues, we can use A.eigenvalues() .   Notice that the multiplicity of an eigenvalue is the number of times it is repeated in the list of eigenvalues.   Finally, we can find eigenvectors by A.eigenvectors_right() . (We are looking for right eigenvalues since the vector appears to the right of in the definition .)   At first glance, the result of this command can be a little confusing to interpret. What we see is a list with one entry for each eigenvalue. For each eigenvalue, there is a triple consisting of (i) the eigenvalue , (ii) a basis for , and (iii) the multiplicity of .   When working with decimal entries, which are called floating point numbers in computer science, we must remember that computers perform only approximate arithmetic. This is a problem when we wish to find the eigenvectors of such a matrix. To illustrate, consider the matrix .  Without using Sage, find the eigenvalues of this matrix.  What do you find for the reduced row echelon form of ?  Let's now use Sage to determine the reduced row echelon form of :   What result does Sage report for the reduced row echelon form? Why is this result not correct?   Because the arithmetic Sage performs with floating point entries is only approximate, we are not able to find the eigenspace . In this next chapter, we will learn how to address this issue. In the meantime, we can get around this problem by writing the entries in the matrix as rational numbers:          The fcp command will return the factored characteristic polynomial lambda^2 - 2*lambda - 3 .  The eigenvalues command returns a list of eigenvalues [-3, -3] .  The eigenvectors_right command returns [(-3, [(1, 0)], 2)] .  If we begin with the matrix , we find  The eigenvalues are and .  The reduced row echelon form is , which shows that is not invertible, as expected.  Sage returns , which is not correct because cannot be invertible if is an eigenvalue of .  Here we find the correct eigenvalues, with basis vector for and with basis vector for .         Summary  In this section, we developed a technique for finding the eigenvalues and eigenvectors of an matrix .  The expression is a degree polynomial, known as the characteristic polynomial of . The eigenvalues of are the roots of the characteristic polynomial found by solving the characteristic equation .  The set of eigenvectors associated to the eigenvalue forms a subspace of , the eigenspace .  If the factor appears times in the characteristic polynomial, we say that the eigenvalue has multiplicity and note that .  If each of the eigenvalues is real and has multiplicity , then we can form a basis of consisting of eigenvectors of .  We can use Sage to find the eigenvalues and eigenvalues of matrices. However, we need to be careful working with floating point numbers since floating point arithmetic is only an approximation.      For each of the following matrices, find its characteristic polynomial, its eigenvalues, and the multiplicity of each eigenvalue.   .   .   .   .      . There is a single eigenvalue having multiplicity .   . There are three eigenvalues , each of multiplicity .   . There is one eigenvalue having multiplicity .   . There are two eigenvalues and , each having multiplicity .     The characteristic polynomial is . There is a single eigenvalue having multiplicity .  The characteristic polynomial is . There are three eigenvalues , each of multiplicity .  The characteristic polynomial is , showing that there is one eigenvalue having multiplicity .  The characteristic polynomial is . There are two eigenvalues and , each having multiplicity .     Given an matrix , an important question, , asks whether we can find a basis of consisting of eigenvectors of . For each of the matrices in the previous exercise, find a basis of consisting of eigenvectors or state why such a basis does not exist.    It is not possible.   .            There is a single eigenvalue and so it is not possible to find a basis for consisting of eigenvectors of .  The three eigenvalues each have multiplicity one so we know that their eigenspaces are one-dimensional. We find a basis vector for is , for is , and for is . A basis for consisting of eigenvectors of is therefore   Since is the zero matrix, every vector is an eigenvector of . This means that is a basis of consisting of eigenvectors.  The multiplicity of each eigenvalue is one so there will be a basis of consisting of eigenvectors. In particular, a basis vector for is and a basis vector for is . This means that is a basis for consisting of eigenvectors of .      Determine whether the following statements are true or false and provide a justification for your response.  The eigenvalues of a matrix are the entries on the diagonal of .  If is an eigenvalue of multiplicity , then is one-dimensional.  If a matrix is invertible, then cannot be an eigenvalue.  If is a matrix, the characteristic polynomial has degree less than .  The eigenspace of is the same as the null space .     False  True  True  False  True     False. This is true for a diagonal matrix, but it is not generally true as we see by considering the matrix whose eigenvalues are and .  True. If is the multiplicity, we have so we must have .  True. If is an eigenvalue, then an associated eigenvector is a nonzero solution to the homogeneous equation . This would say that is not invertible.  False. The degree of the characteristic polynomial equals the number of rows and columns of the square matrix.  True. An eigenvector associated to the eigenvalue satisfies . This is the same equation that characterizes the null space .     Provide a justification for your response to the following questions.  Suppose that is a matrix having eigenvalues . What are the eigenvalues of ?  Suppose that is a diagonal matrix. Why can you guarantee that there is a basis of consisting of eigenvectors of ?  If is a matrix whose eigenvalues are , can you guarantee that there is a basis of consisting of eigenvectors of ?  Suppose that the characteristic polynomial of a matrix is . What are the eigenvalues of ? Is invertible? Is there a basis of consisting of eigenvectors of ?  If the characteristic polynomial of is , what is the characteristic polynomial of ? what is the characteristic polynomial of ?      .  The standard basis vectors are eigenvectors associated to the diagonal entries.  Yes.   . The matrix is not invertible, but there is a basis for consisting of eigenvectors of .   .     If , then . This says that the eigenvalues of are .  The standard basis vectors are eigenvectors associated to the diagonal entries.  Yes. Since there are three distinct eigenvalues, the multiplicity of each eigenvalue must be one. Therefore, the dimension of each eigenspace is one. If we choose a basis vector for each of the eigenspaces, we will obtain a basis for .  The eigenvalues are determined by , which shows that the eigenvalues are . This shows that is not invertible since is an eigenvalue. There is, however, a basis for consisting of eigenvectors of since the three eigenvalues are distinct.  If is an eigenvalue of , then is an eigenvalue of . Therefore, the characteristic polynomial of is .     For each of the following matrices, use Sage to determine its eigenvalues, their multiplicities, and a basis for each eigenspace. For which matrices is it possible to construct a basis for consisting of eigenvectors?                 A basis of eigenvectors is   It is not possible to find a basis of eigenvectors.  A basis of eigenvectors is      Sage tells us there are three distinct eigenvalues , each having multiplicity one. A basis of eigenvectors is   It is not possible to find a basis of eigenvectors because the eigenvalue has multiplicity two but its eigenspace is one-dimensional.  It is possible to find a basis for consisting of eignevectors because one eigenvalue has multiplicity one while the other has multiplicity two and a two-dimensional eigenspace. A basis is      There is a relationship between the determinant of a matrix and the product of its eigenvalues.  We have seen that the eigenvalues of the matrix are . What is ? What is the product of the eigenvalues of ?  Consider the triangular matrix . What are the eigenvalues of ? What is ? What is the product of the eigenvalues of ?  Based on these examples, what do you think is the relationship between the determinant of a matrix and the product of its eigenvalues?  Suppose the characteristic polynomial is written as . By substituting into this equation, explain why the determinant of a matrix equals the product of its eigenvalues.     Both equal .  Both equal .   equals the product of the eigenvalues.  We see that .     We have , and the product of the eigenvalues is .  We have , and the product of the eigenvalues is   We suspect that equals the product of the eigenvalues.  Setting , we have Notice that we mean the product of the eigenvalues, including their multiplicities.     Consider the matrix .  Find the eigenvalues of and a basis for their associated eigenspaces.  Suppose that . Express as a linear combination of eigenvectors of .  Define the vectors . Write , , and as a linear combination of eigenvectors of .  What happens to as grows larger and larger?      with associated eigenvector and with associated eigenvector .   .  We have .   .     We find the eigenvalues with associated eigenvector and with associated eigenvector .  After constructing an augmented matrix, we find that .  Since , we have .  We have . As grows larger, becomes less significant. Eventually, .     Consider the matrix   Find the eigenvalues of and a basis for their associated eigenspaces.  Suppose that . Express as a linear combination of eigenvectors of .  Define the vectors . Write , , and as a linear combination of eigenvectors of .  What happens to as grows larger and larger?      with associated eigenvector and with associated eigenvector .   .  We have .   .     We have eigenvalues with associated eigenvector and with associated eigenvector .  After constructing an augmented matrix, we find .  Since , we have   We have . As grows larger, becomes less significant. Eventually, .     "
},
{
  "id": "sec-eigen-find-2-2",
  "level": "2",
  "url": "sec-eigen-find.html#sec-eigen-find-2-2",
  "type": "Preview Activity",
  "number": "20.2.1",
  "title": "",
  "body": "  Let's begin by reviewing some important ideas that we have seen previously.  Suppose that is a square matrix and that the nonzero vector is a solution to the homogeneous equation . What can we conclude about the invertibility of ?  How does the determinant tell us if there is a nonzero solution to the homogeneous equation ?  Suppose that . Find the determinant . What does this tell us about the solution space to the homogeneous equation ?  Find a basis for .  What is the relationship between the rank of a matrix and the dimension of its null space?      The matrix cannot have a pivot position in every column so it is not invertible.  If there is a nonzero solution to the homogeneous equation , then is not invertible so .  We find that so there is a nonzero solution to the homogeneous equation.  The reduced row echelon form of is so the solution space to the homogeneous equation may be described parametrically as . A basis for is therefore .  If is an matrix, then .    "
},
{
  "id": "sec-eigen-find-3-3",
  "level": "2",
  "url": "sec-eigen-find.html#sec-eigen-find-3-3",
  "type": "Activity",
  "number": "20.2.2",
  "title": "",
  "body": "  The eigenvalues of a square matrix are defined by the condition that there be a nonzero solution to the homogeneous equation .  If there is a nonzero solution to the homogeneous equation , what can we conclude about the invertibility of the matrix ?  If there is a nonzero solution to the homogeneous equation , what can we conclude about the determinant ?  Let's consider the matrix from which we construct . Find the determinant . What kind of equation do you obtain when we set this determinant to zero to obtain ?  Use the determinant you found in the previous part to find the eigenvalues by solving the equation . We considered this matrix in so we should find the same eigenvalues for that we found by reasoning geometrically there.  Consider the matrix and find its eigenvalues by solving the equation .  Consider the matrix and find its eigenvalues by solving the equation .  Find the eigenvalues of the triangular matrix . What is generally true about the eigenvalues of a triangular matrix?         The matrix cannot be invertible.    It must be the case that .    We find that .     so we find eigenvalues and .    For this matrix, we have so there is one eigenvalue, .     so there are complex eigenvalues, and .    Because the determinant of a triangular matrix equals the product of its diagonal entries, The eigenvalues are equal to the entries on the diagonal.      "
},
{
  "id": "example-eigenvalues-poly",
  "level": "2",
  "url": "sec-eigen-find.html#example-eigenvalues-poly",
  "type": "Example",
  "number": "20.2.1",
  "title": "",
  "body": " If we write the characteristic equation for the matrix , we see that This shows us that the eigenvalues are and .  "
},
{
  "id": "sec-eigen-find-3-8",
  "level": "2",
  "url": "sec-eigen-find.html#sec-eigen-find-3-8",
  "type": "Example",
  "number": "20.2.2",
  "title": "",
  "body": " Consider the matrix , whose characteristic equation is In this case, the characteristic polynomial has one real root, which means that this matrix has a single real eigenvalue, .  "
},
{
  "id": "sec-eigen-find-3-9",
  "level": "2",
  "url": "sec-eigen-find.html#sec-eigen-find-3-9",
  "type": "Example",
  "number": "20.2.3",
  "title": "",
  "body": " To find the eigenvalues of a triangular matrix, we remember that the determinant of a triangular matrix is the product of the entries on the diagonal. For instance, the following triangular matrix has the characteristic equation showing that the eigenvalues are the diagonal entries .  "
},
{
  "id": "sec-eigen-find-4-4",
  "level": "2",
  "url": "sec-eigen-find.html#sec-eigen-find-4-4",
  "type": "Activity",
  "number": "20.2.3",
  "title": "",
  "body": "  In this activity, we will find the eigenvectors of a matrix as the null space of the matrix .  Let's begin with the matrix . We have seen that is an eigenvalue. Form the matrix and find a basis for the eigenspace . What is the dimension of this eigenspace? For each of the basis vectors , verify that .  We also saw that is an eigenvalue. Form the matrix and find a basis for the eigenspace . What is the dimension of this eigenspace? For each of the basis vectors , verify that .  Is it possible to form a basis of consisting of eigenvectors of ?  Now consider the matrix . Write the characteristic equation for and use it to find the eigenvalues of . For each eigenvalue, find a basis for its eigenspace . Is it possible to form a basis of consisting of eigenvectors of ?  Next, consider the matrix . Write the characteristic equation for and use it to find the eigenvalues of . For each eigenvalue, find a basis for its eigenspace . Is it possible to form a basis of consisting of eigenvectors of ?  Finally, find the eigenvalues and eigenvectors of the diagonal matrix . Explain your result by considering the geometric effect of the matrix transformation defined by .       We have The null space is one-dimensional with basis .  We have The null space is one-dimensional with basis .  We can form a basis for consisting of eigenvectors of by taking .  The characteristic equation is , which means that there is a single eigenvalue . This eigenspace is two-dimensional with basis . In this case, we can form a basis for consisting of eigenvectors of .  The characteristic equation is so there is again a single eigenvalue . In this case, the eigenspace is one-dimensional with basis vector . It is not possible to form a basis for consisting of eigenvectors.  We have eigenvectors with associated eigenvector and with associated eigenvector .    "
},
{
  "id": "sec-eigen-find-4-6",
  "level": "2",
  "url": "sec-eigen-find.html#sec-eigen-find-4-6",
  "type": "Example",
  "number": "20.2.4",
  "title": "",
  "body": " Revisiting the matrix from , we recall that we found eigenvalues and .  Considering the eigenvalue , we have Since the eigenvectors are the solutions of the equation , we see that they are determined by the single equation or . Therefore the eigenvectors in have the form In other words, is a one-dimensional subspace of with basis vector or basis vector . In the same way, we find that a basis for the eigenspace is .  We note that, for this matrix, it is possible to construct a basis of consisting of eigenvectors, namely,   "
},
{
  "id": "sec-eigen-find-4-7",
  "level": "2",
  "url": "sec-eigen-find.html#sec-eigen-find-4-7",
  "type": "Example",
  "number": "20.2.5",
  "title": "",
  "body": " Consider the matrix whose characteristic equation is   There is a single eigenvalue , and we find that Therefore, the eigenspace is one-dimensional with a basis vector .  "
},
{
  "id": "sec-eigen-find-4-8",
  "level": "2",
  "url": "sec-eigen-find.html#sec-eigen-find-4-8",
  "type": "Example",
  "number": "20.2.6",
  "title": "",
  "body": " If , then which implies that there is a single eigenvalue . We find that which says that every two-dimensional vector satisfies . Therefore, every vector is an eigenvector and so . This eigenspace is two-dimensional.  We can see this in another way. The matrix transformation defined by rotates vectors by , which says that for every vector . In other words, every two-dimensional vector is an eigenvector with associated eigenvalue .  "
},
{
  "id": "sec-eigen-find-5-5",
  "level": "2",
  "url": "sec-eigen-find.html#sec-eigen-find-5-5",
  "type": "Example",
  "number": "20.2.7",
  "title": "",
  "body": "  We have seen that the matrix has the characteristic equation . This matrix has a single eigenvalue , which has multiplicity .   "
},
{
  "id": "sec-eigen-find-5-6",
  "level": "2",
  "url": "sec-eigen-find.html#sec-eigen-find-5-6",
  "type": "Example",
  "number": "20.2.8",
  "title": "",
  "body": "  If a matrix has the characteristic equation , then that matrix has four eigenvalues: having multiplicity 2; having multiplicity 1; having multiplicity 7; and having multiplicity 2. The degree of the characteristic polynomial is the sum of the multiplicities so this matrix must be a matrix.   "
},
{
  "id": "prop-eigen-basis",
  "level": "2",
  "url": "sec-eigen-find.html#prop-eigen-basis",
  "type": "Proposition",
  "number": "20.2.9",
  "title": "",
  "body": "  If is a real eigenvalue of the matrix with multiplicity , then .   "
},
{
  "id": "sec-eigen-find-5-9",
  "level": "2",
  "url": "sec-eigen-find.html#sec-eigen-find-5-9",
  "type": "Example",
  "number": "20.2.10",
  "title": "",
  "body": "  The diagonal matrix has the characteristic equation . There is a single eigenvalue having multiplicity , and we saw earlier that .   "
},
{
  "id": "sec-eigen-find-5-10",
  "level": "2",
  "url": "sec-eigen-find.html#sec-eigen-find-5-10",
  "type": "Example",
  "number": "20.2.11",
  "title": "",
  "body": "  The matrix has the characteristic equation . This tells us that there is a single eigenvalue having multiplicity . In contrast with the previous example, we have .   "
},
{
  "id": "sec-eigen-find-5-11",
  "level": "2",
  "url": "sec-eigen-find.html#sec-eigen-find-5-11",
  "type": "Example",
  "number": "20.2.12",
  "title": "",
  "body": "  We saw earlier that the matrix has the characteristic equation . There are three eigenvalues each having multiplicity . By the proposition, we are guaranteed that the dimension of each eigenspace is ; that is, . It turns out that this is enough to guarantee that there is a basis of consisting of eigenvectors.   "
},
{
  "id": "sec-eigen-find-5-12",
  "level": "2",
  "url": "sec-eigen-find.html#sec-eigen-find-5-12",
  "type": "Example",
  "number": "20.2.13",
  "title": "",
  "body": "  If a matrix has the characteristic equation , we know there are four eigenvalues . Without more information, all we can say about the dimensions of the eigenspaces is We can guarantee that , but we cannot be more specific about the dimensions of the other eigenspaces.   "
},
{
  "id": "sec-eigen-find-5-14",
  "level": "2",
  "url": "sec-eigen-find.html#sec-eigen-find-5-14",
  "type": "Proposition",
  "number": "20.2.14",
  "title": "",
  "body": "  If is an matrix having distinct real eigenvalues, then there is a basis of consisting of eigenvectors of .   "
},
{
  "id": "sec-eigen-find-5-16",
  "level": "2",
  "url": "sec-eigen-find.html#sec-eigen-find-5-16",
  "type": "Activity",
  "number": "20.2.4",
  "title": "",
  "body": "    Identify the eigenvalues, and their multiplicities, of an matrix whose characteristic polynomial is . What can you conclude about the dimensions of the eigenspaces? What is the shape of the matrix? Do you have enough information to guarantee that there is a basis of consisting of eigenvectors?  Find the eigenvalues of and state their multiplicities. Can you find a basis of consisting of eigenvectors of this matrix?  Consider the matrix whose characteristic equation is .  Identify the eigenvalues and their multiplicities.  For each eigenvalue , find a basis of the eigenspace and state its dimension.  Is there a basis of consisting of eigenvectors of ?   Now consider the matrix whose characteristic equation is also .  Identify the eigenvalues and their multiplicities.  For each eigenvalue , find a basis of the eigenspace and state its dimension.  Is there a basis of consisting of eigenvectors of ?   Consider the matrix whose characteristic equation is .  Identify the eigenvalues and their multiplicities.  For each eigenvalue , find a basis of the eigenspace and state its dimension.  Is there a basis of consisting of eigenvectors of ?        There are three eigenvalues, has multiplicity , has multiplicity , and has multiplicity . We know that We can guarantee that , but we can say nothing further about the other two eigenspaces.  The dimension of the matrix is since the degree of the characteristic polynomial is . We cannot guarantee that we can form a basis for consisting of eigenvectors, however.  There is one eigenvalue having multiplicity two. Because the eigenspace is one-dimensional, however, we cannot find a basis for consisting of eigenvectors of .  For the matrix ,  We have eigenvalues with multiplicity and with multiplicity .  The eigenspace is two-dimensional with basis . The eigenspace is one-dimensional with basis vector .  We are able to form a basis for consisting of eigenvectors of .    For the matrix ,  We have eigenvalues with multiplicity and with multiplicity .  The eigenspace is one-dimensional with basis vector . The eigenspace is also one-dimensional with basis vector .  It is not possible to form a basis for consisting of eigenvectors of .    For this matrix,  There are three eigenvalues , , and , each having multiplicity .  A basis vector for the eigenspace is . A basis vector for the eigenspace is . A basis vector for the eigenspace is .  We can form a basis for consisting of eigenvectors of .      "
},
{
  "id": "sec-eigen-find-6-3",
  "level": "2",
  "url": "sec-eigen-find.html#sec-eigen-find-6-3",
  "type": "Activity",
  "number": "20.2.5",
  "title": "",
  "body": "  We will use Sage to find the eigenvalues and eigenvectors of a matrix. Let's begin with the matrix .  We can find the characteristic polynomial of by writing A.charpoly('lambda') . Notice that we have to give Sage a variable in which to write the polynomial; here, we use lambda though x works just as well.   The factored form of the characteristic polynomial may be more useful since it will tell us the eigenvalues and their multiplicities. The factored characteristic polynomial is found with A.fcp('lambda') .    If we only want the eigenvalues, we can use A.eigenvalues() .   Notice that the multiplicity of an eigenvalue is the number of times it is repeated in the list of eigenvalues.   Finally, we can find eigenvectors by A.eigenvectors_right() . (We are looking for right eigenvalues since the vector appears to the right of in the definition .)   At first glance, the result of this command can be a little confusing to interpret. What we see is a list with one entry for each eigenvalue. For each eigenvalue, there is a triple consisting of (i) the eigenvalue , (ii) a basis for , and (iii) the multiplicity of .   When working with decimal entries, which are called floating point numbers in computer science, we must remember that computers perform only approximate arithmetic. This is a problem when we wish to find the eigenvectors of such a matrix. To illustrate, consider the matrix .  Without using Sage, find the eigenvalues of this matrix.  What do you find for the reduced row echelon form of ?  Let's now use Sage to determine the reduced row echelon form of :   What result does Sage report for the reduced row echelon form? Why is this result not correct?   Because the arithmetic Sage performs with floating point entries is only approximate, we are not able to find the eigenspace . In this next chapter, we will learn how to address this issue. In the meantime, we can get around this problem by writing the entries in the matrix as rational numbers:          The fcp command will return the factored characteristic polynomial lambda^2 - 2*lambda - 3 .  The eigenvalues command returns a list of eigenvalues [-3, -3] .  The eigenvectors_right command returns [(-3, [(1, 0)], 2)] .  If we begin with the matrix , we find  The eigenvalues are and .  The reduced row echelon form is , which shows that is not invertible, as expected.  Sage returns , which is not correct because cannot be invertible if is an eigenvalue of .  Here we find the correct eigenvalues, with basis vector for and with basis vector for .      "
},
{
  "id": "sec-eigen-find-8-1",
  "level": "2",
  "url": "sec-eigen-find.html#sec-eigen-find-8-1",
  "type": "Exercise",
  "number": "20.2.6.1",
  "title": "",
  "body": " For each of the following matrices, find its characteristic polynomial, its eigenvalues, and the multiplicity of each eigenvalue.   .   .   .   .      . There is a single eigenvalue having multiplicity .   . There are three eigenvalues , each of multiplicity .   . There is one eigenvalue having multiplicity .   . There are two eigenvalues and , each having multiplicity .     The characteristic polynomial is . There is a single eigenvalue having multiplicity .  The characteristic polynomial is . There are three eigenvalues , each of multiplicity .  The characteristic polynomial is , showing that there is one eigenvalue having multiplicity .  The characteristic polynomial is . There are two eigenvalues and , each having multiplicity .   "
},
{
  "id": "sec-eigen-find-8-2",
  "level": "2",
  "url": "sec-eigen-find.html#sec-eigen-find-8-2",
  "type": "Exercise",
  "number": "20.2.6.2",
  "title": "",
  "body": " Given an matrix , an important question, , asks whether we can find a basis of consisting of eigenvectors of . For each of the matrices in the previous exercise, find a basis of consisting of eigenvectors or state why such a basis does not exist.    It is not possible.   .            There is a single eigenvalue and so it is not possible to find a basis for consisting of eigenvectors of .  The three eigenvalues each have multiplicity one so we know that their eigenspaces are one-dimensional. We find a basis vector for is , for is , and for is . A basis for consisting of eigenvectors of is therefore   Since is the zero matrix, every vector is an eigenvector of . This means that is a basis of consisting of eigenvectors.  The multiplicity of each eigenvalue is one so there will be a basis of consisting of eigenvectors. In particular, a basis vector for is and a basis vector for is . This means that is a basis for consisting of eigenvectors of .    "
},
{
  "id": "sec-eigen-find-8-3",
  "level": "2",
  "url": "sec-eigen-find.html#sec-eigen-find-8-3",
  "type": "Exercise",
  "number": "20.2.6.3",
  "title": "",
  "body": " Determine whether the following statements are true or false and provide a justification for your response.  The eigenvalues of a matrix are the entries on the diagonal of .  If is an eigenvalue of multiplicity , then is one-dimensional.  If a matrix is invertible, then cannot be an eigenvalue.  If is a matrix, the characteristic polynomial has degree less than .  The eigenspace of is the same as the null space .     False  True  True  False  True     False. This is true for a diagonal matrix, but it is not generally true as we see by considering the matrix whose eigenvalues are and .  True. If is the multiplicity, we have so we must have .  True. If is an eigenvalue, then an associated eigenvector is a nonzero solution to the homogeneous equation . This would say that is not invertible.  False. The degree of the characteristic polynomial equals the number of rows and columns of the square matrix.  True. An eigenvector associated to the eigenvalue satisfies . This is the same equation that characterizes the null space .   "
},
{
  "id": "sec-eigen-find-8-4",
  "level": "2",
  "url": "sec-eigen-find.html#sec-eigen-find-8-4",
  "type": "Exercise",
  "number": "20.2.6.4",
  "title": "",
  "body": " Provide a justification for your response to the following questions.  Suppose that is a matrix having eigenvalues . What are the eigenvalues of ?  Suppose that is a diagonal matrix. Why can you guarantee that there is a basis of consisting of eigenvectors of ?  If is a matrix whose eigenvalues are , can you guarantee that there is a basis of consisting of eigenvectors of ?  Suppose that the characteristic polynomial of a matrix is . What are the eigenvalues of ? Is invertible? Is there a basis of consisting of eigenvectors of ?  If the characteristic polynomial of is , what is the characteristic polynomial of ? what is the characteristic polynomial of ?      .  The standard basis vectors are eigenvectors associated to the diagonal entries.  Yes.   . The matrix is not invertible, but there is a basis for consisting of eigenvectors of .   .     If , then . This says that the eigenvalues of are .  The standard basis vectors are eigenvectors associated to the diagonal entries.  Yes. Since there are three distinct eigenvalues, the multiplicity of each eigenvalue must be one. Therefore, the dimension of each eigenspace is one. If we choose a basis vector for each of the eigenspaces, we will obtain a basis for .  The eigenvalues are determined by , which shows that the eigenvalues are . This shows that is not invertible since is an eigenvalue. There is, however, a basis for consisting of eigenvectors of since the three eigenvalues are distinct.  If is an eigenvalue of , then is an eigenvalue of . Therefore, the characteristic polynomial of is .   "
},
{
  "id": "sec-eigen-find-8-5",
  "level": "2",
  "url": "sec-eigen-find.html#sec-eigen-find-8-5",
  "type": "Exercise",
  "number": "20.2.6.5",
  "title": "",
  "body": " For each of the following matrices, use Sage to determine its eigenvalues, their multiplicities, and a basis for each eigenspace. For which matrices is it possible to construct a basis for consisting of eigenvectors?                 A basis of eigenvectors is   It is not possible to find a basis of eigenvectors.  A basis of eigenvectors is      Sage tells us there are three distinct eigenvalues , each having multiplicity one. A basis of eigenvectors is   It is not possible to find a basis of eigenvectors because the eigenvalue has multiplicity two but its eigenspace is one-dimensional.  It is possible to find a basis for consisting of eignevectors because one eigenvalue has multiplicity one while the other has multiplicity two and a two-dimensional eigenspace. A basis is    "
},
{
  "id": "sec-eigen-find-8-6",
  "level": "2",
  "url": "sec-eigen-find.html#sec-eigen-find-8-6",
  "type": "Exercise",
  "number": "20.2.6.6",
  "title": "",
  "body": " There is a relationship between the determinant of a matrix and the product of its eigenvalues.  We have seen that the eigenvalues of the matrix are . What is ? What is the product of the eigenvalues of ?  Consider the triangular matrix . What are the eigenvalues of ? What is ? What is the product of the eigenvalues of ?  Based on these examples, what do you think is the relationship between the determinant of a matrix and the product of its eigenvalues?  Suppose the characteristic polynomial is written as . By substituting into this equation, explain why the determinant of a matrix equals the product of its eigenvalues.     Both equal .  Both equal .   equals the product of the eigenvalues.  We see that .     We have , and the product of the eigenvalues is .  We have , and the product of the eigenvalues is   We suspect that equals the product of the eigenvalues.  Setting , we have Notice that we mean the product of the eigenvalues, including their multiplicities.   "
},
{
  "id": "sec-eigen-find-8-7",
  "level": "2",
  "url": "sec-eigen-find.html#sec-eigen-find-8-7",
  "type": "Exercise",
  "number": "20.2.6.7",
  "title": "",
  "body": " Consider the matrix .  Find the eigenvalues of and a basis for their associated eigenspaces.  Suppose that . Express as a linear combination of eigenvectors of .  Define the vectors . Write , , and as a linear combination of eigenvectors of .  What happens to as grows larger and larger?      with associated eigenvector and with associated eigenvector .   .  We have .   .     We find the eigenvalues with associated eigenvector and with associated eigenvector .  After constructing an augmented matrix, we find that .  Since , we have .  We have . As grows larger, becomes less significant. Eventually, .   "
},
{
  "id": "sec-eigen-find-8-8",
  "level": "2",
  "url": "sec-eigen-find.html#sec-eigen-find-8-8",
  "type": "Exercise",
  "number": "20.2.6.8",
  "title": "",
  "body": " Consider the matrix   Find the eigenvalues of and a basis for their associated eigenspaces.  Suppose that . Express as a linear combination of eigenvectors of .  Define the vectors . Write , , and as a linear combination of eigenvectors of .  What happens to as grows larger and larger?      with associated eigenvector and with associated eigenvector .   .  We have .   .     We have eigenvalues with associated eigenvector and with associated eigenvector .  After constructing an augmented matrix, we find .  Since , we have   We have . As grows larger, becomes less significant. Eventually, .   "
},
{
  "id": "sec-eigen-diag",
  "level": "1",
  "url": "sec-eigen-diag.html",
  "type": "Section",
  "number": "20.3",
  "title": "Diagonalization, similarity, and powers of a matrix",
  "body": " Diagonalization, similarity, and powers of a matrix   The first example we considered in this chapter was the matrix , which has eigenvectors and and associated eigenvalues and . In , we described how is, in some sense, equivalent to the diagonal matrix .  This equivalence is summarized by . The diagonal matrix has the geometric effect of stretching vectors horizontally by a factor of and flipping vectors vertically. The matrix has the geometric effect of stretching vectors by a factor of in the direction and flipping them in the direction. That is, the geometric effect of is the same as that of when viewed in a basis of eigenvectors of .      The matrix has the same geometric effect as the diagonal matrix when viewed in the basis of eigenvectors.   Our goal in this section is to express this geometric observation in algebraic terms. In doing so, we will make precise the sense in which and are equivalent.    In this preview activity, we will review some familiar properties about matrix multiplication that appear in this section.   Remember that matrix-vector multiplication constructs linear combinations of the columns of the matrix. For instance, if , express the product in terms of and .    What is the product in terms of and ?    Next, remember how matrix-matrix multiplication is defined. Suppose that we have matrices and and that . How can we express the matrix product in terms of the columns of ?    Suppose that is a matrix having eigenvectors and with associated eigenvalues and . Express the product in terms of and .    Suppose that is the matrix from the previous part and that . What is the matrix product          .     .     .     .     .        Diagonalization of matrices  When working with an matrix , demonstrated the value of having a basis of consisting of eigenvectors of . In fact, tells us that if the eigenvalues of are real and distinct, then there is a such a basis. As we'll see later, there are other conditions on that guarantee a basis of eigenvectors. For now, suffice it to say that we can find a basis of eigenvectors for many matrices. With this assumption, we will see how the matrix is equivalent to a diagonal matrix .    Suppose that is a matrix having eigenvectors and with associated eigenvalues and . Because the eigenvalues are real and distinct, we know by that these eigenvectors form a basis of .   What are the products and in terms of and ?    If we form the matrix , what is the product in terms of and ?    Use the eigenvalues to form the diagonal matrix and determine the product in terms of and .    The results from the previous two parts of this activity demonstrate that . Using the fact that the eigenvectors and form a basis of , explain why is invertible and that we must have .    Suppose that . Verify that and are eigenvectors of with eigenvalues and .    Use the Sage cell below to define the matrices and and then verify that .        We have and .    .     . Comparing the result of this part of the activity to the previous, we see that .    Since the eigenvectors form a basis, the columns of are linearly independent and their span is . This guarantees that is invertible. Multiplying the equation on the right by gives .    The rest of the activity can be verified using Sage.    More generally, suppose that we have an matrix and that there is a basis of consisting of eigenvectors of with associated eigenvalues . If we use the eigenvectors to form the matrix and the eigenvalues to form the diagonal matrix and apply the same reasoning demonstrated in the activity, we find that and hence   We have now seen the following proposition.    If is an matrix and there is a basis of consisting of eigenvectors of having associated eigenvalues , then we can write where is the diagonal matrix whose diagonal entries are the eigenvalues of  and the matrix .     We have seen that has eigenvectors and with associated eigenvalues and . Forming the matrices we see that .  This is the sense in which we mean that is equivalent to a diagonal matrix . The expression says that , expressed in the basis defined by the columns of , has the same geometric effect as , expressed in the standard basis .     diagonalizable  We say that the matrix is diagonalizable if there is a diagonal matrix and invertible matrix such that       We will try to find a diagonalization of whose characteristic equation is . This shows that the eigenvalues of are and .  By constructing , we find a basis for consisting of the vector . Similarly, a basis for consists of the vector . This shows that we can construct a basis of consisting of eigenvectors of .  We now form the matrices and verify that .  There are, in fact, many ways to diagonalize . For instance, we could change the order of the eigenvalues and eigenvectors and write .  If we choose a different basis for the eigenspaces, we will also find a different matrix that diagonalizes . The point is that there are many ways in which can be written in the form .      We will try to find a diagonalization of .  Once again, we find the eigenvalues by solving the characteristic equation: . In this case, there is a single eigenvalue .  We find a basis for the eigenspace by describing : . This shows that the eigenspace is one-dimensional with forming a basis.  In this case, there is not a basis of consisting of eigenvectors of , which tells us that is not diagonalizable.    In fact, if we only know that , we can say that the columns of are eigenvectors of and that the diagonal entries of are the associated eigenvalues.    An matrix is diagonalizable if and only if there is a basis of consisting of eigenvectors of .      Suppose we know that where The columns of form eigenvectors of so that is an eigenvector of with eigenvalue and is an eigenvector with eigenvalue .  We can verify this by computing and checking that and .        Find a diagonalization of , if one exists, when .  Can the diagonal matrix be diagonalized? If so, explain how to find the matrices and .  Find a diagonalization of , if one exists, when .    Find a diagonalization of , if one exists, when .    Suppose that where .  Explain why is invertible.  Find a diagonalization of .  Find a diagonalization of .        We find that has eigenvectors with associated eigenvalue and with associated eigenvalue . We then have and .  Yes. We know that the eigenvectors are with associated eigenvalue and with associated eigenvalue . Therefore, and . This shows that the diagonalization is ; that is, since is already diagonal, it is diagonalized by the identity matrix.  We find eigenvectors , , and with associated eigenvalues , , and . Therefore, where   Once again, we see that is an eigenvalue with multiplicity one and is an eigenvalue with multiplicity two. However, so we are not able to find a basis for consisting of eigenvalues of . Therefore, is not diagonalizable.  If ,   is invertible since .  We know that and are eigenvectors of with associated eigenvalues and . If is an eigenvector of with associated eigenvalue , then is an eigenvector of with associated eigenvalue . Therefore, where .  We have where .         Powers of a diagonalizable matrix  In several earlier examples, we have been interested in computing powers of a given matrix. For instance, in , we had the matrix and an initial vector , and we wanted to compute In particular, we wanted to find and determine what happens as becomes very large. If a matrix is diagonalizable, writing can help us understand powers of more easily.      Let's begin with the diagonal matrix . Find the powers , , and . What is for a general value of ?  Suppose that is a matrix with eigenvector and associated eigenvalue ; that is, . By considering , explain why is also an eigenvector of with eigenvalue .  Suppose that where . Remembering that the columns of are eigenvectors of , explain why is diagonalizable and find a diagonalization in terms of and .  Give another explanation of the diagonalizability of by writing .  In the same way, find a diagonalization of , , and .  Suppose that is a diagonalizable matrix with eigenvalues and . What happens to as becomes very large?       We have   We know that so that is also an eigenvector of with associated eigenvalue .  Since eigenvectors of are also eigenvectors of , we can use the matrix to diagonalize . The eigenvalues are squared, however, so we have where .  We can also see this by noting that    , , and .  We can write where . Therefore, where . As becomes very large, and become very close to zero. Hence and become very close to the zero matrix.     If is diagonalizable, the activity demonstrates that any power of is as well.    If , then . When is invertible, we also have .     Let's revisit where we had the matrix and the initial vector . We were interested in understanding the sequence of vectors , which means that .  We can verify that and are eigenvectors of having associated eigenvalues and . This means that where Therefore, the powers of have the form .  Notice that . As increases, becomes closer and closer to zero. This means that for very large powers , we have and therefore   Beginning with the vector , we find that when is very large.     Similarity and complex eigenvalues  We have been interested in diagonalizing a matrix because doing so relates a matrix to a simpler diagonal matrix . In particular, the effect of multiplying a vector by , viewed in the basis defined by the columns of , is the same as the effect of multiplying by in the standard basis.  While many matrices are diagonalizable, there are some that are not. For example, if a matrix has complex eigenvalues, it is not possible to find a basis of consisting of eigenvectors, which means that the matrix is not diagonalizable. In this case, however, we can still relate the matrix to a simpler form that explains the geometric effect this matrix has on vectors.    similarity  We say that is similar to if there is an invertible matrix such that .    Notice that a matrix is diagonalizable if and only if it is similar to a diagonal matrix. In case a matrix has complex eigenvalues, we will find a simpler matrix that is similar to and note that has the same effect, when viewed in the basis defined by the columns of , as , when viewed in the standard basis.  To begin, suppose that is a matrix having a complex eigenvalue . It turns out that is similar to .   The next activity shows that has a simple geometric effect on . First, however, we will use polar coordinates to rewrite . As shown in the figure, the point defines , the distance from the origin, and , the angle formed with the positive horizontal axis. We then have Notice that the Pythagorean theorem says that .      We begin by rewriting in terms of and and noting that   Explain why has the geometric effect of rotating vectors by and scaling them by a factor of .  Let's now consider the matrix whose eigenvalues are and . We will choose to focus on one of the eigenvalues   Form the matrix using these values of and . Then rewrite the point in polar coordinates by identifying the values of and . Explain the geometric effect of multiplying vectors by .   Suppose that . Verify that .    Explain why .  We formed the matrix by choosing the eigenvalue . Suppose we had instead chosen . Form the matrix and use polar coordinates to describe the geometric effect of .  Using the matrix , show that .      The matrix has the geometric effect of scaling vectors uniformly by a factor of while the matrix rotates vectors by .  We have so we form the matrix . This shows that will scale vectors by a factor of while rotating them by .  Sage will verify this relationship.  As we saw earlier, we have and hence .  We have and so we form the matrix . This shows that will scale vectors by a factor of while rotating them by .     If the matrix has a complex eigenvalue , it turns out that is always similar to the matrix whose geometric effect on vectors can be described in terms of a rotation and a scaling. There is, in fact, a method for finding the matrix so that that we'll see in . For now, we note that has the same geometric effect as , when viewed in the basis provided by the columns of . We will put this fact to use in the next section to understand certain dynamical systems.    If is a matrix with a complex eigenvalue , then is similar to ; that is, there is a matrix such that .      Summary  Our goal in this section has been to use the eigenvalues and eigenvectors of a matrix to relate to a simpler matrix.  We said that is diagonalizable if we can write where is a diagonal matrix. The columns of consist of eigenvectors of and the diagonal entries of are the associated eigenvalues.  An matrix is diagonalizable if and only if there is a basis of consisting of eigenvectors of .  We said that and are similar if there is an invertible matrix such that . In this case, .  If is a matrix with complex eigenvalue , then is similar to . Writing the point in polar coordinates and , we see that rotates vectors through an angle and scales them by a factor of .       Determine whether the following matrices are diagonalizable. If so, find matrices and such that .   .   .   .   .   .       and    is not diagonalizable.   is not diagonalizable.   and    and      We find that is an eigenvector with associated eigenvalue as is with associated eigenvalue . Therefore, we have where   We see that is an eigenvalue having multiplicity two. However, so we cannot find a basis for consisting of eigenvectors of . Therefore, is not diagonalizable.  We see that has complex eigenvalues so it is not diagonalizable in the form for a diagonal matrix .  The matrix has three distinct eigenvalues , , and with associated eigenvectors , , and . This shows us that where   We have an eigenvalue having multiplicity two, but its eigenspace has dimension two so is diagonalizable. In particular, we choose and have .     Determine whether the following matrices have complex eigenvalues. If so, find the matrix such that .   .   .   .     There are two real eigenvalues.  There is a single real eigenvalue.   or .     We have two real eigenvalues .  We have a single real eigenvalue .  We have complex eigenvalues so we can choose or .     Determine whether the following statements are true or false and provide a justification for your response.  If is invertible, then is diagonalizable.  If and are similar and is invertible, then is also invertible.  If is a diagonalizable matrix, then there is a basis of consisting of eigenvectors of .  If is diagonalizable, then is also diagonalizable.  If is diagonalizable, then is invertible.      False  True  True  True  False     False. A matrix can be invertible without us being able to form a basis consisting of eigenvectors. An example is .  True. If and are similar, then so that . If is invertible, we know that , which also tells us that and hence that is invertible.  True. If , then the columns of are eigenvectors of that form a basis for .  True. If , then .  False. It is possible that has eigenvalue , which would imply that is not invertible.     Provide a justification for your response to the following questions.  If is a matrix having eigenvalues , can you guarantee that is diagonalizable?  If is a matrix with a complex eigenvalue, can you guarantee that is diagonalizable?  If is similar to the matrix , is diagonalizable?  What can you say about a matrix that is similar to the identity matrix?  If is a diagonalizable matrix with a single eigenvalue , what is ?      Yes  No  Yes  Only the identity   .     Yes. If has real and distinct eigenvalues, then there is a basis of consisting of eigenvectors of . Therefore, is diagonalizable.  No. We can write where , but is not diagonalizable.  Yes. Since is diagonal and , then is diagonalizable.  Only the identity because .   . If we denote , then .     Describe geometric effect that the following matrices have on :                     Multiplying by uniformly stretches vectors by a factor of .  This is a shear so that vectors are stretched and then pushed horizontally.  We write where and . Therefore, will stretch vectors by a factor of and rotate them by .  Vectors are stretched in the horizontal direction by a factor of and stretched vertically by a factor of before being reflected in the horizontal axis.  This matrix has eigenvectors with associated eigenvalue and with . Therefore, stretches vectors by a factor of in the direction of and a factor of in the direction of before reflecting in the line defined by .     Multiplying by uniformly stretches vectors by a factor of .  This is a shear so that vectors are stretched and then pushed horizontally.  We write where and . Therefore, will stretch vectors by a factor of and rotate them by .  Vectors are stretched in the horizontal direction by a factor of and stretched vertically by a factor of before being reflected in the horizontal axis.  This matrix has eigenvectors with associated eigenvalue and with . Therefore, stretches vectors by a factor of in the direction of and a factor of in the direction of before reflecting in the line defined by .     We say that is similar to if there is a matrix such that .  If is similar to , explain why is similar to .  If is similar to and is similar to , explain why is similar to .  If is similar to and is diagonalizable, explain why is diagonalizable.  If and are similar, explain why and have the same characteristic polynomial; that is, explain why .  If and are similar, explain why and have the same eigenvalues.      If , then . If we call , then , which shows that is similar to .  If and , then , which shows that is similar to .  If is diagonalizable, then it is similar to a diagonal matrix . Since is similar to and is similar to , we know that is also similar to so that is diagonalizable.  If , then   The eigenvalues of a matrix are the roots of its characteristic polynomial. If and are similar, then they have the same characteristic polynomial and hence the same eigenvalues.     If , then . If we call , then , which shows that is similar to .  If and , then , which shows that is similar to .  If is diagonalizable, then it is similar to a diagonal matrix . Since is similar to and is similar to , we know that is also similar to so that is diagonalizable.  If , then   The eigenvalues of a matrix are the roots of its characteristic polynomial. If and are similar, then they have the same characteristic polynomial and hence the same eigenvalues.     Suppose that where .  Explain the geometric effect that has on vectors in .  Explain the geometric effect that has on vectors in .  What can you say about and other powers of ?  Is invertible?      The matrix projects vectors onto the horizontal axis.   projects vectors onto the line defined by the eigenvector .   .  No     The matrix projects vectors onto the horizontal axis; that is, it produces the shadow of a vector on the horizontal axis from a flashlight shining down the vertical axis.   projects vectors onto the line defined by the eigenvector .  Since and , we have . That is, and .   is not invertible because is an eigenvalue.     When is a matrix with a complex eigenvalue , we have said that there is a matrix such that where . In this exercise, we will learn how to find the matrix . As an example, we will consider the matrix .  Show that the eigenvalues of are complex.  Choose one of the complex eigenvalues and construct the usual matrix .  Using the same eigenvalue, we will find an eigenvector where the entries of are complex numbers. As always, we will describe by constructing the matrix and finding its reduced row echelon form. In doing so, we will necessarily need to use complex arithmetic.  We have now found a complex eigenvector . Write to identify vectors and having real entries.  Construct the matrix and verify that .       .  If , then .      and .       The eigenvalues are .  We will choose so that .  Construct the matrix so that a basis vector for the null space is .  We have giving and .  We let and verify that .     For each of the following matrices, sketch the vector and powers for .     .        .        .     Consider a matrix of the form with . What happens when becomes very large when   .   .   .      In each case, the vectors are rotated by . Note that the figures below are draw at different scales.    The vectors are stretched by a factor of 1.4.       The vectors are stretched by a factor of 0.8.       The length of the vectors is unchanged.     For different values of ,  The vectors are pulled into the origin as their lengths become increasingly small.  The length of the vectors is unchanged.  The vectors are pushed away from the origin as their lengths become increasingly large.      In each case, the vectors are rotated by . Note that the figures below are draw at different scales.    The vectors are stretched by a factor of 1.4.       The vectors are stretched by a factor of 0.8.       The length of the vectors is unchanged.     For different values of ,  The vectors are pulled into the origin as their lengths become increasingly small.  The length of the vectors is unchanged.  The vectors are pushed away from the origin as their lengths become increasingly large.       For each of the following matrices and vectors, sketch the vector along with for .     .                        Find the eigenvalues and eigenvectors of to create your sketch.     If is a matrix with eigenvalues and and is any vector, what happens to when becomes very large?                          Vectors will be pulled into the origin.       Vectors are expanded horizontally and compressed vertically.       Vectors are compressed both horizontally and vertically.       Vectors are expanded both horizontally and vertically.       There are eigenvectors with and with . Vectors are expanded in the direction of and compressed in the direction of .     Because both eigenvalues have an absolute value smaller than , vectors will be compressed in the direction of both eigenvectors and hence be pulled into the origin.     "
},
{
  "id": "fig-eigen-diag-A",
  "level": "2",
  "url": "sec-eigen-diag.html#fig-eigen-diag-A",
  "type": "Figure",
  "number": "20.3.1",
  "title": "",
  "body": "    The matrix has the same geometric effect as the diagonal matrix when viewed in the basis of eigenvectors.  "
},
{
  "id": "sec-eigen-diag-2-5",
  "level": "2",
  "url": "sec-eigen-diag.html#sec-eigen-diag-2-5",
  "type": "Preview Activity",
  "number": "20.3.1",
  "title": "",
  "body": "  In this preview activity, we will review some familiar properties about matrix multiplication that appear in this section.   Remember that matrix-vector multiplication constructs linear combinations of the columns of the matrix. For instance, if , express the product in terms of and .    What is the product in terms of and ?    Next, remember how matrix-matrix multiplication is defined. Suppose that we have matrices and and that . How can we express the matrix product in terms of the columns of ?    Suppose that is a matrix having eigenvectors and with associated eigenvalues and . Express the product in terms of and .    Suppose that is the matrix from the previous part and that . What is the matrix product          .     .     .     .     .     "
},
{
  "id": "sec-eigen-diag-3-3",
  "level": "2",
  "url": "sec-eigen-diag.html#sec-eigen-diag-3-3",
  "type": "Activity",
  "number": "20.3.2",
  "title": "",
  "body": "  Suppose that is a matrix having eigenvectors and with associated eigenvalues and . Because the eigenvalues are real and distinct, we know by that these eigenvectors form a basis of .   What are the products and in terms of and ?    If we form the matrix , what is the product in terms of and ?    Use the eigenvalues to form the diagonal matrix and determine the product in terms of and .    The results from the previous two parts of this activity demonstrate that . Using the fact that the eigenvectors and form a basis of , explain why is invertible and that we must have .    Suppose that . Verify that and are eigenvectors of with eigenvalues and .    Use the Sage cell below to define the matrices and and then verify that .        We have and .    .     . Comparing the result of this part of the activity to the previous, we see that .    Since the eigenvectors form a basis, the columns of are linearly independent and their span is . This guarantees that is invertible. Multiplying the equation on the right by gives .    The rest of the activity can be verified using Sage.   "
},
{
  "id": "prop-diagonalizable",
  "level": "2",
  "url": "sec-eigen-diag.html#prop-diagonalizable",
  "type": "Proposition",
  "number": "20.3.2",
  "title": "",
  "body": "  If is an matrix and there is a basis of consisting of eigenvectors of having associated eigenvalues , then we can write where is the diagonal matrix whose diagonal entries are the eigenvalues of  and the matrix .   "
},
{
  "id": "sec-eigen-diag-3-7",
  "level": "2",
  "url": "sec-eigen-diag.html#sec-eigen-diag-3-7",
  "type": "Example",
  "number": "20.3.3",
  "title": "",
  "body": " We have seen that has eigenvectors and with associated eigenvalues and . Forming the matrices we see that .  This is the sense in which we mean that is equivalent to a diagonal matrix . The expression says that , expressed in the basis defined by the columns of , has the same geometric effect as , expressed in the standard basis .  "
},
{
  "id": "sec-eigen-diag-3-8",
  "level": "2",
  "url": "sec-eigen-diag.html#sec-eigen-diag-3-8",
  "type": "Definition",
  "number": "20.3.4",
  "title": "",
  "body": "  diagonalizable  We say that the matrix is diagonalizable if there is a diagonal matrix and invertible matrix such that    "
},
{
  "id": "sec-eigen-diag-3-9",
  "level": "2",
  "url": "sec-eigen-diag.html#sec-eigen-diag-3-9",
  "type": "Example",
  "number": "20.3.5",
  "title": "",
  "body": "  We will try to find a diagonalization of whose characteristic equation is . This shows that the eigenvalues of are and .  By constructing , we find a basis for consisting of the vector . Similarly, a basis for consists of the vector . This shows that we can construct a basis of consisting of eigenvectors of .  We now form the matrices and verify that .  There are, in fact, many ways to diagonalize . For instance, we could change the order of the eigenvalues and eigenvectors and write .  If we choose a different basis for the eigenspaces, we will also find a different matrix that diagonalizes . The point is that there are many ways in which can be written in the form .   "
},
{
  "id": "sec-eigen-diag-3-10",
  "level": "2",
  "url": "sec-eigen-diag.html#sec-eigen-diag-3-10",
  "type": "Example",
  "number": "20.3.6",
  "title": "",
  "body": "  We will try to find a diagonalization of .  Once again, we find the eigenvalues by solving the characteristic equation: . In this case, there is a single eigenvalue .  We find a basis for the eigenspace by describing : . This shows that the eigenspace is one-dimensional with forming a basis.  In this case, there is not a basis of consisting of eigenvectors of , which tells us that is not diagonalizable.   "
},
{
  "id": "sec-eigen-diag-3-12",
  "level": "2",
  "url": "sec-eigen-diag.html#sec-eigen-diag-3-12",
  "type": "Proposition",
  "number": "20.3.7",
  "title": "",
  "body": "  An matrix is diagonalizable if and only if there is a basis of consisting of eigenvectors of .   "
},
{
  "id": "sec-eigen-diag-3-13",
  "level": "2",
  "url": "sec-eigen-diag.html#sec-eigen-diag-3-13",
  "type": "Example",
  "number": "20.3.8",
  "title": "",
  "body": "  Suppose we know that where The columns of form eigenvectors of so that is an eigenvector of with eigenvalue and is an eigenvector with eigenvalue .  We can verify this by computing and checking that and .   "
},
{
  "id": "sec-eigen-diag-3-14",
  "level": "2",
  "url": "sec-eigen-diag.html#sec-eigen-diag-3-14",
  "type": "Activity",
  "number": "20.3.3",
  "title": "",
  "body": "    Find a diagonalization of , if one exists, when .  Can the diagonal matrix be diagonalized? If so, explain how to find the matrices and .  Find a diagonalization of , if one exists, when .    Find a diagonalization of , if one exists, when .    Suppose that where .  Explain why is invertible.  Find a diagonalization of .  Find a diagonalization of .        We find that has eigenvectors with associated eigenvalue and with associated eigenvalue . We then have and .  Yes. We know that the eigenvectors are with associated eigenvalue and with associated eigenvalue . Therefore, and . This shows that the diagonalization is ; that is, since is already diagonal, it is diagonalized by the identity matrix.  We find eigenvectors , , and with associated eigenvalues , , and . Therefore, where   Once again, we see that is an eigenvalue with multiplicity one and is an eigenvalue with multiplicity two. However, so we are not able to find a basis for consisting of eigenvalues of . Therefore, is not diagonalizable.  If ,   is invertible since .  We know that and are eigenvectors of with associated eigenvalues and . If is an eigenvector of with associated eigenvalue , then is an eigenvector of with associated eigenvalue . Therefore, where .  We have where .      "
},
{
  "id": "sec-eigen-diag-4-3",
  "level": "2",
  "url": "sec-eigen-diag.html#sec-eigen-diag-4-3",
  "type": "Activity",
  "number": "20.3.4",
  "title": "",
  "body": "    Let's begin with the diagonal matrix . Find the powers , , and . What is for a general value of ?  Suppose that is a matrix with eigenvector and associated eigenvalue ; that is, . By considering , explain why is also an eigenvector of with eigenvalue .  Suppose that where . Remembering that the columns of are eigenvectors of , explain why is diagonalizable and find a diagonalization in terms of and .  Give another explanation of the diagonalizability of by writing .  In the same way, find a diagonalization of , , and .  Suppose that is a diagonalizable matrix with eigenvalues and . What happens to as becomes very large?       We have   We know that so that is also an eigenvector of with associated eigenvalue .  Since eigenvectors of are also eigenvectors of , we can use the matrix to diagonalize . The eigenvalues are squared, however, so we have where .  We can also see this by noting that    , , and .  We can write where . Therefore, where . As becomes very large, and become very close to zero. Hence and become very close to the zero matrix.    "
},
{
  "id": "sec-eigen-diag-4-5",
  "level": "2",
  "url": "sec-eigen-diag.html#sec-eigen-diag-4-5",
  "type": "Proposition",
  "number": "20.3.9",
  "title": "",
  "body": "  If , then . When is invertible, we also have .   "
},
{
  "id": "sec-eigen-diag-4-6",
  "level": "2",
  "url": "sec-eigen-diag.html#sec-eigen-diag-4-6",
  "type": "Example",
  "number": "20.3.10",
  "title": "",
  "body": " Let's revisit where we had the matrix and the initial vector . We were interested in understanding the sequence of vectors , which means that .  We can verify that and are eigenvectors of having associated eigenvalues and . This means that where Therefore, the powers of have the form .  Notice that . As increases, becomes closer and closer to zero. This means that for very large powers , we have and therefore   Beginning with the vector , we find that when is very large.  "
},
{
  "id": "sec-eigen-diag-5-4",
  "level": "2",
  "url": "sec-eigen-diag.html#sec-eigen-diag-5-4",
  "type": "Definition",
  "number": "20.3.11",
  "title": "",
  "body": "  similarity  We say that is similar to if there is an invertible matrix such that .   "
},
{
  "id": "sec-eigen-diag-5-8",
  "level": "2",
  "url": "sec-eigen-diag.html#sec-eigen-diag-5-8",
  "type": "Activity",
  "number": "20.3.5",
  "title": "",
  "body": "  We begin by rewriting in terms of and and noting that   Explain why has the geometric effect of rotating vectors by and scaling them by a factor of .  Let's now consider the matrix whose eigenvalues are and . We will choose to focus on one of the eigenvalues   Form the matrix using these values of and . Then rewrite the point in polar coordinates by identifying the values of and . Explain the geometric effect of multiplying vectors by .   Suppose that . Verify that .    Explain why .  We formed the matrix by choosing the eigenvalue . Suppose we had instead chosen . Form the matrix and use polar coordinates to describe the geometric effect of .  Using the matrix , show that .      The matrix has the geometric effect of scaling vectors uniformly by a factor of while the matrix rotates vectors by .  We have so we form the matrix . This shows that will scale vectors by a factor of while rotating them by .  Sage will verify this relationship.  As we saw earlier, we have and hence .  We have and so we form the matrix . This shows that will scale vectors by a factor of while rotating them by .    "
},
{
  "id": "sec-eigen-diag-5-10",
  "level": "2",
  "url": "sec-eigen-diag.html#sec-eigen-diag-5-10",
  "type": "Proposition",
  "number": "20.3.12",
  "title": "",
  "body": "  If is a matrix with a complex eigenvalue , then is similar to ; that is, there is a matrix such that .   "
},
{
  "id": "sec-eigen-diag-7-1",
  "level": "2",
  "url": "sec-eigen-diag.html#sec-eigen-diag-7-1",
  "type": "Exercise",
  "number": "20.3.5.1",
  "title": "",
  "body": " Determine whether the following matrices are diagonalizable. If so, find matrices and such that .   .   .   .   .   .       and    is not diagonalizable.   is not diagonalizable.   and    and      We find that is an eigenvector with associated eigenvalue as is with associated eigenvalue . Therefore, we have where   We see that is an eigenvalue having multiplicity two. However, so we cannot find a basis for consisting of eigenvectors of . Therefore, is not diagonalizable.  We see that has complex eigenvalues so it is not diagonalizable in the form for a diagonal matrix .  The matrix has three distinct eigenvalues , , and with associated eigenvectors , , and . This shows us that where   We have an eigenvalue having multiplicity two, but its eigenspace has dimension two so is diagonalizable. In particular, we choose and have .   "
},
{
  "id": "sec-eigen-diag-7-2",
  "level": "2",
  "url": "sec-eigen-diag.html#sec-eigen-diag-7-2",
  "type": "Exercise",
  "number": "20.3.5.2",
  "title": "",
  "body": " Determine whether the following matrices have complex eigenvalues. If so, find the matrix such that .   .   .   .     There are two real eigenvalues.  There is a single real eigenvalue.   or .     We have two real eigenvalues .  We have a single real eigenvalue .  We have complex eigenvalues so we can choose or .   "
},
{
  "id": "sec-eigen-diag-7-3",
  "level": "2",
  "url": "sec-eigen-diag.html#sec-eigen-diag-7-3",
  "type": "Exercise",
  "number": "20.3.5.3",
  "title": "",
  "body": " Determine whether the following statements are true or false and provide a justification for your response.  If is invertible, then is diagonalizable.  If and are similar and is invertible, then is also invertible.  If is a diagonalizable matrix, then there is a basis of consisting of eigenvectors of .  If is diagonalizable, then is also diagonalizable.  If is diagonalizable, then is invertible.      False  True  True  True  False     False. A matrix can be invertible without us being able to form a basis consisting of eigenvectors. An example is .  True. If and are similar, then so that . If is invertible, we know that , which also tells us that and hence that is invertible.  True. If , then the columns of are eigenvectors of that form a basis for .  True. If , then .  False. It is possible that has eigenvalue , which would imply that is not invertible.   "
},
{
  "id": "sec-eigen-diag-7-4",
  "level": "2",
  "url": "sec-eigen-diag.html#sec-eigen-diag-7-4",
  "type": "Exercise",
  "number": "20.3.5.4",
  "title": "",
  "body": " Provide a justification for your response to the following questions.  If is a matrix having eigenvalues , can you guarantee that is diagonalizable?  If is a matrix with a complex eigenvalue, can you guarantee that is diagonalizable?  If is similar to the matrix , is diagonalizable?  What can you say about a matrix that is similar to the identity matrix?  If is a diagonalizable matrix with a single eigenvalue , what is ?      Yes  No  Yes  Only the identity   .     Yes. If has real and distinct eigenvalues, then there is a basis of consisting of eigenvectors of . Therefore, is diagonalizable.  No. We can write where , but is not diagonalizable.  Yes. Since is diagonal and , then is diagonalizable.  Only the identity because .   . If we denote , then .   "
},
{
  "id": "sec-eigen-diag-7-5",
  "level": "2",
  "url": "sec-eigen-diag.html#sec-eigen-diag-7-5",
  "type": "Exercise",
  "number": "20.3.5.5",
  "title": "",
  "body": " Describe geometric effect that the following matrices have on :                     Multiplying by uniformly stretches vectors by a factor of .  This is a shear so that vectors are stretched and then pushed horizontally.  We write where and . Therefore, will stretch vectors by a factor of and rotate them by .  Vectors are stretched in the horizontal direction by a factor of and stretched vertically by a factor of before being reflected in the horizontal axis.  This matrix has eigenvectors with associated eigenvalue and with . Therefore, stretches vectors by a factor of in the direction of and a factor of in the direction of before reflecting in the line defined by .     Multiplying by uniformly stretches vectors by a factor of .  This is a shear so that vectors are stretched and then pushed horizontally.  We write where and . Therefore, will stretch vectors by a factor of and rotate them by .  Vectors are stretched in the horizontal direction by a factor of and stretched vertically by a factor of before being reflected in the horizontal axis.  This matrix has eigenvectors with associated eigenvalue and with . Therefore, stretches vectors by a factor of in the direction of and a factor of in the direction of before reflecting in the line defined by .   "
},
{
  "id": "sec-eigen-diag-7-6",
  "level": "2",
  "url": "sec-eigen-diag.html#sec-eigen-diag-7-6",
  "type": "Exercise",
  "number": "20.3.5.6",
  "title": "",
  "body": " We say that is similar to if there is a matrix such that .  If is similar to , explain why is similar to .  If is similar to and is similar to , explain why is similar to .  If is similar to and is diagonalizable, explain why is diagonalizable.  If and are similar, explain why and have the same characteristic polynomial; that is, explain why .  If and are similar, explain why and have the same eigenvalues.      If , then . If we call , then , which shows that is similar to .  If and , then , which shows that is similar to .  If is diagonalizable, then it is similar to a diagonal matrix . Since is similar to and is similar to , we know that is also similar to so that is diagonalizable.  If , then   The eigenvalues of a matrix are the roots of its characteristic polynomial. If and are similar, then they have the same characteristic polynomial and hence the same eigenvalues.     If , then . If we call , then , which shows that is similar to .  If and , then , which shows that is similar to .  If is diagonalizable, then it is similar to a diagonal matrix . Since is similar to and is similar to , we know that is also similar to so that is diagonalizable.  If , then   The eigenvalues of a matrix are the roots of its characteristic polynomial. If and are similar, then they have the same characteristic polynomial and hence the same eigenvalues.   "
},
{
  "id": "sec-eigen-diag-7-7",
  "level": "2",
  "url": "sec-eigen-diag.html#sec-eigen-diag-7-7",
  "type": "Exercise",
  "number": "20.3.5.7",
  "title": "",
  "body": " Suppose that where .  Explain the geometric effect that has on vectors in .  Explain the geometric effect that has on vectors in .  What can you say about and other powers of ?  Is invertible?      The matrix projects vectors onto the horizontal axis.   projects vectors onto the line defined by the eigenvector .   .  No     The matrix projects vectors onto the horizontal axis; that is, it produces the shadow of a vector on the horizontal axis from a flashlight shining down the vertical axis.   projects vectors onto the line defined by the eigenvector .  Since and , we have . That is, and .   is not invertible because is an eigenvalue.   "
},
{
  "id": "exercise-complex-eigenvector",
  "level": "2",
  "url": "sec-eigen-diag.html#exercise-complex-eigenvector",
  "type": "Exercise",
  "number": "20.3.5.8",
  "title": "",
  "body": " When is a matrix with a complex eigenvalue , we have said that there is a matrix such that where . In this exercise, we will learn how to find the matrix . As an example, we will consider the matrix .  Show that the eigenvalues of are complex.  Choose one of the complex eigenvalues and construct the usual matrix .  Using the same eigenvalue, we will find an eigenvector where the entries of are complex numbers. As always, we will describe by constructing the matrix and finding its reduced row echelon form. In doing so, we will necessarily need to use complex arithmetic.  We have now found a complex eigenvector . Write to identify vectors and having real entries.  Construct the matrix and verify that .       .  If , then .      and .       The eigenvalues are .  We will choose so that .  Construct the matrix so that a basis vector for the null space is .  We have giving and .  We let and verify that .   "
},
{
  "id": "sec-eigen-diag-7-9",
  "level": "2",
  "url": "sec-eigen-diag.html#sec-eigen-diag-7-9",
  "type": "Exercise",
  "number": "20.3.5.9",
  "title": "",
  "body": " For each of the following matrices, sketch the vector and powers for .     .        .        .     Consider a matrix of the form with . What happens when becomes very large when   .   .   .      In each case, the vectors are rotated by . Note that the figures below are draw at different scales.    The vectors are stretched by a factor of 1.4.       The vectors are stretched by a factor of 0.8.       The length of the vectors is unchanged.     For different values of ,  The vectors are pulled into the origin as their lengths become increasingly small.  The length of the vectors is unchanged.  The vectors are pushed away from the origin as their lengths become increasingly large.      In each case, the vectors are rotated by . Note that the figures below are draw at different scales.    The vectors are stretched by a factor of 1.4.       The vectors are stretched by a factor of 0.8.       The length of the vectors is unchanged.     For different values of ,  The vectors are pulled into the origin as their lengths become increasingly small.  The length of the vectors is unchanged.  The vectors are pushed away from the origin as their lengths become increasingly large.     "
},
{
  "id": "sec-eigen-diag-7-10",
  "level": "2",
  "url": "sec-eigen-diag.html#sec-eigen-diag-7-10",
  "type": "Exercise",
  "number": "20.3.5.10",
  "title": "",
  "body": " For each of the following matrices and vectors, sketch the vector along with for .     .                        Find the eigenvalues and eigenvectors of to create your sketch.     If is a matrix with eigenvalues and and is any vector, what happens to when becomes very large?                          Vectors will be pulled into the origin.       Vectors are expanded horizontally and compressed vertically.       Vectors are compressed both horizontally and vertically.       Vectors are expanded both horizontally and vertically.       There are eigenvectors with and with . Vectors are expanded in the direction of and compressed in the direction of .     Because both eigenvalues have an absolute value smaller than , vectors will be compressed in the direction of both eigenvectors and hence be pulled into the origin.   "
},
{
  "id": "sec-dynamical",
  "level": "1",
  "url": "sec-dynamical.html",
  "type": "Section",
  "number": "20.4",
  "title": "Dynamical systems",
  "body": " Dynamical systems   The last section demonstrated ways in which we may relate a matrix, and the effect that multiplication has on vectors, to a simpler form. For instance, if there is a basis of consisting of eigenvectors of , we saw that is similar to a diagonal matrix . As a result, the effect of multiplying vectors by , when expressed using the basis of eigenvectors, is the same as multiplying by .  In this section, we will put these ideas to use as we explore discrete dynamical systems, first encountered in . Recall that we used a state vector to characterize the state of some system at a particular time, such as the distribution of delivery trucks between two locations. A matrix described the transition of the state vector with characterizing the state of the system at a later time. Since we would like to understand how the state vector evolves over time, we are interested in studying the sequence of vectors .  Our goal in this section is to describe the types of behaviors that dynamical systems exhibit and to develop a means of detecting these behaviors.    Suppose that we have a diagonalizable matrix where .  Find the eigenvalues of and find a basis for the associated eigenspaces.  Form a basis of consisting of eigenvectors of and write the vector as a linear combination of basis vectors.  Write as a linear combination of basis vectors.  For some power , write as a linear combination of basis vectors.  Find the vector .      Since has been diagonalized as , the eigenvalues of are the diagonal entries of and the eigenvectors are the columns of . Therefore, we know the eigenvalues are with associated eigenvector and with associated eigenvector .  The columns of , and , form a basis for . We find that .  Then .  We have .   .       A first example  We will begin with a dynamical system that illustrates how the ideas we've been developing can help us understand the populations of two interacting species. There are several possible ways in which two species may interact. For example, wolves on Isle Royale in northern Michigan prey on moose so this interaction is often called a predator-prey relationship. Other interactions between species, such as bees and flowering plants, are mutually beneficial for both species.    Suppose we have two species and that interact with each another and that we record the change in their populations from year to year. When we begin our study, the populations, measured in thousands, are and ; after years, the populations are and .  If we know the populations in one year, suppose that the populations in the following year are determined by the expressions This is an example of a mutually beneficial relationship between two species. If species is not present, then , which means that the population of species decreases every year. However, species benefits from the presence of species , which helps to grow by 80% of the population of species . In the same way, benefits from the presence of .  We will record the populations in a vector and note that where .  Verify that are eigenvectors of and find their respective eigenvalues.  Suppose that initially . Write as a linear combination of the eigenvectors and .  Write the vectors , , and as linear combinations of the eigenvectors and .  What happens to after a very long time?  When becomes very large, what happens to the ratio of the populations ?  After a very long time, by approximately what factor does the population of grow every year? By approximately what factor does the population of grow every year?  If we begin instead with , what eventually happens to the ratio as becomes very large?      We find that and . This shows that and are eigenvectors with associated eigenvalues and .  Solving for the weights of the linear combination of and that produces , we find that .  Each time we multiply by , the eigenvectors are multiplied by their associated eigenvalues. This gives   After a long time so we have .  More generally, we have . As grows large, becomes insignificantly small so that . This means that and so that the ratio .  We see that so that the populations both grow by a factor of approximately 1.3, which is a 30% growth rate.  Now we have . In the same way, when is very large, we have so that and . This gives the same ratio: .     This activity demonstrates the type of systems we will be considering. In particular, we will have vectors that describe the state of the system at time and a matrix that describes how the state evolves from one time to the next: . The eigenvalues and eigenvectors of provide the key that helps us understand how the vectors evolve and that enables us to make long-range predictions.  Let's look at the specific example in the previous activity more carefully. We see that and that the matrix has eigenvectors and with associated eigenvalues and .  With initial populations , we have   Let's shift our perspective slightly. The eigenvectors and form a basis of , which says that is diagonalizable; that is, where   The coordinate system defined by the basis can be used to express the state vectors. For instance, we can write the initial state vector , which means that . Moreover, so that In the same way,   More generally, we have which is a restatement of the fact that is similar to .   Thinking about this geometrically, we begin with the vector . Subsequent vectors are obtained by scaling horizontally by a factor of and scaling vertically by a factor . Notice how the points move along a curve away from the origin becoming ever closer to the horizontal axis. After a very long time, .     To recover the behavior of the sequence , we change coordinate systems using the basis defined by and . Here, the points move along a curve away from the origin becoming ever closer to the line defined by .    Eventually, the vectors become practically indistinguishable from a scalar multiple of since . This means that This shows that so that we expect the population of species to eventually be about twice that of species .  In addition, so that and , which tells us that both populations are multiplied by 1.3 every year meaning the annual growth rate for both populations is about 30%.  In the same way, we can consider other possible initial populations as shown in . Regardless of , the population vectors, in the coordinates defined by , are scaled horizontally by a factor of and vertically by a factor of . The sequence of points , called trajectories , move along the curves, as shown on the left. In the standard coordinate system, we see that the trajectories converge to the eigenspace .      The trajectories of the dynamical system formed by the matrix in the coordinate system defined by , on the left, and in the standard coordinate system, on the right.   We conclude that, regardless of the initial populations, the ratio of the populations will approach 2 to 1 and that the growth rate for both populations approaches 30%. This example demonstrates the power of using eigenvalues and eigenvectors to rewrite the problem in terms of a new coordinate system. By doing so, we are able to predict the long-term behavior of the populations independently of the initial populations.  Diagrams like those shown in are called phase portraits. On the left of is the phase portrait of the diagonal matrix while the right of that figure shows the phase portrait of . The phase portrait of is relatively easy to understand because it is determined only by the two eigenvalues. Once we have the phase portrait of , however, the phase portrait of has a similar appearance with the eigenvectors replacing the standard basis vectors .    Classifying dynamical systems  In the previous example, we were able to make predictions about the behavior of trajectories by considering the eigenvalues and eigenvectors of the matrix . The next activity looks at a collection of matrices that demonstrate the types of behavior a dynamical system can exhibit.    We will now look at several more examples of dynamical systems. If , we note that the columns of form a basis of . Given below are several matrices written in the form for some matrix . For each matrix, state the eigenvalues of and sketch a phase portrait for the matrix on the left and a phase portrait for on the right. Describe the behavior of as becomes very large for a typical initial vector .    where .        where .        where .        where .        where .        where .                                           This activity demonstrates six possible types of dynamical systems, which are determined by the eigenvalues of .  Suppose that has two real eigenvalues and and that both . In this case, any nonzero vector forms a trajectory that moves away from the origin so we say that the origin is a repellor . This is illustrated in .      The origin is a repellor when .    Suppose that has two real eigenvalues and and that . In this case, most nonzero vectors form trajectories that converge to the eigenspace . In this case, we say that the origin is a saddle as illustrated in .      The origin is a saddle when .    Suppose that has two real eigenvalues and and that both . In this case, any nonzero vector forms a trajectory that moves into the origin so we say that the origin is an attractor . This is illustrated in .      The origin is an attractor when .    Suppose that has a complex eigenvalue where . In this case, a nonzero vector forms a trajectory that spirals away from the origin. We say that the origin is a spiral repellor , as illustrated in .      The origin is a spiral repellor when has an eigenvalue with .    Suppose that has a complex eigenvalue where . In this case, a nonzero vector forms a trajectory that moves on a closed curve around the origin. We say that the origin is a center , as illustrated in .      The origin is a center when has an eigenvalue with .    Suppose that has a complex eigenvalue where . In this case, a nonzero vector forms a trajectory that spirals into the origin. We say that the origin is a spiral attractor , as illustrated in .      The origin is a spiral attractor when has an eigenvalue with .     This list includes many types of expected behavior, but there are other possibilities if, for instance, one of the eigenvalues is 0. The next section explores the situation when one of the eigenvalues is 1.    In this activity, we will consider several ways in which two species might interact with one another. Throughout, we will consider two species and whose populations in year form a vector and which evolve according to the rule .   Suppose that .  Explain why the species do not interact with one another. Which of the six types of dynamical systems do we have? What happens to both species after a long time?    Suppose now that .  Explain why is a beneficial species for . Which of the six types of dynamical systems do we have? What happens to both species after a long time?    If , explain why this describes a predator-prey system. Which of the species is the predator and which is the prey? Which of the six types of dynamical systems do we have? What happens to both species after a long time?    Suppose that . Compare this predator-prey system to the one in the previous part. Which of the six types of dynamical systems do we have? What happens to both species after a long time?        With this matrix , we have This shows that the population of one species does not depend on the other. This dynamical system is a saddle because we have the eigenvalues and . Eventually, species will become extinct while species grows by a factor of 1.6.  Now we have The population of species is not influenced by species . However, we see that the population of species grows in the presence of species . In other words, species helps species to grow so we say that is beneficial for .  Because the eigenvalues are and , this dynamical system is again a saddle. The associated eigenvectors are and . After a long time, the population vector so both populations grow by a factor of 1.6 and with a ratio   Here we have Species helps species to grow, while species inhibits the growth of species . One explanation for this is that species preys on species as a food source.  We have eigenvectors with associated eigenvalue and with . This dynamical system is a repellor so both species will grow arbitrarily large.  This example is similar to the previous one, but the coefficients are slightly different. We see that the growth rate of both species is smaller. For instance, in the previous problem, we had while we now have . This says that the reproduction rate of species has decreased from to . In the same way, that of species has decreased from to . Also, so the presence of species is less beneficial to species .  We now have the eigenvalues and , which means that this dynamical system is an attractor and that both species will become extinct.       A system  Up to this point, we have focused on systems. In fact, the general case is quite similar. As an example, consider a system where the matrix has eigenvalues , , and . Since the eigenvalues are real and distinct, there is a basis consisting of eigenvectors of so we can look at the trajectories in the coordinate system defined by . The phase portraits in show how some representative trajectories will evolve. We see that all the trajectories will converge into the eigenspace .       In a system with , , and , the trajectories move along the curves shown above.   In the same way, suppose we have a system with complex eigenvalues and . Since the complex eigenvalues satisfy , there is a two-dimensional subspace in which the trajectories spiral in toward the origin. The phase portraits in show some of the trajectories. Once again, we see that all the trajectories converge into the eigenspace .       In a system with complex eigenvalues with and , the trajectories move along the curves shown above.     The following type of analysis has been used to study the population of a bison herd. We will divide the population of female bison into three groups: juveniles who are less than one year old; yearlings between one and two years old; and adults who are older than two years.   Each year,  80% of the juveniles survive to become yearlings.  90% of the yearlings survive to become adults.  80% of the adults survive.  40% of the adults give birth to a juvenile.     By , , and , we denote the number of juveniles, yearlings, and adults in year . We have .  Find similar expressions for and in terms of , , and .  As is usual, we write the matrix . Write the matrix such that and find its eigenvalues.  We can write where the matrices and are approximately: Make a prediction about the long-term behavior of . For instance, at what rate does it grow? For every 100 adults, how many juveniles, and yearlings are there?  Suppose that the birth rate decreases so that only 30% of adults give birth to a juvenile. How does this affect the long-term growth rate of the herd?   Suppose that the birth rate decreases further so that only 20% of adults give birth to a juvenile. How does this affect the long-term growth rate of the herd?   Find the smallest birth rate that supports a stable population.      We have the relationships   The matrix .  There is a real eigenvalue , which is larger than . The other eigenvalues are complex and satisfy . Therefore, the complex eigenvalues will pull vectors in toward the line defined by the eigenvector . After a long time, the population is . All the populations grow annually by a factor of or 5.8%, and there are approximately 38 juveniles and 29 yearlings for every 100 adults.  We now have eigenvalues and . This shows that the growth rate is lowered to or % annually.  The first eigenvalue is so the growth rate is about %. In other words, the population decreases every year.  To be stable, we need the first eigenvalue . If we experiment with different birth rates, we see that a birth rate of about 0.278 gives this eigenvalue.       Summary  We have been exploring discrete dynamical systems in which an initial state vector evolves over time according to the rule . The eigenvalues and eigenvectors of help us understand the behavior of the state vectors. In the case, we saw that   produces an attractor so that trajectories are pulled in toward the origin.   and produces a saddle in which most trajectories are pushed away from the origin and in the direction of .   produces a repellor in which trajectories are pushed away from the origin.  The same kind of reasoning allows us to analyze systems as well.     For each of the matrices below, find the eigenvalues and, when appropriate, the eigenvectors to classify the dynamical system . Use this information to sketch the phase portraits.   .   .   .   .     Repellor  Spiral repellor  Saddle  Attractor     There are two real eigenvalues and with associated eigenvectors and . This dynamical system is a repellor so trajectories are pushed away from the origin.     This is a spiral repellor because there are complex eigenvalues whose length is greater than one. Trajectories will spiral away from the origin.     This is a saddle because there are two real eigenvalues and with associated eigenvectors and . Most trajectories will move away from the origin along the line defined by the eigenvector .     This is an attractor because there are two real eigenvalues and with associated eigenvectors and . Trajectories will be pulled in toward the origin.        We will consider matrices that have the form where where is a parameter that we will vary. Sketch phase portraits for and below when   .      .      .     For the different values of , determine which types of dynamical system results. For what range of values do we have an attractor? For what range of values do we have a saddle? For what value does the transition between the two types occur?    The phase portraits are as shown.  When .     When .     When .     If , we have an attractor. If , we have a saddle. The transition between the two types occurs at .    The phase portraits are as shown.  When .     When .     When .     If , we have an attractor. If , we have a saddle. The transition between the two types occurs at .     Suppose that the populations of two species interact according to the relationships where is a parameter. As we saw in the text, this dynamical system represents a typical predator-prey relationship, and the parameter represents the rate at which species preys on . We will denote the matrix .  If , determine the eigenvectors and eigenvalues of the system and classify it as one of the six types. Sketch the phase portraits for the diagonal matrix to which is similar as well as the phase portrait for .     If , determine the eigenvectors and eigenvalues of the system. Sketch the phase portraits for the diagonal matrix to which is similar as well as the phase portrait for .     For what values of is the origin a saddle? What can you say about the populations when this happens?  Describe the evolution of the dynamical system as begins at and increases to .      and       and         As grows, there needs to be a sufficient population to ensure the survival of both species.     If , the eigenvectors are with associated eigenvalue and with eigenvalue . The phase portraits are as     With , we have eigenvectors with eigenvalue and with eigenvalue . The phase portraits appear as     We find the characteristic polynomial of to be which has roots So long as , one root will be real and larger than . We have a saddle when This shows that we have a saddle whenever .  When , we have a saddle. Notice that will go extinct if the initial population of is zero. As we increase , we still have a saddle, but will now go extinct when the initial population is positive. Eventually, when , we see that will go extinct as long as .  As we increase the predation rate, the population of is lowered by . However, is necessary to 's survival. When the predation rate grows too large, there needs to be a sufficiently large initial population to support both species.      Consider the matrices   Find the eigenvalues of . To which of the six types does the system belong?   Using the eigenvalues of , we can write for some matrices and . What is the matrix and what geometric effect does multiplication by have on vectors in the plane?  If we remember that , determine the smallest positive value of for which .  Find the eigenvalues of .  Find a matrix such that for some matrix . What geometric effect does multiplication by have on vectors in the plane?  Determine the smallest positive value of for which .     This dynamical system is a center.   .   .   .      .     The eigenvalues are so this dynamical system is a center.  The matrix is , which is a rotation.  If we rotate by four times, we obtain the identity. Therefore, .  Here, we find the eigenvalues .  The matrix is , which is a rotation.  If we rotate by six times, we obtain the identity. Therefore, .     Suppose we have the female population of a species is divided into juveniles, yearlings, and adults and that each year  90% of the juveniles live to be yearlings.  80% of the yearlings live to be adults.  60% of the adults survive to the next year.  50% of the adults give birth to a juvenile.      Set up a system of the form that describes this situation.  Find the eigenvalues of the matrix .   What prediction can you make about these populations after a very long time?  If the birth rate goes up to 80%, what prediction can you make about these populations after a very long time? For every 100 adults, how many juveniles, and yearlings are there?      If we write , we have    and .  The populations will eventually become extinct.  All the populations grow by %, and there are about juveniles and yearlings for every adults.     If we write , we have   We find the eigenvalues and .  All three eigenvalues have a length less than one. Therefore, the populations will eventually become extinct.  With a birth rate of %, the first eigenvalue goes up to , which means that, eventually, all the populations grow by %. An eigenvector is approximately so there are about juveniles and yearlings for every adults.     Determine whether the following statements are true or false and provide a justification for your response. In each case, we are considering a dynamical system of the form .  If the matrix has a complex eigenvalue, we cannot make a prediction about the behavior of the trajectories.  If has eigenvalues whose absolute value is smaller than 1, then all the trajectories are pulled in toward the origin.  If the origin is a repellor, then it is an attractor for the system .  If a matrix has complex eigenvalues , , , and , all of which satisfy , then all the trajectories are pushed away from the origin.  If the origin is a saddle, then all the trajectories are pushed away from the origin.     False  True  True  True  False     False. We know that the dynamical system is either a spiral repellor, center, or spiral attractor.  True. In this case, we have either an attractor or spiral attractor.  True. If is an attractor, then all the eigenvalues . The eigenvalues for are , which satisfy .  True. We can decompose into two two-dimensional subspaces on which acts as a spiral repellor.  False. There are trajectories along the line defined by the eigenvector associated to the smaller eigenvalue that are pulled in toward the origin.     The Fibonacci numbers form the sequence of numbers that begins . If we let denote the Fibonacci number, then . In general, a Fibonacci number is the sum of the previous two Fibonacci numbers; that is, so that we have     If we write , find the matrix such that .  Show that has eigenvalues with associated eigenvectors and .  Classify this dynamical system as one of the six types that we have seen in this section. What happens to as becomes very large?  Write the initial vector as a linear combination of eigenvectors and .  Write the vector as a linear combinations of and .  Explain why the Fibonacci number   Use this relationship to compute .  Explain why when is very large.    The number is called the golden ratio and is one of mathematics' special numbers.       Find the roots of the characteristic polynomial.   defines a saddle.   .  We have .  From the second component, we see that      For large values of , we have .     We have   The characteristic equation is Applying the quadratic formula, we see that the two eigenvalues are and .  We find the eigenvectors by row reducing the matrices .   Because and , this is a saddle.  We find .  We have   From the second component, we see that   Using this relationship, we compute that .  For large values of , we have , which says that and . This gives the ratio .     This exercise is a continuation of the previous one.  The Lucas numbers are defined by the same relationship as the Fibonacci numbers: . However, we begin with and , which leads to the sequence .  As before, form the vector so that . Express as a linear combination of and , eigenvectors of .  Explain why   Explain why is the closest integer to when is large, where is the golden ratio.  Use this observation to find .       .     Because , larger powers are very close to zero.   .     Constructing the augmented matrix and row reducing shows that .  Then we have . Since is the second component of , we have .  Because , larger powers are very close to zero. Therefore, .  We compute that so that .     Gil Strang defines the Gibonacci numbers  as follows. We begin with and . A subsequent Gibonacci number is the average of the two previous; that is, . We then have   If , find the matrix such that .  Find the eigenvalues and associated eigenvectors of .  Explain why this dynamical system does not neatly fit into one of the six types that we saw in this section.  Write as a linear combination of eigenvectors of .  Write as a linear combination of eigenvectors of .  What happens to as becomes very large?       The eigenvalues are and with associated eigenvectors and .  We have one eigenvalue , which means this system does not fall into any of our cases.         stabilizes at .     If , then we have .  The eigenvalues are and with associated eigenvectors and .  We have one eigenvalue , which means this system does not fall into any of our cases.  After constructing an augmented matrix and row reducing, we find .  We have .  As becomes large, the coefficient becomes very small. Therefore, . This shows that stabilizes at .     Consider a small rodent that lives for three years. Once again, we can separate a population of females into juveniles, yearlings, and adults. Suppose that, each year,  Half of the juveniles live to be yearlings.  One quarter of the yearlings live to be adults.  Adult females produce eight female offspring.  None of the adults survive to the next year.      Writing the populations of juveniles, yearlings, and adults in year using the vector , find the matrix such that .  Show that .  What are the eigenvalues of ? What does this say about the eigenvalues of ?  Verify your observation by finding the eigenvalues of .   What can you say about the trajectories of this dynamical system?  What does this mean about the population of rodents?  Find a population vector that is unchanged from year to year.       .  We can use Sage to compute that .  Any eigenvalue of must satisfy .   and   The trajectories lie on circles about the line defined by .  After every three years, the populations return to their initial values.       We have where .  We can use Sage to compute that .  The only eigenvalues of are so any eigenvalue of must satisfy .  Sage tells us that the eigenvalues are and .  The eigenvector satisfies so it is unchanged. The complex eigenvalues cause a rotation about the axis defined by . Therefore, the trajectories lie on circles about this line.  After every three years, the populations return to their initial values.  If , we know that so this population vector is unchanged.     "
},
{
  "id": "sec-dynamical-2-4",
  "level": "2",
  "url": "sec-dynamical.html#sec-dynamical-2-4",
  "type": "Preview Activity",
  "number": "20.4.1",
  "title": "",
  "body": "  Suppose that we have a diagonalizable matrix where .  Find the eigenvalues of and find a basis for the associated eigenspaces.  Form a basis of consisting of eigenvectors of and write the vector as a linear combination of basis vectors.  Write as a linear combination of basis vectors.  For some power , write as a linear combination of basis vectors.  Find the vector .      Since has been diagonalized as , the eigenvalues of are the diagonal entries of and the eigenvectors are the columns of . Therefore, we know the eigenvalues are with associated eigenvector and with associated eigenvector .  The columns of , and , form a basis for . We find that .  Then .  We have .   .    "
},
{
  "id": "sec-dynamical-3-3",
  "level": "2",
  "url": "sec-dynamical.html#sec-dynamical-3-3",
  "type": "Activity",
  "number": "20.4.2",
  "title": "",
  "body": "  Suppose we have two species and that interact with each another and that we record the change in their populations from year to year. When we begin our study, the populations, measured in thousands, are and ; after years, the populations are and .  If we know the populations in one year, suppose that the populations in the following year are determined by the expressions This is an example of a mutually beneficial relationship between two species. If species is not present, then , which means that the population of species decreases every year. However, species benefits from the presence of species , which helps to grow by 80% of the population of species . In the same way, benefits from the presence of .  We will record the populations in a vector and note that where .  Verify that are eigenvectors of and find their respective eigenvalues.  Suppose that initially . Write as a linear combination of the eigenvectors and .  Write the vectors , , and as linear combinations of the eigenvectors and .  What happens to after a very long time?  When becomes very large, what happens to the ratio of the populations ?  After a very long time, by approximately what factor does the population of grow every year? By approximately what factor does the population of grow every year?  If we begin instead with , what eventually happens to the ratio as becomes very large?      We find that and . This shows that and are eigenvectors with associated eigenvalues and .  Solving for the weights of the linear combination of and that produces , we find that .  Each time we multiply by , the eigenvectors are multiplied by their associated eigenvalues. This gives   After a long time so we have .  More generally, we have . As grows large, becomes insignificantly small so that . This means that and so that the ratio .  We see that so that the populations both grow by a factor of approximately 1.3, which is a 30% growth rate.  Now we have . In the same way, when is very large, we have so that and . This gives the same ratio: .    "
},
{
  "id": "fig-eigen-dynam-saddle",
  "level": "2",
  "url": "sec-dynamical.html#fig-eigen-dynam-saddle",
  "type": "Figure",
  "number": "20.4.1",
  "title": "",
  "body": "    The trajectories of the dynamical system formed by the matrix in the coordinate system defined by , on the left, and in the standard coordinate system, on the right.  "
},
{
  "id": "sec-dynamical-4-3",
  "level": "2",
  "url": "sec-dynamical.html#sec-dynamical-4-3",
  "type": "Activity",
  "number": "20.4.3",
  "title": "",
  "body": "  We will now look at several more examples of dynamical systems. If , we note that the columns of form a basis of . Given below are several matrices written in the form for some matrix . For each matrix, state the eigenvalues of and sketch a phase portrait for the matrix on the left and a phase portrait for on the right. Describe the behavior of as becomes very large for a typical initial vector .    where .        where .        where .        where .        where .        where .                                          "
},
{
  "id": "fig-eigen-phase-repellor",
  "level": "2",
  "url": "sec-dynamical.html#fig-eigen-phase-repellor",
  "type": "Figure",
  "number": "20.4.2",
  "title": "",
  "body": "    The origin is a repellor when .  "
},
{
  "id": "fig-eigen-phase-saddle",
  "level": "2",
  "url": "sec-dynamical.html#fig-eigen-phase-saddle",
  "type": "Figure",
  "number": "20.4.3",
  "title": "",
  "body": "    The origin is a saddle when .  "
},
{
  "id": "fig-eigen-phase-attractor",
  "level": "2",
  "url": "sec-dynamical.html#fig-eigen-phase-attractor",
  "type": "Figure",
  "number": "20.4.4",
  "title": "",
  "body": "    The origin is an attractor when .  "
},
{
  "id": "fig-eigen-phase-spiralr",
  "level": "2",
  "url": "sec-dynamical.html#fig-eigen-phase-spiralr",
  "type": "Figure",
  "number": "20.4.5",
  "title": "",
  "body": "    The origin is a spiral repellor when has an eigenvalue with .  "
},
{
  "id": "fig-eigen-phase-center",
  "level": "2",
  "url": "sec-dynamical.html#fig-eigen-phase-center",
  "type": "Figure",
  "number": "20.4.6",
  "title": "",
  "body": "    The origin is a center when has an eigenvalue with .  "
},
{
  "id": "fig-eigen-phase-spirala",
  "level": "2",
  "url": "sec-dynamical.html#fig-eigen-phase-spirala",
  "type": "Figure",
  "number": "20.4.7",
  "title": "",
  "body": "    The origin is a spiral attractor when has an eigenvalue with .  "
},
{
  "id": "sec-dynamical-4-6",
  "level": "2",
  "url": "sec-dynamical.html#sec-dynamical-4-6",
  "type": "Activity",
  "number": "20.4.4",
  "title": "",
  "body": "  In this activity, we will consider several ways in which two species might interact with one another. Throughout, we will consider two species and whose populations in year form a vector and which evolve according to the rule .   Suppose that .  Explain why the species do not interact with one another. Which of the six types of dynamical systems do we have? What happens to both species after a long time?    Suppose now that .  Explain why is a beneficial species for . Which of the six types of dynamical systems do we have? What happens to both species after a long time?    If , explain why this describes a predator-prey system. Which of the species is the predator and which is the prey? Which of the six types of dynamical systems do we have? What happens to both species after a long time?    Suppose that . Compare this predator-prey system to the one in the previous part. Which of the six types of dynamical systems do we have? What happens to both species after a long time?        With this matrix , we have This shows that the population of one species does not depend on the other. This dynamical system is a saddle because we have the eigenvalues and . Eventually, species will become extinct while species grows by a factor of 1.6.  Now we have The population of species is not influenced by species . However, we see that the population of species grows in the presence of species . In other words, species helps species to grow so we say that is beneficial for .  Because the eigenvalues are and , this dynamical system is again a saddle. The associated eigenvectors are and . After a long time, the population vector so both populations grow by a factor of 1.6 and with a ratio   Here we have Species helps species to grow, while species inhibits the growth of species . One explanation for this is that species preys on species as a food source.  We have eigenvectors with associated eigenvalue and with . This dynamical system is a repellor so both species will grow arbitrarily large.  This example is similar to the previous one, but the coefficients are slightly different. We see that the growth rate of both species is smaller. For instance, in the previous problem, we had while we now have . This says that the reproduction rate of species has decreased from to . In the same way, that of species has decreased from to . Also, so the presence of species is less beneficial to species .  We now have the eigenvalues and , which means that this dynamical system is an attractor and that both species will become extinct.    "
},
{
  "id": "fig-eigen-3d-saddle",
  "level": "2",
  "url": "sec-dynamical.html#fig-eigen-3d-saddle",
  "type": "Figure",
  "number": "20.4.8",
  "title": "",
  "body": "     In a system with , , and , the trajectories move along the curves shown above.  "
},
{
  "id": "fig-eigen-3d-spiral",
  "level": "2",
  "url": "sec-dynamical.html#fig-eigen-3d-spiral",
  "type": "Figure",
  "number": "20.4.9",
  "title": "",
  "body": "     In a system with complex eigenvalues with and , the trajectories move along the curves shown above.  "
},
{
  "id": "sec-dynamical-5-6",
  "level": "2",
  "url": "sec-dynamical.html#sec-dynamical-5-6",
  "type": "Activity",
  "number": "20.4.5",
  "title": "",
  "body": "  The following type of analysis has been used to study the population of a bison herd. We will divide the population of female bison into three groups: juveniles who are less than one year old; yearlings between one and two years old; and adults who are older than two years.   Each year,  80% of the juveniles survive to become yearlings.  90% of the yearlings survive to become adults.  80% of the adults survive.  40% of the adults give birth to a juvenile.     By , , and , we denote the number of juveniles, yearlings, and adults in year . We have .  Find similar expressions for and in terms of , , and .  As is usual, we write the matrix . Write the matrix such that and find its eigenvalues.  We can write where the matrices and are approximately: Make a prediction about the long-term behavior of . For instance, at what rate does it grow? For every 100 adults, how many juveniles, and yearlings are there?  Suppose that the birth rate decreases so that only 30% of adults give birth to a juvenile. How does this affect the long-term growth rate of the herd?   Suppose that the birth rate decreases further so that only 20% of adults give birth to a juvenile. How does this affect the long-term growth rate of the herd?   Find the smallest birth rate that supports a stable population.      We have the relationships   The matrix .  There is a real eigenvalue , which is larger than . The other eigenvalues are complex and satisfy . Therefore, the complex eigenvalues will pull vectors in toward the line defined by the eigenvector . After a long time, the population is . All the populations grow annually by a factor of or 5.8%, and there are approximately 38 juveniles and 29 yearlings for every 100 adults.  We now have eigenvalues and . This shows that the growth rate is lowered to or % annually.  The first eigenvalue is so the growth rate is about %. In other words, the population decreases every year.  To be stable, we need the first eigenvalue . If we experiment with different birth rates, we see that a birth rate of about 0.278 gives this eigenvalue.    "
},
{
  "id": "sec-dynamical-7-1",
  "level": "2",
  "url": "sec-dynamical.html#sec-dynamical-7-1",
  "type": "Exercise",
  "number": "20.4.5.1",
  "title": "",
  "body": " For each of the matrices below, find the eigenvalues and, when appropriate, the eigenvectors to classify the dynamical system . Use this information to sketch the phase portraits.   .   .   .   .     Repellor  Spiral repellor  Saddle  Attractor     There are two real eigenvalues and with associated eigenvectors and . This dynamical system is a repellor so trajectories are pushed away from the origin.     This is a spiral repellor because there are complex eigenvalues whose length is greater than one. Trajectories will spiral away from the origin.     This is a saddle because there are two real eigenvalues and with associated eigenvectors and . Most trajectories will move away from the origin along the line defined by the eigenvector .     This is an attractor because there are two real eigenvalues and with associated eigenvectors and . Trajectories will be pulled in toward the origin.      "
},
{
  "id": "sec-dynamical-7-2",
  "level": "2",
  "url": "sec-dynamical.html#sec-dynamical-7-2",
  "type": "Exercise",
  "number": "20.4.5.2",
  "title": "",
  "body": " We will consider matrices that have the form where where is a parameter that we will vary. Sketch phase portraits for and below when   .      .      .     For the different values of , determine which types of dynamical system results. For what range of values do we have an attractor? For what range of values do we have a saddle? For what value does the transition between the two types occur?    The phase portraits are as shown.  When .     When .     When .     If , we have an attractor. If , we have a saddle. The transition between the two types occurs at .    The phase portraits are as shown.  When .     When .     When .     If , we have an attractor. If , we have a saddle. The transition between the two types occurs at .   "
},
{
  "id": "sec-dynamical-7-3",
  "level": "2",
  "url": "sec-dynamical.html#sec-dynamical-7-3",
  "type": "Exercise",
  "number": "20.4.5.3",
  "title": "",
  "body": " Suppose that the populations of two species interact according to the relationships where is a parameter. As we saw in the text, this dynamical system represents a typical predator-prey relationship, and the parameter represents the rate at which species preys on . We will denote the matrix .  If , determine the eigenvectors and eigenvalues of the system and classify it as one of the six types. Sketch the phase portraits for the diagonal matrix to which is similar as well as the phase portrait for .     If , determine the eigenvectors and eigenvalues of the system. Sketch the phase portraits for the diagonal matrix to which is similar as well as the phase portrait for .     For what values of is the origin a saddle? What can you say about the populations when this happens?  Describe the evolution of the dynamical system as begins at and increases to .      and       and         As grows, there needs to be a sufficient population to ensure the survival of both species.     If , the eigenvectors are with associated eigenvalue and with eigenvalue . The phase portraits are as     With , we have eigenvectors with eigenvalue and with eigenvalue . The phase portraits appear as     We find the characteristic polynomial of to be which has roots So long as , one root will be real and larger than . We have a saddle when This shows that we have a saddle whenever .  When , we have a saddle. Notice that will go extinct if the initial population of is zero. As we increase , we still have a saddle, but will now go extinct when the initial population is positive. Eventually, when , we see that will go extinct as long as .  As we increase the predation rate, the population of is lowered by . However, is necessary to 's survival. When the predation rate grows too large, there needs to be a sufficiently large initial population to support both species.    "
},
{
  "id": "sec-dynamical-7-4",
  "level": "2",
  "url": "sec-dynamical.html#sec-dynamical-7-4",
  "type": "Exercise",
  "number": "20.4.5.4",
  "title": "",
  "body": " Consider the matrices   Find the eigenvalues of . To which of the six types does the system belong?   Using the eigenvalues of , we can write for some matrices and . What is the matrix and what geometric effect does multiplication by have on vectors in the plane?  If we remember that , determine the smallest positive value of for which .  Find the eigenvalues of .  Find a matrix such that for some matrix . What geometric effect does multiplication by have on vectors in the plane?  Determine the smallest positive value of for which .     This dynamical system is a center.   .   .   .      .     The eigenvalues are so this dynamical system is a center.  The matrix is , which is a rotation.  If we rotate by four times, we obtain the identity. Therefore, .  Here, we find the eigenvalues .  The matrix is , which is a rotation.  If we rotate by six times, we obtain the identity. Therefore, .   "
},
{
  "id": "sec-dynamical-7-5",
  "level": "2",
  "url": "sec-dynamical.html#sec-dynamical-7-5",
  "type": "Exercise",
  "number": "20.4.5.5",
  "title": "",
  "body": " Suppose we have the female population of a species is divided into juveniles, yearlings, and adults and that each year  90% of the juveniles live to be yearlings.  80% of the yearlings live to be adults.  60% of the adults survive to the next year.  50% of the adults give birth to a juvenile.      Set up a system of the form that describes this situation.  Find the eigenvalues of the matrix .   What prediction can you make about these populations after a very long time?  If the birth rate goes up to 80%, what prediction can you make about these populations after a very long time? For every 100 adults, how many juveniles, and yearlings are there?      If we write , we have    and .  The populations will eventually become extinct.  All the populations grow by %, and there are about juveniles and yearlings for every adults.     If we write , we have   We find the eigenvalues and .  All three eigenvalues have a length less than one. Therefore, the populations will eventually become extinct.  With a birth rate of %, the first eigenvalue goes up to , which means that, eventually, all the populations grow by %. An eigenvector is approximately so there are about juveniles and yearlings for every adults.   "
},
{
  "id": "sec-dynamical-7-6",
  "level": "2",
  "url": "sec-dynamical.html#sec-dynamical-7-6",
  "type": "Exercise",
  "number": "20.4.5.6",
  "title": "",
  "body": " Determine whether the following statements are true or false and provide a justification for your response. In each case, we are considering a dynamical system of the form .  If the matrix has a complex eigenvalue, we cannot make a prediction about the behavior of the trajectories.  If has eigenvalues whose absolute value is smaller than 1, then all the trajectories are pulled in toward the origin.  If the origin is a repellor, then it is an attractor for the system .  If a matrix has complex eigenvalues , , , and , all of which satisfy , then all the trajectories are pushed away from the origin.  If the origin is a saddle, then all the trajectories are pushed away from the origin.     False  True  True  True  False     False. We know that the dynamical system is either a spiral repellor, center, or spiral attractor.  True. In this case, we have either an attractor or spiral attractor.  True. If is an attractor, then all the eigenvalues . The eigenvalues for are , which satisfy .  True. We can decompose into two two-dimensional subspaces on which acts as a spiral repellor.  False. There are trajectories along the line defined by the eigenvector associated to the smaller eigenvalue that are pulled in toward the origin.   "
},
{
  "id": "sec-dynamical-7-7",
  "level": "2",
  "url": "sec-dynamical.html#sec-dynamical-7-7",
  "type": "Exercise",
  "number": "20.4.5.7",
  "title": "",
  "body": " The Fibonacci numbers form the sequence of numbers that begins . If we let denote the Fibonacci number, then . In general, a Fibonacci number is the sum of the previous two Fibonacci numbers; that is, so that we have     If we write , find the matrix such that .  Show that has eigenvalues with associated eigenvectors and .  Classify this dynamical system as one of the six types that we have seen in this section. What happens to as becomes very large?  Write the initial vector as a linear combination of eigenvectors and .  Write the vector as a linear combinations of and .  Explain why the Fibonacci number   Use this relationship to compute .  Explain why when is very large.    The number is called the golden ratio and is one of mathematics' special numbers.       Find the roots of the characteristic polynomial.   defines a saddle.   .  We have .  From the second component, we see that      For large values of , we have .     We have   The characteristic equation is Applying the quadratic formula, we see that the two eigenvalues are and .  We find the eigenvectors by row reducing the matrices .   Because and , this is a saddle.  We find .  We have   From the second component, we see that   Using this relationship, we compute that .  For large values of , we have , which says that and . This gives the ratio .   "
},
{
  "id": "sec-dynamical-7-8",
  "level": "2",
  "url": "sec-dynamical.html#sec-dynamical-7-8",
  "type": "Exercise",
  "number": "20.4.5.8",
  "title": "",
  "body": " This exercise is a continuation of the previous one.  The Lucas numbers are defined by the same relationship as the Fibonacci numbers: . However, we begin with and , which leads to the sequence .  As before, form the vector so that . Express as a linear combination of and , eigenvectors of .  Explain why   Explain why is the closest integer to when is large, where is the golden ratio.  Use this observation to find .       .     Because , larger powers are very close to zero.   .     Constructing the augmented matrix and row reducing shows that .  Then we have . Since is the second component of , we have .  Because , larger powers are very close to zero. Therefore, .  We compute that so that .   "
},
{
  "id": "sec-dynamical-7-9",
  "level": "2",
  "url": "sec-dynamical.html#sec-dynamical-7-9",
  "type": "Exercise",
  "number": "20.4.5.9",
  "title": "",
  "body": " Gil Strang defines the Gibonacci numbers  as follows. We begin with and . A subsequent Gibonacci number is the average of the two previous; that is, . We then have   If , find the matrix such that .  Find the eigenvalues and associated eigenvectors of .  Explain why this dynamical system does not neatly fit into one of the six types that we saw in this section.  Write as a linear combination of eigenvectors of .  Write as a linear combination of eigenvectors of .  What happens to as becomes very large?       The eigenvalues are and with associated eigenvectors and .  We have one eigenvalue , which means this system does not fall into any of our cases.         stabilizes at .     If , then we have .  The eigenvalues are and with associated eigenvectors and .  We have one eigenvalue , which means this system does not fall into any of our cases.  After constructing an augmented matrix and row reducing, we find .  We have .  As becomes large, the coefficient becomes very small. Therefore, . This shows that stabilizes at .   "
},
{
  "id": "sec-dynamical-7-10",
  "level": "2",
  "url": "sec-dynamical.html#sec-dynamical-7-10",
  "type": "Exercise",
  "number": "20.4.5.10",
  "title": "",
  "body": " Consider a small rodent that lives for three years. Once again, we can separate a population of females into juveniles, yearlings, and adults. Suppose that, each year,  Half of the juveniles live to be yearlings.  One quarter of the yearlings live to be adults.  Adult females produce eight female offspring.  None of the adults survive to the next year.      Writing the populations of juveniles, yearlings, and adults in year using the vector , find the matrix such that .  Show that .  What are the eigenvalues of ? What does this say about the eigenvalues of ?  Verify your observation by finding the eigenvalues of .   What can you say about the trajectories of this dynamical system?  What does this mean about the population of rodents?  Find a population vector that is unchanged from year to year.       .  We can use Sage to compute that .  Any eigenvalue of must satisfy .   and   The trajectories lie on circles about the line defined by .  After every three years, the populations return to their initial values.       We have where .  We can use Sage to compute that .  The only eigenvalues of are so any eigenvalue of must satisfy .  Sage tells us that the eigenvalues are and .  The eigenvector satisfies so it is unchanged. The complex eigenvalues cause a rotation about the axis defined by . Therefore, the trajectories lie on circles about this line.  After every three years, the populations return to their initial values.  If , we know that so this population vector is unchanged.   "
},
{
  "id": "sec-stochastic",
  "level": "1",
  "url": "sec-stochastic.html",
  "type": "Section",
  "number": "20.5",
  "title": "Markov chains and Google’s PageRank algorithm",
  "body": " Markov chains and Google's PageRank algorithm   In the last section, we used our understanding of eigenvalues and eigenvectors to describe the long-term behavior of some discrete dynamical systems. The state of the system, which could record, say, the populations of a few interacting species, at one time is described by a vector . The state vector then evolves according to a linear rule .  This section continues this exploration by looking at Markov chains , which form a specific type of discrete dynamical system. For instance, we could be interested in a rental car company that rents cars from several locations. From one day to the next, the number of cars at different locations can change, but the total number of cars stays the same. Once again, an understanding of eigenvalues and eigenvectors will help us make predictions about the long-term behavior of the system.    Suppose that our rental car company rents from two locations and . We find that 80% of the cars rented from location are returned to while the other 20% are returned to . For cars rented from location , 60% are returned to and 40% to .  We will use and to denote the number of cars at the two locations on day . The following day, the number of cars at equals 80% of and 40% of . This shows that   If we use the vector to represent the distribution of cars on day , find a matrix such that .  Find the eigenvalues and associated eigenvectors of .  Suppose that there are initially 1500 cars, all of which are at location . Write the vector as a linear combination of eigenvectors of .  Write the vectors as a linear combination of eigenvectors of .  What happens to the distribution of cars after a long time?      Expressing the set of equations in matrix form, we see that .  We have eigenvalues and with associated eigenvectors and .  We find that .  Multiplying by the matrix has the effect of multiplying the eigenvectors by their associated eigenvalues. Therefore, .  As becomes large, becomes very close to zero. Therefore . This tells us that cars are at location and are at .       A first example  In the preview activity, the distribution of rental cars was described by the discrete dynamical system . This matrix has some special properties. First, each entry represents the probability that a car rented at one location is returned to another. For instance, there is an 80% chance that a car rented at is returned to , which explains the entry of 0.8 in the upper left corner. Therefore, the entries of the matrix are between 0 and 1.  Second, a car rented at one location must be returned to one of the locations. For example, since 80% of the cars rented at are returned to , it follows that the other 20% of cars rented at are returned to . This implies that the entries in each column must add to 1. This will occur frequently in our discussion so we introduce the following definitions.   probability vector  stochastic matrix  A vector whose entries are nonnegative and add to 1 is called a probability vector . A square matrix whose columns are probability vectors is called a stochastic matrix.     Suppose you live in a country with three political parties , , and . We use , , and to denote the percentage of voters voting for that party in election .   Voters will change parties from one election to the next as shown in the figure. We see that 60% of voters stay with the same party. However, 40% of those who vote for party will vote for party in the next election.      Write expressions for , , and in terms of , , and .  If we write , find the matrix such that .  Explain why is a stochastic matrix.  Suppose that initially 40% of citizens vote for party , 30% vote for party , and 30% vote for party . Form the vector and explain why is a probability vector.  Find , the percentages who vote for the three parties in the next election. Verify that is also a probability vector and explain why will be a probability vector for every .   Find the eigenvalues of the matrix and explain why the eigenspace is a one-dimensional subspace of . Then verify that is a basis vector for .  As every vector in is a scalar multiple of , find a probability vector in and explain why it is the only probability vector in .  Describe what happens to after a very long time.      The solutions to this activity are given in the following text.    The previous activity illustrates some important points that we wish to emphasize.  First, to determine , we note that in election , party retains 60% of its voters from the previous election and adds 20% of those who voted for party . In this way, we see that We therefore define the matrix and note that .  If we consider the first column of , we see that the entries represent the percentages of party 's voters in the last election who vote for each of the three parties in the next election. Since everyone who voted for party previously votes for one of the three parties in the next election, the sum of these percentages must be 1. This is true for each of the columns of , which explains why is a stochastic matrix.  We begin with the vector , the entries of which represent the percentage of voters voting for each of the three parties. Since every voter votes for one of the three parties, the sum of these entries must be 1, which means that is a probability vector. We then find that . Notice that the vectors are also probability vectors and that the sequence seems to be converging to . It is this behavior that we would like to understand more fully by investigating the eigenvalues and eigenvectors of .  We find that the eigenvalues of are . Notice that if is an eigenvector of with associated eigenvalue , then . That is, is unchanged when we multiply it by .   Otherwise, we have where Notice that so the trajectories spiral into the eigenspace as indicated in the figure.    This tells us that the sequence converges to a vector in . In the usual way, we see that is a basis vector for because so we expect that will converge to a scalar multiple of . Indeed, since the vectors are probability vectors, we expect them to converge to a probability vector in .  We can find the probability vector in by finding the appropriate scalar multiple of . Notice that is a probability vector when , which implies that . Therefore, is the unique probability vector in . Since the sequence converges to a probability vector in , we see that converges to , which agrees with the computations we showed above.  The role of the eigenvalues is important in this example. Since , we can find a probability vector that is unchanged by multiplication by . Also, the other eigenvalues satisfy , which means that all the trajectories get pulled in to the eigenspace . Since is a sequence of probability vectors, these vectors converge to the probability vector as they are pulled into .    Markov chains  If we have a stochastic matrix and a probability vector , we can form the sequence where . We call this sequence of vectors a Markov chain . explains why we can guarantee that the vectors are probability vectors. Markov chain   In the example that studied voting patterns, we constructed a Markov chain that described how the percentages of voters choosing different parties changed from one election to the next. We saw that the Markov chain converges to , a probability vector in the eigenspace . In other words, is a probability vector that is unchanged under multiplication by ; that is, . This implies that, after a long time, 20% of voters choose party , 40% choose , and 40% choose .   steady-state vector  stationary vector  If is a stochastic matrix, we say that a probability vector is a steady-state or stationary vector if .   An important question that arises from our previous example is   If is a stochastic matrix and a Markov chain, does converge to a steady-state vector?     Consider the matrices .  Verify that both and are stochastic matrices.  Find the eigenvalues of and then find a steady-state vector for .  We will form the Markov chain beginning with the vector and defining . The Sage cell below constructs the first terms of the Markov chain with the command markov_chain(A, x0, N) . Define the matrix A and vector x0 and evaluate the cell to find the first 10 terms of the Markov chain. What do you notice about the Markov chain? Does it converge to the steady-state vector for ?  Now find the eigenvalues of along with a steady-state vector for .  As before, find the first 10 terms in the Markov chain beginning with and . What do you notice about the Markov chain? Does it converge to the steady-state vector for ?  What condition on the eigenvalues of a stochastic matrix will guarantee that a Markov chain will converge to a steady-state vector?      If we add the entries in each column of and each column of , we obtain . Also, all the entries in both matrices are nonnegative.  The matrix has the eigenvalues and with associated eigenvectors and . The steady-state vector is as this is the unique probability vector in .  The terms in the Markov chain are so the chain does not converge to any vector, much less the steady-state vector.  The matrix has eigenvalues and with associated eigenvectors and . The unique steady-state vector is since this is the only probability vector in .  Here we find which appears to be converging to the steady-state vector .  If there is one eigenvalue having multiplicity one with the other eigenvalues satisfying , we can guarantee that any Markov chain will converge to a unique steady-state vector.     As this activity implies, the eigenvalues of a stochastic matrix tell us whether a Markov chain will converge to a steady-state vector. Here are a few important facts about the eigenvalues of a stochastic matrix.  As is demonstrated in , is an eigenvalue of any stochastic matrix. We usually order the eigenvalues so it is the first eigenvalue meaning that .   All other eigenvalues satisfy the property that .  Any stochastic matrix has at least one steady-state vector .   As illustrated in the activity, a Markov chain could fail to converge to a steady-state vector if . This happens for the matrix , whose eigenvalues are and .  However, if all but the first eigenvalue satisfy , then there is a unique steady-state vector and any Markov chain will converge to . This was the case for the matrix , whose eigenvalues are and . In this case, any Markov chain will converge to the unique steady-state vector .  In this way, we see that the eigenvalues of a stochastic matrix tell us whether a Markov chain will converge to a steady-state vector. However, it is somewhat inconvenient to compute the eigenvalues to answer this question. Is there some way to conclude that every Markov chain will converge to a steady-state vector without actually computing the eigenvalues? It turns out that there is a simple condition on the matrix that guarantees this.   positive matrix  We say that a matrix is positive if either or some power has all positive entries.     The matrix is not positive. We can see this because some of the entries of are zero and therefore not positive. In addition, we see that , and so forth. Therefore, every power of also has some zero entries, which means that is not positive.  The matrix is positive because every entry of is positive.  Also, the matrix clearly has a zero entry. However, , which has all positive entries. Therefore, we see that is a positive matrix.    Positive matrices are important because of the following theorem.   Perron-Frobenius   If is a positive stochastic matrix, then the eigenvalues satisfy and for . This means that has a unique positive, steady-state vector and that every Markov chain defined by will converge to .      We will explore the meaning of the Perron-Frobenius theorem in this activity.  Consider the matrix . This is a positive matrix, as we saw in the previous example. Find the eigenvectors of and verify there is a unique steady-state vector.  Using the Sage cell below, construct the Markov chain with initial vector and describe what happens to as becomes large.   Construct another Markov chain with initial vector and describe what happens to as becomes large.  Consider the matrix and compute several powers of below. Determine whether is a positive matrix.  Find the eigenvalues of and then find the steady-state vectors. Is there a unique steady-state vector?  What happens to the Markov chain defined by with initial vector ? What happens to the Markov chain with initial vector .  Explain how the matrices and , which we have considered in this activity, relate to the Perron-Frobenius theorem.      We find that has eigenvalues and with eigenvectors and . Therefore, the unique steady-state vector is for this is the only probability vector in the eigenspace .  We see that the Markov chain converges to the steady-state vector as the Perron-Frobenius theorem tells us to expect.  Another Markov chain converges to the unique steady-state vector as the Perron-Frobenius theorem tells us to expect.  The matrix is not positive because the first two entries in the bottom row of any power are zero.  The eigenvalues are , which has multiplicity two, and . The eigenspace is two-dimensional and spanned by the probability vectors and . Both of these vectors are steady-state vectors so there is not a unique steady-state vector.  If , then the Markov chain converges to . If , then the Markov chain converges to .  Because is a positive matrix, the Perron-Frobenius theorem tells us that there is a unique steady-state vector to which any Markov chain will converge. Because is not a positive matrix, the Perron-Frobenius theorem does not tell us anything, and, indeed, we see that there is not a unique steady-state vector and different Markov chains can converge to different vectors.       Google's PageRank algorithm  Markov chains and the Perron-Frobenius theorem are the central ingredients in Google's PageRank algorithm, developed by Google to assess the quality of web pages.  Suppose we enter linear algebra into Google's search engine. Google responds by telling us there are 138 million web pages containing those terms. On the first page, however, there are links to ten web pages that Google judges to have the highest quality and to be the ones we are most likely to be interested in. How does Google assess the quality of web pages?  At the time this is being written, Google is tracking 35 trillion web pages. Clearly, this is too many for humans to evaluate. Plus, human evaluators may inject their own biases into their evaluations, perhaps even unintentionally. Google's idea is to use the structure of the Internet to assess the quality of web pages without any human intervention. For instance, if a web page has quality content, other web pages will link to it. This means that the number of links to a page reflect the quality of that page. In addition, we would expect a page to have even higher quality content if those links are coming from pages that are themselves assessed to have high quality. Simply said, if many quality pages link to a page, that page must itself be of high quality. This is the essence of the PageRank algorithm, which we introduce in the next activity.     We will consider a simple model of the Internet that has three pages and links between them as shown here. For instance, page 1 links to both pages 2 and 3, but page 2 only links to page 1.    Our first Internet.    We will measure the quality of the page with a number , which is called the PageRank of page . The PageRank is determined by the following rule: each page divides its PageRank into equal pieces, one for each outgoing link, and gives one piece to each of the pages it links to. A page's PageRank is the sum of all the PageRank it receives from pages linking to it.  For instance, page 3 has two outgoing links. It therefore divides its PageRank in half and gives half to page 1. Page 2 has only one outgoing link so it gives all of its PageRank to page 1. We therefore have .    Find similar expressions for and .  We now form the PageRank vector . Find a matrix such that the expressions for , , and can be written in the form . The matrix is called the Google matrix .  Explain why is a stochastic matrix.  Since is defined by the equation , any vector in the eigenspace satisfies this equation. So that we might work with a specific vector, we will define the PageRank vector to be the steady-state vector of the stochastic matrix . Find this steady state vector.   The PageRank vector is composed of the PageRanks for each of the three pages. Which page of the three is assessed to have the highest quality? By referring to the structure of this small model of the Internet, explain why this is a good choice.  If we begin with the initial vector and form the Markov chain , what does the Perron-Frobenius theorem tell us about the long-term behavior of the Markov chain?  Verify that this Markov chain converges to the steady-state PageRank vector.        Notice that page receives half of the PageRank from pages and . This means that .  Also, page receives half of the PageRank from page and none from page so we have .  This means that   From the equations, we see that .  All of the entries of are nonnegative and the columns each add to . This tells us that is stochastic.  We see that is an eigenvalue and that the eigenspace is spanned by . The unique steady-state vector is then .  Page is assessed to have the highest quality because its PageRank is the largest. This makes sense as a reflection of the structure of this Internet. Since Page only links to Page and not Page , it is not as useful as Page . Furthermore, Page only has one incoming link so it is not viewed by Page as being useful.  Since all the entries of are positive, we can conclude that is a positive matrix. The Perron-Frobenius theorem tells us there is a unique steady-state vector, which we have already seen, and that any Markov chain will converge to it.  We verify that the Markov chain converges to the unique steady-state vector .     This activity shows us two ways to find the PageRank vector. In the first, we determine a steady-state vector directly by finding a description of the eigenspace and then finding the appropriate scalar multiple of a basis vector that gives us the steady-state vector. To find a description of the eigenspace , however, we need to find the null space . Remember that the real Internet has 35 trillion pages so finding requires us to row reduce a matrix with 35 trillion rows and columns. As we saw in , that is not computationally feasible.  As suggested by the activity, the second way to find the PageRank vector is to use a Markov chain that converges to the PageRank vector. Since multiplying a vector by a matrix is significantly less work than row reducing the matrix, this approach is computationally feasible, and it is, in fact, how Google computes the PageRank vector.    Consider the Internet with eight web pages, shown in .      A simple model of the Internet with eight web pages.     Construct the Google matrix for this Internet. Then use a Markov chain to find the steady-state PageRank vector .   What does this vector tell us about the relative quality of the pages in this Internet? Which page has the highest quality and which the lowest?  Now consider the Internet with five pages, shown in .      A model of the Internet with five web pages.   What happens when you begin the Markov chain with the vector ? Explain why this behavior is consistent with the Perron-Frobenius theorem.  What do you think the PageRank vector for this Internet should be? Is any one page of a higher quality than another?  Now consider the Internet with eight web pages, shown in .      Another model of the Internet with eight web pages.   Notice that this version of the Internet is identical to the first one that we saw in this activity, except that a single link from page 7 to page 1 has been removed. We can therefore find its Google matrix by slightly modifying the earlier matrix.  What is the long-term behavior of a Markov chain defined by and why is this behavior not desirable? How is this behavior consistent with the Perron-Frobenius theorem?       After creating a Markov chain to find the steady-state vector, we find   This shows us that page is judged to be the most important and page the least important. The pages , , and , are the most important. This makes sense because there are a lot of links between these pages without many links going out from these pages to the others.  We have the matrix which leads to the Markov chain This Markov chain does not converge to a steady-state vector.  As all of the pages seem equally important, we should expect the PageRank vector to be   After generating some terms of a Markov chain, we see that These are not the components of a steady-state vector because some of the entries are zero. The Google matrix cannot be a positive matrix in this example. This is not desirable because we lose any information about the importance of the first four pages. All the PageRank drains out of the left side into the pages on the right side.     The Perron-Frobenius theorem tells us that a Markov chain converges to a unique steady-state vector when the matrix is positive. This means that or some power of should have only positive entries. Clearly, this is not the case for the matrix formed from the Internet in .  We can understand the problem with the Internet shown in by adding a box around some of the pages as shown in . Here we see that the pages outside of the box give up all of their PageRank to the pages inside the box. This is not desirable because the PageRanks of the pages outside of the box are found to be zero. Once again, the Google matrix is not a positive matrix.      The pages outside the box give up all of their PageRank to the pages inside the box.   Google solves this problem by slightly modifying the Google matrix to obtain a positive matrix . To understand this, think of the entries in the Google matrix as giving the probability that an Internet user follows a link from one page of another. To create a positive matrix, we will allow that user to randomly jump to any other page on the Internet with a small probability.  To make sense of this, suppose that there are pages on our internet. The matrix is a positive stochastic matrix describing a process where we can move from any page to another with equal probability. To form the modified Google matrix , we choose a parameter that is used to mix and together; that is, is the positive stochastic matrix . In practice, it is thought that Google uses a value of (Google doesn't publish this number as it is a trade secret) so that we have . Intuitively, this means that an Internet user will randomly follow a link from one page to another 85% of the time and will randomly jump to any other page on the Internet 15% of the time. Since the matrix is positive, the Perron-Frobenius theorem tells us that any Markov chain will converge to a unique steady-state vector that we call the PageRank vector.    The following Sage cell will generate the Markov chain for the modified Google matrix if you simply enter the original Google matrix in the appropriate line.   Consider the original Internet with three pages shown in and find the PageRank vector using the modified Google matrix in the Sage cell above. How does this modified PageRank vector compare to the vector we found using the original Google matrix ?  Find the modified PageRank vector for the Internet shown in . Explain why this vector seems to be the correct one.  Find the modified PageRank vector for the Internet shown in . Explain why this modified PageRank vector fixes the problem that appeared with the original PageRank vector.      The modified Google matrix has the steady-state vector , which compares to , the steady-state vector of the original Google matrix. These vectors are quite close so it appears that the modification does not change the PageRank significantly.  We see that the Markov chain generated by the modified Google matrix converges to the steady-state vector , which seems correct as all five pages should have the same PageRank.  We find that Now the entries are all positive so we can make assessments about the relative quality of all the pages. Due to the presence of the matrix in the modified Google matrix, the PageRank is allowed to flow back from the four pages on the right to the four pages on the left.     The ability to access almost anything we want to know through the Internet is something we take for granted in today's society. Without Google's PageRank algorithm, however, the Internet would be a chaotic place indeed; imagine trying to find a useful web page among the 30 trillion available pages without it. (There are, of course, other search algorithms, but Google's is the most widely used.) The fundamental role that Markov chains and the Perron-Frobenius theorem play in Google's algorithm demonstrates the vast power that mathematics has to shape our society.    Summary  This section explored stochastic matrices and Markov chains.  A probability vector is one whose entries are nonnegative and whose columns add to 1. A stochastic matrix is a square matrix whose columns are probability vectors.  A Markov chain is formed from a stochastic matrix and an initial probability vector using the rule . We may think of the sequence as describing the evolution of some conserved quantity, such as the number of rental cars or voters, among a number of possible states over time.  A steady-state vector for a stochastic matrix is a probability vector that satisfies .  The Perron-Frobenius theorem tells us that, if is a positive stochastic matrix, then every Markov chain defined by converges to a unique, positive steady-state vector.  Google's PageRank algorithm uses Markov chains and the Perron-Frobenius theorem to assess the relative quality of web pages on the Internet.      Consider the following stochastic matrices.   For each, make a copy of the diagram and label each edge to indicate the probability of that transition. Then find all the steady-state vectors and describe what happens to a Markov chain defined by that matrix.       .   .   .   .      Any Markov chain will converge to .  Any Markov chain will converge to .  The steady-state vectors are those for which for some satisfying .  Any Markov chain will converge to .     The eigenvalues are and so there is a unique steady-state vector to which any Markov chain will converge.  The eigenvalues are and so there is a unique steady-state vector to which any Markov chain will converge.  Every two-dimensional vector is an eigenvector with eigenvalue so that for every . This shows that any Markov chain will remain constant. The steady-state vectors are those for which for some satisfying .  The eigenvalues are and so there is a unique steady-state vector to which any Markov chain will converge.     Every year, people move between urban (U), suburban (S), and rural (R) populations with the probabilities given in .      The flow between urban, suburban, and rural populations.     Construct the stochastic matrix describing the movement of people.  Explain what the Perron-Frobenius theorem tells us about the existence of a steady-state vector and the behavior of a Markov chain.  Use the Sage cell below to find the some terms of a Markov chain.   Describe the long-term distribution of people among urban, suburban, and rural populations.       .  There is a unique steady-state vector and any Markov chain will converge to .  Any Markov chain we create will converge to .  Eventually, about % of the population will be rural, % will be suburban, and % will be urban.     The diagram tells us that giving the stochastic matrix .  Since all the entries of are positive, we know that is a positive matrix. Therefore, the Perron-Frobenius theorem tells us that there is a unique steady-state vector and that any Markov chain will converge to .  Any Markov chain we create will converge to .  Eventually, about % of the population will be rural, % will be suburban, and % will be urban.     Determine whether the following statements are true or false and provide a justification of your response.  Every stochastic matrix has a steady-state vector.  If is a stochastic matrix, then any Markov chain defined by converges to a steady-state vector.  If is a stochastic matrix, then is an eigenvalue and all the other eigenvalues satisfy .  A positive stochastic matrix has a unique steady-state vector.  If is an invertible stochastic matrix, then so is .      True  False  False  True  False     True. Every stochastic matrix has an eigenvalue , and we can find a steady-state vector inside the eigenspace .  False. We have seen examples, such as , for which this is not true. We need to know that is a positive matrix so that the Perron-Frobenius theorem applies if we want to reach this conclusion.   False. Again, we can only guarantee this if the matrix is positive so that the Perron-Frobenius theorem applies.  True. This follows from the Perron-Frobenius theorem.  False. If is an invertible stochastic matrix, the eigenvalues of must satisfy . The eigenvalues of are , which satisfy so is most likely not stochastic.     Consider the stochastic matrix .  Find the eigenvalues of .   Do the conditions of the Perron-Frobenius theorem apply to this matrix?  Find the steady-state vectors of .  What can we guarantee about the long-term behavior of a Markov chain defined by the matrix ?     The eigenvalues are , , and with associated eigenvectors , , and .  No     Any Markov chain will converge to .     The eigenvalues are , , and with associated eigenvectors , , and .  The conditions of the Perron-Frobenius theorem do not apply because the matrix is not positive. The first column of any power of will be .  Since has multiplicity , we know that is one-dimensional. There is, therefore, a unique steady-state vector .  Any Markov chain will converge to because the eigenvalues and are less than .      Explain your responses to the following.  Why does Google use a Markov chain to compute the PageRank vector?  Describe two problems that can happen when Google constructs a Markov chain using the Google matrix .  Describe how these problems are consistent with the Perron-Frobenius theorem.  Describe why the Perron-Frobenius theorem suggests creating a Markov chain using the modified Google matrix .     It is not computationally feasible.  A Markov chain does not converge or it may converge to a probability vector having some zero entries.  In both of these situations, the Google matrix is not positive.   is guaranteed to be positive.     The modified Google matrix is huge, roughly 30 trillion 30 trillion, so it is not computationally feasible to compute the steady-state vector by finding a basis for .  It may happen that a Markov chain does not converge. We saw this with the cyclic web where a Markov chain just moved PageRank from one page to the next around a loop. Also, it may happen that a Markov chain converges to a probability vector with some zero entries. This happens when PageRank drains out of one part of the web.  In both of these situations, the Google matrix is not positive so the Perron-Frobenius theorem does not apply.  When we create in this way, we are guaranteed of having a positive stochastic matrix because all the entries of are positive.    In the next few exercises, we will consider the matrix .   Suppose that is a stochastic matrix and that is a probability vector. We would like to explain why the product is a probability vector.  Explain why is a probability vector and then find the product .  More generally, if is any probability vector, what is the product ?  If is a stochastic matrix, explain why .  Explain why is a probability vector by considering the product .      .  If is a probability vector, .  The entries in are the sums of the columns of , which are all .       Since the components of are nonnegative and add to , is a probability vector. We see that .  Multiplying any vector by just adds the components of . Therefore, if is a probability vector, .  Because the columns of add to , we see that multiplying any column of gives . Consequently, the entries in are the sums of the columns of , which are all . This gives .  Because all the entries in both and are nonnegative, it follows that the components of are nonnegative. Then we have , which shows that the components of add to . Therefore, is a probability vector.     Using the results of the previous exercise, we would like to explain why is a stochastic matrix if is stochastic.  Suppose that and are stochastic matrices. Explain why the product is a stochastic matrix by considering the product .  Explain why is a stochastic matrix.  How do the steady-state vectors of compare to the steady-state vectors of ?       This follows from the previous part.  A steady-state vector for is a steady-state vector for but a steady-state vector for is not necessarily a steady-state vector for .     If both and are stochastic, all the entries of both and are nonnegative. Therefore, the entries of are also nonnegative. In addition, we have , which shows that is stochastic.  This follows from the previous part, which shows that the product of two stochastic matrices is stochastic.  If is a steady-state vector for , it will be a steady-state vector for as well because .  However, it is possible that there are steady-state vectors of that are not steady-state vectors of . If the eigenvalues of are , remember that the eigenvalues of are . Therefore, can have more steady-state vectors if is an eigenvalue of , which leads to . For instance, has a unique steady-state vector whereas every probability vector is a steady-state vector for .     This exercise explains why is an eigenvalue of a stochastic matrix . To conclude that is an eigenvalue, we need to know that is not invertible.  What is the product ?  What is the product ?  Consider the equation . Explain why this equation cannot be consistent by multiplying by to obtain .  What can you say about the span of the columns of ?  Explain why we can conclude that is not invertible and that is an eigenvalue of .         .   while .  The span of the columns of is not .  The span of the columns of is not , we know that is not invertible.     Notice that both and are stochastic. Therefore, .   .  We have This says that the original equation is not consistent.  The span of the columns of is not .  Because the span of the columns of is not , has a row without a pivot position, which says that is not invertible. Therefore, , which says that is an eigenvalue of .     We saw a couple of model Internets in which a Markov chain defined by the Google matrix did not converge to an appropriate PageRank vector. For this reason, Google defines the matrix , where is the number of web pages, and constructs a Markov chain from the modified Google matrix . Since is positive, the Markov chain is guaranteed to converge to a unique steady-state vector.  We said that Google chooses so we might wonder why this is a good choice. We will explore the role of in this exercise. Let's consider the model Internet described in and construct the Google matrix . In the Sage cell below, you can enter the matrix and choose a value for .   Let's begin with . With this choice, what is the matrix ? Construct a Markov chain using the Sage cell above. How many steps are required for the Markov chain to converge to the accuracy with which the vectors are displayed?  Now choose . How many steps are required for the Markov chain to converge to the accuracy at which the vectors are displayed?  Repeat this experiment with and .  What happens if ?   This experiment gives some insight into the choice of . The smaller is, the faster the Markov chain converges. This is important; since the matrix that Google works with is so large, we would like to minimize the number of terms in the Markov chain that we need to compute. On the other hand, as we lower , the matrix begins to resemble more and less. The value is chosen so that the matrix sufficiently resembles while having the Markov chain converge in a reasonable amount of steps.     If , then and we see that any Markov chain converges to the steady-state vector in one term. That is, .  After six terms, we have to three digits.  With , it is not until to three digits. With , it is not until .  With , we have so the Markov chain will never converge.    Remember that the steady-state vector for is .  If , then and we see that any Markov chain converges to in one term. That is, .  After six terms, we have to three digits.  With , it is not until to three digits. With , it is not until .  With , we have so the Markov chain will never converge.     This exercise will analyze the board game Chutes and Ladders , or at least a simplified version of it.   The board for this game consists of 100 squares arranged in a grid and numbered 1 to 100. There are pairs of squares joined by a ladder and pairs joined by a chute. All players begin in square 1 and take turns rolling a die. On their turn, a player will move ahead the number of squares indicated on the die. If they arrive at a square at the bottom of a ladder, they move to the square at the top of the ladder. If they arrive at a square at the top of a chute, they move down to the square at the bottom of the chute. The winner is the first player to reach square 100.      We begin by playing a simpler version of this game with only eight squares laid out in a row as shown in and containing neither chutes nor ladders. Rather than a six-sided die, we will toss a coin and move ahead one or two squares depending on the result of the coin toss. If we are on square 7, we move ahead to square 8 regardless of the coin flip, and if we are on square 8, we will stay there forever.      A simple version of Chutes and Ladders with neither chutes nor ladders.   Construct the matrix that records the probability that a player moves from one square to another on one move. For instance, if a player is on square 2, there is a 50% chance they move to square 3 and a 50% chance they move to square 4 on the next move.  Since we begin the game on square 1, the initial vector . Generate a few terms of the Markov chain .   What is the probability that we arrive at square 8 by the fourth move? By the sixth move? By the seventh move?   We will now modify the game by adding one chute and one ladder as shown in .      A version of Chutes and Ladders with one chute and one ladder.   Even though there are eight squares, we only need to consider six of them. For instance, if we arrive at the first white square, we move up to square 4. Similarly, if we arrive at the second white square, we move down to square 1.  Once again, construct the stochastic matrix that records the probability that we move from one square to another on a given turn and generate some terms in the Markov chain that begins with .     What is the smallest number of moves we can make and arrive at square 6? What is the probability that we arrive at square 6 using this number of moves?  What is the probability that we arrive at square 6 after five moves?  What is the probability that we are still on square 1 after five moves? After seven moves? After nine moves?  After how many moves do we have a 90% chance of having arrived at square 6?  Find the steady-state vector and discuss what this vector implies about the game.       One can analyze the full version of Chutes and Ladders having 100 squares in the same way. Without any chutes or ladders, one finds that the average number of moves required to reach square 100 is 29.0. Once we add the chutes and ladders back in, the average number of moves required to reach square 100 is 27.1. This shows that the average number of moves does not change significantly when we add the chutes and ladders. There is, however, much more variation in the possibilities because it is possible to reach square 100 much more quickly and much more slowly.    The chances are % that we arrive at square after four moves and % that we arrive after five moves. We are guaranteed to arrive after seven moves.  We find that  We can first arrive at square after three moves. The chances of doing so are %.  After five moves, there is a % chance that we have arrived at square .  The chances of being on square after five moves is %. After seven moves, it is still %. After nine moves, however, it is %.  After 23 moves, there is a % chance that we have arrived at square .          We have the matrix Starting a Markov chain with , we find that This says that the chances are % that we arrive at square after four moves and % that we arrive after five moves. We are guaranteed to arrive after seven moves.  Now we have the matrix which leads to the Markov chain with terms   We can first arrive at square after three moves. The chances of doing so are %.  After five moves, there is a % chance that we have arrived at square .  The chances of being on square after five moves is %. After seven moves, it is still %. After nine moves, however, it is %.  After 23 moves, there is a % chance that we have arrived at square .  The steady-state vector is , which means that we will eventually arrive at square .       "
},
{
  "id": "sec-stochastic-2-3",
  "level": "2",
  "url": "sec-stochastic.html#sec-stochastic-2-3",
  "type": "Preview Activity",
  "number": "20.5.1",
  "title": "",
  "body": "  Suppose that our rental car company rents from two locations and . We find that 80% of the cars rented from location are returned to while the other 20% are returned to . For cars rented from location , 60% are returned to and 40% to .  We will use and to denote the number of cars at the two locations on day . The following day, the number of cars at equals 80% of and 40% of . This shows that   If we use the vector to represent the distribution of cars on day , find a matrix such that .  Find the eigenvalues and associated eigenvectors of .  Suppose that there are initially 1500 cars, all of which are at location . Write the vector as a linear combination of eigenvectors of .  Write the vectors as a linear combination of eigenvectors of .  What happens to the distribution of cars after a long time?      Expressing the set of equations in matrix form, we see that .  We have eigenvalues and with associated eigenvectors and .  We find that .  Multiplying by the matrix has the effect of multiplying the eigenvectors by their associated eigenvalues. Therefore, .  As becomes large, becomes very close to zero. Therefore . This tells us that cars are at location and are at .    "
},
{
  "id": "sec-stochastic-3-4",
  "level": "2",
  "url": "sec-stochastic.html#sec-stochastic-3-4",
  "type": "Definition",
  "number": "20.5.1",
  "title": "",
  "body": " probability vector  stochastic matrix  A vector whose entries are nonnegative and add to 1 is called a probability vector . A square matrix whose columns are probability vectors is called a stochastic matrix.  "
},
{
  "id": "sec-stochastic-3-5",
  "level": "2",
  "url": "sec-stochastic.html#sec-stochastic-3-5",
  "type": "Activity",
  "number": "20.5.2",
  "title": "",
  "body": "  Suppose you live in a country with three political parties , , and . We use , , and to denote the percentage of voters voting for that party in election .   Voters will change parties from one election to the next as shown in the figure. We see that 60% of voters stay with the same party. However, 40% of those who vote for party will vote for party in the next election.      Write expressions for , , and in terms of , , and .  If we write , find the matrix such that .  Explain why is a stochastic matrix.  Suppose that initially 40% of citizens vote for party , 30% vote for party , and 30% vote for party . Form the vector and explain why is a probability vector.  Find , the percentages who vote for the three parties in the next election. Verify that is also a probability vector and explain why will be a probability vector for every .   Find the eigenvalues of the matrix and explain why the eigenspace is a one-dimensional subspace of . Then verify that is a basis vector for .  As every vector in is a scalar multiple of , find a probability vector in and explain why it is the only probability vector in .  Describe what happens to after a very long time.      The solutions to this activity are given in the following text.   "
},
{
  "id": "sec-stochastic-4-4",
  "level": "2",
  "url": "sec-stochastic.html#sec-stochastic-4-4",
  "type": "Definition",
  "number": "20.5.2",
  "title": "",
  "body": " steady-state vector  stationary vector  If is a stochastic matrix, we say that a probability vector is a steady-state or stationary vector if .  "
},
{
  "id": "sec-stochastic-4-6",
  "level": "2",
  "url": "sec-stochastic.html#sec-stochastic-4-6",
  "type": "Question",
  "number": "20.5.3",
  "title": "",
  "body": " If is a stochastic matrix and a Markov chain, does converge to a steady-state vector?  "
},
{
  "id": "sec-stochastic-4-7",
  "level": "2",
  "url": "sec-stochastic.html#sec-stochastic-4-7",
  "type": "Activity",
  "number": "20.5.3",
  "title": "",
  "body": "  Consider the matrices .  Verify that both and are stochastic matrices.  Find the eigenvalues of and then find a steady-state vector for .  We will form the Markov chain beginning with the vector and defining . The Sage cell below constructs the first terms of the Markov chain with the command markov_chain(A, x0, N) . Define the matrix A and vector x0 and evaluate the cell to find the first 10 terms of the Markov chain. What do you notice about the Markov chain? Does it converge to the steady-state vector for ?  Now find the eigenvalues of along with a steady-state vector for .  As before, find the first 10 terms in the Markov chain beginning with and . What do you notice about the Markov chain? Does it converge to the steady-state vector for ?  What condition on the eigenvalues of a stochastic matrix will guarantee that a Markov chain will converge to a steady-state vector?      If we add the entries in each column of and each column of , we obtain . Also, all the entries in both matrices are nonnegative.  The matrix has the eigenvalues and with associated eigenvectors and . The steady-state vector is as this is the unique probability vector in .  The terms in the Markov chain are so the chain does not converge to any vector, much less the steady-state vector.  The matrix has eigenvalues and with associated eigenvectors and . The unique steady-state vector is since this is the only probability vector in .  Here we find which appears to be converging to the steady-state vector .  If there is one eigenvalue having multiplicity one with the other eigenvalues satisfying , we can guarantee that any Markov chain will converge to a unique steady-state vector.    "
},
{
  "id": "sec-stochastic-4-12",
  "level": "2",
  "url": "sec-stochastic.html#sec-stochastic-4-12",
  "type": "Definition",
  "number": "20.5.4",
  "title": "",
  "body": " positive matrix  We say that a matrix is positive if either or some power has all positive entries.  "
},
{
  "id": "sec-stochastic-4-13",
  "level": "2",
  "url": "sec-stochastic.html#sec-stochastic-4-13",
  "type": "Example",
  "number": "20.5.5",
  "title": "",
  "body": "  The matrix is not positive. We can see this because some of the entries of are zero and therefore not positive. In addition, we see that , and so forth. Therefore, every power of also has some zero entries, which means that is not positive.  The matrix is positive because every entry of is positive.  Also, the matrix clearly has a zero entry. However, , which has all positive entries. Therefore, we see that is a positive matrix.   "
},
{
  "id": "theorem-perron",
  "level": "2",
  "url": "sec-stochastic.html#theorem-perron",
  "type": "Theorem",
  "number": "20.5.6",
  "title": "Perron-Frobenius.",
  "body": " Perron-Frobenius   If is a positive stochastic matrix, then the eigenvalues satisfy and for . This means that has a unique positive, steady-state vector and that every Markov chain defined by will converge to .   "
},
{
  "id": "sec-stochastic-4-16",
  "level": "2",
  "url": "sec-stochastic.html#sec-stochastic-4-16",
  "type": "Activity",
  "number": "20.5.4",
  "title": "",
  "body": "  We will explore the meaning of the Perron-Frobenius theorem in this activity.  Consider the matrix . This is a positive matrix, as we saw in the previous example. Find the eigenvectors of and verify there is a unique steady-state vector.  Using the Sage cell below, construct the Markov chain with initial vector and describe what happens to as becomes large.   Construct another Markov chain with initial vector and describe what happens to as becomes large.  Consider the matrix and compute several powers of below. Determine whether is a positive matrix.  Find the eigenvalues of and then find the steady-state vectors. Is there a unique steady-state vector?  What happens to the Markov chain defined by with initial vector ? What happens to the Markov chain with initial vector .  Explain how the matrices and , which we have considered in this activity, relate to the Perron-Frobenius theorem.      We find that has eigenvalues and with eigenvectors and . Therefore, the unique steady-state vector is for this is the only probability vector in the eigenspace .  We see that the Markov chain converges to the steady-state vector as the Perron-Frobenius theorem tells us to expect.  Another Markov chain converges to the unique steady-state vector as the Perron-Frobenius theorem tells us to expect.  The matrix is not positive because the first two entries in the bottom row of any power are zero.  The eigenvalues are , which has multiplicity two, and . The eigenspace is two-dimensional and spanned by the probability vectors and . Both of these vectors are steady-state vectors so there is not a unique steady-state vector.  If , then the Markov chain converges to . If , then the Markov chain converges to .  Because is a positive matrix, the Perron-Frobenius theorem tells us that there is a unique steady-state vector to which any Markov chain will converge. Because is not a positive matrix, the Perron-Frobenius theorem does not tell us anything, and, indeed, we see that there is not a unique steady-state vector and different Markov chains can converge to different vectors.    "
},
{
  "id": "subsec-google-5",
  "level": "2",
  "url": "sec-stochastic.html#subsec-google-5",
  "type": "Activity",
  "number": "20.5.5",
  "title": "",
  "body": "   We will consider a simple model of the Internet that has three pages and links between them as shown here. For instance, page 1 links to both pages 2 and 3, but page 2 only links to page 1.    Our first Internet.    We will measure the quality of the page with a number , which is called the PageRank of page . The PageRank is determined by the following rule: each page divides its PageRank into equal pieces, one for each outgoing link, and gives one piece to each of the pages it links to. A page's PageRank is the sum of all the PageRank it receives from pages linking to it.  For instance, page 3 has two outgoing links. It therefore divides its PageRank in half and gives half to page 1. Page 2 has only one outgoing link so it gives all of its PageRank to page 1. We therefore have .    Find similar expressions for and .  We now form the PageRank vector . Find a matrix such that the expressions for , , and can be written in the form . The matrix is called the Google matrix .  Explain why is a stochastic matrix.  Since is defined by the equation , any vector in the eigenspace satisfies this equation. So that we might work with a specific vector, we will define the PageRank vector to be the steady-state vector of the stochastic matrix . Find this steady state vector.   The PageRank vector is composed of the PageRanks for each of the three pages. Which page of the three is assessed to have the highest quality? By referring to the structure of this small model of the Internet, explain why this is a good choice.  If we begin with the initial vector and form the Markov chain , what does the Perron-Frobenius theorem tell us about the long-term behavior of the Markov chain?  Verify that this Markov chain converges to the steady-state PageRank vector.        Notice that page receives half of the PageRank from pages and . This means that .  Also, page receives half of the PageRank from page and none from page so we have .  This means that   From the equations, we see that .  All of the entries of are nonnegative and the columns each add to . This tells us that is stochastic.  We see that is an eigenvalue and that the eigenspace is spanned by . The unique steady-state vector is then .  Page is assessed to have the highest quality because its PageRank is the largest. This makes sense as a reflection of the structure of this Internet. Since Page only links to Page and not Page , it is not as useful as Page . Furthermore, Page only has one incoming link so it is not viewed by Page as being useful.  Since all the entries of are positive, we can conclude that is a positive matrix. The Perron-Frobenius theorem tells us there is a unique steady-state vector, which we have already seen, and that any Markov chain will converge to it.  We verify that the Markov chain converges to the unique steady-state vector .    "
},
{
  "id": "subsec-google-8",
  "level": "2",
  "url": "sec-stochastic.html#subsec-google-8",
  "type": "Activity",
  "number": "20.5.6",
  "title": "",
  "body": "  Consider the Internet with eight web pages, shown in .      A simple model of the Internet with eight web pages.     Construct the Google matrix for this Internet. Then use a Markov chain to find the steady-state PageRank vector .   What does this vector tell us about the relative quality of the pages in this Internet? Which page has the highest quality and which the lowest?  Now consider the Internet with five pages, shown in .      A model of the Internet with five web pages.   What happens when you begin the Markov chain with the vector ? Explain why this behavior is consistent with the Perron-Frobenius theorem.  What do you think the PageRank vector for this Internet should be? Is any one page of a higher quality than another?  Now consider the Internet with eight web pages, shown in .      Another model of the Internet with eight web pages.   Notice that this version of the Internet is identical to the first one that we saw in this activity, except that a single link from page 7 to page 1 has been removed. We can therefore find its Google matrix by slightly modifying the earlier matrix.  What is the long-term behavior of a Markov chain defined by and why is this behavior not desirable? How is this behavior consistent with the Perron-Frobenius theorem?       After creating a Markov chain to find the steady-state vector, we find   This shows us that page is judged to be the most important and page the least important. The pages , , and , are the most important. This makes sense because there are a lot of links between these pages without many links going out from these pages to the others.  We have the matrix which leads to the Markov chain This Markov chain does not converge to a steady-state vector.  As all of the pages seem equally important, we should expect the PageRank vector to be   After generating some terms of a Markov chain, we see that These are not the components of a steady-state vector because some of the entries are zero. The Google matrix cannot be a positive matrix in this example. This is not desirable because we lose any information about the importance of the first four pages. All the PageRank drains out of the left side into the pages on the right side.    "
},
{
  "id": "fig-google-reducible-box",
  "level": "2",
  "url": "sec-stochastic.html#fig-google-reducible-box",
  "type": "Figure",
  "number": "20.5.11",
  "title": "",
  "body": "    The pages outside the box give up all of their PageRank to the pages inside the box.  "
},
{
  "id": "subsec-google-14",
  "level": "2",
  "url": "sec-stochastic.html#subsec-google-14",
  "type": "Activity",
  "number": "20.5.7",
  "title": "",
  "body": "  The following Sage cell will generate the Markov chain for the modified Google matrix if you simply enter the original Google matrix in the appropriate line.   Consider the original Internet with three pages shown in and find the PageRank vector using the modified Google matrix in the Sage cell above. How does this modified PageRank vector compare to the vector we found using the original Google matrix ?  Find the modified PageRank vector for the Internet shown in . Explain why this vector seems to be the correct one.  Find the modified PageRank vector for the Internet shown in . Explain why this modified PageRank vector fixes the problem that appeared with the original PageRank vector.      The modified Google matrix has the steady-state vector , which compares to , the steady-state vector of the original Google matrix. These vectors are quite close so it appears that the modification does not change the PageRank significantly.  We see that the Markov chain generated by the modified Google matrix converges to the steady-state vector , which seems correct as all five pages should have the same PageRank.  We find that Now the entries are all positive so we can make assessments about the relative quality of all the pages. Due to the presence of the matrix in the modified Google matrix, the PageRank is allowed to flow back from the four pages on the right to the four pages on the left.    "
},
{
  "id": "sec-stochastic-7-1",
  "level": "2",
  "url": "sec-stochastic.html#sec-stochastic-7-1",
  "type": "Exercise",
  "number": "20.5.5.1",
  "title": "",
  "body": " Consider the following stochastic matrices.   For each, make a copy of the diagram and label each edge to indicate the probability of that transition. Then find all the steady-state vectors and describe what happens to a Markov chain defined by that matrix.       .   .   .   .      Any Markov chain will converge to .  Any Markov chain will converge to .  The steady-state vectors are those for which for some satisfying .  Any Markov chain will converge to .     The eigenvalues are and so there is a unique steady-state vector to which any Markov chain will converge.  The eigenvalues are and so there is a unique steady-state vector to which any Markov chain will converge.  Every two-dimensional vector is an eigenvector with eigenvalue so that for every . This shows that any Markov chain will remain constant. The steady-state vectors are those for which for some satisfying .  The eigenvalues are and so there is a unique steady-state vector to which any Markov chain will converge.   "
},
{
  "id": "sec-stochastic-7-2",
  "level": "2",
  "url": "sec-stochastic.html#sec-stochastic-7-2",
  "type": "Exercise",
  "number": "20.5.5.2",
  "title": "",
  "body": " Every year, people move between urban (U), suburban (S), and rural (R) populations with the probabilities given in .      The flow between urban, suburban, and rural populations.     Construct the stochastic matrix describing the movement of people.  Explain what the Perron-Frobenius theorem tells us about the existence of a steady-state vector and the behavior of a Markov chain.  Use the Sage cell below to find the some terms of a Markov chain.   Describe the long-term distribution of people among urban, suburban, and rural populations.       .  There is a unique steady-state vector and any Markov chain will converge to .  Any Markov chain we create will converge to .  Eventually, about % of the population will be rural, % will be suburban, and % will be urban.     The diagram tells us that giving the stochastic matrix .  Since all the entries of are positive, we know that is a positive matrix. Therefore, the Perron-Frobenius theorem tells us that there is a unique steady-state vector and that any Markov chain will converge to .  Any Markov chain we create will converge to .  Eventually, about % of the population will be rural, % will be suburban, and % will be urban.   "
},
{
  "id": "sec-stochastic-7-3",
  "level": "2",
  "url": "sec-stochastic.html#sec-stochastic-7-3",
  "type": "Exercise",
  "number": "20.5.5.3",
  "title": "",
  "body": " Determine whether the following statements are true or false and provide a justification of your response.  Every stochastic matrix has a steady-state vector.  If is a stochastic matrix, then any Markov chain defined by converges to a steady-state vector.  If is a stochastic matrix, then is an eigenvalue and all the other eigenvalues satisfy .  A positive stochastic matrix has a unique steady-state vector.  If is an invertible stochastic matrix, then so is .      True  False  False  True  False     True. Every stochastic matrix has an eigenvalue , and we can find a steady-state vector inside the eigenspace .  False. We have seen examples, such as , for which this is not true. We need to know that is a positive matrix so that the Perron-Frobenius theorem applies if we want to reach this conclusion.   False. Again, we can only guarantee this if the matrix is positive so that the Perron-Frobenius theorem applies.  True. This follows from the Perron-Frobenius theorem.  False. If is an invertible stochastic matrix, the eigenvalues of must satisfy . The eigenvalues of are , which satisfy so is most likely not stochastic.   "
},
{
  "id": "sec-stochastic-7-4",
  "level": "2",
  "url": "sec-stochastic.html#sec-stochastic-7-4",
  "type": "Exercise",
  "number": "20.5.5.4",
  "title": "",
  "body": " Consider the stochastic matrix .  Find the eigenvalues of .   Do the conditions of the Perron-Frobenius theorem apply to this matrix?  Find the steady-state vectors of .  What can we guarantee about the long-term behavior of a Markov chain defined by the matrix ?     The eigenvalues are , , and with associated eigenvectors , , and .  No     Any Markov chain will converge to .     The eigenvalues are , , and with associated eigenvectors , , and .  The conditions of the Perron-Frobenius theorem do not apply because the matrix is not positive. The first column of any power of will be .  Since has multiplicity , we know that is one-dimensional. There is, therefore, a unique steady-state vector .  Any Markov chain will converge to because the eigenvalues and are less than .   "
},
{
  "id": "sec-stochastic-7-5",
  "level": "2",
  "url": "sec-stochastic.html#sec-stochastic-7-5",
  "type": "Exercise",
  "number": "20.5.5.5",
  "title": "",
  "body": " Explain your responses to the following.  Why does Google use a Markov chain to compute the PageRank vector?  Describe two problems that can happen when Google constructs a Markov chain using the Google matrix .  Describe how these problems are consistent with the Perron-Frobenius theorem.  Describe why the Perron-Frobenius theorem suggests creating a Markov chain using the modified Google matrix .     It is not computationally feasible.  A Markov chain does not converge or it may converge to a probability vector having some zero entries.  In both of these situations, the Google matrix is not positive.   is guaranteed to be positive.     The modified Google matrix is huge, roughly 30 trillion 30 trillion, so it is not computationally feasible to compute the steady-state vector by finding a basis for .  It may happen that a Markov chain does not converge. We saw this with the cyclic web where a Markov chain just moved PageRank from one page to the next around a loop. Also, it may happen that a Markov chain converges to a probability vector with some zero entries. This happens when PageRank drains out of one part of the web.  In both of these situations, the Google matrix is not positive so the Perron-Frobenius theorem does not apply.  When we create in this way, we are guaranteed of having a positive stochastic matrix because all the entries of are positive.   "
},
{
  "id": "exercise-stochastic-probability",
  "level": "2",
  "url": "sec-stochastic.html#exercise-stochastic-probability",
  "type": "Exercise",
  "number": "20.5.5.6",
  "title": "",
  "body": " Suppose that is a stochastic matrix and that is a probability vector. We would like to explain why the product is a probability vector.  Explain why is a probability vector and then find the product .  More generally, if is any probability vector, what is the product ?  If is a stochastic matrix, explain why .  Explain why is a probability vector by considering the product .      .  If is a probability vector, .  The entries in are the sums of the columns of , which are all .       Since the components of are nonnegative and add to , is a probability vector. We see that .  Multiplying any vector by just adds the components of . Therefore, if is a probability vector, .  Because the columns of add to , we see that multiplying any column of gives . Consequently, the entries in are the sums of the columns of , which are all . This gives .  Because all the entries in both and are nonnegative, it follows that the components of are nonnegative. Then we have , which shows that the components of add to . Therefore, is a probability vector.   "
},
{
  "id": "sec-stochastic-7-8",
  "level": "2",
  "url": "sec-stochastic.html#sec-stochastic-7-8",
  "type": "Exercise",
  "number": "20.5.5.7",
  "title": "",
  "body": " Using the results of the previous exercise, we would like to explain why is a stochastic matrix if is stochastic.  Suppose that and are stochastic matrices. Explain why the product is a stochastic matrix by considering the product .  Explain why is a stochastic matrix.  How do the steady-state vectors of compare to the steady-state vectors of ?       This follows from the previous part.  A steady-state vector for is a steady-state vector for but a steady-state vector for is not necessarily a steady-state vector for .     If both and are stochastic, all the entries of both and are nonnegative. Therefore, the entries of are also nonnegative. In addition, we have , which shows that is stochastic.  This follows from the previous part, which shows that the product of two stochastic matrices is stochastic.  If is a steady-state vector for , it will be a steady-state vector for as well because .  However, it is possible that there are steady-state vectors of that are not steady-state vectors of . If the eigenvalues of are , remember that the eigenvalues of are . Therefore, can have more steady-state vectors if is an eigenvalue of , which leads to . For instance, has a unique steady-state vector whereas every probability vector is a steady-state vector for .   "
},
{
  "id": "exercise-stochastic-eigenvalue",
  "level": "2",
  "url": "sec-stochastic.html#exercise-stochastic-eigenvalue",
  "type": "Exercise",
  "number": "20.5.5.8",
  "title": "",
  "body": " This exercise explains why is an eigenvalue of a stochastic matrix . To conclude that is an eigenvalue, we need to know that is not invertible.  What is the product ?  What is the product ?  Consider the equation . Explain why this equation cannot be consistent by multiplying by to obtain .  What can you say about the span of the columns of ?  Explain why we can conclude that is not invertible and that is an eigenvalue of .         .   while .  The span of the columns of is not .  The span of the columns of is not , we know that is not invertible.     Notice that both and are stochastic. Therefore, .   .  We have This says that the original equation is not consistent.  The span of the columns of is not .  Because the span of the columns of is not , has a row without a pivot position, which says that is not invertible. Therefore, , which says that is an eigenvalue of .   "
},
{
  "id": "sec-stochastic-7-10",
  "level": "2",
  "url": "sec-stochastic.html#sec-stochastic-7-10",
  "type": "Exercise",
  "number": "20.5.5.9",
  "title": "",
  "body": " We saw a couple of model Internets in which a Markov chain defined by the Google matrix did not converge to an appropriate PageRank vector. For this reason, Google defines the matrix , where is the number of web pages, and constructs a Markov chain from the modified Google matrix . Since is positive, the Markov chain is guaranteed to converge to a unique steady-state vector.  We said that Google chooses so we might wonder why this is a good choice. We will explore the role of in this exercise. Let's consider the model Internet described in and construct the Google matrix . In the Sage cell below, you can enter the matrix and choose a value for .   Let's begin with . With this choice, what is the matrix ? Construct a Markov chain using the Sage cell above. How many steps are required for the Markov chain to converge to the accuracy with which the vectors are displayed?  Now choose . How many steps are required for the Markov chain to converge to the accuracy at which the vectors are displayed?  Repeat this experiment with and .  What happens if ?   This experiment gives some insight into the choice of . The smaller is, the faster the Markov chain converges. This is important; since the matrix that Google works with is so large, we would like to minimize the number of terms in the Markov chain that we need to compute. On the other hand, as we lower , the matrix begins to resemble more and less. The value is chosen so that the matrix sufficiently resembles while having the Markov chain converge in a reasonable amount of steps.     If , then and we see that any Markov chain converges to the steady-state vector in one term. That is, .  After six terms, we have to three digits.  With , it is not until to three digits. With , it is not until .  With , we have so the Markov chain will never converge.    Remember that the steady-state vector for is .  If , then and we see that any Markov chain converges to in one term. That is, .  After six terms, we have to three digits.  With , it is not until to three digits. With , it is not until .  With , we have so the Markov chain will never converge.   "
},
{
  "id": "sec-stochastic-7-11",
  "level": "2",
  "url": "sec-stochastic.html#sec-stochastic-7-11",
  "type": "Exercise",
  "number": "20.5.5.10",
  "title": "",
  "body": " This exercise will analyze the board game Chutes and Ladders , or at least a simplified version of it.   The board for this game consists of 100 squares arranged in a grid and numbered 1 to 100. There are pairs of squares joined by a ladder and pairs joined by a chute. All players begin in square 1 and take turns rolling a die. On their turn, a player will move ahead the number of squares indicated on the die. If they arrive at a square at the bottom of a ladder, they move to the square at the top of the ladder. If they arrive at a square at the top of a chute, they move down to the square at the bottom of the chute. The winner is the first player to reach square 100.      We begin by playing a simpler version of this game with only eight squares laid out in a row as shown in and containing neither chutes nor ladders. Rather than a six-sided die, we will toss a coin and move ahead one or two squares depending on the result of the coin toss. If we are on square 7, we move ahead to square 8 regardless of the coin flip, and if we are on square 8, we will stay there forever.      A simple version of Chutes and Ladders with neither chutes nor ladders.   Construct the matrix that records the probability that a player moves from one square to another on one move. For instance, if a player is on square 2, there is a 50% chance they move to square 3 and a 50% chance they move to square 4 on the next move.  Since we begin the game on square 1, the initial vector . Generate a few terms of the Markov chain .   What is the probability that we arrive at square 8 by the fourth move? By the sixth move? By the seventh move?   We will now modify the game by adding one chute and one ladder as shown in .      A version of Chutes and Ladders with one chute and one ladder.   Even though there are eight squares, we only need to consider six of them. For instance, if we arrive at the first white square, we move up to square 4. Similarly, if we arrive at the second white square, we move down to square 1.  Once again, construct the stochastic matrix that records the probability that we move from one square to another on a given turn and generate some terms in the Markov chain that begins with .     What is the smallest number of moves we can make and arrive at square 6? What is the probability that we arrive at square 6 using this number of moves?  What is the probability that we arrive at square 6 after five moves?  What is the probability that we are still on square 1 after five moves? After seven moves? After nine moves?  After how many moves do we have a 90% chance of having arrived at square 6?  Find the steady-state vector and discuss what this vector implies about the game.       One can analyze the full version of Chutes and Ladders having 100 squares in the same way. Without any chutes or ladders, one finds that the average number of moves required to reach square 100 is 29.0. Once we add the chutes and ladders back in, the average number of moves required to reach square 100 is 27.1. This shows that the average number of moves does not change significantly when we add the chutes and ladders. There is, however, much more variation in the possibilities because it is possible to reach square 100 much more quickly and much more slowly.    The chances are % that we arrive at square after four moves and % that we arrive after five moves. We are guaranteed to arrive after seven moves.  We find that  We can first arrive at square after three moves. The chances of doing so are %.  After five moves, there is a % chance that we have arrived at square .  The chances of being on square after five moves is %. After seven moves, it is still %. After nine moves, however, it is %.  After 23 moves, there is a % chance that we have arrived at square .          We have the matrix Starting a Markov chain with , we find that This says that the chances are % that we arrive at square after four moves and % that we arrive after five moves. We are guaranteed to arrive after seven moves.  Now we have the matrix which leads to the Markov chain with terms   We can first arrive at square after three moves. The chances of doing so are %.  After five moves, there is a % chance that we have arrived at square .  The chances of being on square after five moves is %. After seven moves, it is still %. After nine moves, however, it is %.  After 23 moves, there is a % chance that we have arrived at square .  The steady-state vector is , which means that we will eventually arrive at square .     "
},
{
  "id": "sec-gaussian-revisited",
  "level": "1",
  "url": "sec-gaussian-revisited.html",
  "type": "Section",
  "number": "21.1",
  "title": "Gaussian elimination revisited",
  "body": " Gaussian elimination revisited   In this section, we revisit Gaussian elimination and explore some problems with implementing it in the straightforward way that we described back in . In particular, we will see how the fact that computers only approximate arithmetic operations can lead us to find solutions that are far from the actual solutions. Second, we will explore how much work is required to implement Gaussian elimination and devise a more efficient means of implementing it when we want to solve equations for several different vectors .    To begin, let's recall how we implemented Gaussian elimination by considering the matrix   What is the first row operation we perform? If the resulting matrix is , find a matrix such that .  What is the matrix inverse ? You can find this using your favorite technique for finding a matrix inverse. However, it may be easier to think about the effect that the row operation has and how it can be undone.  Perform the next two steps in the Gaussian elimination algorithm to obtain . Represent these steps using multiplication by matrices and so that .  Suppose we need to scale the second row by . What is the matrix that perfoms this row operation by left multiplication?  Suppose that we need to interchange the first and second rows. What is the matrix that performs this row operation by left multiplication?      We first multiply the first row by and add to the second row. This can be represented by multiplying where   To undo the row operation, we multiply the first row by and add to the second row. This shows that .  We have   The matrix performing this scaling is .  The matrix performing this interchange is .       Partial pivoting  The first issue that we address is the fact that computers do not perform arithemtic operations exactly. For instance, Python will evaluate 0.1 + 0.2 and report 0.30000000000000004 even though we know that the true value is 0.3. There are a couple of reasons for this.  First, computers perform arithmetic using base 2 numbers, which means that numbers we enter in decimal form, such as , must be converted to base 2. Even though 0.1 has a simple decimal form, its representation in base 2 is the repeating decimal , To accurately represent this number inside a computer would require infinitely many digits. Since a computer can only hold a finite number of digits, we are necessarily using an approximation just by representing this number in a computer.  In addition, arithmetic operations, such as addition, are prone to error. To keep things simple, suppose we have a computer that represents numbers using only three decimal digits. For instance, the number 1.023 would be represented as 1.02 while 0.023421 would be 0.0234 . If we add these numbers, we have 1.023 + 0.023421 = 1.046421; the computer reports this sum as 1.02 + 0.0234 = 1.04 , whose last digit is not correctly rounded. Generally speaking, we will see this problem, which is called round off error , whenever we add numbers of signficantly different magnitudes. round off error   Remember that Gaussian elimination, when applied to an matrix, requires approximately operations. If we have a matrix, performing Gaussian elimination requires roughly a billion operations, and the errors introduced in each operation could accumulate. How can we have confidence in the final result? We can never completely avoid these errors, but we can take steps to mitigate them. The next activity will introduce one such technique.    Suppose we have a hypothetical computer that represents numbers using only three decimal digits. We will consider the linear system   Show that this system has the unique solution   If we represent this solution inside our computer that only holds 3 decimal digits, what do we find for the solution? This is the best that we can hope to find using our computer.  Let's imagine that we use our computer to find the solution using Gaussian elimination; that is, after every arithmetic operation, we keep only three decimal digits. Our first step is to multiply the first equation by 10000 and subtract it from the second equation. If we represent numbers using only three decimal digits, what does this give for the value of ?  By substituting our value for into the first equation, what do we find for ?  Compare the solution we find on our computer with the actual solution and assess the quality of the approximation.  Let's now modify the linear system by simplying interchanging the equations: Of course, this doesn't change the actual solution. Let's imagine we use our computer to find the solution using Gaussian elimination. Perform the first step where we multiply the first equation by 0.0001 and subtract from the second equation. What does this give for if we represent numbers using only three decimal digits?  Substitute the value you found for into the first equation and solve for . Then compare the approximate solution found with our hypothetical computer to the exact solution.  Which approach produces the most accurate approximation?      We may find this result by forming the augmented matrix and row reducing.  The solution would be rounded to and .  We first multiply the first row by and add to the second row. This gives This tells us that .  The next steps in the Gaussian elimination algorithm give us so have the approximation solution and .  This compares to the actual solution, as represented in our computer, as and . Notice that the value of differs considerably.  Now we have which gives .  Performing one more row operation gives us which shows the approximate solution as and .  The second approach gives an approximate solution that is as accurate as possible given the computer's limited ability to store digits.     This activity demonstrates how the practical aspects of computing differ from the theoretical. We know that the order in which we write the equations has no effect on the solution space; row interchange is one of our three allowed row operations in the Gaussian elimination algorithm. However, when we are only able to perform arithmetic operations approximately, applying row interchanges can dramatically improve the accuracy of our approximations.  If we could compute the solution exactly, we find Since our hypothetical computer represents numbers using only three decimal digits, our computer finds This is the best we can hope to do with our computer since it is impossible to represent the solution exactly.  When the equations are written in their original order and we multiply the first equation by 10000 and subtract from the second, we find   In fact, we find the same value for when we interchange the equations. Here we multiply the first equation by 0.0001 and subtract from the second equation. We then find   The difference occurs when we substitute into the first equation. When the equations are written in their original order, we have When the equations are written in their original order, we find the solution .  When we write the equation in the opposite order, however, substituting into the first equation gives In this case, we find the approximate solution , which is the most accurate solution that our hypothetical computer can find. Simply interchanging the order of the equation produces a much more accurate solution.   We can understand why this works graphically. Each equation represents a line in the plane, and the solution is the intersection point. Notice that the slopes of these lines differ considerably.    When the equations are written in their original order, we substitute into the equation , which is a nearly horizontal line. Along this line, a small change in leads to a large change in . The slight difference in our approximation from the exact value leads to a large difference in the approximation from the exact value .  If we exchange the order in which the equations are written, we substitute our approximation into the equation . Notice that the slope of the associated line is . On this line, a small change in leads to a relatively small change in as well. Therefore, the difference in our approximation from the exact value leads to only a small difference in the approximation from the exact value.  This example motivates the technique that computers usually use to perform Gaussian elimation. We only need to perform a row interchange when a zero occurs in a pivot position, such as . However, we will perform a row interchange to put the entry having the largest possible absolute value into the pivot position. For instance, when performing Gaussian elimination on the following matrix, we begin by interchanging the first and third rows so that the upper left entry has the largest possible absolute value.  partial pivoting This technique is called partial pivoting , and it means that, in practice, we will perform many more row interchange operations than we typically do when computing exactly by hand.     factorizations  In , we saw that the number of arithmetic operations needed to perform Gaussian elimination on an matrix is about . This means that a matrix, requires about two thirds of a billion operations.  Suppose that we have two equations, and , that we would like to solve. Usually, we would form augmented matrices and and apply Gaussian elimination. Of course, the steps we perform in these two computations are nearly identical. Is there a way to store some of the computation we perform in reducing and reuse it in solving subsequent equations? The next activity will point us in the right direction.    We will consider the matrix and begin performing Gaussian elimination without using partial pivoting.   Perform two row replacement operations to find the row equivalent matrix Find elementary matrices and that perform these two operations so that .  Perform a third row replacement to find the upper triangular matrix Find the elementary matrix such that .  We can write . Find the inverse matrices , , and and the product . Then verify that .   Suppose that we want to solve the equation . We will write and introduce an unknown vector such that . Find by noting that and solving this equation.  Now that we have found , find by solving .  Using the factorization and this two-step process, solve the equation .      We have   The third row replacement is performed by   The inverse matrices are found to be giving so that .  Noting that , we find .  To find , we solve the equation to obtain .  We solve to find and to find .     This activity introduces a method for factoring a matrix as a product of two triangular matrices, , where is lower triangular and is upper triangular. The key to finding this factorization is to represent the row operations that we apply in the Gaussian elimination algorithm through multiplication by elementary matrices.   Suppose we have the equation which we write in the form . We begin by applying the Gaussian elimination algorithm to find an factorization of .  The first step is to multiply the first row of by and add it to the second row. The elementary matrix performs this operation so that .  We next apply matrices to obtain the upper triangular matrix .  We can write , which tells us that That is, we have Notice that the matrix is lower triangular, a result of the fact that the elementary matrices , , and are lower triangular.  Now that we have factored into two triangular matrices, we can solve the equation by solving two triangular systems. We write and define the unknown vector , which is determined by the equation . Because is lower triangular, we find the solution using forward substitution, . Finally, we find , the solution to our original system , by applying back substitution to solve . This gives .  If we want to solve for a different right-hand side , we can simply repeat this two-step process.   An factorization allow us to trade in one equation for two simpler equations For instance, the equation in our example has the form Because is a lower-triangular matrix, we can read off the first component of directly from the equations: . We then have , which gives , and , which gives . Solving a triangular system is simplified because we only need to perform a sequence of substitutions.  In fact, solving an equation with an triangular matrix requires approximately operations. Once we have the factorization , we solve the equation by solving two equations involving triangular matrices, which requires about operations. For example, if is a matrix, we solve the equation using about one million steps. The compares with roughly a billion operations needed to perform Gaussian elimination, which represents a significant savings. Of course, we have to first find the factorization of and this requires roughly the same amount of work as performing Gaussian elimination. However, once we have the factorization, we can use it to solve for different right hand sides .  Our discussion so far has ignored one issue, however. Remember that we sometimes have to perform row interchange operations in addition to row replacement. A typical row interchange is represented by multiplication by a matrix such as which has the effect of interchanging the first and third rows. Notice that this matrix is not triangular so performing a row interchange will disrupt the structure of the factorization we seek. Without giving the details, we simply note that linear algebra software packages provide a matrix that describes how the rows are permuted in the Gaussian elimination process. In particular, we will write , where is a permutation matrix, is lower triangular, and is upper triangular.  Therefore, to solve the equation , we first multiply both sides by to obtain . That is, we multiply by and then find using the factorization: and .    Sage will create factorizations; once we have a matrix A , we write P, L, U = A.LU() to obtain the matrices , , and such that .   In , we found the factorization Using Sage, define the matrix , and then ask Sage for the factorization. What are the matrices , , and ?  Notice that Sage finds a different factorization than we found in the previous example because Sage uses partial pivoting, as described in the previous section, when it performs Gaussian elimination.   Define the vector in Sage and compute .  Use the matrices L and U to solve and . You should find the same solution that we found in the previous example.  Use the factorization to solve the equation .  How does the factorization show us that is invertible and that, therefore, every equation has a unique solution?  Suppose that we have the matrix . Use Sage to find the factorization. Explain how the factorization shows that is not invertible.  Consider the matrix and find its factorization. Explain why and have the same null space and use this observation to find a basis for .      Sage gives us the matrices   We find .  Solving , we have . Finally, solving , we obtain .  We find   Because , , and are invertible, it follows that , which means that is invertible.  Sage tells us that Because is not invertible, we see that so is not invertible.  We have We may rewrite as so if , then because and are invertible.  Notice that is upper triangular, which means that it is straightforward to find its reduced row echelon form: This shows that a basis for is .       Summary  We returned to Gaussian elimination, which we have used as a primary tool for finding solutions to linear systems, and explored its practicality, both in terms of numerical accuracy and computational effort.  We saw that the accuracy of computations implemented on a computer could be improved using partial pivoting , a technique that performs row interchanges so that the entry in a pivot position has the largest possible magnitude.  Beginning with a matrix , we used the Gaussian elimination algorithm to write , where is a permutation matrix, is lower triangular, and is upper triangular.  Finding this factorization involves roughly as much work as performing Gaussian elimination. However, once we have the factorization, we are able to quickly solve equations of the form by first solving and then .      In this section, we saw that errors made in computer arithmetic can produce approximate solutions that are far from the exact solutions. Here is another example in which this can happen. Consider the matrix   Find the exact solution to the equation .  Suppose that this linear system arises in the midst of a larger computation except that, due to some error in the computation of the right hand side of the equation, our computer thinks we want to solve . Find the solution to this equation and compare it to the solution of the equation in the previous part of this exercise.    Notice how a small change in the right hand side of the equation leads to a large change in the solution. In this case, we say that the matrix is ill-conditioned because the solutions are extremely sensitive to small changes in the right hand side of the equation. Though we will not do so here, it is possible to create a measure of the matrix that tells us when a matrix is ill-conditioned. Regrettably, there is not much we can do to remedy this problem.     .   .     We find the solution .  Here we find the solution .     In this section, we found the factorization of the matrix in one of the activities, without using partial pivoting. Apply a sequence of row operations, now using partial pivoting, to find an upper triangular matrix that is row equivalent to .       Using partial pivoting, we find This agrees with the result that Sage returns using the LU command.    In the following exercises, use the given factorizations to solve the equations .  Solve the equation   Solve the equation               We solve to find and to find .  We solve to find and to find .     Use Sage to solve the following equation by finding an factorization: .        We obtain the decomposition To solve the equation , we first find , then solve to find , and finally solve to find .    Here is another problem with approximate computer arithmetic that we will encounter in the next section. Consider the matrix   Notice that this is a positive stochastic matrix. What do we know about the eigenvalues of this matrix?  Use Sage to define the matrix using decimals such as 0.2 and the identity matrix . Ask Sage to compute and find the reduced row echelon form of .   Why is the computation that Sage performed incorrect?  Explain why using a computer to find the eigenvectors of a matrix by finding a basis for is problematic.      is an eigenvalue.   .  The computation tells us that is invertible, which is not true.  The approximate arithmetic creates a nonexistent third pivot position.     Since is a positive stochastic matrix, we know that is an eigenvalue. Therefore, should not be invertible.  Sage tells us that the reduced row echelon form of is .  The computation tells us that is invertible. We know that this can't be the case, however, since is an eigenvalue.  Let's trace through the work that goes into finding the reduced row echelon form of : The next step is to multiply the second row by and add it to the third row. The last entry in the third row would be , which we identify as being . The problem is that the computer only approximates this result so it finds something close to but not quite equal to . It therefore, thinks this is a pivot position and scales the row to have in that position.     In practice, one rarely finds the inverse of a matrix . It requires considerable effort to compute, and we can solve any equation of the form using an factorization, which means that the inverse isn't necessary. In any case, the best way to compute an inverse is using an factorization, as this exercise demonstrates.  Suppose that . Explain why .  Since and are triangular, finding their inverses is relatively efficient. That makes this an effective means of finding .   Consider the matrix . Find the factorization of and use it to find .      .    Notice that a row interchange is undone by the same row interchange. Therefore, the permutation matrix is its own inverse so that . If , it then follows that so that .  We find that which gives      Consider the matrix   Find the factorization of .  What conditions on , , , and guarantee that is invertible?     We find    , , , and .     We begin with where Next we have where Finally, we have , an upper triangular matrix where This gives   Notice that is invertible if and only if is invertible and is invertible if and only all the diagonal terms are not equal. This means that is invertible provided that , , , and .     In the factorization of a matrix, the diagonal entries of are all while the diagonal entries of are not necessarily . This exercise will explore that observation by considering the matrix .  Perform Gaussian elimination without partial pivoting to find , an upper triangular matrix that is row equivalent to .  The diagonal entries of are called pivots . Explain why equals the product of the pivots.  What is for our matrix ?  More generally, if we have , explain why equals plus or minus the product of the pivots.      .   .      .     We have This leads to where   Because the diagonal entries of are , we have . Therefore, , which equals the product of the pivots.     More generally, if we have , we can rewrite as so that , which is plus or minus the product of the pivots.     Please provide a justification to your responses to these questions.  In this section, our hypothetical computer could only store numbers using 3 decimal places. Most computers can store numbers using 15 or more decimal places. Why do we still need to be concerned about the accuracy of our computations when solving systems of linear equations?  Finding the factorization of a matrix is roughly the same amount of work as finding its reduced row echelon form. Why is the factorization useful then?  How can we detect whether a matrix is invertible from its factorization?     When working with a large matrix, the errors from individual calculations may accumulate.  We can solve for a couple of different without performing Gaussian elimination again.   is invertible if and only if is.     Since a typical computer has 15 or more decimal places of accuracy, the error in each individual calculation is relatively small. However, if we are working with a large matrix, we are performing many calculations and the errors from each of them may accumulate to become significant.  If we need to solve for a couple of different vectors , we do not have to perform Gaussian elimination again. Some other quantities, like the inverse and determinant of , are more easily computed using an factorization.  The matrix is invertible if and only if is, and we can easily see if is invertible by noting if every row has a pivot.     Consider the matrix   Find the factorization of .  Use the factorization to find a basis for .  We have seen that . Is it true that ?     We see that    .  No     We see that   Because , we see that with a basis vector .  It is not true that . Since there is a nonzero vector such that , we know that is not invertible and hence its column space is not . However, is invertible so we have .     "
},
{
  "id": "sec-gaussian-revisited-2-2",
  "level": "2",
  "url": "sec-gaussian-revisited.html#sec-gaussian-revisited-2-2",
  "type": "Preview Activity",
  "number": "21.1.1",
  "title": "",
  "body": "  To begin, let's recall how we implemented Gaussian elimination by considering the matrix   What is the first row operation we perform? If the resulting matrix is , find a matrix such that .  What is the matrix inverse ? You can find this using your favorite technique for finding a matrix inverse. However, it may be easier to think about the effect that the row operation has and how it can be undone.  Perform the next two steps in the Gaussian elimination algorithm to obtain . Represent these steps using multiplication by matrices and so that .  Suppose we need to scale the second row by . What is the matrix that perfoms this row operation by left multiplication?  Suppose that we need to interchange the first and second rows. What is the matrix that performs this row operation by left multiplication?      We first multiply the first row by and add to the second row. This can be represented by multiplying where   To undo the row operation, we multiply the first row by and add to the second row. This shows that .  We have   The matrix performing this scaling is .  The matrix performing this interchange is .    "
},
{
  "id": "subsec-partial-pivot-6",
  "level": "2",
  "url": "sec-gaussian-revisited.html#subsec-partial-pivot-6",
  "type": "Activity",
  "number": "21.1.2",
  "title": "",
  "body": "  Suppose we have a hypothetical computer that represents numbers using only three decimal digits. We will consider the linear system   Show that this system has the unique solution   If we represent this solution inside our computer that only holds 3 decimal digits, what do we find for the solution? This is the best that we can hope to find using our computer.  Let's imagine that we use our computer to find the solution using Gaussian elimination; that is, after every arithmetic operation, we keep only three decimal digits. Our first step is to multiply the first equation by 10000 and subtract it from the second equation. If we represent numbers using only three decimal digits, what does this give for the value of ?  By substituting our value for into the first equation, what do we find for ?  Compare the solution we find on our computer with the actual solution and assess the quality of the approximation.  Let's now modify the linear system by simplying interchanging the equations: Of course, this doesn't change the actual solution. Let's imagine we use our computer to find the solution using Gaussian elimination. Perform the first step where we multiply the first equation by 0.0001 and subtract from the second equation. What does this give for if we represent numbers using only three decimal digits?  Substitute the value you found for into the first equation and solve for . Then compare the approximate solution found with our hypothetical computer to the exact solution.  Which approach produces the most accurate approximation?      We may find this result by forming the augmented matrix and row reducing.  The solution would be rounded to and .  We first multiply the first row by and add to the second row. This gives This tells us that .  The next steps in the Gaussian elimination algorithm give us so have the approximation solution and .  This compares to the actual solution, as represented in our computer, as and . Notice that the value of differs considerably.  Now we have which gives .  Performing one more row operation gives us which shows the approximate solution as and .  The second approach gives an approximate solution that is as accurate as possible given the computer's limited ability to store digits.    "
},
{
  "id": "sec-gaussian-revisited-4-4",
  "level": "2",
  "url": "sec-gaussian-revisited.html#sec-gaussian-revisited-4-4",
  "type": "Activity",
  "number": "21.1.3",
  "title": "",
  "body": "  We will consider the matrix and begin performing Gaussian elimination without using partial pivoting.   Perform two row replacement operations to find the row equivalent matrix Find elementary matrices and that perform these two operations so that .  Perform a third row replacement to find the upper triangular matrix Find the elementary matrix such that .  We can write . Find the inverse matrices , , and and the product . Then verify that .   Suppose that we want to solve the equation . We will write and introduce an unknown vector such that . Find by noting that and solving this equation.  Now that we have found , find by solving .  Using the factorization and this two-step process, solve the equation .      We have   The third row replacement is performed by   The inverse matrices are found to be giving so that .  Noting that , we find .  To find , we solve the equation to obtain .  We solve to find and to find .    "
},
{
  "id": "example-LU",
  "level": "2",
  "url": "sec-gaussian-revisited.html#example-LU",
  "type": "Example",
  "number": "21.1.1",
  "title": "",
  "body": " Suppose we have the equation which we write in the form . We begin by applying the Gaussian elimination algorithm to find an factorization of .  The first step is to multiply the first row of by and add it to the second row. The elementary matrix performs this operation so that .  We next apply matrices to obtain the upper triangular matrix .  We can write , which tells us that That is, we have Notice that the matrix is lower triangular, a result of the fact that the elementary matrices , , and are lower triangular.  Now that we have factored into two triangular matrices, we can solve the equation by solving two triangular systems. We write and define the unknown vector , which is determined by the equation . Because is lower triangular, we find the solution using forward substitution, . Finally, we find , the solution to our original system , by applying back substitution to solve . This gives .  If we want to solve for a different right-hand side , we can simply repeat this two-step process.  "
},
{
  "id": "sec-gaussian-revisited-4-11",
  "level": "2",
  "url": "sec-gaussian-revisited.html#sec-gaussian-revisited-4-11",
  "type": "Activity",
  "number": "21.1.4",
  "title": "",
  "body": "  Sage will create factorizations; once we have a matrix A , we write P, L, U = A.LU() to obtain the matrices , , and such that .   In , we found the factorization Using Sage, define the matrix , and then ask Sage for the factorization. What are the matrices , , and ?  Notice that Sage finds a different factorization than we found in the previous example because Sage uses partial pivoting, as described in the previous section, when it performs Gaussian elimination.   Define the vector in Sage and compute .  Use the matrices L and U to solve and . You should find the same solution that we found in the previous example.  Use the factorization to solve the equation .  How does the factorization show us that is invertible and that, therefore, every equation has a unique solution?  Suppose that we have the matrix . Use Sage to find the factorization. Explain how the factorization shows that is not invertible.  Consider the matrix and find its factorization. Explain why and have the same null space and use this observation to find a basis for .      Sage gives us the matrices   We find .  Solving , we have . Finally, solving , we obtain .  We find   Because , , and are invertible, it follows that , which means that is invertible.  Sage tells us that Because is not invertible, we see that so is not invertible.  We have We may rewrite as so if , then because and are invertible.  Notice that is upper triangular, which means that it is straightforward to find its reduced row echelon form: This shows that a basis for is .    "
},
{
  "id": "sec-gaussian-revisited-6-1",
  "level": "2",
  "url": "sec-gaussian-revisited.html#sec-gaussian-revisited-6-1",
  "type": "Exercise",
  "number": "21.1.4.1",
  "title": "",
  "body": " In this section, we saw that errors made in computer arithmetic can produce approximate solutions that are far from the exact solutions. Here is another example in which this can happen. Consider the matrix   Find the exact solution to the equation .  Suppose that this linear system arises in the midst of a larger computation except that, due to some error in the computation of the right hand side of the equation, our computer thinks we want to solve . Find the solution to this equation and compare it to the solution of the equation in the previous part of this exercise.    Notice how a small change in the right hand side of the equation leads to a large change in the solution. In this case, we say that the matrix is ill-conditioned because the solutions are extremely sensitive to small changes in the right hand side of the equation. Though we will not do so here, it is possible to create a measure of the matrix that tells us when a matrix is ill-conditioned. Regrettably, there is not much we can do to remedy this problem.     .   .     We find the solution .  Here we find the solution .   "
},
{
  "id": "sec-gaussian-revisited-6-2",
  "level": "2",
  "url": "sec-gaussian-revisited.html#sec-gaussian-revisited-6-2",
  "type": "Exercise",
  "number": "21.1.4.2",
  "title": "",
  "body": " In this section, we found the factorization of the matrix in one of the activities, without using partial pivoting. Apply a sequence of row operations, now using partial pivoting, to find an upper triangular matrix that is row equivalent to .       Using partial pivoting, we find This agrees with the result that Sage returns using the LU command.  "
},
{
  "id": "sec-gaussian-revisited-6-3",
  "level": "2",
  "url": "sec-gaussian-revisited.html#sec-gaussian-revisited-6-3",
  "type": "Exercise",
  "number": "21.1.4.3",
  "title": "",
  "body": " In the following exercises, use the given factorizations to solve the equations .  Solve the equation   Solve the equation               We solve to find and to find .  We solve to find and to find .   "
},
{
  "id": "sec-gaussian-revisited-6-4",
  "level": "2",
  "url": "sec-gaussian-revisited.html#sec-gaussian-revisited-6-4",
  "type": "Exercise",
  "number": "21.1.4.4",
  "title": "",
  "body": " Use Sage to solve the following equation by finding an factorization: .        We obtain the decomposition To solve the equation , we first find , then solve to find , and finally solve to find .  "
},
{
  "id": "exercise-eigenvector-approx",
  "level": "2",
  "url": "sec-gaussian-revisited.html#exercise-eigenvector-approx",
  "type": "Exercise",
  "number": "21.1.4.5",
  "title": "",
  "body": " Here is another problem with approximate computer arithmetic that we will encounter in the next section. Consider the matrix   Notice that this is a positive stochastic matrix. What do we know about the eigenvalues of this matrix?  Use Sage to define the matrix using decimals such as 0.2 and the identity matrix . Ask Sage to compute and find the reduced row echelon form of .   Why is the computation that Sage performed incorrect?  Explain why using a computer to find the eigenvectors of a matrix by finding a basis for is problematic.      is an eigenvalue.   .  The computation tells us that is invertible, which is not true.  The approximate arithmetic creates a nonexistent third pivot position.     Since is a positive stochastic matrix, we know that is an eigenvalue. Therefore, should not be invertible.  Sage tells us that the reduced row echelon form of is .  The computation tells us that is invertible. We know that this can't be the case, however, since is an eigenvalue.  Let's trace through the work that goes into finding the reduced row echelon form of : The next step is to multiply the second row by and add it to the third row. The last entry in the third row would be , which we identify as being . The problem is that the computer only approximates this result so it finds something close to but not quite equal to . It therefore, thinks this is a pivot position and scales the row to have in that position.   "
},
{
  "id": "sec-gaussian-revisited-6-6",
  "level": "2",
  "url": "sec-gaussian-revisited.html#sec-gaussian-revisited-6-6",
  "type": "Exercise",
  "number": "21.1.4.6",
  "title": "",
  "body": " In practice, one rarely finds the inverse of a matrix . It requires considerable effort to compute, and we can solve any equation of the form using an factorization, which means that the inverse isn't necessary. In any case, the best way to compute an inverse is using an factorization, as this exercise demonstrates.  Suppose that . Explain why .  Since and are triangular, finding their inverses is relatively efficient. That makes this an effective means of finding .   Consider the matrix . Find the factorization of and use it to find .      .    Notice that a row interchange is undone by the same row interchange. Therefore, the permutation matrix is its own inverse so that . If , it then follows that so that .  We find that which gives    "
},
{
  "id": "sec-gaussian-revisited-6-7",
  "level": "2",
  "url": "sec-gaussian-revisited.html#sec-gaussian-revisited-6-7",
  "type": "Exercise",
  "number": "21.1.4.7",
  "title": "",
  "body": " Consider the matrix   Find the factorization of .  What conditions on , , , and guarantee that is invertible?     We find    , , , and .     We begin with where Next we have where Finally, we have , an upper triangular matrix where This gives   Notice that is invertible if and only if is invertible and is invertible if and only all the diagonal terms are not equal. This means that is invertible provided that , , , and .   "
},
{
  "id": "sec-gaussian-revisited-6-8",
  "level": "2",
  "url": "sec-gaussian-revisited.html#sec-gaussian-revisited-6-8",
  "type": "Exercise",
  "number": "21.1.4.8",
  "title": "",
  "body": " In the factorization of a matrix, the diagonal entries of are all while the diagonal entries of are not necessarily . This exercise will explore that observation by considering the matrix .  Perform Gaussian elimination without partial pivoting to find , an upper triangular matrix that is row equivalent to .  The diagonal entries of are called pivots . Explain why equals the product of the pivots.  What is for our matrix ?  More generally, if we have , explain why equals plus or minus the product of the pivots.      .   .      .     We have This leads to where   Because the diagonal entries of are , we have . Therefore, , which equals the product of the pivots.     More generally, if we have , we can rewrite as so that , which is plus or minus the product of the pivots.   "
},
{
  "id": "sec-gaussian-revisited-6-9",
  "level": "2",
  "url": "sec-gaussian-revisited.html#sec-gaussian-revisited-6-9",
  "type": "Exercise",
  "number": "21.1.4.9",
  "title": "",
  "body": " Please provide a justification to your responses to these questions.  In this section, our hypothetical computer could only store numbers using 3 decimal places. Most computers can store numbers using 15 or more decimal places. Why do we still need to be concerned about the accuracy of our computations when solving systems of linear equations?  Finding the factorization of a matrix is roughly the same amount of work as finding its reduced row echelon form. Why is the factorization useful then?  How can we detect whether a matrix is invertible from its factorization?     When working with a large matrix, the errors from individual calculations may accumulate.  We can solve for a couple of different without performing Gaussian elimination again.   is invertible if and only if is.     Since a typical computer has 15 or more decimal places of accuracy, the error in each individual calculation is relatively small. However, if we are working with a large matrix, we are performing many calculations and the errors from each of them may accumulate to become significant.  If we need to solve for a couple of different vectors , we do not have to perform Gaussian elimination again. Some other quantities, like the inverse and determinant of , are more easily computed using an factorization.  The matrix is invertible if and only if is, and we can easily see if is invertible by noting if every row has a pivot.   "
},
{
  "id": "sec-gaussian-revisited-6-10",
  "level": "2",
  "url": "sec-gaussian-revisited.html#sec-gaussian-revisited-6-10",
  "type": "Exercise",
  "number": "21.1.4.10",
  "title": "",
  "body": " Consider the matrix   Find the factorization of .  Use the factorization to find a basis for .  We have seen that . Is it true that ?     We see that    .  No     We see that   Because , we see that with a basis vector .  It is not true that . Since there is a nonzero vector such that , we know that is not invertible and hence its column space is not . However, is invertible so we have .   "
},
{
  "id": "sec-power-method",
  "level": "1",
  "url": "sec-power-method.html",
  "type": "Section",
  "number": "21.2",
  "title": "Finding eigenvectors numerically",
  "body": " Finding eigenvectors numerically   We have typically found eigenvalues of a square matrix as the roots of the characteristic polynomial and the associated eigenvectors as the null space . Unfortunately, this approach is not practical when we are working with large matrices. First, finding the charactertic polynomial of a large matrix requires considerable computation, as does finding the roots of that polynomial. Second, finding the null space of a singular matrix is plagued by numerical problems, as we will see in the preview activity.  For this reason, we will explore a technique called the power method that finds numerical approximations to the eigenvalues and eigenvectors of a square matrix.    Let's recall some earlier observations about eigenvalues and eigenvectors.  How are the eigenvalues and associated eigenvectors of related to those of ?  How are the eigenvalues and associated eigenvectors of related to those of ?  If is an eigenvalue of , what can we say about the pivot positions of ?  Suppose that . Explain how we know that is an eigenvalue of and then explain why the following Sage computation is incorrect.   Suppose that , and we define a sequence ; in other words, . What happens to as grows increasingly large?  Explain how the eigenvalues of are responsible for the behavior noted in the previous question.      If is an eigenvalue of , then for an associated eigenvector . Multiplying by and , we obtain , which shows that is an eigenvalue of .  In the same way, if is an eigenvalue of , then for an associated eigenvector . This means that so that is an eigenvalue of .  If is an eigenvalue of , then is not invertible and so has a row without a pivot position.  Since is a positive stochastic matrix, we know that is an eigenvalue and hence that is not invertible. Sage, however, tells us that , which cannot be true since is not invertible.  The vectors form a Markov chain, which must converge to the steady-state vector .  We have eigenvalues and . If we begin with and successively multiply by , we have . When becomes large, the coefficient of becomes insignificantly small so we are left with an eigenvector in .       The power method  Our goal is to find a technique that produces numerical approximations to the eigenvalues and associated eigenvectors of a matrix . We begin by searching for the eigenvalue having the largest absolute value, which is called the dominant eigenvalue. The next two examples demonstrate this technique. eigenvalue, dominant    Let's begin with the positive stochastic matrix . We spent quite a bit of time studying this type of matrix in ; in particular, we saw that any Markov chain will converge to the unique steady-state vector. Let's rephrase this statement in terms of the eigenvectors of .  This matrix has eigenvalues and so the dominant eigenvalue is . The associated eigenvectors are and . Suppose we begin with the vector and find and so forth. Notice that the powers become increasingly small as grows so that when is large. Therefore, the vectors become increasingly close to a vector in the eigenspace , the eigenspace associated to the dominant eigenvalue. If we did not know the eigenvector , we could use a Markov chain in this way to find a basis vector for , which, as seen in , is essentially how the Google PageRank algorithm works.    Let's now look at the matrix , which has eigenvalues and . The dominant eigenvalue is , and the associated eigenvectors are and . Once again, begin with the vector so that    As the figure shows, the vectors are stretched by a factor of in the direction and not at all in the direction. Consequently, the vectors become increasingly long, but their direction becomes closer to the direction of the eigenvector associated to the dominant eigenvalue.    To find an eigenvector associated to the dominant eigenvalue, we will prevent the length of the vectors from growing arbitrarily large by multiplying by an appropriate scaling constant. Here is one way to do this. Given the vector , we identify its component having the largest absolute value and call it . We then define , which means that the component of having the largest absolute value is .  For example, beginning with , we find . The component of having the largest absolute value is so we multiply by to obtain . Then . Now the component having the largest absolute value is so we multiply by to obtain .   The resulting sequence of vectors is shown in the figure. Notice how the vectors now approach the eigenvector , which gives us a way to find the eigenvector . This is the power method for finding an eigenvector associated to the dominant eigenvalue of a matrix. power method        Let's begin by considering the matrix and the initial vector .   Compute the vector .  Find , the component of that has the largest absolute value. Then form . Notice that the component having the largest absolute value of is .  Find the vector . Identify the component of having the largest absolute value. Then form to obtain a vector in which the component with the largest absolute value is .  The Sage cell below defines a function that implements the power method. Define the matrix and initial vector below. The command power(A, x0, N) will print out the multiplier and the vectors for steps of the power method.   How does this computation identify an eigenvector of the matrix ?  What is the corresponding eigenvalue of this eigenvector?  How do the values of the multipliers tell us the eigenvalue associated to the eigenvector we have found?  Consider now the matrix . Use the power method to find the dominant eigenvalue of and an associated eigenvector.      We find .  The first component has the largest absolute value so . Therefore, .  In the same way, we obtain . We see that so we have .  We see that the vectors are getting closer and closer to , which we therefore identify as an eigenvector associated to the dominant eigenvalue.  We see that . Therefore, the dominant eigenvalue is .  More generally, we see that the multiplier will converge to the dominant eigenvalue.  The power method constructs a sequence of vectors converging to an eigenvector . The multipliers converge to , the dominant eigenvalue.     Notice that the power method gives us not only an eigenvector but also its associated eigenvalue. As in the activity, consider the matrix , which has eigenvector . The first component has the largest absolute value so we multiply by to obtain . When we multiply by , we have . Notice that the first component still has the largest absolute value so that the multiplier is the eigenvalue corresponding to the eigenvector. This demonstrates the fact that the multipliers approach the eigenvalue having the largest absolute value.  Notice that the power method requires us to choose an initial vector . For most choices, this method will find the eigenvalue having the largest absolute value. However, an unfortunate choice of may not. For instance, if we had chosen in our example above, the vectors in the sequence will not detect the eigenvector . However, it usually happens that our initial guess has some contribution from that enables us to find it.  The power method, as presented here, will fail for certain unlucky matrices. This is examined in along with a means to improve the power method to work for all matrices.    Finding other eigenvalues  The power method gives a technique for finding the dominant eigenvalue of a matrix. We can modify the method to find the other eigenvalues as well.    The key to finding the eigenvalue of having the smallest absolute value is to note that the eigenvectors of are the same as those of .  If is an eigenvector of with associated eigenvector , explain why is an eigenvector of with associated eigenvalue .  Explain why the eigenvalue of having the smallest absolute value is the reciprocal of the dominant eigenvalue of .  Explain how to use the power method applied to to find the eigenvalue of having the smallest absolute value.  If we apply the power method to , we begin with an intial vector and generate the sequence . It is not computationally efficient to compute , however, so instead we solve the equation . Explain why an factorization of is useful for implementing the power method applied to .  The following Sage cell defines a command called inverse_power that applies the power method to . That is, inverse_power(A, x0, N) prints the vectors , where , and multipliers , which approximate the eigenvalue of . Use it to find the eigenvalue of having the smallest absolute value.   The inverse power method only works if is invertible. If is not invertible, what is its eigenvalue having the smallest absolute value?  Use the power method and the inverse power method to find the eigenvalues and associated eigenvectors of the matrix .      If is an eigenvalue of , then for an associated eigenvector . Multiplying by and , we obtain , which shows that is an eigenvalue of .  If , then . Therefore, the reciprocal of the smallest eigenvalue of is the dominant eigenvalue of .  If we apply the power method to the matrix , we will find the dominant eigenvalue and an associated eigenvector of . We know, however, that will be the eigenvalue of having the smallest absolute value and will be an associated eigenvector.  We would like to solve equations of the form for many different vectors . Using an factorization allows us to recycle for subsequent equations the effort we expend performing Gaussian elimination to solve the first equation.  We obtain the eigenvector and associated eigenvalue .  If is not invertible, then is the eigenvalue having the smallest absolute value.  We find the dominant eigenvalue to be with associated eigenvector . The smallest eigenvalue is with associated eigenvector .     With the power method and the inverse power method, we can now find the eigenvalues of a matrix having the largest and smallest absolute values. With one more modification, we can find all the eigenvalues of .    Remember that the absolute value of a number tells us how far that number is from on the real number line. We may therefore think of the inverse power method as telling us the eigenvalue closest to .  If is an eigenvector of with associated eigenvalue , explain why is an eigenvector of where is some scalar.  What is the eigenvalue of associated to the eigenvector ?  Explain why the eigenvalue of closest to is the eigenvalue of closest to .  Explain why applying the inverse power method to gives the eigenvalue of closest to .  Consider the matrix . If we use the power method and inverse power method, we find two eigenvalues, and . Viewing these eigenvalues on a number line, we know that the other eigenvalues lie in the range between and , as shaded in .      The range of eigenvalues of .   The Sage cell below has a function find_closest_eigenvalue(A, s, x, N) that implements steps of the inverse power method using the matrix and an initial vector . This function prints approximations to the eigenvalue of closest to and its associated eigenvector. By trying different values of in the shaded regions of the number line shown in , find the other two eigenvalues of .   Write a list of the four eigenvalues of in increasing order.      If , then , which shows that is also an eigenvector of .  From the previous part, we see that the associated eigenvalue is .  If is the eigenvalue of closest to , then is an eigenvalue of that must be the closest to .  The inverse power method applied to tells us the eigenvalue of having the smallest absolute value and an associated eigenvector . Therefore, is the eigenvalue of closest to and is an associated eigenvector.  We begin by trying to find the closest eigenvalue to, say, . The power method tells us that this eigenvalue is . If we then try to find the eigenvalue closest to , we find the fourth eigenvalue . It may require some experimentation to find all of the eigenvalues.  The eigenvalues are .     There are some restrictions on the matrices to which this technique applies as we have assumed that the eigenvalues of are real and distinct. If has repeated or complex eigenvalues, this technique will need to be modified, as explored in some of the exercises.    Summary  We have explored the power method as a tool for numerically approximating the eigenvalues and eigenvectors of a matrix.  After choosing an initial vector , we define the sequence . As grows larger, the direction of the vectors closely approximates the direction of the eigenspace corresponding to the eigenvalue having the largest absolute value.  We normalize the vectors by multiplying by , where is the component having the largest absolute value. In this way, the vectors approach an eigenvector associated to , and the multipliers approach the eigenvalue .  To find the eigenvalue having the smallest absolute value, we apply the power method using the matrix .  To find the eigenvalue closest to some number , we apply the power method using the matrix .     This Sage cell has the commands power , inverse_power , and find_closest_eigenvalue that we have developed in this section. After evaluating this cell, these commands will be available in any other cell on this page.    Suppose that is a matrix having eigenvalues , , , and .  What are the eigenvalues of ?  What are the eigenvalues of ?      , , , and .   , , , and .     The eigenvalues of are the reciprocals of the eigenvalues of . They are, therefore, , , , and .  If is an eigenvalue of , then is an eigenvalue of . Therefore, the eigenvalues of are , , , and .     Use the commands power , inverse_power , and find_closest_eigenvalue to approximate the eigenvalues and associated eigenvectors of the following matrices.    .   .   .      and .   and .   , , , and .     The power method tells us that the dominant eigenvalue is with associated eigenvector . The inverse power method tells us that the eigenvalue having the smallest absolute value is with associated eigenvector .  The power method tells us that the dominant eigenvalue is with associated eigenvector . The inverse power method tells us that the eigenvalue having the smallest absolute value is with associated eigenvector .  The power method tells us that the dominant eigenvalue is . The inverse power method tells us that the eigenvalue having the smallest absolute value is . If we now look for the closest eigenvalue to , we see that it is , which we have already found. Let's try again, this time looking to find the closest eigenvalue to . Here, we find . If we next try to find the eigenvalue closest to , we find it to be .  The four eigenvalues are then , , , and .     Use the techniques we have seen in this section to find the eigenvalues of the matrix .     , , , , and .   The power method shows us that is the dominant eigenvalue. The inverse power method tells us that is the eigenvalue having the smallest absolute value. We then probe in between these values to find eigenvalues , , and .    Consider the matrix .   Describe what happens if we apply the power method and the inverse power method using the initial vector .  Find the eigenvalues of this matrix and explain this observed behavior.  How can we apply the techniques of this section to find the eigenvalues of ?     The methods do not converge.  There is not a unique dominant eigenvalue.  Try finding an eigenvalue closest to, say, .     We see that neither the power method nor the inverse power method converge.  The eigenvalues are and . This means that there is not a unique dominant eigenvalue and there is not a unique eigenvalue with the smallest absolute value. The methods try to find first one of them and then the other.  To break the symmetry, we can look for an eigenvalue closest to, say, . When we do this, we find the eigenvalue . Then look for another eigenvalue closest to to find .     We have seen that the matrix has eigenvalues and and associated eigenvectors and .  Describe what happens when we apply the inverse power method using the initial vector .  Explain why this is happening and provide a contrast with how the power method usually works.  How can we modify the power method to give the dominant eigenvalue in this case?     We see that the vectors do not converge and the multipliers converge to the wrong value.  The multipliers are obtained from the first component, then the second component, then the first, and so on.  Choose one of the components, say, the first one and consider the ratio between that component of and in place of the multiplier .     We see that the vectors do not converge but instead flip between approximations to and . Also, the multipliers are converging to rather than .  When applying the power method, the multipliers are usually formed from the same component of the vectors in every iteration. Here, we see that the multipliers are obtained from the first component, then the second component, then the first, and so on.  Choose one of the components, say, the first one and consider the ratio between that component of and in place of the multiplier . The problem is that you must make sure that this component is not approaching zero.     Suppose that is a matrix with eigenvalues and and that is a matrix with eigenvalues and . If we apply the power method to find the dominant eigenvalue of these matrices to the same degree of accuracy, which matrix will require more steps in the algorithm? Explain your response.   We will need more steps when finding the dominant eigenvalue of .   For both matrices, is the dominant eigenvalue and is the eigenvalue closest to . We will construct the initial vector as a linear combination of eigenvectors: . Then . For the matrix , is relatively small compared to so we expect the contribution from to become smaller more quickly. Therefore, we will need more steps in the power method to find the dominant eigenvalue of .    Suppose that we apply the power method to the matrix with an initial vector and find the eigenvalue and eigenvector . Suppose that we then apply the power method again with a different initial vector and find the same eigenvalue but a different eigenvector . What can we conclude about the matrix in this case?    is the dominant eigenvalue with a multiplicity greater than one.   The dominant eigenvector is but it has a multiplicity greater than one and the associated eigenspace has .    The power method we have developed only works if the matrix has real eigenvalues. Suppose that is a matrix that has a complex eigenvalue . What would happen if we apply the power method to ?   The vectors will not converge.   The power method relies on the fact that the vectors attempt to line up in the direction of a vector in the dominant eigenspace. If the eigenvalues are complex, however, the vectors will be rotated with each iteration so they will not converge.    Consider the matrix .  Find the eigenvalues and associated eigenvectors of .  Make a prediction about what happens if we apply the power method and the inverse power method to find eigenvalues of .  Verify your prediction using Sage.      There is a single eigenvalue having multiplicity two with its associated eigenspace being one-dimensional with basis vector .  Applying either the power or inverse power method will find the eigenvalue and a scalar multiple of .     There is a single eigenvalue having multiplicity two with its associated eigenspace being one-dimensional with basis vector .  Applying either the power or inverse power method will find the eigenvalue and a scalar multiple of .     "
},
{
  "id": "sec-power-method-2-3",
  "level": "2",
  "url": "sec-power-method.html#sec-power-method-2-3",
  "type": "Preview Activity",
  "number": "21.2.1",
  "title": "",
  "body": "  Let's recall some earlier observations about eigenvalues and eigenvectors.  How are the eigenvalues and associated eigenvectors of related to those of ?  How are the eigenvalues and associated eigenvectors of related to those of ?  If is an eigenvalue of , what can we say about the pivot positions of ?  Suppose that . Explain how we know that is an eigenvalue of and then explain why the following Sage computation is incorrect.   Suppose that , and we define a sequence ; in other words, . What happens to as grows increasingly large?  Explain how the eigenvalues of are responsible for the behavior noted in the previous question.      If is an eigenvalue of , then for an associated eigenvector . Multiplying by and , we obtain , which shows that is an eigenvalue of .  In the same way, if is an eigenvalue of , then for an associated eigenvector . This means that so that is an eigenvalue of .  If is an eigenvalue of , then is not invertible and so has a row without a pivot position.  Since is a positive stochastic matrix, we know that is an eigenvalue and hence that is not invertible. Sage, however, tells us that , which cannot be true since is not invertible.  The vectors form a Markov chain, which must converge to the steady-state vector .  We have eigenvalues and . If we begin with and successively multiply by , we have . When becomes large, the coefficient of becomes insignificantly small so we are left with an eigenvector in .    "
},
{
  "id": "sec-power-method-3-3",
  "level": "2",
  "url": "sec-power-method.html#sec-power-method-3-3",
  "type": "Example",
  "number": "21.2.1",
  "title": "",
  "body": " Let's begin with the positive stochastic matrix . We spent quite a bit of time studying this type of matrix in ; in particular, we saw that any Markov chain will converge to the unique steady-state vector. Let's rephrase this statement in terms of the eigenvectors of .  This matrix has eigenvalues and so the dominant eigenvalue is . The associated eigenvectors are and . Suppose we begin with the vector and find and so forth. Notice that the powers become increasingly small as grows so that when is large. Therefore, the vectors become increasingly close to a vector in the eigenspace , the eigenspace associated to the dominant eigenvalue. If we did not know the eigenvector , we could use a Markov chain in this way to find a basis vector for , which, as seen in , is essentially how the Google PageRank algorithm works.  "
},
{
  "id": "sec-power-method-3-4",
  "level": "2",
  "url": "sec-power-method.html#sec-power-method-3-4",
  "type": "Example",
  "number": "21.2.2",
  "title": "",
  "body": " Let's now look at the matrix , which has eigenvalues and . The dominant eigenvalue is , and the associated eigenvectors are and . Once again, begin with the vector so that    As the figure shows, the vectors are stretched by a factor of in the direction and not at all in the direction. Consequently, the vectors become increasingly long, but their direction becomes closer to the direction of the eigenvector associated to the dominant eigenvalue.    To find an eigenvector associated to the dominant eigenvalue, we will prevent the length of the vectors from growing arbitrarily large by multiplying by an appropriate scaling constant. Here is one way to do this. Given the vector , we identify its component having the largest absolute value and call it . We then define , which means that the component of having the largest absolute value is .  For example, beginning with , we find . The component of having the largest absolute value is so we multiply by to obtain . Then . Now the component having the largest absolute value is so we multiply by to obtain .   The resulting sequence of vectors is shown in the figure. Notice how the vectors now approach the eigenvector , which gives us a way to find the eigenvector . This is the power method for finding an eigenvector associated to the dominant eigenvalue of a matrix. power method     "
},
{
  "id": "sec-power-method-3-5",
  "level": "2",
  "url": "sec-power-method.html#sec-power-method-3-5",
  "type": "Activity",
  "number": "21.2.2",
  "title": "",
  "body": "  Let's begin by considering the matrix and the initial vector .   Compute the vector .  Find , the component of that has the largest absolute value. Then form . Notice that the component having the largest absolute value of is .  Find the vector . Identify the component of having the largest absolute value. Then form to obtain a vector in which the component with the largest absolute value is .  The Sage cell below defines a function that implements the power method. Define the matrix and initial vector below. The command power(A, x0, N) will print out the multiplier and the vectors for steps of the power method.   How does this computation identify an eigenvector of the matrix ?  What is the corresponding eigenvalue of this eigenvector?  How do the values of the multipliers tell us the eigenvalue associated to the eigenvector we have found?  Consider now the matrix . Use the power method to find the dominant eigenvalue of and an associated eigenvector.      We find .  The first component has the largest absolute value so . Therefore, .  In the same way, we obtain . We see that so we have .  We see that the vectors are getting closer and closer to , which we therefore identify as an eigenvector associated to the dominant eigenvalue.  We see that . Therefore, the dominant eigenvalue is .  More generally, we see that the multiplier will converge to the dominant eigenvalue.  The power method constructs a sequence of vectors converging to an eigenvector . The multipliers converge to , the dominant eigenvalue.    "
},
{
  "id": "sec-power-method-4-3",
  "level": "2",
  "url": "sec-power-method.html#sec-power-method-4-3",
  "type": "Activity",
  "number": "21.2.3",
  "title": "",
  "body": "  The key to finding the eigenvalue of having the smallest absolute value is to note that the eigenvectors of are the same as those of .  If is an eigenvector of with associated eigenvector , explain why is an eigenvector of with associated eigenvalue .  Explain why the eigenvalue of having the smallest absolute value is the reciprocal of the dominant eigenvalue of .  Explain how to use the power method applied to to find the eigenvalue of having the smallest absolute value.  If we apply the power method to , we begin with an intial vector and generate the sequence . It is not computationally efficient to compute , however, so instead we solve the equation . Explain why an factorization of is useful for implementing the power method applied to .  The following Sage cell defines a command called inverse_power that applies the power method to . That is, inverse_power(A, x0, N) prints the vectors , where , and multipliers , which approximate the eigenvalue of . Use it to find the eigenvalue of having the smallest absolute value.   The inverse power method only works if is invertible. If is not invertible, what is its eigenvalue having the smallest absolute value?  Use the power method and the inverse power method to find the eigenvalues and associated eigenvectors of the matrix .      If is an eigenvalue of , then for an associated eigenvector . Multiplying by and , we obtain , which shows that is an eigenvalue of .  If , then . Therefore, the reciprocal of the smallest eigenvalue of is the dominant eigenvalue of .  If we apply the power method to the matrix , we will find the dominant eigenvalue and an associated eigenvector of . We know, however, that will be the eigenvalue of having the smallest absolute value and will be an associated eigenvector.  We would like to solve equations of the form for many different vectors . Using an factorization allows us to recycle for subsequent equations the effort we expend performing Gaussian elimination to solve the first equation.  We obtain the eigenvector and associated eigenvalue .  If is not invertible, then is the eigenvalue having the smallest absolute value.  We find the dominant eigenvalue to be with associated eigenvector . The smallest eigenvalue is with associated eigenvector .    "
},
{
  "id": "sec-power-method-4-5",
  "level": "2",
  "url": "sec-power-method.html#sec-power-method-4-5",
  "type": "Activity",
  "number": "21.2.4",
  "title": "",
  "body": "  Remember that the absolute value of a number tells us how far that number is from on the real number line. We may therefore think of the inverse power method as telling us the eigenvalue closest to .  If is an eigenvector of with associated eigenvalue , explain why is an eigenvector of where is some scalar.  What is the eigenvalue of associated to the eigenvector ?  Explain why the eigenvalue of closest to is the eigenvalue of closest to .  Explain why applying the inverse power method to gives the eigenvalue of closest to .  Consider the matrix . If we use the power method and inverse power method, we find two eigenvalues, and . Viewing these eigenvalues on a number line, we know that the other eigenvalues lie in the range between and , as shaded in .      The range of eigenvalues of .   The Sage cell below has a function find_closest_eigenvalue(A, s, x, N) that implements steps of the inverse power method using the matrix and an initial vector . This function prints approximations to the eigenvalue of closest to and its associated eigenvector. By trying different values of in the shaded regions of the number line shown in , find the other two eigenvalues of .   Write a list of the four eigenvalues of in increasing order.      If , then , which shows that is also an eigenvector of .  From the previous part, we see that the associated eigenvalue is .  If is the eigenvalue of closest to , then is an eigenvalue of that must be the closest to .  The inverse power method applied to tells us the eigenvalue of having the smallest absolute value and an associated eigenvector . Therefore, is the eigenvalue of closest to and is an associated eigenvector.  We begin by trying to find the closest eigenvalue to, say, . The power method tells us that this eigenvalue is . If we then try to find the eigenvalue closest to , we find the fourth eigenvalue . It may require some experimentation to find all of the eigenvalues.  The eigenvalues are .    "
},
{
  "id": "sec-power-method-6-2",
  "level": "2",
  "url": "sec-power-method.html#sec-power-method-6-2",
  "type": "Exercise",
  "number": "21.2.4.1",
  "title": "",
  "body": " Suppose that is a matrix having eigenvalues , , , and .  What are the eigenvalues of ?  What are the eigenvalues of ?      , , , and .   , , , and .     The eigenvalues of are the reciprocals of the eigenvalues of . They are, therefore, , , , and .  If is an eigenvalue of , then is an eigenvalue of . Therefore, the eigenvalues of are , , , and .   "
},
{
  "id": "sec-power-method-6-3",
  "level": "2",
  "url": "sec-power-method.html#sec-power-method-6-3",
  "type": "Exercise",
  "number": "21.2.4.2",
  "title": "",
  "body": " Use the commands power , inverse_power , and find_closest_eigenvalue to approximate the eigenvalues and associated eigenvectors of the following matrices.    .   .   .      and .   and .   , , , and .     The power method tells us that the dominant eigenvalue is with associated eigenvector . The inverse power method tells us that the eigenvalue having the smallest absolute value is with associated eigenvector .  The power method tells us that the dominant eigenvalue is with associated eigenvector . The inverse power method tells us that the eigenvalue having the smallest absolute value is with associated eigenvector .  The power method tells us that the dominant eigenvalue is . The inverse power method tells us that the eigenvalue having the smallest absolute value is . If we now look for the closest eigenvalue to , we see that it is , which we have already found. Let's try again, this time looking to find the closest eigenvalue to . Here, we find . If we next try to find the eigenvalue closest to , we find it to be .  The four eigenvalues are then , , , and .   "
},
{
  "id": "sec-power-method-6-4",
  "level": "2",
  "url": "sec-power-method.html#sec-power-method-6-4",
  "type": "Exercise",
  "number": "21.2.4.3",
  "title": "",
  "body": " Use the techniques we have seen in this section to find the eigenvalues of the matrix .     , , , , and .   The power method shows us that is the dominant eigenvalue. The inverse power method tells us that is the eigenvalue having the smallest absolute value. We then probe in between these values to find eigenvalues , , and .  "
},
{
  "id": "sec-power-method-6-5",
  "level": "2",
  "url": "sec-power-method.html#sec-power-method-6-5",
  "type": "Exercise",
  "number": "21.2.4.4",
  "title": "",
  "body": " Consider the matrix .   Describe what happens if we apply the power method and the inverse power method using the initial vector .  Find the eigenvalues of this matrix and explain this observed behavior.  How can we apply the techniques of this section to find the eigenvalues of ?     The methods do not converge.  There is not a unique dominant eigenvalue.  Try finding an eigenvalue closest to, say, .     We see that neither the power method nor the inverse power method converge.  The eigenvalues are and . This means that there is not a unique dominant eigenvalue and there is not a unique eigenvalue with the smallest absolute value. The methods try to find first one of them and then the other.  To break the symmetry, we can look for an eigenvalue closest to, say, . When we do this, we find the eigenvalue . Then look for another eigenvalue closest to to find .   "
},
{
  "id": "exercise-power-method",
  "level": "2",
  "url": "sec-power-method.html#exercise-power-method",
  "type": "Exercise",
  "number": "21.2.4.5",
  "title": "",
  "body": " We have seen that the matrix has eigenvalues and and associated eigenvectors and .  Describe what happens when we apply the inverse power method using the initial vector .  Explain why this is happening and provide a contrast with how the power method usually works.  How can we modify the power method to give the dominant eigenvalue in this case?     We see that the vectors do not converge and the multipliers converge to the wrong value.  The multipliers are obtained from the first component, then the second component, then the first, and so on.  Choose one of the components, say, the first one and consider the ratio between that component of and in place of the multiplier .     We see that the vectors do not converge but instead flip between approximations to and . Also, the multipliers are converging to rather than .  When applying the power method, the multipliers are usually formed from the same component of the vectors in every iteration. Here, we see that the multipliers are obtained from the first component, then the second component, then the first, and so on.  Choose one of the components, say, the first one and consider the ratio between that component of and in place of the multiplier . The problem is that you must make sure that this component is not approaching zero.   "
},
{
  "id": "sec-power-method-6-7",
  "level": "2",
  "url": "sec-power-method.html#sec-power-method-6-7",
  "type": "Exercise",
  "number": "21.2.4.6",
  "title": "",
  "body": " Suppose that is a matrix with eigenvalues and and that is a matrix with eigenvalues and . If we apply the power method to find the dominant eigenvalue of these matrices to the same degree of accuracy, which matrix will require more steps in the algorithm? Explain your response.   We will need more steps when finding the dominant eigenvalue of .   For both matrices, is the dominant eigenvalue and is the eigenvalue closest to . We will construct the initial vector as a linear combination of eigenvectors: . Then . For the matrix , is relatively small compared to so we expect the contribution from to become smaller more quickly. Therefore, we will need more steps in the power method to find the dominant eigenvalue of .  "
},
{
  "id": "sec-power-method-6-8",
  "level": "2",
  "url": "sec-power-method.html#sec-power-method-6-8",
  "type": "Exercise",
  "number": "21.2.4.7",
  "title": "",
  "body": " Suppose that we apply the power method to the matrix with an initial vector and find the eigenvalue and eigenvector . Suppose that we then apply the power method again with a different initial vector and find the same eigenvalue but a different eigenvector . What can we conclude about the matrix in this case?    is the dominant eigenvalue with a multiplicity greater than one.   The dominant eigenvector is but it has a multiplicity greater than one and the associated eigenspace has .  "
},
{
  "id": "sec-power-method-6-9",
  "level": "2",
  "url": "sec-power-method.html#sec-power-method-6-9",
  "type": "Exercise",
  "number": "21.2.4.8",
  "title": "",
  "body": " The power method we have developed only works if the matrix has real eigenvalues. Suppose that is a matrix that has a complex eigenvalue . What would happen if we apply the power method to ?   The vectors will not converge.   The power method relies on the fact that the vectors attempt to line up in the direction of a vector in the dominant eigenspace. If the eigenvalues are complex, however, the vectors will be rotated with each iteration so they will not converge.  "
},
{
  "id": "sec-power-method-6-10",
  "level": "2",
  "url": "sec-power-method.html#sec-power-method-6-10",
  "type": "Exercise",
  "number": "21.2.4.9",
  "title": "",
  "body": " Consider the matrix .  Find the eigenvalues and associated eigenvectors of .  Make a prediction about what happens if we apply the power method and the inverse power method to find eigenvalues of .  Verify your prediction using Sage.      There is a single eigenvalue having multiplicity two with its associated eigenspace being one-dimensional with basis vector .  Applying either the power or inverse power method will find the eigenvalue and a scalar multiple of .     There is a single eigenvalue having multiplicity two with its associated eigenspace being one-dimensional with basis vector .  Applying either the power or inverse power method will find the eigenvalue and a scalar multiple of .   "
},
{
  "id": "sec-dot-product",
  "level": "1",
  "url": "sec-dot-product.html",
  "type": "Section",
  "number": "22.1",
  "title": "The dot product",
  "body": " The dot product   In this section, we introduce a simple algebraic operation, known as the dot product , that helps us measure the length of vectors and the angle formed by a pair of vectors. For two-dimensional vectors and , their dot product is the scalar defined to be For instance,       Compute the dot product   Sketch the vector below. Then use the Pythagorean theorem to find the length of .     Sketch the vector and find its length.    Compute the dot product . How is the dot product related to the length of ?  Remember that the matrix represents the matrix transformation that rotates vectors counterclockwise by . Beginning with the vector , find , the result of rotating by , and sketch it above.  What is the dot product ?  Suppose that . Find the vector that results from rotating by and find the dot product .  Suppose that and are two perpendicular vectors. What do you think their dot product is?          .    The length of is 5.     , which is the square of the length of .          .     .    The dot product should be zero.         The geometry of the dot product   dot product The dot product is defined, more generally, for any two -dimensional vectors: The important thing to remember is that the dot product will produce a scalar. In other words, the two vectors are combined in such a way as to create a number, and, as we'll see, this number conveys useful geometric information.    We compute the dot product between two four-dimensional vectors as      Properties of dot products  As with ordinary multiplication, the dot product enjoys some familiar algebraic properties, such as commutativity and distributivity. More specifically, it doesn't matter in which order we compute the dot product of two vectors: If is a scalar, we have We may also distribute the dot product across linear combinations:      Suppose that and . Then     The most important property of the dot product, and the real reason for our interest in it, is that it gives us geometric information about vectors and their relationship to one another. Let's first think about the length of a vector by looking at the vector as shown in       The vector .   We may find the length of this vector using the Pythagorean theorem since the vector forms the hypotenuse of a right triangle having a horizontal leg of length 3 and a vertical leg of length 2. The length of , which we denote as , is therefore . Now notice that the dot product of with itself is . This is true in general; that is, we have   More than that, the dot product of two vectors records information about the angle between them. Consider .      The dot product measures the angle .   To see this, we will apply the Law of Cosines, which says that The upshot of this reasoning is that   To summarize:   Geometric properties of the dot product  The dot product gives us the following geometric information: where is the angle between and .        Sketch the vectors and using .      Sketch the vectors and here.    Find the lengths and using the dot product.  Find the dot product and use it to find the angle between and .  Consider the vector . Include it in your sketch in and find the angle between and .  If two vectors are perpendicular, what can you say about their dot product? Explain your thinking.  For what value of is the vector perpendicular to ?  Sage can be used to find lengths of vectors and their dot products. For instance, if v and w are vectors, then v.norm() gives the length of v and v * w gives .  Suppose that Use the Sage cell below to find , , , and the angle between and . You may use arccos to find the angle's measure expressed in radians.              We find that so that and .     so that      so that     If two vectors are perpendicular, then the angle between them is . Since , their dot product must be zero.    The dot product is so .    We find that , , , and the angle between these vectors is .                and .              Their dot product must be zero.     .     .       As we move forward, it will be important for us to recognize when vectors are perpendicular to one another. For instance, when vectors and are perpendicular, the angle between them and we have Therefore, the dot product between perpendicular vectors must be zero. This leads to the following definition.    orthogonal We say that vectors and are orthogonal if .   In practical terms, two perpendicular vectors are orthogonal. However, the concept of orthogonality is somewhat more general because it allows one or both of the vectors to be the zero vector .  We've now seen that the dot product gives us geometric information about vectors. It also provides a way to compare vectors. For example, consider the vectors , , and , shown in . The vectors and seem somewhat similar as the directions they define are nearly the same. By comparison, appears rather dissimilar to both and . We will measure the similarity of vectors by finding the angle between them; the smaller the angle, the more similar the vectors.      Which of the vectors are most similar?     This activity explores two further uses of the dot product beginning with the similarity of vectors.   Our first task is to assess the similarity between various Wikipedia articles by forming vectors from each of five articles. In particular, one may download the text from a Wikipedia article, remove common words, such as the and and , count the number of times the remaining words appear in the article, and represent these counts in a vector.  For example, evaluate the following cell that loads some special commands along with the vectors constructed from the Wikipedia articles on Veteran's Day, Memorial Day, Labor Day, the Golden Globe Awards, and the Super Bowl. For each of the five articles, you will see a list of the number of times 10 words appear in these articles. For instance, the word act appears 3 times in the Veteran's Day article and 0 times in the Labor Day article. For each of the five articles, we obtain 604-dimensional vectors, which are named veterans , memorial , labor , golden , and super .     Suppose that two articles have no words in common. What is the value of the dot product between their corresponding vectors? What does this say about the angle between these vectors?    Suppose there are two articles on the same subject, yet one article is twice as long. What approximate relationship would you expect to hold between the two vectors? What does this say about the angle between them?    Use the Sage cell below to find the angle between the vector veterans and the other four vectors. To express the angle in degrees, use the degrees(x) command, which gives the number of degrees in x radians.     Compare the four angles you have found and discuss what they mean about the similarity between the Veteran's Day article and the other four. How do your findings reflect the nature of these five events?       Vectors are often used to represent how a quantity changes over time. For instance, the vector might represent the value of a company's stock on four consecutive days. When interpreted in this way, we call the vector a time series. Evaluate the Sage cell below to see a representation of two time series , in blue, and , in orange, which we imagine represent the value of two stocks over a period of time. (This cell relies on some data loaded by the first cell in this activity.) Even though one stock has a higher value than the other, the two appear to be related since they seem to rise and fall at roughly similar ways. We often say that they are correlated , and we would like to measure the degree to which they are correlated.   In order to compare the ways in which they rise and fall, we will first demean the time series; that is, for each time series, we will subtract its average value to obtain a new time series. There is a command, demean(s) , that returns the demeaned time series of s . Use the Sage cell below to demean the series and and plot.     If the demeaned series are and , then the correlation between and is defined to be Given the geometric interpretation of the dot product, the correlation equals the cosine of the angle between the demeaned time series, and therefore is between -1 and 1.  Find the correlation between and .     Suppose that two time series are such that their demeaned time series are scalar multiples of one another, as in        On the left, the demeaned time series are positive scalar multiples of one another. On the right, they are negative scalar multiples.   For instance, suppose we have time series and whose demeaned time series and are positive scalar multiples of one another. What is the angle between the demeaned vectors? What does this say about the correlation ?    Suppose the demeaned time series and are negative scalar multiples of one another, what is the angle between the demeaned vectors? What does this say about the correlation ?    Use the Sage cell below to plot the time series and and find their correlation.     Use the Sage cell below to plot the time series and and find their correlation.                 If there are no words in common, then the dot product between the two vectors will be zero. This means that they are perpendicular to one another.    The vectors should be, at least approximately, scalar multiples of one another, which means that the angle between them is zero.    The angle veterans makes with memorial is , with labor is , with golden is , and with super is .    It appears that the articles on Veteran's Day and Memorial Day are most similar. This makes sense because both are U.S. national holidays that honor military service. The second most similar article is Labor Day, which is also a national holiday. The other two are quite dissimilar as they are entertainment events.          The graphs are now lowered so that their averages are zero.    The correlation is , which is quite close to 1.    The angle should be zero, which means that the correlation will be .    The angle should be , which means that the correlation should be .    The correlation is .    The correlation is .                They are perpendicular.    The angle should be close to 0.    The angle veterans makes with memorial is , with labor is , with golden is , and with super is .    The articles on Veteran's Day and Memorial Day are most similar.          The graphs are now lowered so that their averages are zero.                                      -means clustering  A typical problem in data science is to find some underlying patterns in a dataset. Suppose, for instance, that we have the set of 177 data points plotted in . Notice that the points are not scattered around haphazardly; instead, they seem to form clusters. Our goal here is to develop a strategy for detecting the clusters.      A set of 177 data points.   To see how this could be useful, suppose we have medical data describing a group of patients, some of whom have been diagnosed with a specific condition, such as diabetes. Perhaps we have a record of age, weight, blood sugar, cholesterol, and other attributes for each patient. It could be that the data points for the group diagnosed as having the condition form a cluster that is somewhat distinct from the rest of the data. Suppose that we are able to identify that cluster and that we are then presented with a new patient that has not been tested for the condition. If the attributes for that patient place them in that cluster, we might identify them as being at risk for the condition and prioritize them for appropriate screenings.  If there are many attributes for each patient, the data may be high-dimensional and not easily visualized. We would therefore like to develop an algorithm that separates the data points into clusters without human intervention. We call the result a clustering .  The next activity introduces a technique, called -means clustering, that helps us find clusterings. To do so, we will view the data points as vectors so that the distance between two data points equals the length of the vector joining them. That is, if two points are represented by the vectors and , then the distance between the points is .    To begin, we identify the centroid , or the average, of a set of vectors as    Find the centroid of the vectors and sketch the vectors and the centroid using . You may wish to simply plot the points represented by the tips of the vectors rather than drawing the vectors themselves.      The vectors , , and their centroid.   Notice that the centroid lies in the center of the points defined by the vectors.    Now we'll illustrate an algorithm that forms clusterings. To begin, consider the following points, represented as vectors, which are shown in .      We will group this set of four points into two clusters.   Suppose that we would like to group these points into clusters. (Later on, we'll see how to choose an appropriate value for , the number of clusters.) We begin by choosing two points and at random and declaring them to be the centers ' of the two clusters.  For example, suppose we randomly choose and as the center of two clusters. The cluster centered on will be the set of points that are closer to than to . Determine which of the four data points are in this cluster, which we denote by , and circle them in .    The second cluster will consist of the data points that are closer to than . Determine which of the four points are in this cluster, which we denote by , and circle them in .    We now have a clustering with two clusters, but we will try to improve upon it in the following way. First, find the centroids of the two clusters; that is, redefine to be the centroid of cluster and to be the centroid of . Find those centroids and indicate them in       Indicate the new centroids and clusters.   Now update the cluster to be the set of points closer to than . Update the cluster in a similar way and indicate the clusters in .    Let's perform this last step again. That is, update the centroids and from the new clusters and then update the clusters and . Indicate your centroids and clusters in .      Indicate the new centroids and clusters.   Notice that this last step produces the same set of clusters so there is no point in repeating it. We declare this to be our final clustering.          The centroid is .       The first cluster is .    The second cluster is .    We redefine and . This leads to new clusters and .    We have new centroids and , and the clusters and are unchanged.          The centroid is .        .     .     and    and .     and . The clusters and are unchanged.       This activity demonstrates our algorithm for finding a clustering. We first choose a value and seek to break the data points into clusters. The algorithm proceeds in the following way:   Choose points at random from our dataset.    Construct the cluster as the set of data points closest to , as the set of data points closest to , and so forth.    Repeat the following until the clusters no longer change:   Find the centroids of the current clusters.    Update the clusters .        The clusterings we find depend on the initial random choice of points . For instance, in the previous activity, we arrived, with the initial choice and , at the clustering:   If we instead choose the initial points to be and , we eventually find the clustering:   Is there a way that we can determine which clustering is the better of the two? It seems like a better clustering will be one for which the points in a cluster are, on average, closer to the centroid of their cluster. If we have a clustering, we therefore define a function, called the objective , which measures the average of the square of the distance from each point to the centroid of the cluster to which that point belongs. A clustering with a smaller objective will have clusters more tightly centered around their centroids, which should result in a better clustering.  For example, when we obtain the clustering: with centroids and , we find the objective to be     We'll now use the objective to compare clusterings and to choose an appropriate value of .   In the previous activity, one initial choice of and led to the clustering: with centroids and . Find the objective of this clustering.    We have now seen two clusterings and computed their objectives. Recall that our dataset is shown in . Which of the two clusterings feels like the better fit? How is this fit reflected in the values of the objectives?    Evaluating the following cell will load and display a dataset consisting of 177 data points. This dataset has the name data . Given this plot of the data, what would seem like a reasonable number of clusters?    In the following cell, you may choose a value of and then run the algorithm to determine and display a clustering and its objective. If you run the algorithm a few times with the same value of , you will likely see different clusterings having different objectives. This is natural since our algorithm starts by making a random choice of points , and a different choices may lead to different clusterings. Choose a value of and run the algorithm a few times. Notice that clusterings having lower objectives seem to fit the data better. Repeat this experiment with a few different values of .     For a given value of , our strategy is to run the algorithm several times and choose the clustering with the smallest objective. After choosing a value of , the following cell will run the algorithm 10 times and display the clustering having the smallest objective.   For each value of between 2 and 9, find the clustering having the smallest objective and plot your findings in .      Construct a plot of the minimal objective as it depends on the choice of .   This plot is called an elbow plot due to its shape. Notice how the objective decreases sharply when is small and then flattens out. This leads to a location, called the elbow, where the objective transitions from being sharply decreasing to relatively flat. This means that increasing beyond the elbow does not significantly decrease the objective, which makes the elbow a good choice for .  Where does the elbow occur in your plot above? How does this compare to the best value of that you estimated by simply looking at the data in .     Of course, we could increase until each data point is its own cluster. However, this defeats the point of the technique, which is to group together nearby data points in the hope that they share common features, thus providing insight into the structure of the data.       The objective is     The clustering with and appears to be a tighter clustering and has a smaller objective.    It appears that the best clustering is either or .    With a fixed value of , running the algorithm several times leads to different clusterings with different objectives. If we increase , the objective generally decreases.    The elbow occurs around or , which are the values that we felt led to the best clusterings.             The objective is .    The clustering and has a smaller objective.     or .    With a fixed value of , running the algorithm several times leads to different clusterings with different objectives. If we increase , the objective generally decreases.    The elbow occurs around or .          We have now seen how our algorithm and the objective identify a reasonable value for , the number of the clusters, and produce a good clustering having clusters. Notice that we don't claim to have found the best clustering as the true test of any clustering will be in how it helps us understand the dataset and helps us make predictions about any new data that we may encounter.    Summary  This section introduced the dot product and the ability to investigate geometric relationships between vectors.   The dot product of two vectors and satisfies these properties: where is the angle between and .    The vectors and are orthogonal when .    We explored some applications of the dot product to the similarity of vectors, correlation of time series, and -means clustering.        Consider the vectors    Find the lengths of the vectors, and .    Find the dot product and use it to find the angle between and .          , , , and .     and .         We have and so that and .    Since , we find that .       Consider the three vectors    Find the dot products , , and .    Use the dot products you just found to evaluate:    .     .     .     .       For what value of is orthogonal to ?          , , and .                                    , , and .                                 Suppose that and are vectors where    What is ?    What is the angle between and ?    Suppose that is a scalar. Find the value of for which is orthogonal to ?                    .                    so .       Suppose that .   What is the relationship between and ?    What is the relationship between and ?    If for some scalar , what is the relationship between and ? What is the relationship between and ?    Suppose that . Find a scalar so that has length 1.                                             so that     We know so        Given vectors and , explain why Sketch two vectors and and explain why this fact is called the parallelogram law .   Use the relationship .              Consider the vectors and a general vector .   Write an equation in terms of , , and that describes all the vectors orthogonal to .    Write a linear system that describes all the vectors orthogonal to both and .    Write the solution set to this linear system in parametric form. What type of geometric object does this solution set represent? Indicate with a rough sketch why this makes sense.    Give a parametric description of all vectors orthogonal to . What type of geometric object does this represent? Indicate with a rough sketch why this makes sense.                                             , which describes a line     , which describes a plane.       Explain your responses to these questions.   Suppose that is orthogonal to both and . Can you guarantee that is also orthogonal to any linear combination ?    Suppose that is orthogonal to itself. What can you say about ?         Yes              Yes, because      so        Suppose that , , and form a basis for and that each vector is orthogonal to the other two. Suppose also that is another vector in .   Explain why for some scalars , , and .    Beginning with the expression apply the distributive property of dot products to explain why Find similar expressions for and .    Verify that form a basis for and that each vector is orthogonal to the other two. Use what you've discovered in this problem to write the vector as a linear combination of , , and .         Since , , and form a basis for , any vector in can be written as a linear combination of them.    Apply the distributive property.   and               Since , , and form a basis for , any vector in can be written as a linear combination of them.     so that .  In the same way, and     Check that the vectors are orthogonal by computing their dot products. Then by computing the ratios of dot products.       Suppose that , , and are three nonzero vectors that are pairwise orthogonal; that is, each vector is orthogonal to the other two.   Explain why cannot be a linear combination of and .    Explain why this set of three vectors is linearly independent.         If , then and .    None of the vectors is a linear combination of the others.         If , then . In the same way, so must be the zero vector. We're told, however, that is nonzero.    We've seen that is not a linear combination of and . The same thinking shows that none of the vectors is a linear combination of the others so they form a linearly independent set.       In the next chapter, we will consider certain matrices and define a function where is a vector in .   Suppose that and . Evaluate .    For a general vector , evaluate as an expression involving and .    Suppose that is an eigenvector of a matrix with associated eigenvalue and that has length 1. What is the value of the function ?                                        .       Back in , we saw that equations of the form represent lines in the plane. In this exercise, we will see how this expression arises geometrically.     A line, a point on the line, and a vector perpendicular to the line.     Find the slope and vertical intercept of the line shown in . Then write an equation for the line in the form .    Suppose that is a point on the line, that is a vector perpendicular to the line, and that is a general point on the line. Sketch the vector and describe the angle between this vector and the vector .    What is the value of the dot product ?    Explain why the equation of the line can be written in the form .    Identify the vectors and for the line illustrated in and use them to write the equation of the line in terms of and . Verify that this expression is algebraically equivalent to the equation that you earlier found for this line.    Explain why any line in the plane can be described by an equation having the form . What is the significance of the vector ?          .     is orthogonal to .         Apply the distributive property          is perpendicular to the line.         The slope and the intercept so .    The vector is in the direction of the line so it is orthogonal to .    Since these vectors are orthogonal, their dot product is .    Since , we have , which implies that .     and . This gives . We can rearrange this to have the form .    If we choose a vector that is perpendicular to the line and a point on the line, we have .       "
},
{
  "id": "sec-dot-product-2-2",
  "level": "2",
  "url": "sec-dot-product.html#sec-dot-product-2-2",
  "type": "Preview Activity",
  "number": "22.1.1",
  "title": "",
  "body": "    Compute the dot product   Sketch the vector below. Then use the Pythagorean theorem to find the length of .     Sketch the vector and find its length.    Compute the dot product . How is the dot product related to the length of ?  Remember that the matrix represents the matrix transformation that rotates vectors counterclockwise by . Beginning with the vector , find , the result of rotating by , and sketch it above.  What is the dot product ?  Suppose that . Find the vector that results from rotating by and find the dot product .  Suppose that and are two perpendicular vectors. What do you think their dot product is?          .    The length of is 5.     , which is the square of the length of .          .     .    The dot product should be zero.      "
},
{
  "id": "sec-dot-product-3-3",
  "level": "2",
  "url": "sec-dot-product.html#sec-dot-product-3-3",
  "type": "Example",
  "number": "22.1.2",
  "title": "",
  "body": "  We compute the dot product between two four-dimensional vectors as    "
},
{
  "id": "sec-dot-product-3-5",
  "level": "2",
  "url": "sec-dot-product.html#sec-dot-product-3-5",
  "type": "Example",
  "number": "22.1.3",
  "title": "",
  "body": "  Suppose that and . Then    "
},
{
  "id": "fig-dot-length",
  "level": "2",
  "url": "sec-dot-product.html#fig-dot-length",
  "type": "Figure",
  "number": "22.1.4",
  "title": "",
  "body": "    The vector .  "
},
{
  "id": "fig-dot-angle",
  "level": "2",
  "url": "sec-dot-product.html#fig-dot-angle",
  "type": "Figure",
  "number": "22.1.5",
  "title": "",
  "body": "    The dot product measures the angle .  "
},
{
  "id": "sec-dot-product-3-14",
  "level": "2",
  "url": "sec-dot-product.html#sec-dot-product-3-14",
  "type": "Activity",
  "number": "22.1.2",
  "title": "",
  "body": "     Sketch the vectors and using .      Sketch the vectors and here.    Find the lengths and using the dot product.  Find the dot product and use it to find the angle between and .  Consider the vector . Include it in your sketch in and find the angle between and .  If two vectors are perpendicular, what can you say about their dot product? Explain your thinking.  For what value of is the vector perpendicular to ?  Sage can be used to find lengths of vectors and their dot products. For instance, if v and w are vectors, then v.norm() gives the length of v and v * w gives .  Suppose that Use the Sage cell below to find , , , and the angle between and . You may use arccos to find the angle's measure expressed in radians.              We find that so that and .     so that      so that     If two vectors are perpendicular, then the angle between them is . Since , their dot product must be zero.    The dot product is so .    We find that , , , and the angle between these vectors is .                and .              Their dot product must be zero.     .     .      "
},
{
  "id": "sec-dot-product-3-16",
  "level": "2",
  "url": "sec-dot-product.html#sec-dot-product-3-16",
  "type": "Definition",
  "number": "22.1.7",
  "title": "",
  "body": "  orthogonal We say that vectors and are orthogonal if .  "
},
{
  "id": "fig-similar-vectors",
  "level": "2",
  "url": "sec-dot-product.html#fig-similar-vectors",
  "type": "Figure",
  "number": "22.1.8",
  "title": "",
  "body": "    Which of the vectors are most similar?  "
},
{
  "id": "sec-dot-product-3-20",
  "level": "2",
  "url": "sec-dot-product.html#sec-dot-product-3-20",
  "type": "Activity",
  "number": "22.1.3",
  "title": "",
  "body": "  This activity explores two further uses of the dot product beginning with the similarity of vectors.   Our first task is to assess the similarity between various Wikipedia articles by forming vectors from each of five articles. In particular, one may download the text from a Wikipedia article, remove common words, such as the and and , count the number of times the remaining words appear in the article, and represent these counts in a vector.  For example, evaluate the following cell that loads some special commands along with the vectors constructed from the Wikipedia articles on Veteran's Day, Memorial Day, Labor Day, the Golden Globe Awards, and the Super Bowl. For each of the five articles, you will see a list of the number of times 10 words appear in these articles. For instance, the word act appears 3 times in the Veteran's Day article and 0 times in the Labor Day article. For each of the five articles, we obtain 604-dimensional vectors, which are named veterans , memorial , labor , golden , and super .     Suppose that two articles have no words in common. What is the value of the dot product between their corresponding vectors? What does this say about the angle between these vectors?    Suppose there are two articles on the same subject, yet one article is twice as long. What approximate relationship would you expect to hold between the two vectors? What does this say about the angle between them?    Use the Sage cell below to find the angle between the vector veterans and the other four vectors. To express the angle in degrees, use the degrees(x) command, which gives the number of degrees in x radians.     Compare the four angles you have found and discuss what they mean about the similarity between the Veteran's Day article and the other four. How do your findings reflect the nature of these five events?       Vectors are often used to represent how a quantity changes over time. For instance, the vector might represent the value of a company's stock on four consecutive days. When interpreted in this way, we call the vector a time series. Evaluate the Sage cell below to see a representation of two time series , in blue, and , in orange, which we imagine represent the value of two stocks over a period of time. (This cell relies on some data loaded by the first cell in this activity.) Even though one stock has a higher value than the other, the two appear to be related since they seem to rise and fall at roughly similar ways. We often say that they are correlated , and we would like to measure the degree to which they are correlated.   In order to compare the ways in which they rise and fall, we will first demean the time series; that is, for each time series, we will subtract its average value to obtain a new time series. There is a command, demean(s) , that returns the demeaned time series of s . Use the Sage cell below to demean the series and and plot.     If the demeaned series are and , then the correlation between and is defined to be Given the geometric interpretation of the dot product, the correlation equals the cosine of the angle between the demeaned time series, and therefore is between -1 and 1.  Find the correlation between and .     Suppose that two time series are such that their demeaned time series are scalar multiples of one another, as in        On the left, the demeaned time series are positive scalar multiples of one another. On the right, they are negative scalar multiples.   For instance, suppose we have time series and whose demeaned time series and are positive scalar multiples of one another. What is the angle between the demeaned vectors? What does this say about the correlation ?    Suppose the demeaned time series and are negative scalar multiples of one another, what is the angle between the demeaned vectors? What does this say about the correlation ?    Use the Sage cell below to plot the time series and and find their correlation.     Use the Sage cell below to plot the time series and and find their correlation.                 If there are no words in common, then the dot product between the two vectors will be zero. This means that they are perpendicular to one another.    The vectors should be, at least approximately, scalar multiples of one another, which means that the angle between them is zero.    The angle veterans makes with memorial is , with labor is , with golden is , and with super is .    It appears that the articles on Veteran's Day and Memorial Day are most similar. This makes sense because both are U.S. national holidays that honor military service. The second most similar article is Labor Day, which is also a national holiday. The other two are quite dissimilar as they are entertainment events.          The graphs are now lowered so that their averages are zero.    The correlation is , which is quite close to 1.    The angle should be zero, which means that the correlation will be .    The angle should be , which means that the correlation should be .    The correlation is .    The correlation is .                They are perpendicular.    The angle should be close to 0.    The angle veterans makes with memorial is , with labor is , with golden is , and with super is .    The articles on Veteran's Day and Memorial Day are most similar.          The graphs are now lowered so that their averages are zero.                                  "
},
{
  "id": "fig-clusters",
  "level": "2",
  "url": "sec-dot-product.html#fig-clusters",
  "type": "Figure",
  "number": "22.1.10",
  "title": "",
  "body": "    A set of 177 data points.  "
},
{
  "id": "sec-dot-product-4-7",
  "level": "2",
  "url": "sec-dot-product.html#sec-dot-product-4-7",
  "type": "Activity",
  "number": "22.1.4",
  "title": "",
  "body": "  To begin, we identify the centroid , or the average, of a set of vectors as    Find the centroid of the vectors and sketch the vectors and the centroid using . You may wish to simply plot the points represented by the tips of the vectors rather than drawing the vectors themselves.      The vectors , , and their centroid.   Notice that the centroid lies in the center of the points defined by the vectors.    Now we'll illustrate an algorithm that forms clusterings. To begin, consider the following points, represented as vectors, which are shown in .      We will group this set of four points into two clusters.   Suppose that we would like to group these points into clusters. (Later on, we'll see how to choose an appropriate value for , the number of clusters.) We begin by choosing two points and at random and declaring them to be the centers ' of the two clusters.  For example, suppose we randomly choose and as the center of two clusters. The cluster centered on will be the set of points that are closer to than to . Determine which of the four data points are in this cluster, which we denote by , and circle them in .    The second cluster will consist of the data points that are closer to than . Determine which of the four points are in this cluster, which we denote by , and circle them in .    We now have a clustering with two clusters, but we will try to improve upon it in the following way. First, find the centroids of the two clusters; that is, redefine to be the centroid of cluster and to be the centroid of . Find those centroids and indicate them in       Indicate the new centroids and clusters.   Now update the cluster to be the set of points closer to than . Update the cluster in a similar way and indicate the clusters in .    Let's perform this last step again. That is, update the centroids and from the new clusters and then update the clusters and . Indicate your centroids and clusters in .      Indicate the new centroids and clusters.   Notice that this last step produces the same set of clusters so there is no point in repeating it. We declare this to be our final clustering.          The centroid is .       The first cluster is .    The second cluster is .    We redefine and . This leads to new clusters and .    We have new centroids and , and the clusters and are unchanged.          The centroid is .        .     .     and    and .     and . The clusters and are unchanged.      "
},
{
  "id": "sec-dot-product-4-13",
  "level": "2",
  "url": "sec-dot-product.html#sec-dot-product-4-13",
  "type": "Activity",
  "number": "22.1.5",
  "title": "",
  "body": "  We'll now use the objective to compare clusterings and to choose an appropriate value of .   In the previous activity, one initial choice of and led to the clustering: with centroids and . Find the objective of this clustering.    We have now seen two clusterings and computed their objectives. Recall that our dataset is shown in . Which of the two clusterings feels like the better fit? How is this fit reflected in the values of the objectives?    Evaluating the following cell will load and display a dataset consisting of 177 data points. This dataset has the name data . Given this plot of the data, what would seem like a reasonable number of clusters?    In the following cell, you may choose a value of and then run the algorithm to determine and display a clustering and its objective. If you run the algorithm a few times with the same value of , you will likely see different clusterings having different objectives. This is natural since our algorithm starts by making a random choice of points , and a different choices may lead to different clusterings. Choose a value of and run the algorithm a few times. Notice that clusterings having lower objectives seem to fit the data better. Repeat this experiment with a few different values of .     For a given value of , our strategy is to run the algorithm several times and choose the clustering with the smallest objective. After choosing a value of , the following cell will run the algorithm 10 times and display the clustering having the smallest objective.   For each value of between 2 and 9, find the clustering having the smallest objective and plot your findings in .      Construct a plot of the minimal objective as it depends on the choice of .   This plot is called an elbow plot due to its shape. Notice how the objective decreases sharply when is small and then flattens out. This leads to a location, called the elbow, where the objective transitions from being sharply decreasing to relatively flat. This means that increasing beyond the elbow does not significantly decrease the objective, which makes the elbow a good choice for .  Where does the elbow occur in your plot above? How does this compare to the best value of that you estimated by simply looking at the data in .     Of course, we could increase until each data point is its own cluster. However, this defeats the point of the technique, which is to group together nearby data points in the hope that they share common features, thus providing insight into the structure of the data.       The objective is     The clustering with and appears to be a tighter clustering and has a smaller objective.    It appears that the best clustering is either or .    With a fixed value of , running the algorithm several times leads to different clusterings with different objectives. If we increase , the objective generally decreases.    The elbow occurs around or , which are the values that we felt led to the best clusterings.             The objective is .    The clustering and has a smaller objective.     or .    With a fixed value of , running the algorithm several times leads to different clusterings with different objectives. If we increase , the objective generally decreases.    The elbow occurs around or .         "
},
{
  "id": "sec-dot-product-6-1",
  "level": "2",
  "url": "sec-dot-product.html#sec-dot-product-6-1",
  "type": "Exercise",
  "number": "22.1.4.1",
  "title": "",
  "body": " Consider the vectors    Find the lengths of the vectors, and .    Find the dot product and use it to find the angle between and .          , , , and .     and .         We have and so that and .    Since , we find that .     "
},
{
  "id": "sec-dot-product-6-2",
  "level": "2",
  "url": "sec-dot-product.html#sec-dot-product-6-2",
  "type": "Exercise",
  "number": "22.1.4.2",
  "title": "",
  "body": " Consider the three vectors    Find the dot products , , and .    Use the dot products you just found to evaluate:    .     .     .     .       For what value of is orthogonal to ?          , , and .                                    , , and .                               "
},
{
  "id": "sec-dot-product-6-3",
  "level": "2",
  "url": "sec-dot-product.html#sec-dot-product-6-3",
  "type": "Exercise",
  "number": "22.1.4.3",
  "title": "",
  "body": " Suppose that and are vectors where    What is ?    What is the angle between and ?    Suppose that is a scalar. Find the value of for which is orthogonal to ?                    .                    so .     "
},
{
  "id": "sec-dot-product-6-4",
  "level": "2",
  "url": "sec-dot-product.html#sec-dot-product-6-4",
  "type": "Exercise",
  "number": "22.1.4.4",
  "title": "",
  "body": " Suppose that .   What is the relationship between and ?    What is the relationship between and ?    If for some scalar , what is the relationship between and ? What is the relationship between and ?    Suppose that . Find a scalar so that has length 1.                                             so that     We know so      "
},
{
  "id": "sec-dot-product-6-5",
  "level": "2",
  "url": "sec-dot-product.html#sec-dot-product-6-5",
  "type": "Exercise",
  "number": "22.1.4.5",
  "title": "",
  "body": " Given vectors and , explain why Sketch two vectors and and explain why this fact is called the parallelogram law .   Use the relationship .            "
},
{
  "id": "sec-dot-product-6-6",
  "level": "2",
  "url": "sec-dot-product.html#sec-dot-product-6-6",
  "type": "Exercise",
  "number": "22.1.4.6",
  "title": "",
  "body": " Consider the vectors and a general vector .   Write an equation in terms of , , and that describes all the vectors orthogonal to .    Write a linear system that describes all the vectors orthogonal to both and .    Write the solution set to this linear system in parametric form. What type of geometric object does this solution set represent? Indicate with a rough sketch why this makes sense.    Give a parametric description of all vectors orthogonal to . What type of geometric object does this represent? Indicate with a rough sketch why this makes sense.                                             , which describes a line     , which describes a plane.     "
},
{
  "id": "sec-dot-product-6-7",
  "level": "2",
  "url": "sec-dot-product.html#sec-dot-product-6-7",
  "type": "Exercise",
  "number": "22.1.4.7",
  "title": "",
  "body": " Explain your responses to these questions.   Suppose that is orthogonal to both and . Can you guarantee that is also orthogonal to any linear combination ?    Suppose that is orthogonal to itself. What can you say about ?         Yes              Yes, because      so      "
},
{
  "id": "sec-dot-product-6-8",
  "level": "2",
  "url": "sec-dot-product.html#sec-dot-product-6-8",
  "type": "Exercise",
  "number": "22.1.4.8",
  "title": "",
  "body": " Suppose that , , and form a basis for and that each vector is orthogonal to the other two. Suppose also that is another vector in .   Explain why for some scalars , , and .    Beginning with the expression apply the distributive property of dot products to explain why Find similar expressions for and .    Verify that form a basis for and that each vector is orthogonal to the other two. Use what you've discovered in this problem to write the vector as a linear combination of , , and .         Since , , and form a basis for , any vector in can be written as a linear combination of them.    Apply the distributive property.   and               Since , , and form a basis for , any vector in can be written as a linear combination of them.     so that .  In the same way, and     Check that the vectors are orthogonal by computing their dot products. Then by computing the ratios of dot products.     "
},
{
  "id": "sec-dot-product-6-9",
  "level": "2",
  "url": "sec-dot-product.html#sec-dot-product-6-9",
  "type": "Exercise",
  "number": "22.1.4.9",
  "title": "",
  "body": " Suppose that , , and are three nonzero vectors that are pairwise orthogonal; that is, each vector is orthogonal to the other two.   Explain why cannot be a linear combination of and .    Explain why this set of three vectors is linearly independent.         If , then and .    None of the vectors is a linear combination of the others.         If , then . In the same way, so must be the zero vector. We're told, however, that is nonzero.    We've seen that is not a linear combination of and . The same thinking shows that none of the vectors is a linear combination of the others so they form a linearly independent set.     "
},
{
  "id": "sec-dot-product-6-10",
  "level": "2",
  "url": "sec-dot-product.html#sec-dot-product-6-10",
  "type": "Exercise",
  "number": "22.1.4.10",
  "title": "",
  "body": " In the next chapter, we will consider certain matrices and define a function where is a vector in .   Suppose that and . Evaluate .    For a general vector , evaluate as an expression involving and .    Suppose that is an eigenvector of a matrix with associated eigenvalue and that has length 1. What is the value of the function ?                                        .     "
},
{
  "id": "sec-dot-product-6-11",
  "level": "2",
  "url": "sec-dot-product.html#sec-dot-product-6-11",
  "type": "Exercise",
  "number": "22.1.4.11",
  "title": "",
  "body": " Back in , we saw that equations of the form represent lines in the plane. In this exercise, we will see how this expression arises geometrically.     A line, a point on the line, and a vector perpendicular to the line.     Find the slope and vertical intercept of the line shown in . Then write an equation for the line in the form .    Suppose that is a point on the line, that is a vector perpendicular to the line, and that is a general point on the line. Sketch the vector and describe the angle between this vector and the vector .    What is the value of the dot product ?    Explain why the equation of the line can be written in the form .    Identify the vectors and for the line illustrated in and use them to write the equation of the line in terms of and . Verify that this expression is algebraically equivalent to the equation that you earlier found for this line.    Explain why any line in the plane can be described by an equation having the form . What is the significance of the vector ?          .     is orthogonal to .         Apply the distributive property          is perpendicular to the line.         The slope and the intercept so .    The vector is in the direction of the line so it is orthogonal to .    Since these vectors are orthogonal, their dot product is .    Since , we have , which implies that .     and . This gives . We can rearrange this to have the form .    If we choose a vector that is perpendicular to the line and a point on the line, we have .     "
},
{
  "id": "sec-transpose",
  "level": "1",
  "url": "sec-transpose.html",
  "type": "Section",
  "number": "22.2",
  "title": "Orthogonal complements and the matrix transpose",
  "body": " Orthogonal complements and the matrix transpose   We've now seen how the dot product enables us to determine the angle between two vectors and, more specifically, when two vectors are orthogonal. Moving forward, we will explore how the orthogonality condition simplifies many common tasks, such as expressing a vector as a linear combination of a given set of vectors.  This section introduces the notion of an orthogonal complement, the set of vectors each of which is orthogonal to a prescribed subspace. We'll also find a way to describe dot products using matrix products, which allows us to study orthogonality using many of the tools for understanding linear systems that we developed earlier.     Sketch the vector on and one vector that is orthogonal to it.     Sketch the vector and one vector orthogonal to it.    If a vector is orthogonal to , what do we know about the dot product ?  If we write , use the dot product to write an equation for the vectors orthogonal to in terms of and .  Use this equation to sketch the set of all vectors orthogonal to in .   introduced the column space and null space of a matrix . If is a matrix, what is the meaning of the null space ?  What is the meaning of the column space ?        The vector is an example of a vector orthogonal to .    The dot product must be zero.     .    This is the line .    It is the set of vectors for which .    It is the set of vector for which the equation is consistent.         Orthogonal complements  The preview activity presented us with a vector and led us through the process of describing all the vectors orthogonal to . Notice that the set of scalar multiples of describes a line , a 1-dimensional subspace of . We then described a second line consisting of all the vectors orthogonal to . Notice that every vector on this line is orthogonal to every vector on the line . We call this new line the orthogonal complement of and denote it by . The lines and are illustrated on the left of .       On the left is a line and its orthogonal complement . On the right is a plane and its orthogonal complement in .   The next definition places this example into a more general context.   orthogonal complement   Given a subspace of , the orthogonal complement of is the set of vectors in each of which is orthogonal to every vector in . We denote the orthogonal complement by .    A typical example appears on the right of . Here we see a plane , a two-dimensional subspace of , and its orthogonal complement , which is a line in .  As the next activity demonstrates, the orthogonal complement of a subspace is itself a subspace of .    Suppose that and form a basis for , a two-dimensional subspace of . We will find a description of the orthogonal complement .   Suppose that the vector is orthogonal to . If we write , use the fact that to write a linear equation for , , and .    Suppose that is also orthogonal to . In the same way, write a linear equation for , , and that arises from the fact that .    If is orthogonal to both and , these two equations give us a linear system for some matrix . Identify the matrix and write a parametric description of the solution space to the equation .    Since and form a basis for the two-dimensional subspace , any vector in can be written as a linear combination If is orthogonal to both and , use the distributive property of dot products to explain why is orthogonal to .    Give a basis for the orthogonal complement and state the dimension .    Describe , the orthogonal complement of .          We have the equation .    We have the equation .    These two equations give where whose solutions have the parametric form .    By distributivity, .     is the solution space to the equation . Therefore, a basis consists of the single vector , and is one-dimensional.    Since every vector in is orthogonal to every vector in , the orthogonal complement of is .           .     .     so .    By distributivity, .    A basis consists of , and is one-dimensional.    The orthogonal complement of is .         If is the line defined by in , we will describe the orthogonal complement , the set of vectors orthogonal to .  If is orthogonal to , it must be orthogonal to so we have   We can describe the solutions to this equation parametrically as Therefore, the orthogonal complement is a plane, a two-dimensional subspace of , spanned by the vectors and .      Suppose that is the -dimensional subspace of with basis We will give a description of the orthogonal complement .  If is in , we know that is orthogonal to both and . Therefore, In other words, where The solutions may be described parametrically as The distributive property of dot products implies that any vector that is orthogonal to both and is also orthogonal to any linear combination of and since Therefore, is a -dimensional subspace of with basis One may check that the vectors , , and are each orthogonal to both and .      The matrix transpose  The previous activity and examples show how we can describe the orthogonal complement of a subspace as the solution set of a particular linear system. We will make this connection more explicit by defining a new matrix operation called the transpose .   transpose   The transpose of the matrix is the matrix whose rows are the columns of .      If , then      This activity illustrates how multiplying a vector by is related to computing dot products with the columns of . You'll develop a better understanding of this relationship if you compute the dot products and matrix products in this activity without using technology.  If , write the matrix .  Suppose that Find the dot products and .  Now write the matrix and its transpose . Find the product and describe how this product computes both dot products and .  Suppose that is a vector that is orthogonal to both and . What does this say about the dot products and ? What does this say about the product ?  Use the matrix to give a parametric description of all the vectors that are orthogonal to and .   Remember that , the null space of , is the solution set of the equation . If is a vector in , explain why must be orthogonal to both and .    Remember that , the column space of , is the set of linear combinations of the columns of . Therefore, any vector in can be written as . If is a vector in , explain why is orthogonal to every vector in .                         Both dot products are 0 so we have .    We need to solve the equation so we find the reduced row echelon form The vectors orthogonal to both and have the form .     tells us that and .    Since is orthogonal to both and , we have                 , .          .     .     .    Apply the distributive property of dot products.       The previous activity demonstrates an important connection between the matrix transpose and dot products. More specifically, the components of the product are simply the dot products of the columns of with . We will make frequent use of this observation so let's record it as a proposition.    If is the matrix whose columns are , then       Suppose that is a subspace of having basis and that we wish to describe the orthogonal complement .  If is the matrix and is in , we have Describing vectors that are orthogonal to both and is therefore equivalent to the more familiar task of describing the solution set . To do so, we find the reduced row echelon form of and write the solution set parametrically as Once again, the distributive property of dot products tells us that such a vector is also orthogonal to any linear combination of and so this solution set is, in fact, the orthogonal complement . Indeed, we see that the vectors form a basis for , which is a two-dimensional subspace of .    To place this example in a slightly more general context, note that and , the columns of , form a basis of . Since , the column space of is the subspace of linear combinations of the columns of , we have .  This example also shows that the orthogonal complement is described by the solution set of . This solution set is what we have called , the null space of . In this way, we see the following proposition, which is visually represented in .    For any matrix , the orthogonal complement of is ; that is,         The orthogonal complement of the column space of is the null space of .     Properties of the matrix transpose  The transpose is a simple algebraic operation performed on a matrix. The next activity explores some of its properties.    In Sage, the transpose of a matrix A is given by A.T . Define the matrices    Evaluate and . What do you notice about the relationship between these two matrices?  What happens if you transpose a matrix twice; that is, what is ?  Find and . What do you notice about the relationship between these determinants?    Find the product and its transpose .  Is it possible to compute the product ? Explain why or why not.  Find the product and compare it to . What do you notice about the relationship between these two matrices?    What is the transpose of the identity matrix ?  If a square matrix is invertible, explain why you can guarantee that is invertible and why .         .     .             and     The product is not defined because has two columns and has three rows.                 We have so . This means that .           .     .          .          .       In spite of the fact that we are looking at some specific examples, this activity demonstrates the following general properties of the transpose, which may be verified with a little effort.   Properties of the transpose  Here are some properties of the matrix transpose, expressed in terms of general matrices , , and . We assume that is a square matrix.  If is defined, then .   .   .   .  If is defined, then . Notice that the order of the multiplication is reversed.  If is invertible, then .    There is one final property we wish to record though we will wait until to explain why it is true.   For any matrix , we have    This proposition is important because it implies a relationship between the dimensions of a subspace and its orthogonal complement. For instance, if is an matrix, we saw in that and .  Now suppose that is an -dimensional subspace of with basis . If we form the matrix , then so that   The transpose is an matrix having . Since , we have This explains the following proposition.    If is a subspace of , then       In , we constructed the orthogonal complement of a line in . The dimension of the orthogonal complement should be , which explains why we found the orthogonal complement to be a plane.      In , we looked at , a -dimensional subspace of and found its orthogonal complement to be a -dimensional subspace of .         Suppose that is a -dimensional subspace of and that is a matrix whose columns form a basis for ; that is, .  What is the shape of ?  What is the rank of ?  What is the shape of ?  What is the rank of ?  What is ?  What is ?  How are the dimensions of and related?      Suppose that is a subspace of having basis    Find the dimensions and .    Find a basis for . It may be helpful to know that the Sage command A.right_kernel() produces a basis for .     Verify that each of the basis vectors you found for are orthogonal to the basis vectors for .                 is .     .     is .                    since the subspaces live in .           so .    A basis is and .    You can verify by computing the four dot products.                                                               and .    Verify by computing the four dot products.            Summary  This section introduced the matrix transpose, its connection to dot products, and its use in describing the orthogonal complement of a subspace.   The columns of the matrix are the rows of the matrix transpose .    The components of the product are the dot products of with the columns of .    The orthogonal complement of the column space of equals the null space of ; that is, .    If is a subspace of , then         Suppose that is a subspace of with basis    What are the dimensions and ?    Find a basis for .    Verify that each of the basis vectors for are orthogonal to and .          and .     and     Use the dot product.          and .     and     Verify that for all and .       Consider the matrix .   Find and a basis for .    Determine the dimension of and find a basis for it.          and a basis for is and      with basis .          so since there are two pivot positions. The reduced row echelon form shows that the third column is a linear combination of the first two so and form a basis for .    We know that . To find a basis, solve the equation to obtain .       Suppose that is the subspace of defined as the solution set of the equation    What are the dimensions and ?    Find a basis for .    Find a basis for .    In general, how can you easily find a basis for when is defined by           , .     , , and .                    where so . Therefore, .    A basis for is , , and .    Since every vector in satisfies , a basis for is             Determine whether the following statements are true or false and explain your reasoning.   If , then is in .    If is a matrix and is a matrix, then is a matrix.    If the columns of are , , and and , then is orthogonal to .    If is a matrix with , then is a line in .    If is a matrix with , then .         True    False    True    True    False         True, since , is in .    False, is a matrix, but it is given by .    True, because the second component of .    True, because .    False, .       Apply properties of matrix operations to simplify the following expressions.                                                                            A symmetric matrix is one for which .   Explain why a symmetric matrix must be square.    If and are general matrices and is a square diagonal matrix, which of the following matrices can you guarantee are symmetric?              .                 They must have the same number of rows and columns.       Yes    No    No    Yes            If is an matrix, then is If these matrices are the same, then .       If is diagonal, then .     so this matrix need not be symmetric.     so this matrix is symmetric.     so this matrix is symmetric.          If is a square matrix, remember that the characteristic polynomial of is and that the roots of the characteristic polynomial are the eigenvalues of .   Explain why and have the same characteristic polynomial.    Explain why and have the same set of eigenvalues.    Suppose that is diagonalizable with diagonalization . Explain why is diagonalizable and find a diagonalization.         Use properties of the matrix transpose.    Because they have the same characteristic polynomial.               and therefore .    The eigenvalues of a matrix are given by the roots of its characteristic polynomial. Since and have the same characteristic polynomial, they have the same eigenvalues.            This exercise introduces a version of the Pythagorean theorem that we'll use later.   Suppose that and are orthogonal to one another. Use the dot product to explain why     Suppose that is a subspace of and that is a vector in for which where is in and is in . Explain why which is an expression of the Pythagorean theorem.              Because and are orthogonal.              Because and are orthogonal.       In the next chapter, symmetric matrices---that is, matrices for which ---play an important role. It turns out that eigenvectors of a symmetric matrix that are associated to different eigenvalues are orthogonal. We will explain this fact in this exercise.   Viewing a vector as a matrix having one column, we may write . If is a matrix, explain why .    We have seen that the matrix has eigenvectors , with associated eigenvalue , and , with associated eigenvalue . Verify that is symmetric and that and are orthogonal.    Suppose that is a general symmetric matrix and that is an eigenvector associated to eigenvalue and that is an eigenvector associated to a different eigenvalue . Beginning with , apply the identity from the first part of this exercise to explain why and are orthogonal.         Use properties of the transpose.    Compute the dot product of and .    It follows that           .     and .     so since .       Given an matrix , the row space of is the column space of ; that is, .   Suppose that is a matrix. For what is a subspace of ?    How can help us describe ?    Suppose that . Find bases for and .                   A basis for are the three rows of . A basis for is .         Since is , it follows that is a subspace of .          so . A basis for are the three rows of . is one-dimensional with basis .       "
},
{
  "id": "sec-transpose-2-3",
  "level": "2",
  "url": "sec-transpose.html#sec-transpose-2-3",
  "type": "Preview Activity",
  "number": "22.2.1",
  "title": "",
  "body": "   Sketch the vector on and one vector that is orthogonal to it.     Sketch the vector and one vector orthogonal to it.    If a vector is orthogonal to , what do we know about the dot product ?  If we write , use the dot product to write an equation for the vectors orthogonal to in terms of and .  Use this equation to sketch the set of all vectors orthogonal to in .   introduced the column space and null space of a matrix . If is a matrix, what is the meaning of the null space ?  What is the meaning of the column space ?        The vector is an example of a vector orthogonal to .    The dot product must be zero.     .    This is the line .    It is the set of vectors for which .    It is the set of vector for which the equation is consistent.      "
},
{
  "id": "fig-orthog-comps",
  "level": "2",
  "url": "sec-transpose.html#fig-orthog-comps",
  "type": "Figure",
  "number": "22.2.2",
  "title": "",
  "body": "     On the left is a line and its orthogonal complement . On the right is a plane and its orthogonal complement in .  "
},
{
  "id": "sec-transpose-3-5",
  "level": "2",
  "url": "sec-transpose.html#sec-transpose-3-5",
  "type": "Definition",
  "number": "22.2.3",
  "title": "",
  "body": " orthogonal complement   Given a subspace of , the orthogonal complement of is the set of vectors in each of which is orthogonal to every vector in . We denote the orthogonal complement by .   "
},
{
  "id": "sec-transpose-3-8",
  "level": "2",
  "url": "sec-transpose.html#sec-transpose-3-8",
  "type": "Activity",
  "number": "22.2.2",
  "title": "",
  "body": "  Suppose that and form a basis for , a two-dimensional subspace of . We will find a description of the orthogonal complement .   Suppose that the vector is orthogonal to . If we write , use the fact that to write a linear equation for , , and .    Suppose that is also orthogonal to . In the same way, write a linear equation for , , and that arises from the fact that .    If is orthogonal to both and , these two equations give us a linear system for some matrix . Identify the matrix and write a parametric description of the solution space to the equation .    Since and form a basis for the two-dimensional subspace , any vector in can be written as a linear combination If is orthogonal to both and , use the distributive property of dot products to explain why is orthogonal to .    Give a basis for the orthogonal complement and state the dimension .    Describe , the orthogonal complement of .          We have the equation .    We have the equation .    These two equations give where whose solutions have the parametric form .    By distributivity, .     is the solution space to the equation . Therefore, a basis consists of the single vector , and is one-dimensional.    Since every vector in is orthogonal to every vector in , the orthogonal complement of is .           .     .     so .    By distributivity, .    A basis consists of , and is one-dimensional.    The orthogonal complement of is .      "
},
{
  "id": "example-orthog-comp-line",
  "level": "2",
  "url": "sec-transpose.html#example-orthog-comp-line",
  "type": "Example",
  "number": "22.2.4",
  "title": "",
  "body": "  If is the line defined by in , we will describe the orthogonal complement , the set of vectors orthogonal to .  If is orthogonal to , it must be orthogonal to so we have   We can describe the solutions to this equation parametrically as Therefore, the orthogonal complement is a plane, a two-dimensional subspace of , spanned by the vectors and .   "
},
{
  "id": "example-orthog-comp-gen",
  "level": "2",
  "url": "sec-transpose.html#example-orthog-comp-gen",
  "type": "Example",
  "number": "22.2.5",
  "title": "",
  "body": "  Suppose that is the -dimensional subspace of with basis We will give a description of the orthogonal complement .  If is in , we know that is orthogonal to both and . Therefore, In other words, where The solutions may be described parametrically as The distributive property of dot products implies that any vector that is orthogonal to both and is also orthogonal to any linear combination of and since Therefore, is a -dimensional subspace of with basis One may check that the vectors , , and are each orthogonal to both and .   "
},
{
  "id": "sec-transpose-4-3",
  "level": "2",
  "url": "sec-transpose.html#sec-transpose-4-3",
  "type": "Definition",
  "number": "22.2.6",
  "title": "",
  "body": " transpose   The transpose of the matrix is the matrix whose rows are the columns of .   "
},
{
  "id": "sec-transpose-4-4",
  "level": "2",
  "url": "sec-transpose.html#sec-transpose-4-4",
  "type": "Example",
  "number": "22.2.7",
  "title": "",
  "body": "  If , then   "
},
{
  "id": "sec-transpose-4-5",
  "level": "2",
  "url": "sec-transpose.html#sec-transpose-4-5",
  "type": "Activity",
  "number": "22.2.3",
  "title": "",
  "body": "  This activity illustrates how multiplying a vector by is related to computing dot products with the columns of . You'll develop a better understanding of this relationship if you compute the dot products and matrix products in this activity without using technology.  If , write the matrix .  Suppose that Find the dot products and .  Now write the matrix and its transpose . Find the product and describe how this product computes both dot products and .  Suppose that is a vector that is orthogonal to both and . What does this say about the dot products and ? What does this say about the product ?  Use the matrix to give a parametric description of all the vectors that are orthogonal to and .   Remember that , the null space of , is the solution set of the equation . If is a vector in , explain why must be orthogonal to both and .    Remember that , the column space of , is the set of linear combinations of the columns of . Therefore, any vector in can be written as . If is a vector in , explain why is orthogonal to every vector in .                         Both dot products are 0 so we have .    We need to solve the equation so we find the reduced row echelon form The vectors orthogonal to both and have the form .     tells us that and .    Since is orthogonal to both and , we have                 , .          .     .     .    Apply the distributive property of dot products.      "
},
{
  "id": "prop-transpose-multiplication",
  "level": "2",
  "url": "sec-transpose.html#prop-transpose-multiplication",
  "type": "Proposition",
  "number": "22.2.8",
  "title": "",
  "body": "  If is the matrix whose columns are , then    "
},
{
  "id": "sec-transpose-4-8",
  "level": "2",
  "url": "sec-transpose.html#sec-transpose-4-8",
  "type": "Example",
  "number": "22.2.9",
  "title": "",
  "body": "  Suppose that is a subspace of having basis and that we wish to describe the orthogonal complement .  If is the matrix and is in , we have Describing vectors that are orthogonal to both and is therefore equivalent to the more familiar task of describing the solution set . To do so, we find the reduced row echelon form of and write the solution set parametrically as Once again, the distributive property of dot products tells us that such a vector is also orthogonal to any linear combination of and so this solution set is, in fact, the orthogonal complement . Indeed, we see that the vectors form a basis for , which is a two-dimensional subspace of .   "
},
{
  "id": "prop-col-orthog",
  "level": "2",
  "url": "sec-transpose.html#prop-col-orthog",
  "type": "Proposition",
  "number": "22.2.10",
  "title": "",
  "body": "  For any matrix , the orthogonal complement of is ; that is,    "
},
{
  "id": "fig-orthog-comp",
  "level": "2",
  "url": "sec-transpose.html#fig-orthog-comp",
  "type": "Figure",
  "number": "22.2.11",
  "title": "",
  "body": "    The orthogonal complement of the column space of is the null space of .  "
},
{
  "id": "sec-transpose-5-3",
  "level": "2",
  "url": "sec-transpose.html#sec-transpose-5-3",
  "type": "Activity",
  "number": "22.2.4",
  "title": "",
  "body": "  In Sage, the transpose of a matrix A is given by A.T . Define the matrices    Evaluate and . What do you notice about the relationship between these two matrices?  What happens if you transpose a matrix twice; that is, what is ?  Find and . What do you notice about the relationship between these determinants?    Find the product and its transpose .  Is it possible to compute the product ? Explain why or why not.  Find the product and compare it to . What do you notice about the relationship between these two matrices?    What is the transpose of the identity matrix ?  If a square matrix is invertible, explain why you can guarantee that is invertible and why .         .     .             and     The product is not defined because has two columns and has three rows.                 We have so . This means that .           .     .          .          .      "
},
{
  "id": "prop-col-row-rank",
  "level": "2",
  "url": "sec-transpose.html#prop-col-row-rank",
  "type": "Proposition",
  "number": "22.2.12",
  "title": "",
  "body": " For any matrix , we have   "
},
{
  "id": "prop-orthog-dim",
  "level": "2",
  "url": "sec-transpose.html#prop-orthog-dim",
  "type": "Proposition",
  "number": "22.2.13",
  "title": "",
  "body": "  If is a subspace of , then    "
},
{
  "id": "sec-transpose-5-12",
  "level": "2",
  "url": "sec-transpose.html#sec-transpose-5-12",
  "type": "Example",
  "number": "22.2.14",
  "title": "",
  "body": "  In , we constructed the orthogonal complement of a line in . The dimension of the orthogonal complement should be , which explains why we found the orthogonal complement to be a plane.   "
},
{
  "id": "sec-transpose-5-13",
  "level": "2",
  "url": "sec-transpose.html#sec-transpose-5-13",
  "type": "Example",
  "number": "22.2.15",
  "title": "",
  "body": "  In , we looked at , a -dimensional subspace of and found its orthogonal complement to be a -dimensional subspace of .   "
},
{
  "id": "sec-transpose-5-14",
  "level": "2",
  "url": "sec-transpose.html#sec-transpose-5-14",
  "type": "Activity",
  "number": "22.2.5",
  "title": "",
  "body": "     Suppose that is a -dimensional subspace of and that is a matrix whose columns form a basis for ; that is, .  What is the shape of ?  What is the rank of ?  What is the shape of ?  What is the rank of ?  What is ?  What is ?  How are the dimensions of and related?      Suppose that is a subspace of having basis    Find the dimensions and .    Find a basis for . It may be helpful to know that the Sage command A.right_kernel() produces a basis for .     Verify that each of the basis vectors you found for are orthogonal to the basis vectors for .                 is .     .     is .                    since the subspaces live in .           so .    A basis is and .    You can verify by computing the four dot products.                                                               and .    Verify by computing the four dot products.         "
},
{
  "id": "sec-transpose-7-1",
  "level": "2",
  "url": "sec-transpose.html#sec-transpose-7-1",
  "type": "Exercise",
  "number": "22.2.5.1",
  "title": "",
  "body": " Suppose that is a subspace of with basis    What are the dimensions and ?    Find a basis for .    Verify that each of the basis vectors for are orthogonal to and .          and .     and     Use the dot product.          and .     and     Verify that for all and .     "
},
{
  "id": "sec-transpose-7-2",
  "level": "2",
  "url": "sec-transpose.html#sec-transpose-7-2",
  "type": "Exercise",
  "number": "22.2.5.2",
  "title": "",
  "body": " Consider the matrix .   Find and a basis for .    Determine the dimension of and find a basis for it.          and a basis for is and      with basis .          so since there are two pivot positions. The reduced row echelon form shows that the third column is a linear combination of the first two so and form a basis for .    We know that . To find a basis, solve the equation to obtain .     "
},
{
  "id": "sec-transpose-7-3",
  "level": "2",
  "url": "sec-transpose.html#sec-transpose-7-3",
  "type": "Exercise",
  "number": "22.2.5.3",
  "title": "",
  "body": " Suppose that is the subspace of defined as the solution set of the equation    What are the dimensions and ?    Find a basis for .    Find a basis for .    In general, how can you easily find a basis for when is defined by           , .     , , and .                    where so . Therefore, .    A basis for is , , and .    Since every vector in satisfies , a basis for is           "
},
{
  "id": "sec-transpose-7-4",
  "level": "2",
  "url": "sec-transpose.html#sec-transpose-7-4",
  "type": "Exercise",
  "number": "22.2.5.4",
  "title": "",
  "body": " Determine whether the following statements are true or false and explain your reasoning.   If , then is in .    If is a matrix and is a matrix, then is a matrix.    If the columns of are , , and and , then is orthogonal to .    If is a matrix with , then is a line in .    If is a matrix with , then .         True    False    True    True    False         True, since , is in .    False, is a matrix, but it is given by .    True, because the second component of .    True, because .    False, .     "
},
{
  "id": "sec-transpose-7-5",
  "level": "2",
  "url": "sec-transpose.html#sec-transpose-7-5",
  "type": "Exercise",
  "number": "22.2.5.5",
  "title": "",
  "body": " Apply properties of matrix operations to simplify the following expressions.                                                                          "
},
{
  "id": "sec-transpose-7-6",
  "level": "2",
  "url": "sec-transpose.html#sec-transpose-7-6",
  "type": "Exercise",
  "number": "22.2.5.6",
  "title": "",
  "body": " A symmetric matrix is one for which .   Explain why a symmetric matrix must be square.    If and are general matrices and is a square diagonal matrix, which of the following matrices can you guarantee are symmetric?              .                 They must have the same number of rows and columns.       Yes    No    No    Yes            If is an matrix, then is If these matrices are the same, then .       If is diagonal, then .     so this matrix need not be symmetric.     so this matrix is symmetric.     so this matrix is symmetric.        "
},
{
  "id": "sec-transpose-7-7",
  "level": "2",
  "url": "sec-transpose.html#sec-transpose-7-7",
  "type": "Exercise",
  "number": "22.2.5.7",
  "title": "",
  "body": " If is a square matrix, remember that the characteristic polynomial of is and that the roots of the characteristic polynomial are the eigenvalues of .   Explain why and have the same characteristic polynomial.    Explain why and have the same set of eigenvalues.    Suppose that is diagonalizable with diagonalization . Explain why is diagonalizable and find a diagonalization.         Use properties of the matrix transpose.    Because they have the same characteristic polynomial.               and therefore .    The eigenvalues of a matrix are given by the roots of its characteristic polynomial. Since and have the same characteristic polynomial, they have the same eigenvalues.          "
},
{
  "id": "sec-transpose-7-8",
  "level": "2",
  "url": "sec-transpose.html#sec-transpose-7-8",
  "type": "Exercise",
  "number": "22.2.5.8",
  "title": "",
  "body": " This exercise introduces a version of the Pythagorean theorem that we'll use later.   Suppose that and are orthogonal to one another. Use the dot product to explain why     Suppose that is a subspace of and that is a vector in for which where is in and is in . Explain why which is an expression of the Pythagorean theorem.              Because and are orthogonal.              Because and are orthogonal.     "
},
{
  "id": "sec-transpose-7-9",
  "level": "2",
  "url": "sec-transpose.html#sec-transpose-7-9",
  "type": "Exercise",
  "number": "22.2.5.9",
  "title": "",
  "body": " In the next chapter, symmetric matrices---that is, matrices for which ---play an important role. It turns out that eigenvectors of a symmetric matrix that are associated to different eigenvalues are orthogonal. We will explain this fact in this exercise.   Viewing a vector as a matrix having one column, we may write . If is a matrix, explain why .    We have seen that the matrix has eigenvectors , with associated eigenvalue , and , with associated eigenvalue . Verify that is symmetric and that and are orthogonal.    Suppose that is a general symmetric matrix and that is an eigenvector associated to eigenvalue and that is an eigenvector associated to a different eigenvalue . Beginning with , apply the identity from the first part of this exercise to explain why and are orthogonal.         Use properties of the transpose.    Compute the dot product of and .    It follows that           .     and .     so since .     "
},
{
  "id": "sec-transpose-7-10",
  "level": "2",
  "url": "sec-transpose.html#sec-transpose-7-10",
  "type": "Exercise",
  "number": "22.2.5.10",
  "title": "",
  "body": " Given an matrix , the row space of is the column space of ; that is, .   Suppose that is a matrix. For what is a subspace of ?    How can help us describe ?    Suppose that . Find bases for and .                   A basis for are the three rows of . A basis for is .         Since is , it follows that is a subspace of .          so . A basis for are the three rows of . is one-dimensional with basis .     "
},
{
  "id": "sec-orthogonal-bases",
  "level": "1",
  "url": "sec-orthogonal-bases.html",
  "type": "Section",
  "number": "22.3",
  "title": "Orthogonal bases and projections",
  "body": " Orthogonal bases and projections   We know that a linear system is inconsistent when is not in , the column space of . Later in this chapter, we'll develop a strategy for dealing with inconsistent systems by finding , the vector in that minimizes the distance to . The equation is therefore consistent and its solution set can provide us with useful information about the original system .  In this section and the next, we'll develop some techniques that enable us to find , the vector in a given subspace that is closest to a given vector .    For this activity, it will be helpful to recall the distributive property of dot products: We'll work with the basis of formed by the vectors    Verify that the vectors and are orthogonal.    Suppose that and find the dot products and .    We would like to express as a linear combination of and , which means that we need to find weights and such that To find the weight , dot both sides of this expression with : and apply the distributive property.    In a similar fashion, find the weight .    Verify that using the weights you have found.          We can compute that .     and .     .     .     .       We frequently ask to write a given vector as a linear combination of given basis vectors. In the past, we have done this by solving a linear system. The preview activity illustrates how this task can be simplified when the basis vectors are orthogonal to each other. We'll explore this and other uses of orthogonal bases in this section.    Orthogonal sets  The preview activity dealt with a basis of formed by two orthogonal vectors. More generally, we will consider a set of orthogonal vectors, as described in the next definition.   orthogonal set   By an orthogonal set of vectors, we mean a set of nonzero vectors each of which is orthogonal to the others.      The 3-dimensional vectors form an orthogonal set, which can be verified by computing Notice that this set of vectors forms a basis for .      The vectors form an orthogonal set of 4-dimensional vectors. Since there are only three vectors, this set does not form a basis for . It does, however, form a basis for a 3-dimensional subspace of .    Suppose that a vector is a linear combination of an orthogonal set of vectors ; that is, suppose that Just as in the preview activity, we can find the weight by dotting both sides with and applying the distributive property of dot products: Notice how the presence of an orthogonal set causes most of the terms in the sum to vanish. In the same way, we find that so that   We'll record this fact in the following proposition.    If a vector is a linear combination of an orthogonal set of vectors , then     Using this proposition, we can see that an orthogonal set of vectors must be linearly independent. Suppose, for instance, that is a set of nonzero orthogonal vectors and that one of the vectors is a linear combination of the others, say, We therefore know that which cannot happen since we know that is nonzero. This tells us that    An orthogonal set of vectors is linearly independent.    If the vectors in an orthogonal set have dimension , they form a linearly independent set in and are therefore a basis for the subspace . If there are vectors in the orthogonal set, they form a basis for .    Consider the vectors    Verify that this set forms an orthogonal set of -dimensional vectors.     Explain why we know that this set of vectors forms a basis for .    Suppose that . Find the weights , , and that express as a linear combination using .    If we multiply a vector by a positive scalar , the length of is also multiplied by ; that is, .   unit vector Using this observation, find a vector that is parallel to and has length 1. Such vectors are called unit vectors .     Similarly, find a unit vector that is parallel to and a unit vector that is parallel to .    Construct the matrix and find the product . Use to explain your result.          We compute the dot products , , and .    We know that an orthogonal set of vectors is linearly independent. Therefore, we have a set of three linearly independent vectors in so they must form a basis for .    We find that .    Since , we find     We find that     We find since each entry in this matrix product is the dot product of two columns of .          We compute the dot products , , and .    An orthogonal set of vectors is linearly independent.     .         We find that             This activity introduces an important way of modifying an orthogonal set so that the vectors in the set have unit length. Recall that we may multiply any nonzero vector by a scalar so that the new vector has length 1. For instance, we know that if is a positive scalar, then . To obtain a vector having unit length, we want so that . Therefore, becomes a unit vector parallel to .  Orthogonal sets in which the vectors have unit length are called orthonormal and are especially convenient.   othonormal set   An orthonormal set is an orthogonal set of vectors each of which has unit length.      The vectors are an orthonormal set of vectors in and form an orthonormal basis for .  If we form the matrix we find that since tells us that     The previous activity and example illustrate the next proposition.    If the columns of the matrix form an orthonormal set, then , the identity matrix.      Orthogonal projections  We now turn to an important problem that will appear in many forms in the rest of our explorations. Suppose, as shown in , that we have a subspace of and a vector that is not in that subspace. We would like to find the vector in that is closest to , meaning the distance between and is as small as possible.      Given a plane in and a vector not in the plane, we wish to find the vector in the plane that is closest to .   To get started, let's consider a simpler problem where we have a line in , defined by the vector , and another vector that is not on the line, as shown on the left of . We wish to find , the vector on the line that is closest to , as illustrated in the right of .       Given a line and a vector , we seek the vector on that is closest to .   To find , we require that be orthogonal to . For instance, if is another vector on the line, as shown in , then the Pythagorean theorem implies that which means that . Therefore, is closer to than any other vector on the line .      The vector is closer to than because is orthogonal to .    orthogonal projection   Given a vector in and a subspace of , the orthogonal projection of onto is the vector in that is closest to . It is characterized by the property that is orthogonal to .      This activity demonstrates how to determine the orthogonal projection of a vector onto a subspace of .   Let's begin by considering a line , defined by the vector , and a vector not on , as illustrated in .       Finding the orthogonal projection of onto the line defined by .      To find , first notice that for some scalar . Since is orthogonal to , what do we know about the dot product     Apply the distributive property of dot products to find the scalar . What is the vector , the orthogonal projection of onto ?    More generally, explain why the orthogonal projection of onto the line defined by is        The same ideas apply more generally. Suppose we have an orthogonal set of vectors and that define a plane in . If another vector in , we seek the vector on the plane closest to . As before, the vector will be orthogonal to , as illustrated in .       Given a plane defined by the orthogonal vectors and and another vector , we seek the vector on closest to .      The vector is orthogonal to . What does this say about the dot products: and ?    Since is in the plane , we can write it as a linear combination . Then Find the weight by dotting with and applying the distributive property of dot products. Similarly, find the weight .    What is the vector , the orthogonal projection of onto the plane ?       Suppose that is a subspace of with orthogonal basis and that is a vector in . Explain why the orthogonal projection of onto is the vector     Suppose that is an orthonormal basis for ; that is, the vectors are orthogonal to one another and have unit length. Explain why the orthogonal projection is     If is the matrix whose columns are an orthonormal basis of , use to explain why .             This dot product should be 0 since the vectors are orthogonal.     .    As before,           These dot products are 0.                    We know and we can find by requiring that be orthogonal to every vector .    The vectors form an orthogonal set and since , the weights are .    We have so that              0     .               0                    We require that be orthogonal to every vector .         Use the fact that        In all the cases considered in the activity, we are looking for , the vector in a subspace closest to a vector , which is found by requiring that be orthogonal to . This means that for any vector in .  If we have an orthogonal basis for , then . Therefore, This leads to the projection formula:   Projection formula   If is a subspace of having an orthogonal basis and is a vector in , then the orthogonal projection of onto is      Caution  Remember that the projection formula given in applies only when the basis of is orthogonal .   If we have an orthonormal basis for , the projection formula simplifies to If we then form the matrix this expression may be succintly written   This leads to the following proposition.    If is an orthonormal basis for a subspace of , then the matrix transformation that projects vectors in orthogonally onto is represented by the matrix where       In the previous activity, we looked at the plane defined by the two orthogonal vectors We can form an orthonormal basis by scalar multiplying these vectors to have unit length: Using these vectors, we form the matrix The projection onto the plane is then given by the matrix   Let's check that this works by considering the vector and finding , its orthogonal projection onto the plane . In terms of the original basis and , the projection formula from tells us that   Alternatively, we use the matrix , as in , to find that          Suppose that is the line in defined by the vector .    Find an orthonormal basis for .    Construct the matrix and use it to construct the matrix that projects vectors orthogonally onto .    Use your matrix to find , the orthogonal projection of onto .    Find and explain its geometric significance.       The vectors form an orthogonal basis of , a two-dimensional subspace of .    Use the projection formula from to find , the orthogonal projection of onto .    Find an orthonormal basis and for and use it to construct the matrix that projects vectors orthogonally onto . Check that , the orthogonal projection you found in the previous part of this activity.    Find and explain its geometric significance.    Find a basis for .    Find a vector in such that     If is the matrix whose columns are and , find the product and explain your result.                               We find that , which makes sense because , a 1-dimensional subspace of .                     since     Since , then , which gives and     We can find      since this product computes the dot products between the columns of .                                                          and                     This activity demonstrates one issue of note. We found , the orthogonal projection of onto , by requiring that be orthogonal to . In other words, is a vector in the orthogonal complement , which we may denote . This explains the following proposition, which is illustrated in    If is a subspace of with orthogonal complement , then any -dimensional vector can be uniquely written as where is in and is in . The vector is the orthogonal projection of onto and is the orthogonal projection of onto .       A vector along with , its orthogonal projection onto the line , and , its orthogonal projection onto the orthogonal complement .   Let's summarize what we've found. If is a matrix whose columns form an orthonormal set in , then    , the identity matrix, because this product computes the dot products between the columns of .     is the matrix the projects vectors orthogonally onto , the subspace of spanned by .   As we've said before, matrix multiplication depends on the order in which we multiply the matrices, and we see this clearly here.  Because , there is a temptation to say that is invertible. This is usually not the case, however. Remember that an invertible matrix must be a square matrix, and the matrix will only be square if . In this case, there are vectors in the orthonormal set so the subspace spanned by the vectors is . If is a vector in , then is the orthogonal projection of onto . In other words, is the closest vector in to , and this closest vector must be itself. Therefore, , which means that . In this case, is an invertible matrix.    Consider the orthonormal set of vectors and the matrix they define In this case, and span a plane, a 2-dimensional subspace of . We know that and projects vectors orthogonally onto the plane. However, is not a square matrix so it cannot be invertible.      Now consider the orthonormal set of vectors and the matrix they define Here, , , and form a basis for so that both and . Therefore, is a square matrix and is invertible.  Moreover, since , we see that so finding the inverse of is as simple as writing its transpose. Matrices with this property are very special and will play an important role in our upcoming work. We will therefore give them a special name.      orthogonal matrix  A square matrix whose columns form an orthonormal basis for is called orthogonal .    This terminology can be a little confusing. We call a basis orthogonal if the basis vectors are orthogonal to one another. However, a matrix is orthogonal if the columns are orthogonal to one another and have unit length. It pays to keep this in mind when reading statements about orthogonal bases and orthogonal matrices. In the meantime, we record the following proposition.    An orthogonal matrix is invertible and its inverse .      Summary  This section introduced orthogonal sets and the projection formula that allows us to project vectors orthogonally onto a subspace.   Given an orthogonal set that spans an -dimensional subspace of , the orthogonal projection of onto is the vector in closest to and may be written as     If is an orthonormal basis of and is the matrix whose columns are , then the matrix projects vectors orthogonally onto .    If the columns of form an orthonormal basis for an -dimensional subspace of , then .    An orthogonal matrix is a square matrix whose columns form an orthonormal basis. In this case, so that .        Suppose that    Verify that and form an orthogonal basis for a plane in .    Use to find , the orthogonal projection of onto .    Find an orthonormal basis , for .    Find the matrix representing the matrix transformation that projects vectors in orthogonally onto . Verify that .    Determine and explain its geometric significance.          .     .     and                    Check that .    Applying the Projection Formula gives .     and     Form so that     Since , we have        Consider the vectors    Explain why these vectors form an orthogonal basis for .    Suppose that and evaluate the product . Why is this product a diagonal matrix and what is the significance of the diagonal entries?    Express the vector as a linear combination of , , and .    Multiply the vectors , , by appropriate scalars to find an orthonormal basis , , of .    If , find the matrix product and explain the result.          , , and .          .                   Check that all three dot products , , and .     . This matrix is diagonal because the vectors form an orthogonal set.    We may find the weights of this linear combination by finding so that .         The columns of form an orthonormal basis for so is orthogonal. Therefore, .       Suppose that form an orthogonal basis for a subspace of .   Find , the orthogonal projection of onto .    Find the vector in such that .    Find a basis for . and express as a linear combination of the basis vectors.                   A basis for is and . We then have .         Apply the Projection Formula to find .     .    Constructing a matrix whose columns are and allows us to find a basis for . This gives the basis and . We then have .       Consider the vectors    If is the line defined by the vector , find the vector in closest to . Call this vector .    If is the subspace spanned by and , find the vector in closest to . Call this vector .    Determine whether or is closer to and explain why.                             Applying the Projection Formula gives     Applying the Projection Formula gives      is the closest vector in to . Since is contained in , cannot be closer. Therefore, must be closer to than .       Suppose that defines a line in .   Find the orthogonal projections of the vectors , , onto .    Find the matrix .    Use to explain why the columns of are related to the orthogonal projections you found in the first part of this exercise.                   The columns of are the results of projecting the standard basis vectors onto .         Applying the Projection Formula gives the projections          If , then , which projects vectors orthogonally onto . The columns of are the results of projecting the standard basis vectors onto .       Suppose that form the basis for a plane in .   Find a basis for the line that is the orthogonal complement .    Given the vector , find , the orthogonal projection of onto the line .    Explain why the vector must be in and write as a linear combination of and .                    .         A basis vector is           is the orthogonal projection of onto so must be orthogonal to , which means that it is in . We see that .       Determine whether the following statements are true or false and explain your thinking.   If the columns of form an orthonormal basis for a subspace and is a vector in , then .    An orthogonal set of vectors in can have no more than 8 vectors.    If is a matrix whose columns are orthonormal, then .    If is a matrix whose columns are orthonormal, then .    If the orthogonal projection of onto a subspace satisfies , then is in .         True    True    False    True    True        True, because is the closest vector in to . Therefore, .    True, because the orthogonal set of vectors is linearly independent.    False, projects vectors orthogonally onto the 5-dimensional subspace .    True, because computes the dot products between the columns of     True, because . Therefore, is in .      Suppose that is an orthogonal matrix.   Remembering that , explain why     Explain why .  This means that the length of a vector is unchanged after multiplying by an orthogonal matrix.    If is a real eigenvalue of , explain why .          .         If , then so           .         If , then so        Explain why the following statements are true.   If is an orthogonal matrix, then .    If is a matrix whose columns are orthonormal, then is an matrix whose rank is 4.    If is the orthogonal projection of onto a subspace , then is the orthogonal projection of onto .         Since , we have .    The four columns of form a basis for the column space     If , then is in .         Since , we have     The columns of form an orthonormal basis for , a 4-dimensional subspace of . Therefore, projects vectors orthogonally onto so and .    If , then is in , the orthogonal complement of . This means that is the orthogonal projection of onto .       This exercise is about orthogonal matrices.   In , we saw that the matrix represents a rotation by an angle . Explain why this matrix is an orthogonal matrix.    We also saw that the matrix represents a reflection in a line. Explain why this matrix is an orthogonal matrix.    Suppose that is a 2-dimensional unit vector. Use a sketch to indicate all the possible vectors such that and form an orthonormal basis of .    Explain why every orthogonal matrix is either a rotation or a reflection.          .     .     or     The first column of has the form . Now apply the result of the last part of this problem.         If this matrix is , we have .    If this matrix is , we have .     or     If is an orthogonal matrix, then is a unit vector and has the form for some angle . By the last part of this problem, there are only two choices for , one of which gives a rotation and one of which gives a reflection.       "
},
{
  "id": "preview-orthogonal-basis",
  "level": "2",
  "url": "sec-orthogonal-bases.html#preview-orthogonal-basis",
  "type": "Preview Activity",
  "number": "22.3.1",
  "title": "",
  "body": "  For this activity, it will be helpful to recall the distributive property of dot products: We'll work with the basis of formed by the vectors    Verify that the vectors and are orthogonal.    Suppose that and find the dot products and .    We would like to express as a linear combination of and , which means that we need to find weights and such that To find the weight , dot both sides of this expression with : and apply the distributive property.    In a similar fashion, find the weight .    Verify that using the weights you have found.          We can compute that .     and .     .     .     .      "
},
{
  "id": "sec-orthogonal-bases-3-3",
  "level": "2",
  "url": "sec-orthogonal-bases.html#sec-orthogonal-bases-3-3",
  "type": "Definition",
  "number": "22.3.1",
  "title": "",
  "body": " orthogonal set   By an orthogonal set of vectors, we mean a set of nonzero vectors each of which is orthogonal to the others.   "
},
{
  "id": "example-orthogonal-basis",
  "level": "2",
  "url": "sec-orthogonal-bases.html#example-orthogonal-basis",
  "type": "Example",
  "number": "22.3.2",
  "title": "",
  "body": "  The 3-dimensional vectors form an orthogonal set, which can be verified by computing Notice that this set of vectors forms a basis for .   "
},
{
  "id": "example-orthogonal-set",
  "level": "2",
  "url": "sec-orthogonal-bases.html#example-orthogonal-set",
  "type": "Example",
  "number": "22.3.3",
  "title": "",
  "body": "  The vectors form an orthogonal set of 4-dimensional vectors. Since there are only three vectors, this set does not form a basis for . It does, however, form a basis for a 3-dimensional subspace of .   "
},
{
  "id": "prop-orthog-lincomb",
  "level": "2",
  "url": "sec-orthogonal-bases.html#prop-orthog-lincomb",
  "type": "Proposition",
  "number": "22.3.4",
  "title": "",
  "body": "  If a vector is a linear combination of an orthogonal set of vectors , then    "
},
{
  "id": "prop-orthog-lin-indep",
  "level": "2",
  "url": "sec-orthogonal-bases.html#prop-orthog-lin-indep",
  "type": "Proposition",
  "number": "22.3.5",
  "title": "",
  "body": "  An orthogonal set of vectors is linearly independent.   "
},
{
  "id": "sec-orthogonal-bases-3-12",
  "level": "2",
  "url": "sec-orthogonal-bases.html#sec-orthogonal-bases-3-12",
  "type": "Activity",
  "number": "22.3.2",
  "title": "",
  "body": "  Consider the vectors    Verify that this set forms an orthogonal set of -dimensional vectors.     Explain why we know that this set of vectors forms a basis for .    Suppose that . Find the weights , , and that express as a linear combination using .    If we multiply a vector by a positive scalar , the length of is also multiplied by ; that is, .   unit vector Using this observation, find a vector that is parallel to and has length 1. Such vectors are called unit vectors .     Similarly, find a unit vector that is parallel to and a unit vector that is parallel to .    Construct the matrix and find the product . Use to explain your result.          We compute the dot products , , and .    We know that an orthogonal set of vectors is linearly independent. Therefore, we have a set of three linearly independent vectors in so they must form a basis for .    We find that .    Since , we find     We find that     We find since each entry in this matrix product is the dot product of two columns of .          We compute the dot products , , and .    An orthogonal set of vectors is linearly independent.     .         We find that            "
},
{
  "id": "sec-orthogonal-bases-3-15",
  "level": "2",
  "url": "sec-orthogonal-bases.html#sec-orthogonal-bases-3-15",
  "type": "Definition",
  "number": "22.3.6",
  "title": "",
  "body": " othonormal set   An orthonormal set is an orthogonal set of vectors each of which has unit length.   "
},
{
  "id": "sec-orthogonal-bases-3-16",
  "level": "2",
  "url": "sec-orthogonal-bases.html#sec-orthogonal-bases-3-16",
  "type": "Example",
  "number": "22.3.7",
  "title": "",
  "body": "  The vectors are an orthonormal set of vectors in and form an orthonormal basis for .  If we form the matrix we find that since tells us that    "
},
{
  "id": "prop-orthonormal-QTQ",
  "level": "2",
  "url": "sec-orthogonal-bases.html#prop-orthonormal-QTQ",
  "type": "Proposition",
  "number": "22.3.8",
  "title": "",
  "body": "  If the columns of the matrix form an orthonormal set, then , the identity matrix.   "
},
{
  "id": "fig-3d-orthog-proj",
  "level": "2",
  "url": "sec-orthogonal-bases.html#fig-3d-orthog-proj",
  "type": "Figure",
  "number": "22.3.9",
  "title": "",
  "body": "    Given a plane in and a vector not in the plane, we wish to find the vector in the plane that is closest to .  "
},
{
  "id": "fig-projection-line-a",
  "level": "2",
  "url": "sec-orthogonal-bases.html#fig-projection-line-a",
  "type": "Figure",
  "number": "22.3.10",
  "title": "",
  "body": "     Given a line and a vector , we seek the vector on that is closest to .  "
},
{
  "id": "fig-projection-line-b",
  "level": "2",
  "url": "sec-orthogonal-bases.html#fig-projection-line-b",
  "type": "Figure",
  "number": "22.3.11",
  "title": "",
  "body": "    The vector is closer to than because is orthogonal to .  "
},
{
  "id": "sec-orthogonal-bases-4-8",
  "level": "2",
  "url": "sec-orthogonal-bases.html#sec-orthogonal-bases-4-8",
  "type": "Definition",
  "number": "22.3.12",
  "title": "",
  "body": " orthogonal projection   Given a vector in and a subspace of , the orthogonal projection of onto is the vector in that is closest to . It is characterized by the property that is orthogonal to .   "
},
{
  "id": "sec-orthogonal-bases-4-9",
  "level": "2",
  "url": "sec-orthogonal-bases.html#sec-orthogonal-bases-4-9",
  "type": "Activity",
  "number": "22.3.3",
  "title": "",
  "body": "  This activity demonstrates how to determine the orthogonal projection of a vector onto a subspace of .   Let's begin by considering a line , defined by the vector , and a vector not on , as illustrated in .       Finding the orthogonal projection of onto the line defined by .      To find , first notice that for some scalar . Since is orthogonal to , what do we know about the dot product     Apply the distributive property of dot products to find the scalar . What is the vector , the orthogonal projection of onto ?    More generally, explain why the orthogonal projection of onto the line defined by is        The same ideas apply more generally. Suppose we have an orthogonal set of vectors and that define a plane in . If another vector in , we seek the vector on the plane closest to . As before, the vector will be orthogonal to , as illustrated in .       Given a plane defined by the orthogonal vectors and and another vector , we seek the vector on closest to .      The vector is orthogonal to . What does this say about the dot products: and ?    Since is in the plane , we can write it as a linear combination . Then Find the weight by dotting with and applying the distributive property of dot products. Similarly, find the weight .    What is the vector , the orthogonal projection of onto the plane ?       Suppose that is a subspace of with orthogonal basis and that is a vector in . Explain why the orthogonal projection of onto is the vector     Suppose that is an orthonormal basis for ; that is, the vectors are orthogonal to one another and have unit length. Explain why the orthogonal projection is     If is the matrix whose columns are an orthonormal basis of , use to explain why .             This dot product should be 0 since the vectors are orthogonal.     .    As before,           These dot products are 0.                    We know and we can find by requiring that be orthogonal to every vector .    The vectors form an orthogonal set and since , the weights are .    We have so that              0     .               0                    We require that be orthogonal to every vector .         Use the fact that       "
},
{
  "id": "prop-proj-formula",
  "level": "2",
  "url": "sec-orthogonal-bases.html#prop-proj-formula",
  "type": "Proposition",
  "number": "22.3.15",
  "title": "Projection formula.",
  "body": " Projection formula   If is a subspace of having an orthogonal basis and is a vector in , then the orthogonal projection of onto is    "
},
{
  "id": "prop-proj-orthonormal",
  "level": "2",
  "url": "sec-orthogonal-bases.html#prop-proj-orthonormal",
  "type": "Proposition",
  "number": "22.3.16",
  "title": "",
  "body": "  If is an orthonormal basis for a subspace of , then the matrix transformation that projects vectors in orthogonally onto is represented by the matrix where    "
},
{
  "id": "example-projection-matrix",
  "level": "2",
  "url": "sec-orthogonal-bases.html#example-projection-matrix",
  "type": "Example",
  "number": "22.3.17",
  "title": "",
  "body": "  In the previous activity, we looked at the plane defined by the two orthogonal vectors We can form an orthonormal basis by scalar multiplying these vectors to have unit length: Using these vectors, we form the matrix The projection onto the plane is then given by the matrix   Let's check that this works by considering the vector and finding , its orthogonal projection onto the plane . In terms of the original basis and , the projection formula from tells us that   Alternatively, we use the matrix , as in , to find that    "
},
{
  "id": "sec-orthogonal-bases-4-18",
  "level": "2",
  "url": "sec-orthogonal-bases.html#sec-orthogonal-bases-4-18",
  "type": "Activity",
  "number": "22.3.4",
  "title": "",
  "body": "     Suppose that is the line in defined by the vector .    Find an orthonormal basis for .    Construct the matrix and use it to construct the matrix that projects vectors orthogonally onto .    Use your matrix to find , the orthogonal projection of onto .    Find and explain its geometric significance.       The vectors form an orthogonal basis of , a two-dimensional subspace of .    Use the projection formula from to find , the orthogonal projection of onto .    Find an orthonormal basis and for and use it to construct the matrix that projects vectors orthogonally onto . Check that , the orthogonal projection you found in the previous part of this activity.    Find and explain its geometric significance.    Find a basis for .    Find a vector in such that     If is the matrix whose columns are and , find the product and explain your result.                               We find that , which makes sense because , a 1-dimensional subspace of .                     since     Since , then , which gives and     We can find      since this product computes the dot products between the columns of .                                                          and                    "
},
{
  "id": "prop-orthog-decomp",
  "level": "2",
  "url": "sec-orthogonal-bases.html#prop-orthog-decomp",
  "type": "Proposition",
  "number": "22.3.18",
  "title": "",
  "body": " If is a subspace of with orthogonal complement , then any -dimensional vector can be uniquely written as where is in and is in . The vector is the orthogonal projection of onto and is the orthogonal projection of onto .  "
},
{
  "id": "fig-orthog-decomp",
  "level": "2",
  "url": "sec-orthogonal-bases.html#fig-orthog-decomp",
  "type": "Figure",
  "number": "22.3.19",
  "title": "",
  "body": "    A vector along with , its orthogonal projection onto the line , and , its orthogonal projection onto the orthogonal complement .  "
},
{
  "id": "sec-orthogonal-bases-4-24",
  "level": "2",
  "url": "sec-orthogonal-bases.html#sec-orthogonal-bases-4-24",
  "type": "Example",
  "number": "22.3.20",
  "title": "",
  "body": "  Consider the orthonormal set of vectors and the matrix they define In this case, and span a plane, a 2-dimensional subspace of . We know that and projects vectors orthogonally onto the plane. However, is not a square matrix so it cannot be invertible.   "
},
{
  "id": "sec-orthogonal-bases-4-25",
  "level": "2",
  "url": "sec-orthogonal-bases.html#sec-orthogonal-bases-4-25",
  "type": "Example",
  "number": "22.3.21",
  "title": "",
  "body": "  Now consider the orthonormal set of vectors and the matrix they define Here, , , and form a basis for so that both and . Therefore, is a square matrix and is invertible.  Moreover, since , we see that so finding the inverse of is as simple as writing its transpose. Matrices with this property are very special and will play an important role in our upcoming work. We will therefore give them a special name.   "
},
{
  "id": "sec-orthogonal-bases-4-26",
  "level": "2",
  "url": "sec-orthogonal-bases.html#sec-orthogonal-bases-4-26",
  "type": "Definition",
  "number": "22.3.22",
  "title": "",
  "body": "  orthogonal matrix  A square matrix whose columns form an orthonormal basis for is called orthogonal .   "
},
{
  "id": "prop-orthog-matrix",
  "level": "2",
  "url": "sec-orthogonal-bases.html#prop-orthog-matrix",
  "type": "Proposition",
  "number": "22.3.23",
  "title": "",
  "body": "  An orthogonal matrix is invertible and its inverse .   "
},
{
  "id": "sec-orthogonal-bases-6-1",
  "level": "2",
  "url": "sec-orthogonal-bases.html#sec-orthogonal-bases-6-1",
  "type": "Exercise",
  "number": "22.3.4.1",
  "title": "",
  "body": " Suppose that    Verify that and form an orthogonal basis for a plane in .    Use to find , the orthogonal projection of onto .    Find an orthonormal basis , for .    Find the matrix representing the matrix transformation that projects vectors in orthogonally onto . Verify that .    Determine and explain its geometric significance.          .     .     and                    Check that .    Applying the Projection Formula gives .     and     Form so that     Since , we have      "
},
{
  "id": "sec-orthogonal-bases-6-2",
  "level": "2",
  "url": "sec-orthogonal-bases.html#sec-orthogonal-bases-6-2",
  "type": "Exercise",
  "number": "22.3.4.2",
  "title": "",
  "body": " Consider the vectors    Explain why these vectors form an orthogonal basis for .    Suppose that and evaluate the product . Why is this product a diagonal matrix and what is the significance of the diagonal entries?    Express the vector as a linear combination of , , and .    Multiply the vectors , , by appropriate scalars to find an orthonormal basis , , of .    If , find the matrix product and explain the result.          , , and .          .                   Check that all three dot products , , and .     . This matrix is diagonal because the vectors form an orthogonal set.    We may find the weights of this linear combination by finding so that .         The columns of form an orthonormal basis for so is orthogonal. Therefore, .     "
},
{
  "id": "sec-orthogonal-bases-6-3",
  "level": "2",
  "url": "sec-orthogonal-bases.html#sec-orthogonal-bases-6-3",
  "type": "Exercise",
  "number": "22.3.4.3",
  "title": "",
  "body": " Suppose that form an orthogonal basis for a subspace of .   Find , the orthogonal projection of onto .    Find the vector in such that .    Find a basis for . and express as a linear combination of the basis vectors.                   A basis for is and . We then have .         Apply the Projection Formula to find .     .    Constructing a matrix whose columns are and allows us to find a basis for . This gives the basis and . We then have .     "
},
{
  "id": "sec-orthogonal-bases-6-4",
  "level": "2",
  "url": "sec-orthogonal-bases.html#sec-orthogonal-bases-6-4",
  "type": "Exercise",
  "number": "22.3.4.4",
  "title": "",
  "body": " Consider the vectors    If is the line defined by the vector , find the vector in closest to . Call this vector .    If is the subspace spanned by and , find the vector in closest to . Call this vector .    Determine whether or is closer to and explain why.                             Applying the Projection Formula gives     Applying the Projection Formula gives      is the closest vector in to . Since is contained in , cannot be closer. Therefore, must be closer to than .     "
},
{
  "id": "sec-orthogonal-bases-6-5",
  "level": "2",
  "url": "sec-orthogonal-bases.html#sec-orthogonal-bases-6-5",
  "type": "Exercise",
  "number": "22.3.4.5",
  "title": "",
  "body": " Suppose that defines a line in .   Find the orthogonal projections of the vectors , , onto .    Find the matrix .    Use to explain why the columns of are related to the orthogonal projections you found in the first part of this exercise.                   The columns of are the results of projecting the standard basis vectors onto .         Applying the Projection Formula gives the projections          If , then , which projects vectors orthogonally onto . The columns of are the results of projecting the standard basis vectors onto .     "
},
{
  "id": "sec-orthogonal-bases-6-6",
  "level": "2",
  "url": "sec-orthogonal-bases.html#sec-orthogonal-bases-6-6",
  "type": "Exercise",
  "number": "22.3.4.6",
  "title": "",
  "body": " Suppose that form the basis for a plane in .   Find a basis for the line that is the orthogonal complement .    Given the vector , find , the orthogonal projection of onto the line .    Explain why the vector must be in and write as a linear combination of and .                    .         A basis vector is           is the orthogonal projection of onto so must be orthogonal to , which means that it is in . We see that .     "
},
{
  "id": "sec-orthogonal-bases-6-7",
  "level": "2",
  "url": "sec-orthogonal-bases.html#sec-orthogonal-bases-6-7",
  "type": "Exercise",
  "number": "22.3.4.7",
  "title": "",
  "body": " Determine whether the following statements are true or false and explain your thinking.   If the columns of form an orthonormal basis for a subspace and is a vector in , then .    An orthogonal set of vectors in can have no more than 8 vectors.    If is a matrix whose columns are orthonormal, then .    If is a matrix whose columns are orthonormal, then .    If the orthogonal projection of onto a subspace satisfies , then is in .         True    True    False    True    True        True, because is the closest vector in to . Therefore, .    True, because the orthogonal set of vectors is linearly independent.    False, projects vectors orthogonally onto the 5-dimensional subspace .    True, because computes the dot products between the columns of     True, because . Therefore, is in .    "
},
{
  "id": "sec-orthogonal-bases-6-8",
  "level": "2",
  "url": "sec-orthogonal-bases.html#sec-orthogonal-bases-6-8",
  "type": "Exercise",
  "number": "22.3.4.8",
  "title": "",
  "body": " Suppose that is an orthogonal matrix.   Remembering that , explain why     Explain why .  This means that the length of a vector is unchanged after multiplying by an orthogonal matrix.    If is a real eigenvalue of , explain why .          .         If , then so           .         If , then so      "
},
{
  "id": "sec-orthogonal-bases-6-9",
  "level": "2",
  "url": "sec-orthogonal-bases.html#sec-orthogonal-bases-6-9",
  "type": "Exercise",
  "number": "22.3.4.9",
  "title": "",
  "body": " Explain why the following statements are true.   If is an orthogonal matrix, then .    If is a matrix whose columns are orthonormal, then is an matrix whose rank is 4.    If is the orthogonal projection of onto a subspace , then is the orthogonal projection of onto .         Since , we have .    The four columns of form a basis for the column space     If , then is in .         Since , we have     The columns of form an orthonormal basis for , a 4-dimensional subspace of . Therefore, projects vectors orthogonally onto so and .    If , then is in , the orthogonal complement of . This means that is the orthogonal projection of onto .     "
},
{
  "id": "sec-orthogonal-bases-6-10",
  "level": "2",
  "url": "sec-orthogonal-bases.html#sec-orthogonal-bases-6-10",
  "type": "Exercise",
  "number": "22.3.4.10",
  "title": "",
  "body": " This exercise is about orthogonal matrices.   In , we saw that the matrix represents a rotation by an angle . Explain why this matrix is an orthogonal matrix.    We also saw that the matrix represents a reflection in a line. Explain why this matrix is an orthogonal matrix.    Suppose that is a 2-dimensional unit vector. Use a sketch to indicate all the possible vectors such that and form an orthonormal basis of .    Explain why every orthogonal matrix is either a rotation or a reflection.          .     .     or     The first column of has the form . Now apply the result of the last part of this problem.         If this matrix is , we have .    If this matrix is , we have .     or     If is an orthogonal matrix, then is a unit vector and has the form for some angle . By the last part of this problem, there are only two choices for , one of which gives a rotation and one of which gives a reflection.     "
},
{
  "id": "sec-gram-schmidt",
  "level": "1",
  "url": "sec-gram-schmidt.html",
  "type": "Section",
  "number": "22.4",
  "title": "Finding orthogonal bases",
  "body": " Finding orthogonal bases   The last section demonstrated the value of working with orthogonal, and especially orthonormal, sets. If we have an orthogonal basis for a subspace , the Projection Formula tells us that the orthogonal projection of a vector onto is An orthonormal basis is even more convenient: after forming the matrix , we have .  In the examples we've seen so far, however, orthogonal bases were given to us. What we need now is a way to form orthogonal bases. In this section, we'll explore an algorithm that begins with a basis for a subspace and creates an orthogonal basis. Once we have an orthogonal basis, we can scale each of the vectors appropriately to produce an orthonormal basis.    Suppose we have a basis for consisting of the vectors as shown in . Notice that this basis is not orthogonal.      A basis for .      Find the vector that is the orthogonal projection of onto the line defined by .    Explain why is orthogonal to .    Define the new vectors and and sketch them in . Explain why and define an orthogonal basis for .      Sketch the new basis and .     Write the vector as a linear combination of and .    Scale the vectors and to produce an orthonormal basis and for .           .    The orthogonal projection is defined so that is orthogonal to .     and are othogonal, which can be checked with the dot product.    The projection formula gives .     and .         Gram-Schmidt orthogonalization   Gram-Schmidt The preview activity illustrates the main idea behind an algorithm, known as Gram-Schmidt orthogonalization , that begins with a basis for some subspace of and produces an orthogonal or orthonormal basis. The algorithm relies on our construction of the orthogonal projection. Remember that we formed the orthogonal projection of onto a subspace by requiring that is orthogonal to as shown in .      If is the orthogonal projection of onto , then is orthogonal to .   This observation guides our construction of an orthogonal basis for it allows us to create a vector that is orthogonal to a given subspace. Let's see how the Gram-Schmidt algorithm works.    Suppose that is a three-dimensional subspace of with basis: We can see that this basis is not orthogonal by noting that . Our goal is to create an orthogonal basis , , and for .  To begin, we declare that , and we call the line defined by .      Find the vector that is the orthogonal projection of onto , the line defined by .    Form the vector and verify that it is orthogonal to .    Explain why by showing that any linear combination of and can be written as a linear combination of and and vice versa.    The vectors and are an orthogonal basis for a two-dimensional subspace of . Find the vector that is the orthogonal projection of onto .    Verify that is orthogonal to both and .    Explain why , , and form an orthogonal basis for .    Now find an orthonormal basis for .           .         We have and . Therefore, a linear combination of and can be rewritten as In the same way, and so any linear combination of and can be rewritten as a linear combination of and .    By the Projection Formula, .         We can check that , , and form an orthogonal set. Since can be written in terms of and vice-versa, these new vectors form a basis for .                .         We have and so a linear combination of and can be rewritten as a linear combination of and .     .         Since can be written in terms of and vice-versa, these new vectors form a basis for .            As this activity illustrates, Gram-Schmidt orthogonalization begins with a basis for a subspace of and creates an orthogonal basis for . Let's work through a second example.    Let's start with the basis which is a basis for .  To get started, we'll simply set . We construct from by subtracting its orthogonal projection onto , the line defined by . This gives   Notice that we found . Therefore, we can rewrite any linear combination of and as a linear combination of and . This tells us that In other words, and is a orthogonal basis for , the 2-dimensional subspace that is the span of and .  Finally, we form from by subtracting its orthogonal projection onto :   We can now check that is an orthogonal set. Furthermore, we have, as before, , which says that we have found a new orthogonal basis for .  To create an orthonormal basis, we form unit vectors parallel to each of the vectors in the orthogonal basis:     More generally, if we have a basis for a subspace of , the Gram-Schmidt algorithm creates an orthogonal basis for in the following way:   From here, we may form an orthonormal basis by constructing a unit vector parallel to each vector in the orthogonal basis: .    Sage can automate these computations for us. Before we begin, however, it will be helpful to understand how we can combine things using a list in Python. For instance, if the vectors v1 , v2 , and v3 form a basis for a subspace, we can bundle them together using square brackets: [v1, v2, v3] . Furthermore, we could assign this to a variable, such as basis = [v1, v2, v3] .  Evaluating the following cell will load in some special commands.    There is a command to apply the projection formula: projection(b, basis) returns the orthogonal projection of b onto the subspace spanned by basis , which is a list of vectors.    The command unit(w) returns a unit vector parallel to w .    Given a collection of vectors, say, v1 and v2 , we can form the matrix whose columns are v1 and v2 using matrix([v1, v2]).T . When given a list of vectors, Sage constructs a matrix whose rows are the given vectors. For this reason, we need to apply the transpose.     Let's now consider , the subspace of having basis    Apply the Gram-Schmidt algorithm to find an orthogonal basis , , and for .     Find , the orthogonal projection of onto .    Explain why we know that is a linear combination of the original vectors , , and and then find weights so that     Find an orthonormal basis , , for for and form the matrix whose columns are these vectors.     Find the product and explain the result.    Find the matrix that projects vectors orthogonally onto and verify that gives , the orthogonal projection that you found earlier.                    We know that is in and , , and is a basis for . We find .          since the matrix product computes the dot products of the columns of .                          .                         factorizations  Now that we've seen how the Gram-Schmidt algorithm forms an orthonormal basis for a given subspace, we will explore how the algorithm leads to an important matrix factorization known as the factorization.    Suppose that is the matrix whose columns are These vectors form a basis for , the subspace of that we encountered in . Since these vectors are the columns of , we have .    When we implemented Gram-Schmidt, we first found an orthogonal basis , , and using Use these expressions to write , , and as linear combinations of , , and .    We next normalized the orthogonal basis , , and to obtain an orthonormal basis , , and .  Write the vectors as scalar multiples of . Then use these expressions to write , , and as linear combinations of , , and .    Suppose that . Use the result of the previous part to find a vector so that .    Then find vectors and such that and .    Construct the matrix . Remembering that , explain why .    What is special about the shape of ?    Suppose that is a matrix whose columns are linearly independent. This means that the columns of form a basis for , a 6-dimensional subspace of . Suppose that we apply Gram-Schmidt orthogonalization to create an orthonormal basis whose vectors form the columns of and that we write . What are the shape of and what the shape of ?           Therefore,      so we have This leads to     Since , we have .    In the same way, we have and .    We have .     so is upper triangular.     will be and will be a upper triangular matrix.                     .     and .    We have .     is upper triangular.     will be and will be .       When the columns of a matrix are linearly independent, they form a basis for so that we can perform the Gram-Schmidt algorithm. The previous activity shows how this leads to a factorization of as the product of a matrix whose columns are an orthonormal basis for and an upper triangular matrix .    factorization  If is an matrix whose columns are linearly independent, we may write where is an matrix whose columns form an orthonormal basis for and is an upper triangular matrix.     We'll consider the matrix whose columns, which we'll denote , , and , are the basis of that we considered in . There we found an orthogonal basis , , and that satisfied   In terms of the resulting orthonormal basis , , and , we had so that   Therefore, if , we have the factorization     The value of the factorization will become clear in the next section where we use it to solve least-squares problems.    As before, we would like to use Sage to automate the process of finding and using the factorization of a matrix . Evaluating the following cell provides a command QR(A) that returns the factorization, which may be stored using, for example, Q, R = QR(A) .   Suppose that is the following matrix whose columns are linearly independent.    If , what is the shape of and ? What is special about the form of ?    Find the factorization using Q, R = QR(A) and verify that has the predicted shape and that .     Find the matrix that orthogonally projects vectors onto .    Find , the orthogonal projection of onto .    Explain why the equation must be consistent and then find .           is and is a upper triangular matrix.    We see that               Since is in , the system must be consistent. We find a solution by augmenting by and row reducing: .           is and is a upper triangular matrix.    We see that                .       In fact, Sage provides its own version of the factorization that is a bit different than the way we've developed the factorization here. For this reason, we have provided our own version of the factorization.    Summary  This section explored the Gram-Schmidt orthogonalization algorithm and how it leads to the matrix factorization when the columns of are linearly independent.   Beginning with a basis for a subspace of , the vectors form an orthogonal basis for .    We may scale each vector appropriately to obtain an orthonormal basis .    Expressing the Gram-Schmidt algorithm in matrix form shows that, if the columns of are linearly independent, then we can write , where the columns of form an orthonormal basis for and is upper triangular.        Suppose that a subspace of has a basis formed by    Find an orthogonal basis for .    Find an orthonormal basis for .    Find the matrix that projects vectors orthogonally onto .    Find the orthogonal projection of onto .          and      and                     and      and                  Find the factorization of .    and    Applying Gram-Schmidt, we have and , which leads to and . It therefore follows that and .  This leads to where and .    Consider the basis of given by the vectors     Apply the Gram-Schmit orthogonalization algorithm to find an orthonormal basis , , for .    If is the whose columns are , , and , find the factorization of .    Suppose that we want to solve the equation , which we can rewrite as .   If we set , the equation becomes . Explain how to solve the equation in a computationally efficient manner.    Explain how to solve the equation in a computationally efficient manner.    Find the solution by first solving and then .                          .     is upper triangular                 We find that             Since , we have .     is upper triangular so this equation can be solved using back substitution.               Consider the vectors and the subspace of that they span.    Find an orthonormal basis for .    Find the matrix that projects vectors orthogonally onto .    Find , the orthogonal projection of onto .    Express as a linear combination of , , and .                                  Applying Gram-Schmidt gives us     Let be the matrix whose columns are the orthonormal basis that results from Gram-Schmidt. Then                  Consider the set of vectors    What happens when we apply the Gram-Schmit orthogonalization algorithm?    Why does the algorithm fail to produce an orthogonal basis for ?         We find .    This set of vectors is linearly dependent.         We find Since , this does not produce a basis for .    The vector so this set of vectors is linearly dependent. Since is in the span of and , projecting into that subspace gives so that .       Suppose that is a matrix with linearly independent columns and having the factorization . Determine whether the following statements are true or false and explain your thinking.   It follows that .    The matrix is invertible.    The product projects vectors orthogonally onto .    The columns of are an orthogonal basis for .    The orthogonal complement .        True  True  False  True  True        True. Since , we have .    True. Since is upper triangular and the diagonal entries of are the lengths of the nonzero vectors , we have , which means that is invertible.    False, because .    True. In fact, they are an orthonormal basis for .    True. If , then for every vector in an orthonormal basis of . Therefore, is orthogonal to .       Suppose we have the factorization , where is a matrix.   What is the shape of the product ? Explain the significance of this product.    What is the shape of the product ? Explain the significance of this product.    What is the shape of the matrix ?    If is a diagonal matrix, what can you say about the columns of ?                        They form an orthogonal set.          is and projects vectors orthogonally onto .     is the identity matrix because the product computes dot products between the columns of .     is a upper triangular matrix.    The columns of form an orthogonal set.       Suppose we have the factorization where the columns of are and the columns of are .   How can the matrix product be expressed in terms of dot products?    How can the matrix product be expressed in terms of dot products?    Explain why .    Explain why the dot products .                        This follows from the previous parts of this exercise.         The entries of are the dot products .    The entries of are the dot products .         This follows from the previous parts of this exercise.       "
},
{
  "id": "sec-gram-schmidt-2-3",
  "level": "2",
  "url": "sec-gram-schmidt.html#sec-gram-schmidt-2-3",
  "type": "Preview Activity",
  "number": "22.4.1",
  "title": "",
  "body": "  Suppose we have a basis for consisting of the vectors as shown in . Notice that this basis is not orthogonal.      A basis for .      Find the vector that is the orthogonal projection of onto the line defined by .    Explain why is orthogonal to .    Define the new vectors and and sketch them in . Explain why and define an orthogonal basis for .      Sketch the new basis and .     Write the vector as a linear combination of and .    Scale the vectors and to produce an orthonormal basis and for .           .    The orthogonal projection is defined so that is orthogonal to .     and are othogonal, which can be checked with the dot product.    The projection formula gives .     and .      "
},
{
  "id": "fig-proj-orthog",
  "level": "2",
  "url": "sec-gram-schmidt.html#fig-proj-orthog",
  "type": "Figure",
  "number": "22.4.3",
  "title": "",
  "body": "    If is the orthogonal projection of onto , then is orthogonal to .  "
},
{
  "id": "activity-gram-schmidt",
  "level": "2",
  "url": "sec-gram-schmidt.html#activity-gram-schmidt",
  "type": "Activity",
  "number": "22.4.2",
  "title": "",
  "body": "  Suppose that is a three-dimensional subspace of with basis: We can see that this basis is not orthogonal by noting that . Our goal is to create an orthogonal basis , , and for .  To begin, we declare that , and we call the line defined by .      Find the vector that is the orthogonal projection of onto , the line defined by .    Form the vector and verify that it is orthogonal to .    Explain why by showing that any linear combination of and can be written as a linear combination of and and vice versa.    The vectors and are an orthogonal basis for a two-dimensional subspace of . Find the vector that is the orthogonal projection of onto .    Verify that is orthogonal to both and .    Explain why , , and form an orthogonal basis for .    Now find an orthonormal basis for .           .         We have and . Therefore, a linear combination of and can be rewritten as In the same way, and so any linear combination of and can be rewritten as a linear combination of and .    By the Projection Formula, .         We can check that , , and form an orthogonal set. Since can be written in terms of and vice-versa, these new vectors form a basis for .                .         We have and so a linear combination of and can be rewritten as a linear combination of and .     .         Since can be written in terms of and vice-versa, these new vectors form a basis for .           "
},
{
  "id": "example-gram-schmidt",
  "level": "2",
  "url": "sec-gram-schmidt.html#example-gram-schmidt",
  "type": "Example",
  "number": "22.4.4",
  "title": "",
  "body": "  Let's start with the basis which is a basis for .  To get started, we'll simply set . We construct from by subtracting its orthogonal projection onto , the line defined by . This gives   Notice that we found . Therefore, we can rewrite any linear combination of and as a linear combination of and . This tells us that In other words, and is a orthogonal basis for , the 2-dimensional subspace that is the span of and .  Finally, we form from by subtracting its orthogonal projection onto :   We can now check that is an orthogonal set. Furthermore, we have, as before, , which says that we have found a new orthogonal basis for .  To create an orthonormal basis, we form unit vectors parallel to each of the vectors in the orthogonal basis:    "
},
{
  "id": "sec-gram-schmidt-3-10",
  "level": "2",
  "url": "sec-gram-schmidt.html#sec-gram-schmidt-3-10",
  "type": "Activity",
  "number": "22.4.3",
  "title": "",
  "body": "  Sage can automate these computations for us. Before we begin, however, it will be helpful to understand how we can combine things using a list in Python. For instance, if the vectors v1 , v2 , and v3 form a basis for a subspace, we can bundle them together using square brackets: [v1, v2, v3] . Furthermore, we could assign this to a variable, such as basis = [v1, v2, v3] .  Evaluating the following cell will load in some special commands.    There is a command to apply the projection formula: projection(b, basis) returns the orthogonal projection of b onto the subspace spanned by basis , which is a list of vectors.    The command unit(w) returns a unit vector parallel to w .    Given a collection of vectors, say, v1 and v2 , we can form the matrix whose columns are v1 and v2 using matrix([v1, v2]).T . When given a list of vectors, Sage constructs a matrix whose rows are the given vectors. For this reason, we need to apply the transpose.     Let's now consider , the subspace of having basis    Apply the Gram-Schmidt algorithm to find an orthogonal basis , , and for .     Find , the orthogonal projection of onto .    Explain why we know that is a linear combination of the original vectors , , and and then find weights so that     Find an orthonormal basis , , for for and form the matrix whose columns are these vectors.     Find the product and explain the result.    Find the matrix that projects vectors orthogonally onto and verify that gives , the orthogonal projection that you found earlier.                    We know that is in and , , and is a basis for . We find .          since the matrix product computes the dot products of the columns of .                          .                     "
},
{
  "id": "sec-gram-schmidt-4-3",
  "level": "2",
  "url": "sec-gram-schmidt.html#sec-gram-schmidt-4-3",
  "type": "Activity",
  "number": "22.4.4",
  "title": "",
  "body": "  Suppose that is the matrix whose columns are These vectors form a basis for , the subspace of that we encountered in . Since these vectors are the columns of , we have .    When we implemented Gram-Schmidt, we first found an orthogonal basis , , and using Use these expressions to write , , and as linear combinations of , , and .    We next normalized the orthogonal basis , , and to obtain an orthonormal basis , , and .  Write the vectors as scalar multiples of . Then use these expressions to write , , and as linear combinations of , , and .    Suppose that . Use the result of the previous part to find a vector so that .    Then find vectors and such that and .    Construct the matrix . Remembering that , explain why .    What is special about the shape of ?    Suppose that is a matrix whose columns are linearly independent. This means that the columns of form a basis for , a 6-dimensional subspace of . Suppose that we apply Gram-Schmidt orthogonalization to create an orthonormal basis whose vectors form the columns of and that we write . What are the shape of and what the shape of ?           Therefore,      so we have This leads to     Since , we have .    In the same way, we have and .    We have .     so is upper triangular.     will be and will be a upper triangular matrix.                     .     and .    We have .     is upper triangular.     will be and will be .      "
},
{
  "id": "prop-qr",
  "level": "2",
  "url": "sec-gram-schmidt.html#prop-qr",
  "type": "Proposition",
  "number": "22.4.5",
  "title": "<span xmlns:pf=\"https:\/\/prefigure.org\" class=\"process-math\">\\(QR\\)<\/span> factorization.",
  "body": "  factorization  If is an matrix whose columns are linearly independent, we may write where is an matrix whose columns form an orthonormal basis for and is an upper triangular matrix.  "
},
{
  "id": "sec-gram-schmidt-4-6",
  "level": "2",
  "url": "sec-gram-schmidt.html#sec-gram-schmidt-4-6",
  "type": "Example",
  "number": "22.4.6",
  "title": "",
  "body": "  We'll consider the matrix whose columns, which we'll denote , , and , are the basis of that we considered in . There we found an orthogonal basis , , and that satisfied   In terms of the resulting orthonormal basis , , and , we had so that   Therefore, if , we have the factorization    "
},
{
  "id": "sec-gram-schmidt-4-8",
  "level": "2",
  "url": "sec-gram-schmidt.html#sec-gram-schmidt-4-8",
  "type": "Activity",
  "number": "22.4.5",
  "title": "",
  "body": "  As before, we would like to use Sage to automate the process of finding and using the factorization of a matrix . Evaluating the following cell provides a command QR(A) that returns the factorization, which may be stored using, for example, Q, R = QR(A) .   Suppose that is the following matrix whose columns are linearly independent.    If , what is the shape of and ? What is special about the form of ?    Find the factorization using Q, R = QR(A) and verify that has the predicted shape and that .     Find the matrix that orthogonally projects vectors onto .    Find , the orthogonal projection of onto .    Explain why the equation must be consistent and then find .           is and is a upper triangular matrix.    We see that               Since is in , the system must be consistent. We find a solution by augmenting by and row reducing: .           is and is a upper triangular matrix.    We see that                .      "
},
{
  "id": "sec-gram-schmidt-6-1",
  "level": "2",
  "url": "sec-gram-schmidt.html#sec-gram-schmidt-6-1",
  "type": "Exercise",
  "number": "22.4.4.1",
  "title": "",
  "body": " Suppose that a subspace of has a basis formed by    Find an orthogonal basis for .    Find an orthonormal basis for .    Find the matrix that projects vectors orthogonally onto .    Find the orthogonal projection of onto .          and      and                     and      and                "
},
{
  "id": "sec-gram-schmidt-6-2",
  "level": "2",
  "url": "sec-gram-schmidt.html#sec-gram-schmidt-6-2",
  "type": "Exercise",
  "number": "22.4.4.2",
  "title": "",
  "body": " Find the factorization of .    and    Applying Gram-Schmidt, we have and , which leads to and . It therefore follows that and .  This leads to where and .  "
},
{
  "id": "sec-gram-schmidt-6-3",
  "level": "2",
  "url": "sec-gram-schmidt.html#sec-gram-schmidt-6-3",
  "type": "Exercise",
  "number": "22.4.4.3",
  "title": "",
  "body": " Consider the basis of given by the vectors     Apply the Gram-Schmit orthogonalization algorithm to find an orthonormal basis , , for .    If is the whose columns are , , and , find the factorization of .    Suppose that we want to solve the equation , which we can rewrite as .   If we set , the equation becomes . Explain how to solve the equation in a computationally efficient manner.    Explain how to solve the equation in a computationally efficient manner.    Find the solution by first solving and then .                          .     is upper triangular                 We find that             Since , we have .     is upper triangular so this equation can be solved using back substitution.             "
},
{
  "id": "sec-gram-schmidt-6-4",
  "level": "2",
  "url": "sec-gram-schmidt.html#sec-gram-schmidt-6-4",
  "type": "Exercise",
  "number": "22.4.4.4",
  "title": "",
  "body": " Consider the vectors and the subspace of that they span.    Find an orthonormal basis for .    Find the matrix that projects vectors orthogonally onto .    Find , the orthogonal projection of onto .    Express as a linear combination of , , and .                                  Applying Gram-Schmidt gives us     Let be the matrix whose columns are the orthonormal basis that results from Gram-Schmidt. Then                "
},
{
  "id": "sec-gram-schmidt-6-5",
  "level": "2",
  "url": "sec-gram-schmidt.html#sec-gram-schmidt-6-5",
  "type": "Exercise",
  "number": "22.4.4.5",
  "title": "",
  "body": " Consider the set of vectors    What happens when we apply the Gram-Schmit orthogonalization algorithm?    Why does the algorithm fail to produce an orthogonal basis for ?         We find .    This set of vectors is linearly dependent.         We find Since , this does not produce a basis for .    The vector so this set of vectors is linearly dependent. Since is in the span of and , projecting into that subspace gives so that .     "
},
{
  "id": "sec-gram-schmidt-6-6",
  "level": "2",
  "url": "sec-gram-schmidt.html#sec-gram-schmidt-6-6",
  "type": "Exercise",
  "number": "22.4.4.6",
  "title": "",
  "body": " Suppose that is a matrix with linearly independent columns and having the factorization . Determine whether the following statements are true or false and explain your thinking.   It follows that .    The matrix is invertible.    The product projects vectors orthogonally onto .    The columns of are an orthogonal basis for .    The orthogonal complement .        True  True  False  True  True        True. Since , we have .    True. Since is upper triangular and the diagonal entries of are the lengths of the nonzero vectors , we have , which means that is invertible.    False, because .    True. In fact, they are an orthonormal basis for .    True. If , then for every vector in an orthonormal basis of . Therefore, is orthogonal to .     "
},
{
  "id": "sec-gram-schmidt-6-7",
  "level": "2",
  "url": "sec-gram-schmidt.html#sec-gram-schmidt-6-7",
  "type": "Exercise",
  "number": "22.4.4.7",
  "title": "",
  "body": " Suppose we have the factorization , where is a matrix.   What is the shape of the product ? Explain the significance of this product.    What is the shape of the product ? Explain the significance of this product.    What is the shape of the matrix ?    If is a diagonal matrix, what can you say about the columns of ?                        They form an orthogonal set.          is and projects vectors orthogonally onto .     is the identity matrix because the product computes dot products between the columns of .     is a upper triangular matrix.    The columns of form an orthogonal set.     "
},
{
  "id": "sec-gram-schmidt-6-8",
  "level": "2",
  "url": "sec-gram-schmidt.html#sec-gram-schmidt-6-8",
  "type": "Exercise",
  "number": "22.4.4.8",
  "title": "",
  "body": " Suppose we have the factorization where the columns of are and the columns of are .   How can the matrix product be expressed in terms of dot products?    How can the matrix product be expressed in terms of dot products?    Explain why .    Explain why the dot products .                        This follows from the previous parts of this exercise.         The entries of are the dot products .    The entries of are the dot products .         This follows from the previous parts of this exercise.     "
},
{
  "id": "sec-least-squares",
  "level": "1",
  "url": "sec-least-squares.html",
  "type": "Section",
  "number": "22.5",
  "title": "Orthogonal least squares",
  "body": " Orthogonal least squares   Suppose we collect some data when performing an experiment and plot it as shown on the left of . Notice that there is no line on which all the points lie; in fact, it would be surprising if there were since we can expect some uncertainty in the measurements recorded. There does, however, appear to be a line, as shown on the right, on which the points almost lie.       A collection of points and a line approximating the linear relationship implied by them.   In this section, we'll explore how the techniques developed in this chapter enable us to find the line that best approximates the data. More specifically, we'll see how the search for a line passing through the data points leads to an inconsistent system . Since we are unable to find a solution, we instead seek the vector where is as close as possible to . Orthogonal projection gives us just the right tool for doing this.       Is there a solution to the equation where and are such that      We know that and form a basis for . Find an orthogonal basis for .    Find the orthogonal projection of onto .    Explain why the equation must be consistent and then find its solution.          The reduced row echelon form shows that there is no solution.    Applying Gram-Schmidt, we find an orthogonal basis consisting of and .    The projection formula gives .    The equation is consistent because is in . We find the solution .         A first example  When we've encountered inconsistent systems in the past, we've simply said there is no solution and moved on. The preview activity, however, shows how we can find approximate solutions to an inconsistent system: if there are no solutions to , we instead solve the consistent system , the orthogonal projection of onto . As we'll see, this solution is, in a specific sense, the best possible.    Suppose we have three data points , , and and that we would like to find a line passing through them.   Plot these three points in . Are you able to draw a line that passes through all three points?     Plot the three data points here.      Remember that the equation of a line can be written as where is the slope and is the -intercept. We will try to find and so that the three points lie on the line.  The first data point gives an equation for and . In particular, we know that when , then so we have or . Use the other two data points to create a linear system describing and .    We have obtained a linear system having three equations, one from each data point, for the two unknowns and . Identify a matrix and vector so that the system has the form , where .  Notice that the unknown vector describes the line that we seek.    Is there a solution to this linear system? How does this question relate to your attempt to draw a line through the three points above?     Since this system is inconsistent, we know that is not in the column space . Find an orthogonal basis for and use it to find the orthogonal projection of onto .    Since is in , the equation is consistent. Find its solution and sketch the line in . We say that this is the line of best fit.          After plotting the points, we see that it's not possible to draw a line through all three points.    We have the equations     We have and .    Finding the reduced row echelon form of the associated augmented matrix tells us this is an inconsistent system. Since a solution would describe a line passing through the three points, we should expect this.    Applying Gram-Schmidt gives us the orthogonal basis and . Projecting onto gives .    Solving the equation gives , which describes a line having vertical intercept and the slope . This line is shown in .      The line that best approximates the three data points.           It's not possible to draw a line through all three points.    We have the equations     We have and .    This linear system is inconsistent.     .     . This line is shown in .      The line that best approximates the three data points.        This activity illustrates the idea behind a technique known as orthogonal least squares , which we have been working toward throughout this chapter. If the data points are denoted as , we construct the matrix and vector as With the vector representing the line , we see that the equation describes a line passing through all the data points. In our activity, it is visually apparent that there is no such line, which agrees with the fact that the equation is inconsistent.  Remember that , the orthogonal projection of onto , is the closest vector in to . Therefore, when we solve the equation , we are finding the vector so that is as close to as possible. Let's think about what this means within the context of this problem.  The difference so that the square of the distance between and is Our approach finds the values for and that make this sum of squares as small as possible, which is why we call this a least-squares problem.  Drawing the line defined by the vector , the quantity reflects the vertical distance between the line and the data point , as shown in . Seen in this way, the square of the distance is a measure of how much the line defined by the vector misses the data points. The solution to the least-squares problem is the line that misses the data points by the smallest amount possible.      The solution of the least-squares problem and the vertical distances between the line and the data points.     Solving least-squares problems  Now that we've seen an example of what we're trying to accomplish, let's put this technique into a more general framework.  Given an inconsistent system , we seek the vector that minimizes the distance from to . In other words, satisfies , where is the orthogonal projection of onto the column space . We know the equation is consistent since is in , and we know there is only one solution if we assume that the columns of are linearly independent.  We will usually denote the solution of by and call this vector the least-squares approximate solution of to distinguish it from a (possibly non-existent) solution of .  There is an alternative method for finding that does not involve first finding the orthogonal projection . Remember that is defined by the fact that is orthogonal to . In other words, is in the orthogonal complement , which tells us is the same as . Since is in , it follows that Because the least-squares approximate solution is the vector such that , we can rearrange this equation to see that  normal equation This equation is called the normal equation , and we have the following proposition.    If the columns of are linearly independent, then there is a unique least-squares approximate solution to the equation given by the normal equation      Consider the equation with matrix and vector . Since this equation is inconsistent, we will find the least-squares approximate solution by solving the normal equation , which has the form and the solution .     The rate at which a cricket chirps is related to the outdoor temperature, as reflected in some experimental data that we'll study in this activity. The chirp rate is expressed in chirps per second while the temperature is in degrees Fahrenheit. Evaluate the following cell to load the data: Evaluating this cell also provides:   the vectors chirps and temps formed from the columns of the dataset.    the command onesvec(n) , which creates an -dimensional vector whose entries are all one.    Remember that you can form a matrix whose columns are the vectors v1 and v2 with matrix([v1, v2]).T .     We would like to represent this relationship by a linear function    Use the first data point to write an equation involving and .    Suppose that we represent the unknowns using a vector . Use the 15 data points to create the matrix and vector so that the linear system describes the unknown vector .     Write the normal equations ; that is, find the matrix and the vector .    Solve the normal equations to find , the least-squares approximate solution to the equation . Call your solution xhat since x has another meaning in Sage.   What are the values of and that you found?    If the chirp rate is 22 chirps per second, what is your prediction for the temperature?  You can plot the data and your line, assuming you called the solution xhat , using the cell below.           We have the equation .     is the matrix whose first column consists only of 1's and whose second column is the vector of chirp rates. The vector is the vector of temperatures.     and      .    The predicted temperature is degrees.           .     is the matrix whose first column consists only of 1's and whose second column is the vector of chirp rates. The vector is the vector of temperatures.     and      .     degrees.       This example demonstrates an approach, called linear regression , in which a collection of data is modeled using a linear function found by solving a least-squares problem. Once we have the linear function that best fits the data, we can make predictions about situations that we haven't encountered in the data.  If we're going to use our function to make predictions, it's natural to ask how much confidence we have in these predictions. This is a statistical question that leads to a rich and well-developed theory For example, see Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani. An Introduction to Statistical Learning: with Applications in R. Springer, 2013. , which we won't explore in much detail here. However, there is one simple measure of how well our linear function fits the data that is known as the coefficient of determination and denoted by .  We have seen that the square of the distance measures the amount by which the line fails to pass through the data points. When the line is close to the data points, we expect this number to be small. However, the size of this measure depends on the scale of the data. For instance, the two lines shown in seem to fit the data equally well, but is 100 times larger on the right.       The lines appear to fit equally well in spite of the fact that differs by a factor of 100.   The coefficient of determination is defined by normalizing so that it is independent of the scale. Recall that we described how to demean a vector in : given a vector , we obtain by subtracting the average of the components from each component.   Coefficient of determination  coefficient of determination  R squared   The coefficient of determination is where is the vector obtained by demeaning .    A more complete explanation of this definition relies on the concept of variance, which we explore in and the next chapter. For the time being, it's enough to know that and that the closer is to 1, the better the line fits the data. In our original example, illustrated in , we find that , and in our study of cricket chirp rates, we have . However, assessing the confidence we have in predictions made by solving a least-squares problem can require considerable thought, and it would be naive to rely only on the value of .    Using factorizations  As we've seen, the least-squares approximate solution to may be found by solving the normal equation , and this can be a practical strategy for some problems. However, this approach can be problematic as small rounding errors can accumulate and lead to inaccurate final results.  As the next activity demonstrates, there is an alternate method for finding the least-squares approximate solution using a factorization of the matrix , and this method is preferable as it is numerically more reliable.       Suppose we are interested in finding the least-squares approximate solution to the equation and that we have the factorization . Explain why the least-squares approximate solution is given by solving     Multiply both sides of the second expression by and explain why   Since is upper triangular, this is a relatively simple equation to solve using back substitution, as we saw in . We will therefore write the least-squares approximate solution as and put this to use in the following context.    Brozak’s formula, which is used to calculate a person's body fat index , is where denotes a person's body density in grams per cubic centimeter. Obtaining an accurate measure of is difficult, however, because it requires submerging the person in water and measuring the volume of water displaced. Instead, we will gather several other body measurements, which are more easily obtained, and use it to predict .  For instance, suppose we take 10 patients and measure their weight in pounds, height in inches, abdomen in centimeters, wrist circumference in centimeters, neck circumference in centimeters, and . Evaluating the following cell loads and displays the data. In addition, that cell provides:   vectors weight , height , abdomen , wrist , neck , and BFI formed from the columns of the dataset.    the command onesvec(n) , which returns an -dimensional vector whose entries are all one.    the command QR(A) that returns the factorization of as Q, R = QR(A) .    the command demean(v) , which returns the demeaned vector .     We would like to find the linear function that best fits the data.  Use the first data point to write an equation for the parameters .    Describe the linear system for these parameters. More specifically, describe how the matrix and the vector are formed.    Construct the matrix and find its factorization in the cell below.     Find the least-squares approximate solution by solving the equation . You may want to use N(xhat) to display a decimal approximation of the vector. What are the parameters that best fit the data?    Find the coefficient of determination for your parameters. What does this imply about the quality of the fit?     Suppose a person's measurements are: weight 190, height 70, abdomen 90, wrist 18, and neck 35. Estimate this person's .          The columns of form an orthonormal basis for so that . The equation then becomes .    Since , we have , which gives .          is the matrix whose columns are a vector of all 1's followed by the vectors of weights, heights, abdominal, wrist, and neck measurements. The vector is the vector of BFI readings.     is a matrix and is a upper triangular matrix.    We find that          Evaluating           Use the fact that .    Use the fact that .          is the matrix whose columns are a vector of all 1's followed by the vectors of weights, heights, abdominal, wrist, and neck measurements. The vector is the vector of BFI readings.     is a matrix and is a upper triangular matrix.    We find that          Evaluating        To summarize, we have seen that    If the columns of are linearly independent and we have the factorization , then the least-squares approximate solution to the equation is given by       Polynomial Regression  In the examples we've seen so far, we have fit a linear function to a dataset. Sometimes, however, a polynomial, such as a quadratic function, may be more appropriate. It turns out that the techniques we've developed in this section are still useful as the next activity demonstrates.       Suppose that we have a small dataset containing the points , , , and , such as appear when the following cell is evaluated. In addition to loading and plotting the data, evaluating that cell provides the following commands:    Q, R = QR(A) returns the factorization of .     demean(v) returns the demeaned vector .     Let's fit a quadratic function of the form to this dataset.  Write four equations, one for each data point, that describe the coefficients , , and .    Express these four equations as a linear system where .  Find the factorization of and use it to find the least-squares approximate solution .     Use the parameters , , and that you found to write the quadratic function that fits the data. You can plot this function, along with the data, by entering your function in the place indicated below.     What is your predicted value when ?    Find the coefficient of determination for the quadratic function. What does this say about the quality of the fit?    Now fit a cubic polynomial of the form to this dataset.     Find the coefficient of determination for the cubic function. What does this say about the quality of the fit?    What do you notice when you plot the cubic function along with the data? How does this reflect the value of that you found?           We have the equations     With and , we find     The quadratic function is .    The predicted value is .         We find .     , which means that we have a perfect fit.    The graph of the cubic function passes through each data point.          We have the equations           .     .          .         The graph of the cubic function passes through each data point.       The matrices that you created in the last activity when fitting a quadratic and cubic function to a dataset have a special form. In particular, if the data points are labeled and we seek a degree polynomial, then This is called a Vandermonde matrix of degree .    This activity explores a dataset describing Arctic sea ice and that comes from Sustainability Math.   Evaluating the cell below will plot the extent of Arctic sea ice, in millions of square kilometers, during the twelve months of 2012. In addition, you have access to a few special variables and commands:    month is the vector of month values and ice is the vector of sea ice values from the table above.     vandermonde(x, k) constructs the Vandermonde matrix of degree using the points in the vector x .     Q, R = QR(A) provides the factorization of .     demean(v) returns the demeaned vector .        Find the vector , the least-squares approximate solution to the linear system that results from fitting a degree 5 polynomial to the data.     If your result is stored in the variable xhat , you may plot the polynomial and the data together using the following cell.     Find the coefficient of determination for this polynomial fit.    Repeat these steps to fit a degree 8 polynomial to the data, plot the polynomial with the data, and find .     Repeat one more time by fitting a degree 11 polynomial to the data, creating a plot, and finding .   It's certainly true that higher degree polynomials fit the data better, as seen by the increasing values of , but that's not always a good thing. For instance, when , you may notice that the graph of the polynomial wiggles a little more than we would expect. In this case, the polynomial is trying too hard to fit the data, which usually contains some uncertainty, especially if it's obtained from measurements. The error built in to the data is called noise, and its presence means that we shouldn't expect our polynomial to fit the data perfectly. When we choose a polynomial whose degree is too high, we give the noise too much weight in the model, which leads to some undesirable behavior, like the wiggles in the graph.  Fitting the data with a polynomial whose degree is too high is called overfitting , a phenomenon that can appear in many machine learning applications. Generally speaking, we would like to choose large enough to capture the essential features of the data but not so large that we overfit and build the noise into the model. There are ways to determine the optimal value of , but we won't pursue that here.    Choosing a reasonable value of , estimate the extent of Arctic sea ice at month 6.5, roughly at the Summer Solstice.               The fifth degree polynomial fits the data fairly well.          .          seems like a good choice, and this gives the prediction of million square kilometers of sea ice.               The fifth degree polynomial fits the data fairly well.          .          million square kilometers of sea ice.         Summary  This section introduced some types of least-squares problems and a framework for working with them.   Given an inconsistent system , we find , the least-squares approximate solution, by requiring that be as close to as possible. In other words, where is the orthogonal projection of onto .    One way to find is by solving the normal equations This is not our preferred method since numerical problems can arise.    A second way to find uses a factorization of . If , then and finding is computationally feasible since is upper triangular.    This technique may be applied widely and is useful for modeling data. We saw examples in this section where linear functions of several input variables and polynomials provided effective models for different datasets.    A simple measure of the quality of the fit is the coefficient of determination though some additional thought should be given in real applications.       Evaluating the following cell loads in some commands that will be helpful in the following exercises. In particular, there are commands:    QR(A) that returns the factorization of A as Q, R = QR(A) ,     onesvec(n) that returns the -dimensional vector whose entries are all 1,     demean(v) that demeans the vector v ,     vandermonde(x, k) that returns the Vandermonde matrix of degree formed from the components of the vector x , and     plot_model(xhat, data) that plots the data and the model xhat .       Suppose we write the linear system as .   Find an orthogonal basis for .    Find , the orthogonal projection of onto .    Find a solution to the linear system .          and      .     .         Applying Gram-Schmidt gives the orthogonal basis     Applying the Projection Formula gives .    Solving the linear system gives .       Consider the data in .  A dataset with four points.            1  1    2  1    3  1    4  2        Set up the linear system that describes the line passing through these points.    Write the normal equations that describe the least-squares approximate solution to .    Find the least-squares approximate solution and plot the data and the resulting line.    What is your predicted -value when ?    Find the coefficient of determination .          , .               .              The matrix and the vector .              The predicted value is .            Consider the four points in .    Set up a linear system that describes a quadratic function passing through the points.    Use a factorization to find the least-squares approximate solution and plot the data and the graph of the resulting quadratic function.    What is your predicted -value when ?    Find the coefficient of determination .         We have the Vandermonde matrix and vector .    The quadratic function is .    This gives the predicted value .            Consider the data in .  A simple dataset               1  1  4.2    1  2  3.3    2  1  5.9    2  2  5.1    3  2  7.5    3  3  6.3        Set up a linear system that describes the relationship     Find the least-squares approximate solution .    What is your predicted -value when and ?    Find the coefficient of determination .                    .                         .            Determine whether the following statements are true or false and explain your thinking.   If is consistent, then is a solution to .    If , then the least-squares approximate solution is also a solution to the original equation .    Given the factorization , we have .    A factorization provides a method for finding the least-squares approximate solution to that is more reliable than solving the normal equations.    A solution to is the least-squares approximate solution to .        True  True  False  True  False        True. If , then is in so .    True. If , then . Therefore, .    False. The product rather than the matrix that projects vectors orthogonally onto .    True, numerical issues are more likely to arise when solving the normal equations.    False. The normal equations gives the least-squares approximate solution.       Explain your response to the following questions.   If , what does this say about the vector ?    If the columns of are orthonormal, how can you easily find the least-squares approximate solution to ?          is in .     .         Since and , we know that . This says that is in .    In this case, , which means we have . If we multiply both sides by , we have .       The following cell loads in some data showing the number of people in Bangladesh living without electricity over 27 years. It also defines vectors year , which records the years in the dataset, and people , which records the number of people.    Suppose we want to write where is the year and is the number of people. Construct the matrix and vector so that the linear system describes the vector .    Using a factorization of , find the values of and in the least-squares approximate solution .    What is the coefficient of determination and what does this tell us about the quality of the approximation?    What is your prediction for the number of people living without electricity in 1985?    Estimate the year in which there will be no people living without electricity.         The matrix will be the matrix whose first column is all 1's and whose second column is the vector of years. The vector is the vector that records the number of people.     .          .    2045         The matrix will be the matrix whose first column is all 1's and whose second column is the vector of years. The vector is the vector that records the number of people.    We obtain .          .    Solve to obtain 2045.       This problem concerns a dataset describing planets in our Solar system. For each planet, we have the length of the semi-major axis, essentially the distance from the planet to the Sun in AU (astronomical units), and the period , the length of time in years required to complete one orbit around the Sun.  We would like to model this data using the function where and are parameters we need to determine. Since this isn't a linear function, we will transform this relationship by taking the natural logarithm of both sides to obtain   Evaluating the following cell loads the dataset and defines two vectors logaxis , whose components are , and logperiod , whose components are .    Construct the matrix and vector so that the solution to is the vector .    Find the least-squares approximate solution . What does this give for the values of and ?    Find the coefficient of determination . What does this tell us about the quality of the approximation?   Suppose that the orbit of an asteroid has a semi-major axis whose length is AU. Estimate the period of the asteroid's orbit.   Halley's Comet has a period of years. Estimate the length of its semi-major axis.         The matrix is the matrix whose first column is all 1's and whose second column is the vector of the logarithms of the semi-major axes. The vector is the vector that records the logarithms of the periods.     and .          years.     AU.         The matrix is the matrix whose first column is all 1's and whose second column is the vector of the logarithms of the semi-major axes. The vector is the vector that records the logarithms of the periods.    We find so that and . This means that .     , which means that we have a very good fit because this data reflects a physical law.     years.    The expression means that . Solving for AU.       Evaluating the following cell loads a dataset describing the temperature in the Earth's atmosphere at various altitudes. There are also two vectors altitude , expressed in kilometers, and temperature , in degrees Celsius.    Describe how to form the matrix and vector so that the linear system describes a degree polynomial fitting the data.    After choosing a value of , construct the matrix and vector , and find the least-squares approximate solution .    Plot the polynomial and data using plot_model(xhat, data) .    Now examine what happens as you vary the degree of the polynomial . Choose an appropriate value of that seems to capture the most important features of the data while avoiding overfitting, and explain your choice.    Use your value of to estimate the temperature at an altitude of 55 kilometers.         The matrix will be the Vandermonde matrix of degree using the altitude readings while is the vector of temperatures.     appears to give a good fit without overfitting.    We have     A small value of does not adequately track important features of the data. If we increase too far, the graph tries too hard to match the data, and we see spurious features in the graph.    We estimate the temperature to be degrees Celsius.         The matrix will be the Vandermonde matrix of degree using the altitude readings while is the vector of temperatures.     appears to give a good fit without overfitting.    We have     A small value of does not adequately track important features of the data. If we increase too far, the graph tries too hard to match the data, and we see spurious features in the graph.     seems to work well. With this function, we estimate the temperature to be degrees Celsius.       The following cell loads some data describing 1057 houses in a particular real estate market. For each house, we record the living area in square feet, the lot size in acres, the age in years, and the price in dollars. The cell also defines variables area , size , age , and price . We will use linear regression to predict the price of a house given its living area, lot size, and age:    Use a factorization to find the least-squares approximate solution .    Discuss the significance of the signs of , , and .    If two houses are identical except for differing in age by one year, how would you predict that their prices compare to each another?    Find the coefficient of determination . What does this say about the quality of the fit?    Predict the price of a house whose living area is 2000 square feet, lot size is 1.5 acres, and age is 50 years.              Increasing the living area or the lot size will cause the house price to increase, but an older house will cost less.    A house that's one year older will cost about less.         We estimate .               and are positive, which means that increasing the living area or the lot size will cause the house price to increase. However, is negative, which means that an older house will cost less.    Because , we would expect a house that's one year older to cost less.     , which is far from perfect without being terrible. There are other factors that are not included in the dataset that can also influence the house price, such as number of bedrooms and the location.    We estimate .       We observed that if the columns of are linearly independent, then there is a unique least-squares approximate solution to the equation because the equation has a unique solution. We also said that is the unique solution to the normal equation without explaining why this equation has a unique solution. This exercise offers an explanation.  Assuming that the columns of are linearly independent, we would like to conclude that the equation has a unique solution.   Suppose that is a vector for which . Explain why the following argument is valid and allows us to conclude that . In other words, if , we know that .    If the columns of are linearly independent and , what do we know about the vector ?    Explain why can only happen when .    Assuming that the columns of are linearly independent, explain why has a unique solution.         Use the fact that .    It must be true that .    This follows from the previous two parts of this exercise.    The columns of must be linearly independent.         Starting with the assumption that , we dot both sides of the equation with to obtain . From here, we write This says that the length of must be zero, which tells us that .    If the columns of are linearly independent, we know that the only solution to the homogeneous equation is .    The previous two parts of this exercise tell us that implies that , which implies that .    Since the only solution to the homogeneous equation is , the columns of are linearly independent, which means that has only one solution.       This problem is about the meaning of the coefficient of determination and its connection to variance, a topic that appears in the next section. Throughout this problem, we consider the linear system and the approximate least-squares solution , where . We suppose that is an matrix, and we will denote the -dimensional vector .     Explain why , the mean of the components of , can be found as the dot product     In the examples we have seen in this section, explain why is in .    If we write , explain why and hence why the mean of the components of is zero.    The variance of an -dimensional vector is , where is the vector obtained by demeaning .  Explain why     Explain why and hence   These expressions indicate why it is sometimes said that measures the fraction of variance explained by the function we are using to fit the data. As seen in the previous exercise, there may be other features that are not recorded in the dataset that influence the quantity we wish to predict.    Explain why .          simply sums the components of .     is a column of .     is in .    Use the fact that and are orthogonal.    This follows from .    This follow from .          simply sums the components of .    The examples we've seen fit the data using functions that have a constant term such as . This means that will be a column of the matrix and hence in .    Since is in , any vector in , such as , will be orthogonal to .    Notice that which says that Since and are orthogonal, we have . This means that so that .     , which explains why Then     The variances are nonnegative so we have . Also,        "
},
{
  "id": "lst-squares-intro",
  "level": "2",
  "url": "sec-least-squares.html#lst-squares-intro",
  "type": "Figure",
  "number": "22.5.1",
  "title": "",
  "body": "     A collection of points and a line approximating the linear relationship implied by them.  "
},
{
  "id": "sec-least-squares-2-4",
  "level": "2",
  "url": "sec-least-squares.html#sec-least-squares-2-4",
  "type": "Preview Activity",
  "number": "22.5.1",
  "title": "",
  "body": "     Is there a solution to the equation where and are such that      We know that and form a basis for . Find an orthogonal basis for .    Find the orthogonal projection of onto .    Explain why the equation must be consistent and then find its solution.          The reduced row echelon form shows that there is no solution.    Applying Gram-Schmidt, we find an orthogonal basis consisting of and .    The projection formula gives .    The equation is consistent because is in . We find the solution .      "
},
{
  "id": "sec-least-squares-3-3",
  "level": "2",
  "url": "sec-least-squares.html#sec-least-squares-3-3",
  "type": "Activity",
  "number": "22.5.2",
  "title": "",
  "body": "  Suppose we have three data points , , and and that we would like to find a line passing through them.   Plot these three points in . Are you able to draw a line that passes through all three points?     Plot the three data points here.      Remember that the equation of a line can be written as where is the slope and is the -intercept. We will try to find and so that the three points lie on the line.  The first data point gives an equation for and . In particular, we know that when , then so we have or . Use the other two data points to create a linear system describing and .    We have obtained a linear system having three equations, one from each data point, for the two unknowns and . Identify a matrix and vector so that the system has the form , where .  Notice that the unknown vector describes the line that we seek.    Is there a solution to this linear system? How does this question relate to your attempt to draw a line through the three points above?     Since this system is inconsistent, we know that is not in the column space . Find an orthogonal basis for and use it to find the orthogonal projection of onto .    Since is in , the equation is consistent. Find its solution and sketch the line in . We say that this is the line of best fit.          After plotting the points, we see that it's not possible to draw a line through all three points.    We have the equations     We have and .    Finding the reduced row echelon form of the associated augmented matrix tells us this is an inconsistent system. Since a solution would describe a line passing through the three points, we should expect this.    Applying Gram-Schmidt gives us the orthogonal basis and . Projecting onto gives .    Solving the equation gives , which describes a line having vertical intercept and the slope . This line is shown in .      The line that best approximates the three data points.           It's not possible to draw a line through all three points.    We have the equations     We have and .    This linear system is inconsistent.     .     . This line is shown in .      The line that best approximates the three data points.       "
},
{
  "id": "fig-least-squares-def",
  "level": "2",
  "url": "sec-least-squares.html#fig-least-squares-def",
  "type": "Figure",
  "number": "22.5.5",
  "title": "",
  "body": "    The solution of the least-squares problem and the vertical distances between the line and the data points.  "
},
{
  "id": "sec-least-squares-4-6",
  "level": "2",
  "url": "sec-least-squares.html#sec-least-squares-4-6",
  "type": "Proposition",
  "number": "22.5.6",
  "title": "",
  "body": "  If the columns of are linearly independent, then there is a unique least-squares approximate solution to the equation given by the normal equation    "
},
{
  "id": "sec-least-squares-4-7",
  "level": "2",
  "url": "sec-least-squares.html#sec-least-squares-4-7",
  "type": "Example",
  "number": "22.5.7",
  "title": "",
  "body": " Consider the equation with matrix and vector . Since this equation is inconsistent, we will find the least-squares approximate solution by solving the normal equation , which has the form and the solution .  "
},
{
  "id": "sec-least-squares-4-8",
  "level": "2",
  "url": "sec-least-squares.html#sec-least-squares-4-8",
  "type": "Activity",
  "number": "22.5.3",
  "title": "",
  "body": "  The rate at which a cricket chirps is related to the outdoor temperature, as reflected in some experimental data that we'll study in this activity. The chirp rate is expressed in chirps per second while the temperature is in degrees Fahrenheit. Evaluate the following cell to load the data: Evaluating this cell also provides:   the vectors chirps and temps formed from the columns of the dataset.    the command onesvec(n) , which creates an -dimensional vector whose entries are all one.    Remember that you can form a matrix whose columns are the vectors v1 and v2 with matrix([v1, v2]).T .     We would like to represent this relationship by a linear function    Use the first data point to write an equation involving and .    Suppose that we represent the unknowns using a vector . Use the 15 data points to create the matrix and vector so that the linear system describes the unknown vector .     Write the normal equations ; that is, find the matrix and the vector .    Solve the normal equations to find , the least-squares approximate solution to the equation . Call your solution xhat since x has another meaning in Sage.   What are the values of and that you found?    If the chirp rate is 22 chirps per second, what is your prediction for the temperature?  You can plot the data and your line, assuming you called the solution xhat , using the cell below.           We have the equation .     is the matrix whose first column consists only of 1's and whose second column is the vector of chirp rates. The vector is the vector of temperatures.     and      .    The predicted temperature is degrees.           .     is the matrix whose first column consists only of 1's and whose second column is the vector of chirp rates. The vector is the vector of temperatures.     and      .     degrees.      "
},
{
  "id": "fig-regression-scale",
  "level": "2",
  "url": "sec-least-squares.html#fig-regression-scale",
  "type": "Figure",
  "number": "22.5.8",
  "title": "",
  "body": "     The lines appear to fit equally well in spite of the fact that differs by a factor of 100.  "
},
{
  "id": "sec-least-squares-4-14",
  "level": "2",
  "url": "sec-least-squares.html#sec-least-squares-4-14",
  "type": "Definition",
  "number": "22.5.9",
  "title": "Coefficient of determination.",
  "body": " Coefficient of determination  coefficient of determination  R squared   The coefficient of determination is where is the vector obtained by demeaning .   "
},
{
  "id": "activity-BFI",
  "level": "2",
  "url": "sec-least-squares.html#activity-BFI",
  "type": "Activity",
  "number": "22.5.4",
  "title": "",
  "body": "     Suppose we are interested in finding the least-squares approximate solution to the equation and that we have the factorization . Explain why the least-squares approximate solution is given by solving     Multiply both sides of the second expression by and explain why   Since is upper triangular, this is a relatively simple equation to solve using back substitution, as we saw in . We will therefore write the least-squares approximate solution as and put this to use in the following context.    Brozak’s formula, which is used to calculate a person's body fat index , is where denotes a person's body density in grams per cubic centimeter. Obtaining an accurate measure of is difficult, however, because it requires submerging the person in water and measuring the volume of water displaced. Instead, we will gather several other body measurements, which are more easily obtained, and use it to predict .  For instance, suppose we take 10 patients and measure their weight in pounds, height in inches, abdomen in centimeters, wrist circumference in centimeters, neck circumference in centimeters, and . Evaluating the following cell loads and displays the data. In addition, that cell provides:   vectors weight , height , abdomen , wrist , neck , and BFI formed from the columns of the dataset.    the command onesvec(n) , which returns an -dimensional vector whose entries are all one.    the command QR(A) that returns the factorization of as Q, R = QR(A) .    the command demean(v) , which returns the demeaned vector .     We would like to find the linear function that best fits the data.  Use the first data point to write an equation for the parameters .    Describe the linear system for these parameters. More specifically, describe how the matrix and the vector are formed.    Construct the matrix and find its factorization in the cell below.     Find the least-squares approximate solution by solving the equation . You may want to use N(xhat) to display a decimal approximation of the vector. What are the parameters that best fit the data?    Find the coefficient of determination for your parameters. What does this imply about the quality of the fit?     Suppose a person's measurements are: weight 190, height 70, abdomen 90, wrist 18, and neck 35. Estimate this person's .          The columns of form an orthonormal basis for so that . The equation then becomes .    Since , we have , which gives .          is the matrix whose columns are a vector of all 1's followed by the vectors of weights, heights, abdominal, wrist, and neck measurements. The vector is the vector of BFI readings.     is a matrix and is a upper triangular matrix.    We find that          Evaluating           Use the fact that .    Use the fact that .          is the matrix whose columns are a vector of all 1's followed by the vectors of weights, heights, abdominal, wrist, and neck measurements. The vector is the vector of BFI readings.     is a matrix and is a upper triangular matrix.    We find that          Evaluating       "
},
{
  "id": "sec-least-squares-5-6",
  "level": "2",
  "url": "sec-least-squares.html#sec-least-squares-5-6",
  "type": "Proposition",
  "number": "22.5.10",
  "title": "",
  "body": "  If the columns of are linearly independent and we have the factorization , then the least-squares approximate solution to the equation is given by    "
},
{
  "id": "sec-least-squares-6-3",
  "level": "2",
  "url": "sec-least-squares.html#sec-least-squares-6-3",
  "type": "Activity",
  "number": "22.5.5",
  "title": "",
  "body": "     Suppose that we have a small dataset containing the points , , , and , such as appear when the following cell is evaluated. In addition to loading and plotting the data, evaluating that cell provides the following commands:    Q, R = QR(A) returns the factorization of .     demean(v) returns the demeaned vector .     Let's fit a quadratic function of the form to this dataset.  Write four equations, one for each data point, that describe the coefficients , , and .    Express these four equations as a linear system where .  Find the factorization of and use it to find the least-squares approximate solution .     Use the parameters , , and that you found to write the quadratic function that fits the data. You can plot this function, along with the data, by entering your function in the place indicated below.     What is your predicted value when ?    Find the coefficient of determination for the quadratic function. What does this say about the quality of the fit?    Now fit a cubic polynomial of the form to this dataset.     Find the coefficient of determination for the cubic function. What does this say about the quality of the fit?    What do you notice when you plot the cubic function along with the data? How does this reflect the value of that you found?           We have the equations     With and , we find     The quadratic function is .    The predicted value is .         We find .     , which means that we have a perfect fit.    The graph of the cubic function passes through each data point.          We have the equations           .     .          .         The graph of the cubic function passes through each data point.      "
},
{
  "id": "sec-least-squares-6-5",
  "level": "2",
  "url": "sec-least-squares.html#sec-least-squares-6-5",
  "type": "Activity",
  "number": "22.5.6",
  "title": "",
  "body": "  This activity explores a dataset describing Arctic sea ice and that comes from Sustainability Math.   Evaluating the cell below will plot the extent of Arctic sea ice, in millions of square kilometers, during the twelve months of 2012. In addition, you have access to a few special variables and commands:    month is the vector of month values and ice is the vector of sea ice values from the table above.     vandermonde(x, k) constructs the Vandermonde matrix of degree using the points in the vector x .     Q, R = QR(A) provides the factorization of .     demean(v) returns the demeaned vector .        Find the vector , the least-squares approximate solution to the linear system that results from fitting a degree 5 polynomial to the data.     If your result is stored in the variable xhat , you may plot the polynomial and the data together using the following cell.     Find the coefficient of determination for this polynomial fit.    Repeat these steps to fit a degree 8 polynomial to the data, plot the polynomial with the data, and find .     Repeat one more time by fitting a degree 11 polynomial to the data, creating a plot, and finding .   It's certainly true that higher degree polynomials fit the data better, as seen by the increasing values of , but that's not always a good thing. For instance, when , you may notice that the graph of the polynomial wiggles a little more than we would expect. In this case, the polynomial is trying too hard to fit the data, which usually contains some uncertainty, especially if it's obtained from measurements. The error built in to the data is called noise, and its presence means that we shouldn't expect our polynomial to fit the data perfectly. When we choose a polynomial whose degree is too high, we give the noise too much weight in the model, which leads to some undesirable behavior, like the wiggles in the graph.  Fitting the data with a polynomial whose degree is too high is called overfitting , a phenomenon that can appear in many machine learning applications. Generally speaking, we would like to choose large enough to capture the essential features of the data but not so large that we overfit and build the noise into the model. There are ways to determine the optimal value of , but we won't pursue that here.    Choosing a reasonable value of , estimate the extent of Arctic sea ice at month 6.5, roughly at the Summer Solstice.               The fifth degree polynomial fits the data fairly well.          .          seems like a good choice, and this gives the prediction of million square kilometers of sea ice.               The fifth degree polynomial fits the data fairly well.          .          million square kilometers of sea ice.      "
},
{
  "id": "sec-least-squares-8-2",
  "level": "2",
  "url": "sec-least-squares.html#sec-least-squares-8-2",
  "type": "Exercise",
  "number": "22.5.6.1",
  "title": "",
  "body": " Suppose we write the linear system as .   Find an orthogonal basis for .    Find , the orthogonal projection of onto .    Find a solution to the linear system .          and      .     .         Applying Gram-Schmidt gives the orthogonal basis     Applying the Projection Formula gives .    Solving the linear system gives .     "
},
{
  "id": "ex-lst-squares-line",
  "level": "2",
  "url": "sec-least-squares.html#ex-lst-squares-line",
  "type": "Exercise",
  "number": "22.5.6.2",
  "title": "",
  "body": " Consider the data in .  A dataset with four points.            1  1    2  1    3  1    4  2        Set up the linear system that describes the line passing through these points.    Write the normal equations that describe the least-squares approximate solution to .    Find the least-squares approximate solution and plot the data and the resulting line.    What is your predicted -value when ?    Find the coefficient of determination .          , .               .              The matrix and the vector .              The predicted value is .          "
},
{
  "id": "sec-least-squares-8-4",
  "level": "2",
  "url": "sec-least-squares.html#sec-least-squares-8-4",
  "type": "Exercise",
  "number": "22.5.6.3",
  "title": "",
  "body": " Consider the four points in .    Set up a linear system that describes a quadratic function passing through the points.    Use a factorization to find the least-squares approximate solution and plot the data and the graph of the resulting quadratic function.    What is your predicted -value when ?    Find the coefficient of determination .         We have the Vandermonde matrix and vector .    The quadratic function is .    This gives the predicted value .          "
},
{
  "id": "sec-least-squares-8-5",
  "level": "2",
  "url": "sec-least-squares.html#sec-least-squares-8-5",
  "type": "Exercise",
  "number": "22.5.6.4",
  "title": "",
  "body": " Consider the data in .  A simple dataset               1  1  4.2    1  2  3.3    2  1  5.9    2  2  5.1    3  2  7.5    3  3  6.3        Set up a linear system that describes the relationship     Find the least-squares approximate solution .    What is your predicted -value when and ?    Find the coefficient of determination .                    .                         .          "
},
{
  "id": "sec-least-squares-8-6",
  "level": "2",
  "url": "sec-least-squares.html#sec-least-squares-8-6",
  "type": "Exercise",
  "number": "22.5.6.5",
  "title": "",
  "body": " Determine whether the following statements are true or false and explain your thinking.   If is consistent, then is a solution to .    If , then the least-squares approximate solution is also a solution to the original equation .    Given the factorization , we have .    A factorization provides a method for finding the least-squares approximate solution to that is more reliable than solving the normal equations.    A solution to is the least-squares approximate solution to .        True  True  False  True  False        True. If , then is in so .    True. If , then . Therefore, .    False. The product rather than the matrix that projects vectors orthogonally onto .    True, numerical issues are more likely to arise when solving the normal equations.    False. The normal equations gives the least-squares approximate solution.     "
},
{
  "id": "sec-least-squares-8-7",
  "level": "2",
  "url": "sec-least-squares.html#sec-least-squares-8-7",
  "type": "Exercise",
  "number": "22.5.6.6",
  "title": "",
  "body": " Explain your response to the following questions.   If , what does this say about the vector ?    If the columns of are orthonormal, how can you easily find the least-squares approximate solution to ?          is in .     .         Since and , we know that . This says that is in .    In this case, , which means we have . If we multiply both sides by , we have .     "
},
{
  "id": "sec-least-squares-8-8",
  "level": "2",
  "url": "sec-least-squares.html#sec-least-squares-8-8",
  "type": "Exercise",
  "number": "22.5.6.7",
  "title": "",
  "body": " The following cell loads in some data showing the number of people in Bangladesh living without electricity over 27 years. It also defines vectors year , which records the years in the dataset, and people , which records the number of people.    Suppose we want to write where is the year and is the number of people. Construct the matrix and vector so that the linear system describes the vector .    Using a factorization of , find the values of and in the least-squares approximate solution .    What is the coefficient of determination and what does this tell us about the quality of the approximation?    What is your prediction for the number of people living without electricity in 1985?    Estimate the year in which there will be no people living without electricity.         The matrix will be the matrix whose first column is all 1's and whose second column is the vector of years. The vector is the vector that records the number of people.     .          .    2045         The matrix will be the matrix whose first column is all 1's and whose second column is the vector of years. The vector is the vector that records the number of people.    We obtain .          .    Solve to obtain 2045.     "
},
{
  "id": "sec-least-squares-8-9",
  "level": "2",
  "url": "sec-least-squares.html#sec-least-squares-8-9",
  "type": "Exercise",
  "number": "22.5.6.8",
  "title": "",
  "body": " This problem concerns a dataset describing planets in our Solar system. For each planet, we have the length of the semi-major axis, essentially the distance from the planet to the Sun in AU (astronomical units), and the period , the length of time in years required to complete one orbit around the Sun.  We would like to model this data using the function where and are parameters we need to determine. Since this isn't a linear function, we will transform this relationship by taking the natural logarithm of both sides to obtain   Evaluating the following cell loads the dataset and defines two vectors logaxis , whose components are , and logperiod , whose components are .    Construct the matrix and vector so that the solution to is the vector .    Find the least-squares approximate solution . What does this give for the values of and ?    Find the coefficient of determination . What does this tell us about the quality of the approximation?   Suppose that the orbit of an asteroid has a semi-major axis whose length is AU. Estimate the period of the asteroid's orbit.   Halley's Comet has a period of years. Estimate the length of its semi-major axis.         The matrix is the matrix whose first column is all 1's and whose second column is the vector of the logarithms of the semi-major axes. The vector is the vector that records the logarithms of the periods.     and .          years.     AU.         The matrix is the matrix whose first column is all 1's and whose second column is the vector of the logarithms of the semi-major axes. The vector is the vector that records the logarithms of the periods.    We find so that and . This means that .     , which means that we have a very good fit because this data reflects a physical law.     years.    The expression means that . Solving for AU.     "
},
{
  "id": "sec-least-squares-8-10",
  "level": "2",
  "url": "sec-least-squares.html#sec-least-squares-8-10",
  "type": "Exercise",
  "number": "22.5.6.9",
  "title": "",
  "body": " Evaluating the following cell loads a dataset describing the temperature in the Earth's atmosphere at various altitudes. There are also two vectors altitude , expressed in kilometers, and temperature , in degrees Celsius.    Describe how to form the matrix and vector so that the linear system describes a degree polynomial fitting the data.    After choosing a value of , construct the matrix and vector , and find the least-squares approximate solution .    Plot the polynomial and data using plot_model(xhat, data) .    Now examine what happens as you vary the degree of the polynomial . Choose an appropriate value of that seems to capture the most important features of the data while avoiding overfitting, and explain your choice.    Use your value of to estimate the temperature at an altitude of 55 kilometers.         The matrix will be the Vandermonde matrix of degree using the altitude readings while is the vector of temperatures.     appears to give a good fit without overfitting.    We have     A small value of does not adequately track important features of the data. If we increase too far, the graph tries too hard to match the data, and we see spurious features in the graph.    We estimate the temperature to be degrees Celsius.         The matrix will be the Vandermonde matrix of degree using the altitude readings while is the vector of temperatures.     appears to give a good fit without overfitting.    We have     A small value of does not adequately track important features of the data. If we increase too far, the graph tries too hard to match the data, and we see spurious features in the graph.     seems to work well. With this function, we estimate the temperature to be degrees Celsius.     "
},
{
  "id": "sec-least-squares-8-11",
  "level": "2",
  "url": "sec-least-squares.html#sec-least-squares-8-11",
  "type": "Exercise",
  "number": "22.5.6.10",
  "title": "",
  "body": " The following cell loads some data describing 1057 houses in a particular real estate market. For each house, we record the living area in square feet, the lot size in acres, the age in years, and the price in dollars. The cell also defines variables area , size , age , and price . We will use linear regression to predict the price of a house given its living area, lot size, and age:    Use a factorization to find the least-squares approximate solution .    Discuss the significance of the signs of , , and .    If two houses are identical except for differing in age by one year, how would you predict that their prices compare to each another?    Find the coefficient of determination . What does this say about the quality of the fit?    Predict the price of a house whose living area is 2000 square feet, lot size is 1.5 acres, and age is 50 years.              Increasing the living area or the lot size will cause the house price to increase, but an older house will cost less.    A house that's one year older will cost about less.         We estimate .               and are positive, which means that increasing the living area or the lot size will cause the house price to increase. However, is negative, which means that an older house will cost less.    Because , we would expect a house that's one year older to cost less.     , which is far from perfect without being terrible. There are other factors that are not included in the dataset that can also influence the house price, such as number of bedrooms and the location.    We estimate .     "
},
{
  "id": "sec-least-squares-8-12",
  "level": "2",
  "url": "sec-least-squares.html#sec-least-squares-8-12",
  "type": "Exercise",
  "number": "22.5.6.11",
  "title": "",
  "body": " We observed that if the columns of are linearly independent, then there is a unique least-squares approximate solution to the equation because the equation has a unique solution. We also said that is the unique solution to the normal equation without explaining why this equation has a unique solution. This exercise offers an explanation.  Assuming that the columns of are linearly independent, we would like to conclude that the equation has a unique solution.   Suppose that is a vector for which . Explain why the following argument is valid and allows us to conclude that . In other words, if , we know that .    If the columns of are linearly independent and , what do we know about the vector ?    Explain why can only happen when .    Assuming that the columns of are linearly independent, explain why has a unique solution.         Use the fact that .    It must be true that .    This follows from the previous two parts of this exercise.    The columns of must be linearly independent.         Starting with the assumption that , we dot both sides of the equation with to obtain . From here, we write This says that the length of must be zero, which tells us that .    If the columns of are linearly independent, we know that the only solution to the homogeneous equation is .    The previous two parts of this exercise tell us that implies that , which implies that .    Since the only solution to the homogeneous equation is , the columns of are linearly independent, which means that has only one solution.     "
},
{
  "id": "ex-r2-meaning",
  "level": "2",
  "url": "sec-least-squares.html#ex-r2-meaning",
  "type": "Exercise",
  "number": "22.5.6.12",
  "title": "",
  "body": " This problem is about the meaning of the coefficient of determination and its connection to variance, a topic that appears in the next section. Throughout this problem, we consider the linear system and the approximate least-squares solution , where . We suppose that is an matrix, and we will denote the -dimensional vector .     Explain why , the mean of the components of , can be found as the dot product     In the examples we have seen in this section, explain why is in .    If we write , explain why and hence why the mean of the components of is zero.    The variance of an -dimensional vector is , where is the vector obtained by demeaning .  Explain why     Explain why and hence   These expressions indicate why it is sometimes said that measures the fraction of variance explained by the function we are using to fit the data. As seen in the previous exercise, there may be other features that are not recorded in the dataset that influence the quantity we wish to predict.    Explain why .          simply sums the components of .     is a column of .     is in .    Use the fact that and are orthogonal.    This follows from .    This follow from .          simply sums the components of .    The examples we've seen fit the data using functions that have a constant term such as . This means that will be a column of the matrix and hence in .    Since is in , any vector in , such as , will be orthogonal to .    Notice that which says that Since and are orthogonal, we have . This means that so that .     , which explains why Then     The variances are nonnegative so we have . Also,      "
},
{
  "id": "sec-symmetric-matrices",
  "level": "1",
  "url": "sec-symmetric-matrices.html",
  "type": "Section",
  "number": "23.1",
  "title": "Symmetric matrices and variance",
  "body": " Symmetric matrices and variance   In this section, we will revisit the theory of eigenvalues and eigenvectors for the special class of matrices that are symmetric , meaning that the matrix equals its transpose. This understanding of symmetric matrices will enable us to form singular value decompositions later in the chapter. We'll also begin studying variance in this section as it provides an important context that motivates some of our later work.  To begin, remember that if is a square matrix, we say that is an eigenvector of with associated eigenvalue if . In other words, for these special vectors, the operation of matrix multiplication simplifies to scalar multiplication.    This preview activity reminds us how a basis of eigenvectors can be used to relate a square matrix to a diagonal one.       Use these plots to sketch the vectors requested in the preview activity.      Suppose that and that and .   Sketch the vectors and on the left side of .    Sketch the vectors and on the left side of .    Sketch the vectors and on the left side.    Give a geometric description of the matrix transformation defined by .       Now suppose we have vectors and and that is a matrix such that That is, and are eigenvectors of with associated eigenvalues and .   Sketch the vectors and on the right side of .    Sketch the vectors and on the right side of .    Sketch the vectors and on the right side.    Give a geometric description of the matrix transformation defined by .       In what ways are the matrix transformations defined by and related to one another?                             stretches vectors horizontally by a factor of 3 and reflects them in the horizontal axis.                          stretches vectors in the direction of by a factor of 3 and reflects them in the line defined by .       The effect of the two transformations are the same when viewed in the coordinate systems given by the appropriate set of vectors.       The preview activity asks us to compare the matrix transformations defined by two matrices, a diagonal matrix and a matrix whose eigenvectors are given to us. The transformation defined by stretches horizontally by a factor of 3 and reflects in the horizontal axis, as shown in       The matrix transformation defined by .   By contrast, the transformation defined by stretches the plane by a factor of 3 in the direction of and reflects in the line defined by , as seen in .      The matrix transformation defined by .   In this way, we see that the matrix transformations defined by these two matrices are equivalent after a rotation. This notion of equivalence is what we called similarity in . There we considered a square matrix that provided enough eigenvectors to form a basis of . For example, suppose we can construct a basis for using eigenvectors having associated eigenvalues . Forming the matrices, enables us to write . This is what it means for to be diagonalizable.  For the example in the preview activity, we are led to form which tells us that .  Notice that the matrix has eigenvectors and that not only form a basis for but, in fact, form an orthogonal basis for . Given the prominent role played by orthogonal bases in the last chapter, we would like to understand what conditions on a matrix enable us to form an orthogonal basis of eigenvectors.    Symmetric matrices and orthogonal diagonalization  Let's begin by looking at some examples in the next activity.    Remember that the Sage command A.right_eigenmatrix() attempts to find a basis for consisting of eigenvectors of . In particular, the assignment D, P = A.right_eigenmatrix() provides a diagonal matrix constructed from the eigenvalues of with the columns of containing the associated eigenvectors.    For each of the following matrices, determine whether there is a basis for consisting of eigenvectors of that matrix. When there is such a basis, form the matrices and and verify that the matrix equals .    .     .     .     .       For which of these examples is it possible to form an orthogonal basis for consisting of eigenvectors?    For any such matrix , find an orthonormal basis of eigenvectors and explain why where is an orthogonal matrix.    Finally, explain why in this case.    When , what is the relationship between and ?             The eigenvalues of this matrix are complex so there is no such basis.    There is one eigenvalue with multiplicity two. The associated eigenspace is one-dimensional so there is not a basis of consisting of eigenvectors.    This matrix is diagonalizable with     This matrix is also diagonalizable with        Only the last matrix .    We form an orthonormal basis by scaling the eigenvectors to have length 1. This gives , which is orthogonal since the columns form an orthonormal basis of .    Orthogonal matrices are invertible and have     If , we have . This means that the matrix is symmetric.             There is no such basis.    There is no such basis.    This matrix is diagonalizable with     This matrix is also diagonalizable with        Only the last matrix.                      The examples in this activity illustrate a range of possibilities. First, a matrix may have complex eigenvalues, in which case it will not be diagonalizable. Second, even if all the eigenvalues are real, there may not be a basis of eigenvalues if the dimension of one of the eigenspaces is less than the algebraic multiplicity of the associated eigenvalue.  We are interested in matrices for which there is an orthogonal basis of eigenvectors. When this happens, we can create an orthonormal basis of eigenvectors by scaling each eigenvector in the basis so that its length is 1. Putting these orthonormal vectors into a matrix produces an orthogonal matrix, which means that . We then have In this case, we say that is orthogonally diagonalizable .   orthogonal diagonalization   If there is an orthonormal basis of consisting of eigenvectors of the matrix , we say that is orthogonally diagonalizable . In particular, we can write where is an orthogonal matrix.    When is orthogonally diagonalizable, notice that That is, when is orthogonally diagonalizable, and we say that is symmetric .   symmetric matrix   A symmetric matrix is one for which .      Consider the matrix , which has eigenvectors , with associated eigenvalue , and , with associated eigenvalue . Notice that and are orthogonal so we can form an orthonormal basis of eigenvectors:   In this way, we construct the matrices and note that .  Notice also that, as expected, is symmetric; that is, .      If , then there is an orthogonal basis of eigenvectors and with eigenvalues and . Using these eigenvectors, we form the orthogonal matrix consisting of eigenvectors and the diagonal matrix , where Then we have .  Notice that the matrix transformation represented by is a rotation while that represented by is a rotation. Therefore, if we multiply a vector by , we can decompose the multiplication as That is, we first rotate by , then apply the diagonal matrix , which stretches and reflects, and finally rotate by . We may visualize this factorization as in .      The transformation defined by can be interpreted as a sequence of geometric transformations: rotates by , stretches and reflects, and rotates by .   In fact, a similar picture holds any time the matrix is orthogonally diagonalizable.    We have seen that a matrix that is orthogonally diagonalizable must be symmetric. In fact, it turns out that any symmetric matrix is orthogonally diagonalizable. We record this fact in the next theorem.   The Spectral Theorem   The matrix is orthogonally diagonalizable if and only if is symmetric.      Each of the following matrices is symmetric so the Spectral Theorem tells us that each is orthogonally diagonalizable. The point of this activity is to find an orthogonal diagonalization for each matrix.  To begin, find a basis for each eigenspace. Use this basis to find an orthogonal basis for each eigenspace and put these bases together to find an orthogonal basis for consisting of eigenvectors. Use this basis to write an orthogonal diagonalization of the matrix.     .     .     .    Consider the matrix where . Explain how we know that is symmetric and then find an orthogonal diagonalization of .          We have eigenvectors and with associated eigenvalues and . We form an orthonormal basis of eigenvectors, and . This gives     We find     We have eigenvalues with associated eigenvector and with associated eigenvectors and . Notice that is orthogonal to both and , but and are not orthogonal to one another. We can, however, apply Gram-Schmidt to create an orthogonal basis of the eigenspace . We can then form an orthonormal basis so that     We have so must be symmetric. Then we find the orthogonal diagonalization                                  As the examples in illustrate, the Spectral Theorem implies a number of things. Namely, if is a symmetric matrix, then   the eigenvalues of are real.    there is a basis of consisting of eigenvectors.    two eigenvectors that are associated to different eigenvalues are orthogonal.     We won't justify the first two facts here since that would take us rather far afield. However, it will be helpful to explain the third fact. To begin, notice the following: This is a useful fact that we'll employ quite a bit in the future so let's summarize it in the following proposition.    For any matrix , we have In particular, if is symmetric, then       Suppose a symmetric matrix has eigenvectors , with associated eigenvalue , and , with associated eigenvalue . Notice that Since by , we have which can only happen if . Therefore, and are orthogonal.  More generally, the same argument shows that two eigenvectors of a symmetric matrix associated to distinct eigenvalues are orthogonal.      Variance  Many of the ideas we'll encounter in this chapter, such as orthogonal diagonalizations, can be applied to the study of data. In fact, it can be useful to understand these applications because they provide an important context in which mathematical ideas have a more concrete meaning and their motivation appears more clearly. For that reason, we will now introduce the statistical concept of variance as a way to gain insight into the significance of orthogonal diagonalizations.  Given a set of data points, their variance measures how spread out the points are. The next activity looks at some examples.    We'll begin with a set of three data points    Find the centroid, or mean, . Then plot the data points and their centroid in .      Plot the data points and their centroid here.     Notice that the centroid lies in the center of the data so the spread of the data will be measured by how far away the points are from the centroid. To simplify our calculations, find the demeaned data points and plot them in .      Plot the demeaned data points here.     Now that the data has been demeaned, we will define the total variance as the average of the squares of the distances from the origin; that is, the total variance is Find the total variance for our set of three points.    Now plot the projections of the demeaned data onto the and axes using and find the variances and of the projected points.       Plot the projections of the demeaned data onto the and axes.     Which of the variances, and , is larger and how does the plot of the projected points explain your response?    What do you notice about the relationship between , , and ? How does the Pythagorean theorem explain this relationship?    Plot the projections of the demeaned data points onto the lines defined by vectors and using and find the variances and of these projected points.      Plot the projections of the deameaned data onto the lines defined by and .     What is the relationship between the total variance and and ? How does the Pythagorean theorem explain your response?          The centroid is .    The demeaned data points are     The total variance is .    We find and . Notice that is larger because the points are more spread out in the vertical direction.    We have due to the Pythagorean theorem.    The points projected onto the line defined by are , , and . This gives the variance .  The points projected onto the line defined by are , , and . This gives the variance .    Once again, because of the Pythagorean theorem.           .          .     and                          Notice that variance enjoys an additivity property. Consider, for instance, the situation where our data points are two-dimensional and suppose that the demeaned points are . We have If we take the average over all data points, we find that the total variance is the sum of the variances in the and directions:   More generally, suppose that we have an orthonormal basis and . If we project the demeaned points onto the line defined by , we obtain the points so that   For each of our demeaned data points, the Projection Formula tells us that We then have since . When we average over all the data points, we find that the total variance is the sum of the variances in the and directions. This leads to the following proposition, in which this observation is expressed more generally.   Additivity of Variance   If is a subspace with orthonormal basis , , , , then the variance of the points projected onto is the sum of the variances in the directions:     The next activity demonstrates a more efficient way to find the variance in a particular direction and connects our discussion of variance with symmetric matrices.    Let's return to the dataset from the previous activity in which we have demeaned data points: Our goal is to compute the variance in the direction defined by a unit vector .  To begin, form the demeaned data matrix and suppose that is a unit vector.   Write the vector in terms of the dot products .    Explain why .    Apply to explain why     In general, the matrix is called the covariance matrix of the dataset, and it is useful because the variance , as we have just seen. Find the matrix for our dataset with three points.     Use the covariance matrix to find the variance when .    Use the covariance matrix to find the variance when . Since and are orthogonal, verify that the sum of and gives the total variance.    Explain why the covariance matrix is a symmetric matrix.               Projecting onto gives , whose length squared is . Then                   . Then , which is the total variance.                                       .            This activity introduced the covariance matrix of a dataset, which is defined to be where is the matrix of demeaned data points. Notice that which tells us that is symmetric. In particular, we know that it is orthogonally diagonalizable, an observation that will play an important role in the future.  This activity also demonstrates the significance of the covariance matrix, which is recorded in the following proposition.    If is the covariance matrix associated to a demeaned dataset and is a unit vector, then the variance of the demeaned points projected onto the line defined by is     Our goal in the future will be to find directions where the variance is as large as possible and directions where it is as small as possible. The next activity demonstrates why this is useful.       Evaluating the following Sage cell loads a dataset consisting of 100 demeaned data points and provides a plot of them. It also provides the demeaned data matrix .   What is the shape of the covariance matrix ? Find and verify your response.     By visually inspecting the data, determine which is larger, or . Then compute both of these quantities to verify your response.    What is the total variance ?    In approximately what direction is the variance greatest? Choose a reasonable vector that points in approximately that direction and find .    In approximately what direction is the variance smallest? Choose a reasonable vector that points in approximately that direction and find .    How are the directions and in the last two parts of this problem related to one another? Why does this relationship hold?           will be the matrix      and , which agrees with the fact that the data is more spread out in the horizontal than vertical direction.         It looks like the direction defined by the unit vector . We find that , which is almost all of the total variance.    It looks like the direction defined by the unit vector . We find that .    They are orthogonal to one another. Since the total variance when and are orthogonal, will be as large as possible when is as small as possible.                and          If , then .    If , then .    They are orthogonal to one another.       This activity illustrates how variance can identify a line along which the data are concentrated. When the data primarily lie along a line defined by a vector , then the variance in that direction will be large while the variance in an orthogonal direction will be small.  Remember that variance is additive, according to , so that if and are orthogonal unit vectors, then the total variance is Therefore, if we choose to be the direction where is a maximum, then will be a minimum.  In the next section, we will use an orthogonal diagonalization of the covariance matrix to find the directions having the greatest and smallest variances. In this way, we will be able to determine when data are concentrated along a line or subspace.    Summary  This section explored both symmetric matrices and variance. In particular, we saw that   A matrix is orthogonally diagonalizable if there is an orthonormal basis of eigenvectors. In particular, we can write , where is a diagonal matrix of eigenvalues and is an orthogonal matrix of eigenvectors.    The Spectral Theorem tells us that a matrix is orthogonally diagonalizable if and only if it is symmetric; that is, .    The variance of a dataset can be computed using the covariance matrix , where is the matrix of demeaned data points. In particular, the variance of the demeaned data points projected onto the line defined by the unit vector is .    Variance is additive so that if is a subspace with orthonormal basis , then         For each of the following matrices, find the eigenvalues and a basis for each eigenspace. Determine whether the matrix is diagonalizable and, if so, find a diagonalization. Determine whether the matrix is orthogonally diagonalizable and, if so, find an orthogonal diagonalization.                             Not diagonalizable         Diagonalizable, but not orthogonally diagonalizable.               This matrix is not diagonalizable because there is not a basis of consisting of eigenvectors.    This matrix is symmetric so it is orthogonally diagonalizable:     This matrix is diagonalizable but not orthogonally diagonalizable.     This matrix is symmetric so it's orthogonally diagonalizable.        Consider the matrix whose eigenvalues are , , and .    Explain why is orthogonally diagonalizable.    Find an orthonormal basis for the eigenspace .    Find a basis for the eigenspace .    Now find an orthonormal basis for .    Find matrices and such that .         Because of the Spectral Theorem          and      and               Since the matrix is symmetric, the Spectral Theorem says it is orthogonally diagonalizable.          and      and             Find an orthogonal diagonalization, if one exists, for the following matrices.     .     .     .              Not orthogonally diagonalizable                   This matrix is not symmetric so it is not orthogonally diagonalizable.            Suppose that is an matrix and that .   Explain why is orthogonally diagonalizable.    Explain why .    Suppose that is an eigenvector of with associated eigenvalue and that has unit length. Explain why .    Explain why the eigenvalues of are nonnegative.    If is the covariance matrix associated to a demeaned dataset, explain why the eigenvalues of are nonnegative.          is symmetric               .     .         Because , the matrix is symmetric and hence orthogonally diagonalizable.     .         Because .    In the same way, so that . Therefore, .       Suppose that you have the data points    Find the demeaned data points.    Find the total variance of the dataset.    Find the variance in the direction and the variance in the direction .    Project the demeaned data points onto the line defined by and find the variance of these projected points.    Project the demeaned data points onto the line defined by and find the variance of these projected points.    How and why are the results of from the last two parts related to the total variance?                   and               The variances add to the toal variance.                  and     Let be the unit vector parallel to so that .    Let be the unit vector parallel to so that .    The vectors are parallel so the variances add to the toal variance.      Suppose you have six 2-dimensional data points arranged in the matrix    Find the matrix of demeaned data points and plot the points in .     A plot for the demeaned data points.      Construct the covariance matrix and explain why you know that it is orthogonally diagonalizable.    Find an orthogonal diagonalization of .    Sketch the lines corresponding to the two eigenvectors on the plot above.    Find the variances in the directions of the eigenvectors.                         and .     and                is orthonally diagonalizable because it is symmetric.         Sketch the lines defined by and .    The variances are the eigenvalues and        Suppose that is the covariance matrix of a demeaned dataset.   Suppose that is an eigenvector of with associated eigenvalue and that has unit length. Explain why .    Suppose that the covariance matrix of a demeaned dataset can be written as where What is ? What does this tell you about the demeaned data?    Explain why the total variance of a dataset equals the sum of the eigenvalues of the covariance matrix.          .                   The variance is .                 Determine whether the following statements are true or false and explain your thinking.   If is an invertible, orthogonally diagonalizable matrix, then so is .    If is an eigenvalue of , then cannot be orthogonally diagonalizable.    If there is a basis for consisting of eigenvectors of , then is orthogonally diagonalizable.    If and are eigenvectors of a symmetric matrix associated to eigenvalues -2 and 3, then .    If is a square matrix, then .         True.    True.    False.    True.    False.         True. If is invertible, then the eigenvalues are nonzero, which means that is invertible. Therefore, , which says that is orthogonally diagonalizable.    True. In this case, there cannot be a basis for consisting of eigenvalues of so is not diagonalizable.    False. This condition implies that is diagonalizable, but it may not be orthogonally diagonalizable.    True. The eigenvectors of a symmetric matrix associated to different eigenvalues are orthogonal.    False. This is only true if is symmetric.       Suppose that is a noninvertible, symmetric matrix having eigenvectors and associated eigenvalues and . Find matrices and such that .       Since is not invertible, the third eigenvalue must be zero: . Also, an eigenvector associated to must be orthogonal to both and . We can find such a vector by finding where . This leads to     Suppose that is a plane in and that is the matrix that projects vectors orthogonally onto .   Explain why is orthogonally diagonalizable.    What are the eigenvalues of ?    Explain the relationship between the eigenvectors of and the plane .          is symmetric    0 or 1    The eigenspaces and          If is a matrix whose columns are an orthonormal basis for , then . This means that so is symmetric and hence orthogonally diagonalizable.    If is in , then and if is in , then . This means that the eigenvalues of are either 0 or 1.    The eigenspaces and        "
},
{
  "id": "sec-symmetric-matrices-2-3",
  "level": "2",
  "url": "sec-symmetric-matrices.html#sec-symmetric-matrices-2-3",
  "type": "Preview Activity",
  "number": "23.1.1",
  "title": "",
  "body": "  This preview activity reminds us how a basis of eigenvectors can be used to relate a square matrix to a diagonal one.       Use these plots to sketch the vectors requested in the preview activity.      Suppose that and that and .   Sketch the vectors and on the left side of .    Sketch the vectors and on the left side of .    Sketch the vectors and on the left side.    Give a geometric description of the matrix transformation defined by .       Now suppose we have vectors and and that is a matrix such that That is, and are eigenvectors of with associated eigenvalues and .   Sketch the vectors and on the right side of .    Sketch the vectors and on the right side of .    Sketch the vectors and on the right side.    Give a geometric description of the matrix transformation defined by .       In what ways are the matrix transformations defined by and related to one another?                             stretches vectors horizontally by a factor of 3 and reflects them in the horizontal axis.                          stretches vectors in the direction of by a factor of 3 and reflects them in the line defined by .       The effect of the two transformations are the same when viewed in the coordinate systems given by the appropriate set of vectors.      "
},
{
  "id": "fig-eigen-diag-D",
  "level": "2",
  "url": "sec-symmetric-matrices.html#fig-eigen-diag-D",
  "type": "Figure",
  "number": "23.1.2",
  "title": "",
  "body": "    The matrix transformation defined by .  "
},
{
  "id": "fig-eigen-diag-general",
  "level": "2",
  "url": "sec-symmetric-matrices.html#fig-eigen-diag-general",
  "type": "Figure",
  "number": "23.1.3",
  "title": "",
  "body": "    The matrix transformation defined by .  "
},
{
  "id": "sec-symmetric-matrices-3-3",
  "level": "2",
  "url": "sec-symmetric-matrices.html#sec-symmetric-matrices-3-3",
  "type": "Activity",
  "number": "23.1.2",
  "title": "",
  "body": "  Remember that the Sage command A.right_eigenmatrix() attempts to find a basis for consisting of eigenvectors of . In particular, the assignment D, P = A.right_eigenmatrix() provides a diagonal matrix constructed from the eigenvalues of with the columns of containing the associated eigenvectors.    For each of the following matrices, determine whether there is a basis for consisting of eigenvectors of that matrix. When there is such a basis, form the matrices and and verify that the matrix equals .    .     .     .     .       For which of these examples is it possible to form an orthogonal basis for consisting of eigenvectors?    For any such matrix , find an orthonormal basis of eigenvectors and explain why where is an orthogonal matrix.    Finally, explain why in this case.    When , what is the relationship between and ?             The eigenvalues of this matrix are complex so there is no such basis.    There is one eigenvalue with multiplicity two. The associated eigenspace is one-dimensional so there is not a basis of consisting of eigenvectors.    This matrix is diagonalizable with     This matrix is also diagonalizable with        Only the last matrix .    We form an orthonormal basis by scaling the eigenvectors to have length 1. This gives , which is orthogonal since the columns form an orthonormal basis of .    Orthogonal matrices are invertible and have     If , we have . This means that the matrix is symmetric.             There is no such basis.    There is no such basis.    This matrix is diagonalizable with     This matrix is also diagonalizable with        Only the last matrix.                     "
},
{
  "id": "def-orthog-diag",
  "level": "2",
  "url": "sec-symmetric-matrices.html#def-orthog-diag",
  "type": "Definition",
  "number": "23.1.4",
  "title": "",
  "body": " orthogonal diagonalization   If there is an orthonormal basis of consisting of eigenvectors of the matrix , we say that is orthogonally diagonalizable . In particular, we can write where is an orthogonal matrix.   "
},
{
  "id": "sec-symmetric-matrices-3-8",
  "level": "2",
  "url": "sec-symmetric-matrices.html#sec-symmetric-matrices-3-8",
  "type": "Definition",
  "number": "23.1.5",
  "title": "",
  "body": " symmetric matrix   A symmetric matrix is one for which .   "
},
{
  "id": "sec-symmetric-matrices-3-9",
  "level": "2",
  "url": "sec-symmetric-matrices.html#sec-symmetric-matrices-3-9",
  "type": "Example",
  "number": "23.1.6",
  "title": "",
  "body": "  Consider the matrix , which has eigenvectors , with associated eigenvalue , and , with associated eigenvalue . Notice that and are orthogonal so we can form an orthonormal basis of eigenvectors:   In this way, we construct the matrices and note that .  Notice also that, as expected, is symmetric; that is, .   "
},
{
  "id": "sec-symmetric-matrices-3-10",
  "level": "2",
  "url": "sec-symmetric-matrices.html#sec-symmetric-matrices-3-10",
  "type": "Example",
  "number": "23.1.7",
  "title": "",
  "body": "  If , then there is an orthogonal basis of eigenvectors and with eigenvalues and . Using these eigenvectors, we form the orthogonal matrix consisting of eigenvectors and the diagonal matrix , where Then we have .  Notice that the matrix transformation represented by is a rotation while that represented by is a rotation. Therefore, if we multiply a vector by , we can decompose the multiplication as That is, we first rotate by , then apply the diagonal matrix , which stretches and reflects, and finally rotate by . We may visualize this factorization as in .      The transformation defined by can be interpreted as a sequence of geometric transformations: rotates by , stretches and reflects, and rotates by .   In fact, a similar picture holds any time the matrix is orthogonally diagonalizable.   "
},
{
  "id": "sec-symmetric-matrices-3-12",
  "level": "2",
  "url": "sec-symmetric-matrices.html#sec-symmetric-matrices-3-12",
  "type": "Theorem",
  "number": "23.1.9",
  "title": "The Spectral Theorem.",
  "body": " The Spectral Theorem   The matrix is orthogonally diagonalizable if and only if is symmetric.   "
},
{
  "id": "activity-orthog-diag",
  "level": "2",
  "url": "sec-symmetric-matrices.html#activity-orthog-diag",
  "type": "Activity",
  "number": "23.1.3",
  "title": "",
  "body": "  Each of the following matrices is symmetric so the Spectral Theorem tells us that each is orthogonally diagonalizable. The point of this activity is to find an orthogonal diagonalization for each matrix.  To begin, find a basis for each eigenspace. Use this basis to find an orthogonal basis for each eigenspace and put these bases together to find an orthogonal basis for consisting of eigenvectors. Use this basis to write an orthogonal diagonalization of the matrix.     .     .     .    Consider the matrix where . Explain how we know that is symmetric and then find an orthogonal diagonalization of .          We have eigenvectors and with associated eigenvalues and . We form an orthonormal basis of eigenvectors, and . This gives     We find     We have eigenvalues with associated eigenvector and with associated eigenvectors and . Notice that is orthogonal to both and , but and are not orthogonal to one another. We can, however, apply Gram-Schmidt to create an orthogonal basis of the eigenspace . We can then form an orthonormal basis so that     We have so must be symmetric. Then we find the orthogonal diagonalization                                 "
},
{
  "id": "prop-symmetric-dot",
  "level": "2",
  "url": "sec-symmetric-matrices.html#prop-symmetric-dot",
  "type": "Proposition",
  "number": "23.1.10",
  "title": "",
  "body": "  For any matrix , we have In particular, if is symmetric, then    "
},
{
  "id": "sec-symmetric-matrices-3-17",
  "level": "2",
  "url": "sec-symmetric-matrices.html#sec-symmetric-matrices-3-17",
  "type": "Example",
  "number": "23.1.11",
  "title": "",
  "body": "  Suppose a symmetric matrix has eigenvectors , with associated eigenvalue , and , with associated eigenvalue . Notice that Since by , we have which can only happen if . Therefore, and are orthogonal.  More generally, the same argument shows that two eigenvectors of a symmetric matrix associated to distinct eigenvalues are orthogonal.   "
},
{
  "id": "sec-symmetric-matrices-4-4",
  "level": "2",
  "url": "sec-symmetric-matrices.html#sec-symmetric-matrices-4-4",
  "type": "Activity",
  "number": "23.1.4",
  "title": "",
  "body": "  We'll begin with a set of three data points    Find the centroid, or mean, . Then plot the data points and their centroid in .      Plot the data points and their centroid here.     Notice that the centroid lies in the center of the data so the spread of the data will be measured by how far away the points are from the centroid. To simplify our calculations, find the demeaned data points and plot them in .      Plot the demeaned data points here.     Now that the data has been demeaned, we will define the total variance as the average of the squares of the distances from the origin; that is, the total variance is Find the total variance for our set of three points.    Now plot the projections of the demeaned data onto the and axes using and find the variances and of the projected points.       Plot the projections of the demeaned data onto the and axes.     Which of the variances, and , is larger and how does the plot of the projected points explain your response?    What do you notice about the relationship between , , and ? How does the Pythagorean theorem explain this relationship?    Plot the projections of the demeaned data points onto the lines defined by vectors and using and find the variances and of these projected points.      Plot the projections of the deameaned data onto the lines defined by and .     What is the relationship between the total variance and and ? How does the Pythagorean theorem explain your response?          The centroid is .    The demeaned data points are     The total variance is .    We find and . Notice that is larger because the points are more spread out in the vertical direction.    We have due to the Pythagorean theorem.    The points projected onto the line defined by are , , and . This gives the variance .  The points projected onto the line defined by are , , and . This gives the variance .    Once again, because of the Pythagorean theorem.           .          .     and                         "
},
{
  "id": "prop-variance-additivity",
  "level": "2",
  "url": "sec-symmetric-matrices.html#prop-variance-additivity",
  "type": "Proposition",
  "number": "23.1.16",
  "title": "Additivity of Variance.",
  "body": " Additivity of Variance   If is a subspace with orthonormal basis , , , , then the variance of the points projected onto is the sum of the variances in the directions:    "
},
{
  "id": "sec-symmetric-matrices-4-10",
  "level": "2",
  "url": "sec-symmetric-matrices.html#sec-symmetric-matrices-4-10",
  "type": "Activity",
  "number": "23.1.5",
  "title": "",
  "body": "  Let's return to the dataset from the previous activity in which we have demeaned data points: Our goal is to compute the variance in the direction defined by a unit vector .  To begin, form the demeaned data matrix and suppose that is a unit vector.   Write the vector in terms of the dot products .    Explain why .    Apply to explain why     In general, the matrix is called the covariance matrix of the dataset, and it is useful because the variance , as we have just seen. Find the matrix for our dataset with three points.     Use the covariance matrix to find the variance when .    Use the covariance matrix to find the variance when . Since and are orthogonal, verify that the sum of and gives the total variance.    Explain why the covariance matrix is a symmetric matrix.               Projecting onto gives , whose length squared is . Then                   . Then , which is the total variance.                                       .           "
},
{
  "id": "prop-covariance",
  "level": "2",
  "url": "sec-symmetric-matrices.html#prop-covariance",
  "type": "Proposition",
  "number": "23.1.17",
  "title": "",
  "body": "  If is the covariance matrix associated to a demeaned dataset and is a unit vector, then the variance of the demeaned points projected onto the line defined by is    "
},
{
  "id": "sec-symmetric-matrices-4-15",
  "level": "2",
  "url": "sec-symmetric-matrices.html#sec-symmetric-matrices-4-15",
  "type": "Activity",
  "number": "23.1.6",
  "title": "",
  "body": "     Evaluating the following Sage cell loads a dataset consisting of 100 demeaned data points and provides a plot of them. It also provides the demeaned data matrix .   What is the shape of the covariance matrix ? Find and verify your response.     By visually inspecting the data, determine which is larger, or . Then compute both of these quantities to verify your response.    What is the total variance ?    In approximately what direction is the variance greatest? Choose a reasonable vector that points in approximately that direction and find .    In approximately what direction is the variance smallest? Choose a reasonable vector that points in approximately that direction and find .    How are the directions and in the last two parts of this problem related to one another? Why does this relationship hold?           will be the matrix      and , which agrees with the fact that the data is more spread out in the horizontal than vertical direction.         It looks like the direction defined by the unit vector . We find that , which is almost all of the total variance.    It looks like the direction defined by the unit vector . We find that .    They are orthogonal to one another. Since the total variance when and are orthogonal, will be as large as possible when is as small as possible.                and          If , then .    If , then .    They are orthogonal to one another.      "
},
{
  "id": "sec-symmetric-matrices-6-1",
  "level": "2",
  "url": "sec-symmetric-matrices.html#sec-symmetric-matrices-6-1",
  "type": "Exercise",
  "number": "23.1.4.1",
  "title": "",
  "body": " For each of the following matrices, find the eigenvalues and a basis for each eigenspace. Determine whether the matrix is diagonalizable and, if so, find a diagonalization. Determine whether the matrix is orthogonally diagonalizable and, if so, find an orthogonal diagonalization.                             Not diagonalizable         Diagonalizable, but not orthogonally diagonalizable.               This matrix is not diagonalizable because there is not a basis of consisting of eigenvectors.    This matrix is symmetric so it is orthogonally diagonalizable:     This matrix is diagonalizable but not orthogonally diagonalizable.     This matrix is symmetric so it's orthogonally diagonalizable.      "
},
{
  "id": "sec-symmetric-matrices-6-2",
  "level": "2",
  "url": "sec-symmetric-matrices.html#sec-symmetric-matrices-6-2",
  "type": "Exercise",
  "number": "23.1.4.2",
  "title": "",
  "body": " Consider the matrix whose eigenvalues are , , and .    Explain why is orthogonally diagonalizable.    Find an orthonormal basis for the eigenspace .    Find a basis for the eigenspace .    Now find an orthonormal basis for .    Find matrices and such that .         Because of the Spectral Theorem          and      and               Since the matrix is symmetric, the Spectral Theorem says it is orthogonally diagonalizable.          and      and           "
},
{
  "id": "sec-symmetric-matrices-6-3",
  "level": "2",
  "url": "sec-symmetric-matrices.html#sec-symmetric-matrices-6-3",
  "type": "Exercise",
  "number": "23.1.4.3",
  "title": "",
  "body": " Find an orthogonal diagonalization, if one exists, for the following matrices.     .     .     .              Not orthogonally diagonalizable                   This matrix is not symmetric so it is not orthogonally diagonalizable.          "
},
{
  "id": "sec-symmetric-matrices-6-4",
  "level": "2",
  "url": "sec-symmetric-matrices.html#sec-symmetric-matrices-6-4",
  "type": "Exercise",
  "number": "23.1.4.4",
  "title": "",
  "body": " Suppose that is an matrix and that .   Explain why is orthogonally diagonalizable.    Explain why .    Suppose that is an eigenvector of with associated eigenvalue and that has unit length. Explain why .    Explain why the eigenvalues of are nonnegative.    If is the covariance matrix associated to a demeaned dataset, explain why the eigenvalues of are nonnegative.          is symmetric               .     .         Because , the matrix is symmetric and hence orthogonally diagonalizable.     .         Because .    In the same way, so that . Therefore, .     "
},
{
  "id": "sec-symmetric-matrices-6-5",
  "level": "2",
  "url": "sec-symmetric-matrices.html#sec-symmetric-matrices-6-5",
  "type": "Exercise",
  "number": "23.1.4.5",
  "title": "",
  "body": " Suppose that you have the data points    Find the demeaned data points.    Find the total variance of the dataset.    Find the variance in the direction and the variance in the direction .    Project the demeaned data points onto the line defined by and find the variance of these projected points.    Project the demeaned data points onto the line defined by and find the variance of these projected points.    How and why are the results of from the last two parts related to the total variance?                   and               The variances add to the toal variance.                  and     Let be the unit vector parallel to so that .    Let be the unit vector parallel to so that .    The vectors are parallel so the variances add to the toal variance.    "
},
{
  "id": "sec-symmetric-matrices-6-6",
  "level": "2",
  "url": "sec-symmetric-matrices.html#sec-symmetric-matrices-6-6",
  "type": "Exercise",
  "number": "23.1.4.6",
  "title": "",
  "body": " Suppose you have six 2-dimensional data points arranged in the matrix    Find the matrix of demeaned data points and plot the points in .     A plot for the demeaned data points.      Construct the covariance matrix and explain why you know that it is orthogonally diagonalizable.    Find an orthogonal diagonalization of .    Sketch the lines corresponding to the two eigenvectors on the plot above.    Find the variances in the directions of the eigenvectors.                         and .     and                is orthonally diagonalizable because it is symmetric.         Sketch the lines defined by and .    The variances are the eigenvalues and      "
},
{
  "id": "sec-symmetric-matrices-6-7",
  "level": "2",
  "url": "sec-symmetric-matrices.html#sec-symmetric-matrices-6-7",
  "type": "Exercise",
  "number": "23.1.4.7",
  "title": "",
  "body": " Suppose that is the covariance matrix of a demeaned dataset.   Suppose that is an eigenvector of with associated eigenvalue and that has unit length. Explain why .    Suppose that the covariance matrix of a demeaned dataset can be written as where What is ? What does this tell you about the demeaned data?    Explain why the total variance of a dataset equals the sum of the eigenvalues of the covariance matrix.          .                   The variance is .               "
},
{
  "id": "sec-symmetric-matrices-6-8",
  "level": "2",
  "url": "sec-symmetric-matrices.html#sec-symmetric-matrices-6-8",
  "type": "Exercise",
  "number": "23.1.4.8",
  "title": "",
  "body": " Determine whether the following statements are true or false and explain your thinking.   If is an invertible, orthogonally diagonalizable matrix, then so is .    If is an eigenvalue of , then cannot be orthogonally diagonalizable.    If there is a basis for consisting of eigenvectors of , then is orthogonally diagonalizable.    If and are eigenvectors of a symmetric matrix associated to eigenvalues -2 and 3, then .    If is a square matrix, then .         True.    True.    False.    True.    False.         True. If is invertible, then the eigenvalues are nonzero, which means that is invertible. Therefore, , which says that is orthogonally diagonalizable.    True. In this case, there cannot be a basis for consisting of eigenvalues of so is not diagonalizable.    False. This condition implies that is diagonalizable, but it may not be orthogonally diagonalizable.    True. The eigenvectors of a symmetric matrix associated to different eigenvalues are orthogonal.    False. This is only true if is symmetric.     "
},
{
  "id": "sec-symmetric-matrices-6-9",
  "level": "2",
  "url": "sec-symmetric-matrices.html#sec-symmetric-matrices-6-9",
  "type": "Exercise",
  "number": "23.1.4.9",
  "title": "",
  "body": " Suppose that is a noninvertible, symmetric matrix having eigenvectors and associated eigenvalues and . Find matrices and such that .       Since is not invertible, the third eigenvalue must be zero: . Also, an eigenvector associated to must be orthogonal to both and . We can find such a vector by finding where . This leads to   "
},
{
  "id": "sec-symmetric-matrices-6-10",
  "level": "2",
  "url": "sec-symmetric-matrices.html#sec-symmetric-matrices-6-10",
  "type": "Exercise",
  "number": "23.1.4.10",
  "title": "",
  "body": " Suppose that is a plane in and that is the matrix that projects vectors orthogonally onto .   Explain why is orthogonally diagonalizable.    What are the eigenvalues of ?    Explain the relationship between the eigenvectors of and the plane .          is symmetric    0 or 1    The eigenspaces and          If is a matrix whose columns are an orthonormal basis for , then . This means that so is symmetric and hence orthogonally diagonalizable.    If is in , then and if is in , then . This means that the eigenvalues of are either 0 or 1.    The eigenspaces and      "
},
{
  "id": "sec-quadratic-forms",
  "level": "1",
  "url": "sec-quadratic-forms.html",
  "type": "Section",
  "number": "23.2",
  "title": "Quadratic forms",
  "body": " Quadratic forms   With our understanding of symmetric matrices and variance in hand, we'll now explore how to determine the directions in which the variance of a dataset is as large as possible and where it is as small as possible. This is part of a much larger story involving a type of function, called a quadratic form , that we'll introduce here.    Let's begin by looking at an example. Suppose we have three data points that form the demeaned data matrix    Plot the demeaned data points in . In which direction does the variance appear to be largest and in which does it appear to be smallest?      Use this coordinate grid to plot the demeaned data points.     Construct the covariance matrix and determine the variance in the direction of and the variance in the direction of .     What is the total variance of this dataset?    Generally speaking, if is the covariance matrix of a dataset and is an eigenvector of having unit length and with associated eigenvalue , what is ?          The variance appears to be greatest in the direction of and smallest in the direction of .    In the direction of , the variance is , while in the direction of , the variance is .    The total variance is .              Quadratic forms  Given a matrix of demeaned data points, the symmetric covariance matrix determines the variance in a particular direction where is a unit vector defining the direction.  More generally, a symmetric matrix defines a function by Notice that this expression is similar to the one we use to find the variance in terms of the covariance matrix . The only difference is that we allow to be any vector rather than requiring it to be a unit vector.    Suppose that . If we write , then we have   We may evaluate the quadratic form using some input vectors: Notice that the value of the quadratic form is a scalar.     quadratic form   If is a symmetric matrix, the quadratic form defined by is the function .      Let's look at some more examples of quadratic forms.   Consider the symmetric matrix . Write the quadratic form defined by in terms of the components of . What is the value of ?    Given the symmetric matrix , write the quadratic form defined by and evaluate .    Suppose that . Find a symmetric matrix such that is the quadratic form defined by .    Suppose that is a quadratic form and that . What is ? ? ?    Suppose that is a symmetric matrix and is the quadratic form defined by . Suppose that is an eigenvector of with associated eigenvalue -4 and with length 7. What is ?           and      and          Notice that . In the same way, we have and .                and      and           , and             Linear algebra is principally about things that are linear. However, quadratic forms, as the name implies, have a distinctly non-linear character. First, if , is a symmetric matrix, then the associated quadratic form is Notice how the variables and are multiplied together, which tells us this isn't a linear function.  This expression assumes an especially simple form when is a diagonal matrix. In particular, if , then . This is special because there is no cross-term involving .  Remember that matrix transformations have the property that . Quadratic forms behave differently: For instance, when we multiply by the scalar 2, then . Also, notice that since the scalar is squared.  Finally, evaluating a quadratic form on an eigenvector has a particularly simple form. Suppose that is an eigenvector of with associated eigenvalue . We then have   Let's now return to our motivating question: in which direction is the variance of a dataset as large as possible and in which is it as small as possible. Remembering that the vector is a unit vector, we can now state a more general form of this question: If is a quadratic form, for which unit vectors is as large as possible and for which is it as small as possible? Since a unit vector specifies a direction, we will often ask for the directions in which the quadratic form is at its maximum or minimum value.    We can gain some intuition about this problem by graphing the quadratic form and paying particular attention to the unit vectors.   Evaluating the following cell defines the matrix and displays the graph of the associated quadratic form . In addition, the points corresponding to vectors with unit length are displayed as a curve. Notice that the matrix is diagonal. In which directions does the quadratic form have its maximum and minimum values?    Write the quadratic form associated to . What is the value of ? What is the value of ?    Consider a unit vector so that , an expression we can rewrite as . Write the quadratic form and replace by . Now explain why the maximum of is 3. In which direction does the maximum occur? Does this agree with what you observed from the graph above?    Write the quadratic form and replace by . What is the minimum value of and in which direction does the minimum occur?    Use the previous Sage cell to change the matrix to and display the graph of the quadratic form . Determine the directions in which the maximum and minimum occur.    Remember that is symmetric so that where is the diagonal matrix above and is the orthogonal matrix that rotates vectors by . Notice that where . That is, we have .  Explain why is also a unit vector; that is, explain why     Using the fact that , explain how we now know the maximum value of is 3 and determine the direction in which it occurs. Also, determine the minimum value of and determine the direction in which it occurs.          The maximum appears to occur in the direction of and the minimum appears to occur in the direction of .     so that and .    We have so that the quadratic form only depends on . The graph of this function of is a parabola that has a maximum of when . Since , this means that the maximum occurs when . We therefore see that the maximum value of is in the direction as we saw from the graph.    Now , which has a minimum value of when . Therefore, the minimum value of is in the direction .    The graph of appears to be similar to the graph of only rotated by . This means the maximum appears to occur in the direction and the minimum in the direction .    Since is orthogonal, we have so that     Since , the maximum of is , which occurs when . This means that , the eigenvector of associated to .  In the same way, the minimum value of is , which occurs when , the eigenvector of associated to .          The maximum appears to occur in the direction of and the minimum appears to occur in the direction of .     so that and .    The maximum value of is in the direction .    The minimum value of is in the direction .    The maximum appears to occur in the direction and the minimum in the direction .    Use the fact that     The maximum of is , which occurs when .  The maximum of is , which occurs when .       This activity demonstrates how the eigenvalues of determine the maximum and minimum values of the quadratic form when evaluated on unit vectors and how the associated eigenvectors determine the directions in which the maximum and minimum values occur. Let's look at another example so that this connection is clear.    Consider the symmetric matrix . Because is symmetric, we know that it can be orthogonally diagonalized. In fact, we have where From this diagonalization, we know that is the largest eigenvalue of with associated eigenvector and that is the smallest eigenvalue with associated eigenvector .  Let's first study the quadratic form because the absence of the cross-term makes it comparatively simple. Remembering that is a unit vector, we have , which means that . Therefore, This tells us that has a maximum value of , which occurs when or in the direction .  In the same way, rewriting allows us to conclude that the minimum value of is , which occurs in the direction .  Let's now return to the matrix whose quadratic form is related to because . In particular, we have In other words, we have where . This is quite useful because it allows us to relate the values of to those of , which we already understand quite well.  Now it turns out that is also a unit vector because Therefore, the maximum value of is the same as , which we know to be and which occurs in the direction . This means that the maximum value of is also and that this occurs in the direction . We now know that the maximum value of is the largest eigenvalue and that this maximum value occurs in the direction of an associated eigenvector.  In the same way, we see that the minimum value of is the smallest eigenvalue and that this minimum occurs in the direction of , an associated eigenvector.    More generally, we have    Suppose that is a symmetric matrix, that we list its eigenvalues in decreasing order , and that is a basis of associated eigenvectors. The maximum value of among all unit vectors is , which occurs in the direction . Similarly, the minimum value of is , which occurs in the direction .      Suppose that is the symmetric matrix , which may be orthogonally diagonalized as where We see that the maximum value of is 12, which occurs in the direction , and the minimum value is -6, which occurs in the direction .      Suppose we have the matrix of demeaned data points that we considered in . The data points are shown in .      The set of demeaned data points from .   Constructing the covariance matrix gives , which has eigenvalues , with associated eigenvector , and , with associated eigenvector .  Remember that the variance in a direction is . Therefore, the variance attains a maximum value of 9 in the direction and a minimum value of 1\/3 in the direction . shows the data projected onto the lines defined by these vectors.       The demeaned data from is shown projected onto the lines of maximal and minimal variance.   Remember that variance is additive, as stated in , which tells us that the total variance is .    We've been focused on finding the directions in which a quadratic form attains its maximum and minimum values, but there's another important observation to make after this activity. Recall how we used the fact that a symmetric matrix is orthogonally diagonalizable: if , then where .  More generally, if we define , we have Remembering that the quadratic form associated to a diagonal form has no cross terms, we obtain In other words, after a change of coordinates, the quadratic form can be written without cross terms. This is known as the Principal Axes Theorem.   Principal Axes Theorem   If is a symmetric matrix with eigenvalues , then the quadratic form can be written, after an orthogonal change of coordinates , as     We will put this to use in the next section.    Definite symmetric matrices  While our questions about variance provide some motivation for exploring quadratic forms, these functions appear in a variety of other contexts so it's worth spending some more time with them. For example, quadratic forms appear in multivariable calculus when describing the behavior of a function of several variables near a critical point and in physics when describing the kinetic energy of a rigid body.  The following definition will be important in this section.    A symmetric matrix is called positive definite if its associated quadratic form satisfies for any nonzero vector . If for all nonzero vectors , we say that is positive semidefinite .  Likewise, we say that is negative definite if for all nonzero vectors .  Finally, is called indefinite if for some and for others.      This activity explores the relationship between the eigenvalues of a symmetric matrix and its definiteness.   Consider the diagonal matrix and write its quadratic form in terms of the components of . How does this help you decide whether is positive definite or not?    Now consider and write its quadratic form in terms of and . What can you say about the definiteness of ?    If is a diagonal matrix, what condition on the diagonal entries guarantee that is   positive definite?   positive semidefinite?   negative definite?   negative semidefinite?   indefinite?      Suppose that is a symmetric matrix with eigenvalues 4 and 2 so that where . If , then we have . Explain why this tells us that is positive definite.    Suppose that is a symmetric matrix with eigenvalues 4 and 0. What can you say about the definiteness of in this case?    What condition on the eigenvalues of a symmetric matrix guarantees that is   positive definite?   positive semidefinite?   negative definite?   negative semidefinite?   indefinite?             . Both addends are nonnegative, and one of them is positive if is nonzero. This means that when is nonzero and so is positive definite.     , which is always nonnegative. However, so is positive semidefinite.       They are all positive.    They are all nonnegative.    They are all negative.    They are all nonpositive.    They are some positive eigenvalues and some negative ones .       Since we know that when is nonzero, we know that when is nonzero. Therefore, is positive definite.    It will be positive semidefinite.    They will be the same as before.           so is positive definite.     so is positive semidefinite.       They are all positive.    They are all nonnegative.    They are all negative.    They are all nonpositive.    They are some positive eigenvalues and some negative ones .        is positive definite.     is positive semidefinite.    They will be the same as before.       As seen in this activity, it is straightforward to determine the definiteness of a diagonal matrix. For instance, if , then This shows that when either or is not zero so we conclude that is positive definite. In the same way, we see that is positive semidefinite if all the diagonal entries are nonnegative.  Understanding this behavior for diagonal matrices enables us to understand more general symmetric matrices. As we saw previously, the quadratic form for a symmetric matrix agrees with the quadratic form for the diagonal matrix after a change of coordinates. In particular, where . Now the diagonal entries of are the eigenvalues of from which we conclude that if all the eigenvalues of are positive. Likewise, if all the eigenvalues are nonnegative.    A symmetric matrix is positive definite if all its eigenvalues are positive. It is positive semidefinite if all its eigenvalues are nonnegative.  Likewise, a symmetric matrix is indefinite if some eigenvalues are positive and some are negative.    We will now apply what we've learned about quadratic forms to study the nature of critical points in multivariable calculus. The rest of this section assumes that the reader is familiar with ideas from multivariable calculus and can be skipped by others.  First, suppose that is a differentiable function. We will use and to denote the partial derivatives of with respect to and . Similarly, , , and denote the second partial derivatives. You may recall that the mixed partials, and are equal under a mild assumption on the function . A typical question in calculus is to determine where this function has its maximum and minimum values.  Any local maximum or minimum of appears at a critical point where Near a critical point, the quadratic approximation of tells us that     Let's explore how our understanding of quadratic forms helps us determine the behavior of a function near a critical point.   Consider the function . Find the partial derivatives and and use these expressions to determine the critical points of .    Evaluate the second partial derivatives , , and .    Let's first consider the critical point . Use the quadratic approximation as written above to find an expression approximating near the critical point.    Using the vector , rewrite your approximation as for some matrix . What is the matrix in this case?    Find the eigenvalues of . What can you conclude about the definiteness of ?    Recall that is a local minimum for if for nearby points . Explain why our understanding of the eigenvalues of shows that is a local minimum for .           We have     We have     This gives     The matrix is .    The eigenvalues are and , both of which are positive, which means that is positive definite.    We have if is nonzero so for points near to .          We have     We have     This gives      .     is positive definite.     for points near to .       Near a critical point of a function , we can write where and . If is positive definite, then , which tells us that and that the critical point is therefore a local minimum.  The matrix is called the Hessian of , and we see now that the eigenvalues of this symmetric matrix determine the nature of the critical point . In particular, if the eigenvalues are both positive, then is positive definite, and the critical point is a local minimum.  This observation leads to the Second Derivative Test for multivariable functions.   Second Derivative Test   The nature of a critical point of a multivariable function is determined by the Hessian of the function at the critical point. If    has all positive eigenvalues, the critical point is a local minimum.     has all negative eigenvalues, the critical point is a local maximum.     has both positive and negative eigenvalues, the critical point is neither a local maximum nor minimum.       Most multivariable calculus texts assume that the reader is not familiar with linear algebra and so write the second derivative test for functions of two variables in terms of . If    and , then is a local minimum.     and , then is a local maximum.     , then is neither a local maximum nor minimum.   The conditions in this version of the second derivative test are simply algebraic criteria that tell us about the definiteness of the Hessian matrix .    Summary  This section explored quadratic forms, functions that are defined by symmetric matrices.   If is a symmetric matrix, then the quadratic form defined by is the function . Quadratic forms appear when studying the variance of a dataset. If is the covariance matrix, then the variance in the direction defined by a unit vector is .  Similarly, quadratic forms appear in multivariable calculus when analyzing the behavior of a function of several variables near a critical point.    If is the largest eigenvalue of a symmetric matrix and the smallest, then the maximum value of among unit vectors , is , and this maximum value occurs in the direction of , a unit eigenvector associated to .  Similarly, the minimum value of is , which appears in the direction of , an eigenvector associated to .    A symmetric matrix is positive definite if its eigenvalues are all positive, positive semidefinite if its eigenvalues are all nonnegative, and indefinite if it has both positive and negative eigenvalues.    If the Hessian of a multivariable function is positive definite at a critical point, then the critical point is a local minimum. Likewise, if the Hessian is negative definite, the critical point is a local maximum.        Suppose that .   Find an orthogonal diagonalization of .    Evaluate the quadratic form .    Find the unit vector for which is as large as possible. What is the value of in this direction?    Find the unit vector for which is as small as possible. What is the value of in this direction?          where          The maximum value is in the direction .    The minimum value is in the direction .          where          The maximum value is in the direction .    The minimum value is in the direction .       Consider the quadratic form    Find a matrix such that .    Find the maximum and minimum values of among all unit vectors and describe the directions in which they occur.              The maximum value is in the direction . The minimum value is in the direction .              The maximum value is in the direction . The minimum value is in the direction .       Suppose that is a demeaned data matrix:    Find the covariance matrix .    What is the variance of the data projected onto the line defined by .    What is the total variance?    In which direction is the variance greatest and what is the variance in this direction?                    .    In the direction where .                   The total variance is .    The variance is greatest in the direction where .       Consider the matrix .   Find and such that .    Find the maximum and minimum values of among all unit vectors .    Describe the direction in which the minimum value occurs. What can you say about the direction in which the maximum occurs?              The maximum is 7 and the minimum is -2.    The minimum occurs in the direction of . The maximum occurs along a circle defined by where and and are the first two columns of .              The maximum is 7 and the minimum is -2.    The minimum occurs in the direction of . The maximum occurs along a circle defined by where and and are the first two columns of .       Consider the matrix .   Find the matrix so that .    Find the maximum and minimum values of among all unit vectors and describe the directions in which they occur.    What does the minimum value of tell you about the matrix ?          .    The maximum value is in the direction and the minimum value is in the direction .    The columns of are linearly dependent.          so .    The maximum value is in the direction and the minimum value is in the direction .    The minimum value of says there is a vector such that , which means that . So there is a nonzero vector that is a solution to the homogeneous equations, which implies that the columns of are linearly dependent.       Consider the quadratic form    What can you say about the definiteness of the matrix that defines the quadratic form?    Find a matrix so that the change of coordinates transforms the quadratic form into one that has no cross terms. Write the quadratic form in terms of .    What are the maximum and minimum values for among all unit vectors ?          is positive definite.     and     The maximum value is and the minimum value is .         The matrix has eigenvalues , , and so is positive definite.     . If , then     The maximum value is and the minimum value is .       Explain why the following statements are true.   Given any matrix , the matrix is a symmetric, positive semidefinite matrix.    If both and are symmetric, positive definite matrices, then is a symmetric, positive definite matrix.    If is a symmetric, invertible, positive definite matrix, then is also.          .     .    The eigenvalues of are the reciprocals of the eigenvalues of .         The matrix is symmetric because . If is an eigenvector having unit length, then and .    If is a nonzero vector, then .    In this case, we know that the eigenvalues of are all positive. If is an eigenvalue of , then is an eigenvalue of so is also positive definite.       Determine whether the following statements are true or false and explain your reasoning.   If is an indefinite matrix, we can't know whether it is positive definite or not.    If the smallest eigenvalue of is 3, then is positive definite.    If is the covariance matrix associated with a dataset, then is positive semidefinite.    If is a symmetric matrix and the maximum and minimum values of occur at and , then is diagonal.    If is negative definite and is an orthogonal matrix with , then is negative definite.         False    True    True    True    True         False. We know that it is neither positive nor negative definite.    True. This implies that for all unit vectors . Then for any other vector .    True. If is an eigenvalue with associated unit eigenvector , then     True. The matrix is the identity so .    True. In this case, and are similar so they have the same eigenvalues, all of which are negative.       Determine the critical points for each of the following functions. At each critical point, determine the Hessian , describe the definiteness of , and determine whether the critical point is a local maximum or minimum.    .     .          is neither a local maximum nor minimum.     is neither a local maximum nor minimum. Both and are local minima.            Also, , , and so that the Hessian matrix is . The eigenvalues are and so this matrix is indefinite and the critical point is neither a local maximum nor minimum.     so and . Therefore, so , , or .  Also, , , and so that the Hessian matrix is .  At , the Hessian is indefinite so this critical point is neither a local maximum nor minimum. At both and , it is positive definite so the critical points are local minima.       Consider the function .   Show that has a critical point at and construct the Hessian at that point.    Find the eigenvalues of . Is this a definite matrix of some kind?    What does this imply about whether is a local maximum or minimum?         The Hessian matrix at that point is      is positive definite.    The critical point is a local minimum.          and all three equations are satisfied at the point .  The Hessian matrix at that point is     The eigenvalues are , , and so is positive definite    This implies that the critical point is a local minimum.       "
},
{
  "id": "preview-quadforms",
  "level": "2",
  "url": "sec-quadratic-forms.html#preview-quadforms",
  "type": "Preview Activity",
  "number": "23.2.1",
  "title": "",
  "body": "  Let's begin by looking at an example. Suppose we have three data points that form the demeaned data matrix    Plot the demeaned data points in . In which direction does the variance appear to be largest and in which does it appear to be smallest?      Use this coordinate grid to plot the demeaned data points.     Construct the covariance matrix and determine the variance in the direction of and the variance in the direction of .     What is the total variance of this dataset?    Generally speaking, if is the covariance matrix of a dataset and is an eigenvector of having unit length and with associated eigenvalue , what is ?          The variance appears to be greatest in the direction of and smallest in the direction of .    In the direction of , the variance is , while in the direction of , the variance is .    The total variance is .           "
},
{
  "id": "sec-quadratic-forms-3-4",
  "level": "2",
  "url": "sec-quadratic-forms.html#sec-quadratic-forms-3-4",
  "type": "Example",
  "number": "23.2.2",
  "title": "",
  "body": "  Suppose that . If we write , then we have   We may evaluate the quadratic form using some input vectors: Notice that the value of the quadratic form is a scalar.   "
},
{
  "id": "sec-quadratic-forms-3-5",
  "level": "2",
  "url": "sec-quadratic-forms.html#sec-quadratic-forms-3-5",
  "type": "Definition",
  "number": "23.2.3",
  "title": "",
  "body": " quadratic form   If is a symmetric matrix, the quadratic form defined by is the function .   "
},
{
  "id": "sec-quadratic-forms-3-6",
  "level": "2",
  "url": "sec-quadratic-forms.html#sec-quadratic-forms-3-6",
  "type": "Activity",
  "number": "23.2.2",
  "title": "",
  "body": "  Let's look at some more examples of quadratic forms.   Consider the symmetric matrix . Write the quadratic form defined by in terms of the components of . What is the value of ?    Given the symmetric matrix , write the quadratic form defined by and evaluate .    Suppose that . Find a symmetric matrix such that is the quadratic form defined by .    Suppose that is a quadratic form and that . What is ? ? ?    Suppose that is a symmetric matrix and is the quadratic form defined by . Suppose that is an eigenvector of with associated eigenvalue -4 and with length 7. What is ?           and      and          Notice that . In the same way, we have and .                and      and           , and            "
},
{
  "id": "sec-quadratic-forms-3-12",
  "level": "2",
  "url": "sec-quadratic-forms.html#sec-quadratic-forms-3-12",
  "type": "Activity",
  "number": "23.2.3",
  "title": "",
  "body": "  We can gain some intuition about this problem by graphing the quadratic form and paying particular attention to the unit vectors.   Evaluating the following cell defines the matrix and displays the graph of the associated quadratic form . In addition, the points corresponding to vectors with unit length are displayed as a curve. Notice that the matrix is diagonal. In which directions does the quadratic form have its maximum and minimum values?    Write the quadratic form associated to . What is the value of ? What is the value of ?    Consider a unit vector so that , an expression we can rewrite as . Write the quadratic form and replace by . Now explain why the maximum of is 3. In which direction does the maximum occur? Does this agree with what you observed from the graph above?    Write the quadratic form and replace by . What is the minimum value of and in which direction does the minimum occur?    Use the previous Sage cell to change the matrix to and display the graph of the quadratic form . Determine the directions in which the maximum and minimum occur.    Remember that is symmetric so that where is the diagonal matrix above and is the orthogonal matrix that rotates vectors by . Notice that where . That is, we have .  Explain why is also a unit vector; that is, explain why     Using the fact that , explain how we now know the maximum value of is 3 and determine the direction in which it occurs. Also, determine the minimum value of and determine the direction in which it occurs.          The maximum appears to occur in the direction of and the minimum appears to occur in the direction of .     so that and .    We have so that the quadratic form only depends on . The graph of this function of is a parabola that has a maximum of when . Since , this means that the maximum occurs when . We therefore see that the maximum value of is in the direction as we saw from the graph.    Now , which has a minimum value of when . Therefore, the minimum value of is in the direction .    The graph of appears to be similar to the graph of only rotated by . This means the maximum appears to occur in the direction and the minimum in the direction .    Since is orthogonal, we have so that     Since , the maximum of is , which occurs when . This means that , the eigenvector of associated to .  In the same way, the minimum value of is , which occurs when , the eigenvector of associated to .          The maximum appears to occur in the direction of and the minimum appears to occur in the direction of .     so that and .    The maximum value of is in the direction .    The minimum value of is in the direction .    The maximum appears to occur in the direction and the minimum in the direction .    Use the fact that     The maximum of is , which occurs when .  The maximum of is , which occurs when .      "
},
{
  "id": "sec-quadratic-forms-3-14",
  "level": "2",
  "url": "sec-quadratic-forms.html#sec-quadratic-forms-3-14",
  "type": "Example",
  "number": "23.2.4",
  "title": "",
  "body": "  Consider the symmetric matrix . Because is symmetric, we know that it can be orthogonally diagonalized. In fact, we have where From this diagonalization, we know that is the largest eigenvalue of with associated eigenvector and that is the smallest eigenvalue with associated eigenvector .  Let's first study the quadratic form because the absence of the cross-term makes it comparatively simple. Remembering that is a unit vector, we have , which means that . Therefore, This tells us that has a maximum value of , which occurs when or in the direction .  In the same way, rewriting allows us to conclude that the minimum value of is , which occurs in the direction .  Let's now return to the matrix whose quadratic form is related to because . In particular, we have In other words, we have where . This is quite useful because it allows us to relate the values of to those of , which we already understand quite well.  Now it turns out that is also a unit vector because Therefore, the maximum value of is the same as , which we know to be and which occurs in the direction . This means that the maximum value of is also and that this occurs in the direction . We now know that the maximum value of is the largest eigenvalue and that this maximum value occurs in the direction of an associated eigenvector.  In the same way, we see that the minimum value of is the smallest eigenvalue and that this minimum occurs in the direction of , an associated eigenvector.   "
},
{
  "id": "prop-quadform-extrema",
  "level": "2",
  "url": "sec-quadratic-forms.html#prop-quadform-extrema",
  "type": "Proposition",
  "number": "23.2.5",
  "title": "",
  "body": "  Suppose that is a symmetric matrix, that we list its eigenvalues in decreasing order , and that is a basis of associated eigenvectors. The maximum value of among all unit vectors is , which occurs in the direction . Similarly, the minimum value of is , which occurs in the direction .   "
},
{
  "id": "sec-quadratic-forms-3-17",
  "level": "2",
  "url": "sec-quadratic-forms.html#sec-quadratic-forms-3-17",
  "type": "Example",
  "number": "23.2.6",
  "title": "",
  "body": "  Suppose that is the symmetric matrix , which may be orthogonally diagonalized as where We see that the maximum value of is 12, which occurs in the direction , and the minimum value is -6, which occurs in the direction .   "
},
{
  "id": "sec-quadratic-forms-3-18",
  "level": "2",
  "url": "sec-quadratic-forms.html#sec-quadratic-forms-3-18",
  "type": "Example",
  "number": "23.2.7",
  "title": "",
  "body": "  Suppose we have the matrix of demeaned data points that we considered in . The data points are shown in .      The set of demeaned data points from .   Constructing the covariance matrix gives , which has eigenvalues , with associated eigenvector , and , with associated eigenvector .  Remember that the variance in a direction is . Therefore, the variance attains a maximum value of 9 in the direction and a minimum value of 1\/3 in the direction . shows the data projected onto the lines defined by these vectors.       The demeaned data from is shown projected onto the lines of maximal and minimal variance.   Remember that variance is additive, as stated in , which tells us that the total variance is .   "
},
{
  "id": "sec-quadratic-forms-3-21",
  "level": "2",
  "url": "sec-quadratic-forms.html#sec-quadratic-forms-3-21",
  "type": "Theorem",
  "number": "23.2.10",
  "title": "Principal Axes Theorem.",
  "body": " Principal Axes Theorem   If is a symmetric matrix with eigenvalues , then the quadratic form can be written, after an orthogonal change of coordinates , as    "
},
{
  "id": "sec-quadratic-forms-4-4",
  "level": "2",
  "url": "sec-quadratic-forms.html#sec-quadratic-forms-4-4",
  "type": "Definition",
  "number": "23.2.11",
  "title": "",
  "body": "  A symmetric matrix is called positive definite if its associated quadratic form satisfies for any nonzero vector . If for all nonzero vectors , we say that is positive semidefinite .  Likewise, we say that is negative definite if for all nonzero vectors .  Finally, is called indefinite if for some and for others.   "
},
{
  "id": "sec-quadratic-forms-4-5",
  "level": "2",
  "url": "sec-quadratic-forms.html#sec-quadratic-forms-4-5",
  "type": "Activity",
  "number": "23.2.4",
  "title": "",
  "body": "  This activity explores the relationship between the eigenvalues of a symmetric matrix and its definiteness.   Consider the diagonal matrix and write its quadratic form in terms of the components of . How does this help you decide whether is positive definite or not?    Now consider and write its quadratic form in terms of and . What can you say about the definiteness of ?    If is a diagonal matrix, what condition on the diagonal entries guarantee that is   positive definite?   positive semidefinite?   negative definite?   negative semidefinite?   indefinite?      Suppose that is a symmetric matrix with eigenvalues 4 and 2 so that where . If , then we have . Explain why this tells us that is positive definite.    Suppose that is a symmetric matrix with eigenvalues 4 and 0. What can you say about the definiteness of in this case?    What condition on the eigenvalues of a symmetric matrix guarantees that is   positive definite?   positive semidefinite?   negative definite?   negative semidefinite?   indefinite?             . Both addends are nonnegative, and one of them is positive if is nonzero. This means that when is nonzero and so is positive definite.     , which is always nonnegative. However, so is positive semidefinite.       They are all positive.    They are all nonnegative.    They are all negative.    They are all nonpositive.    They are some positive eigenvalues and some negative ones .       Since we know that when is nonzero, we know that when is nonzero. Therefore, is positive definite.    It will be positive semidefinite.    They will be the same as before.           so is positive definite.     so is positive semidefinite.       They are all positive.    They are all nonnegative.    They are all negative.    They are all nonpositive.    They are some positive eigenvalues and some negative ones .        is positive definite.     is positive semidefinite.    They will be the same as before.      "
},
{
  "id": "prop-definite-matrices",
  "level": "2",
  "url": "sec-quadratic-forms.html#prop-definite-matrices",
  "type": "Proposition",
  "number": "23.2.12",
  "title": "",
  "body": "  A symmetric matrix is positive definite if all its eigenvalues are positive. It is positive semidefinite if all its eigenvalues are nonnegative.  Likewise, a symmetric matrix is indefinite if some eigenvalues are positive and some are negative.   "
},
{
  "id": "sec-quadratic-forms-4-12",
  "level": "2",
  "url": "sec-quadratic-forms.html#sec-quadratic-forms-4-12",
  "type": "Activity",
  "number": "23.2.5",
  "title": "",
  "body": "  Let's explore how our understanding of quadratic forms helps us determine the behavior of a function near a critical point.   Consider the function . Find the partial derivatives and and use these expressions to determine the critical points of .    Evaluate the second partial derivatives , , and .    Let's first consider the critical point . Use the quadratic approximation as written above to find an expression approximating near the critical point.    Using the vector , rewrite your approximation as for some matrix . What is the matrix in this case?    Find the eigenvalues of . What can you conclude about the definiteness of ?    Recall that is a local minimum for if for nearby points . Explain why our understanding of the eigenvalues of shows that is a local minimum for .           We have     We have     This gives     The matrix is .    The eigenvalues are and , both of which are positive, which means that is positive definite.    We have if is nonzero so for points near to .          We have     We have     This gives      .     is positive definite.     for points near to .      "
},
{
  "id": "sec-quadratic-forms-4-16",
  "level": "2",
  "url": "sec-quadratic-forms.html#sec-quadratic-forms-4-16",
  "type": "Proposition",
  "number": "23.2.13",
  "title": "Second Derivative Test.",
  "body": " Second Derivative Test   The nature of a critical point of a multivariable function is determined by the Hessian of the function at the critical point. If    has all positive eigenvalues, the critical point is a local minimum.     has all negative eigenvalues, the critical point is a local maximum.     has both positive and negative eigenvalues, the critical point is neither a local maximum nor minimum.      "
},
{
  "id": "sec-quadratic-forms-6-1",
  "level": "2",
  "url": "sec-quadratic-forms.html#sec-quadratic-forms-6-1",
  "type": "Exercise",
  "number": "23.2.4.1",
  "title": "",
  "body": " Suppose that .   Find an orthogonal diagonalization of .    Evaluate the quadratic form .    Find the unit vector for which is as large as possible. What is the value of in this direction?    Find the unit vector for which is as small as possible. What is the value of in this direction?          where          The maximum value is in the direction .    The minimum value is in the direction .          where          The maximum value is in the direction .    The minimum value is in the direction .     "
},
{
  "id": "sec-quadratic-forms-6-2",
  "level": "2",
  "url": "sec-quadratic-forms.html#sec-quadratic-forms-6-2",
  "type": "Exercise",
  "number": "23.2.4.2",
  "title": "",
  "body": " Consider the quadratic form    Find a matrix such that .    Find the maximum and minimum values of among all unit vectors and describe the directions in which they occur.              The maximum value is in the direction . The minimum value is in the direction .              The maximum value is in the direction . The minimum value is in the direction .     "
},
{
  "id": "sec-quadratic-forms-6-3",
  "level": "2",
  "url": "sec-quadratic-forms.html#sec-quadratic-forms-6-3",
  "type": "Exercise",
  "number": "23.2.4.3",
  "title": "",
  "body": " Suppose that is a demeaned data matrix:    Find the covariance matrix .    What is the variance of the data projected onto the line defined by .    What is the total variance?    In which direction is the variance greatest and what is the variance in this direction?                    .    In the direction where .                   The total variance is .    The variance is greatest in the direction where .     "
},
{
  "id": "sec-quadratic-forms-6-4",
  "level": "2",
  "url": "sec-quadratic-forms.html#sec-quadratic-forms-6-4",
  "type": "Exercise",
  "number": "23.2.4.4",
  "title": "",
  "body": " Consider the matrix .   Find and such that .    Find the maximum and minimum values of among all unit vectors .    Describe the direction in which the minimum value occurs. What can you say about the direction in which the maximum occurs?              The maximum is 7 and the minimum is -2.    The minimum occurs in the direction of . The maximum occurs along a circle defined by where and and are the first two columns of .              The maximum is 7 and the minimum is -2.    The minimum occurs in the direction of . The maximum occurs along a circle defined by where and and are the first two columns of .     "
},
{
  "id": "sec-quadratic-forms-6-5",
  "level": "2",
  "url": "sec-quadratic-forms.html#sec-quadratic-forms-6-5",
  "type": "Exercise",
  "number": "23.2.4.5",
  "title": "",
  "body": " Consider the matrix .   Find the matrix so that .    Find the maximum and minimum values of among all unit vectors and describe the directions in which they occur.    What does the minimum value of tell you about the matrix ?          .    The maximum value is in the direction and the minimum value is in the direction .    The columns of are linearly dependent.          so .    The maximum value is in the direction and the minimum value is in the direction .    The minimum value of says there is a vector such that , which means that . So there is a nonzero vector that is a solution to the homogeneous equations, which implies that the columns of are linearly dependent.     "
},
{
  "id": "sec-quadratic-forms-6-6",
  "level": "2",
  "url": "sec-quadratic-forms.html#sec-quadratic-forms-6-6",
  "type": "Exercise",
  "number": "23.2.4.6",
  "title": "",
  "body": " Consider the quadratic form    What can you say about the definiteness of the matrix that defines the quadratic form?    Find a matrix so that the change of coordinates transforms the quadratic form into one that has no cross terms. Write the quadratic form in terms of .    What are the maximum and minimum values for among all unit vectors ?          is positive definite.     and     The maximum value is and the minimum value is .         The matrix has eigenvalues , , and so is positive definite.     . If , then     The maximum value is and the minimum value is .     "
},
{
  "id": "sec-quadratic-forms-6-7",
  "level": "2",
  "url": "sec-quadratic-forms.html#sec-quadratic-forms-6-7",
  "type": "Exercise",
  "number": "23.2.4.7",
  "title": "",
  "body": " Explain why the following statements are true.   Given any matrix , the matrix is a symmetric, positive semidefinite matrix.    If both and are symmetric, positive definite matrices, then is a symmetric, positive definite matrix.    If is a symmetric, invertible, positive definite matrix, then is also.          .     .    The eigenvalues of are the reciprocals of the eigenvalues of .         The matrix is symmetric because . If is an eigenvector having unit length, then and .    If is a nonzero vector, then .    In this case, we know that the eigenvalues of are all positive. If is an eigenvalue of , then is an eigenvalue of so is also positive definite.     "
},
{
  "id": "sec-quadratic-forms-6-8",
  "level": "2",
  "url": "sec-quadratic-forms.html#sec-quadratic-forms-6-8",
  "type": "Exercise",
  "number": "23.2.4.8",
  "title": "",
  "body": " Determine whether the following statements are true or false and explain your reasoning.   If is an indefinite matrix, we can't know whether it is positive definite or not.    If the smallest eigenvalue of is 3, then is positive definite.    If is the covariance matrix associated with a dataset, then is positive semidefinite.    If is a symmetric matrix and the maximum and minimum values of occur at and , then is diagonal.    If is negative definite and is an orthogonal matrix with , then is negative definite.         False    True    True    True    True         False. We know that it is neither positive nor negative definite.    True. This implies that for all unit vectors . Then for any other vector .    True. If is an eigenvalue with associated unit eigenvector , then     True. The matrix is the identity so .    True. In this case, and are similar so they have the same eigenvalues, all of which are negative.     "
},
{
  "id": "sec-quadratic-forms-6-9",
  "level": "2",
  "url": "sec-quadratic-forms.html#sec-quadratic-forms-6-9",
  "type": "Exercise",
  "number": "23.2.4.9",
  "title": "",
  "body": " Determine the critical points for each of the following functions. At each critical point, determine the Hessian , describe the definiteness of , and determine whether the critical point is a local maximum or minimum.    .     .          is neither a local maximum nor minimum.     is neither a local maximum nor minimum. Both and are local minima.            Also, , , and so that the Hessian matrix is . The eigenvalues are and so this matrix is indefinite and the critical point is neither a local maximum nor minimum.     so and . Therefore, so , , or .  Also, , , and so that the Hessian matrix is .  At , the Hessian is indefinite so this critical point is neither a local maximum nor minimum. At both and , it is positive definite so the critical points are local minima.     "
},
{
  "id": "sec-quadratic-forms-6-10",
  "level": "2",
  "url": "sec-quadratic-forms.html#sec-quadratic-forms-6-10",
  "type": "Exercise",
  "number": "23.2.4.10",
  "title": "",
  "body": " Consider the function .   Show that has a critical point at and construct the Hessian at that point.    Find the eigenvalues of . Is this a definite matrix of some kind?    What does this imply about whether is a local maximum or minimum?         The Hessian matrix at that point is      is positive definite.    The critical point is a local minimum.          and all three equations are satisfied at the point .  The Hessian matrix at that point is     The eigenvalues are , , and so is positive definite    This implies that the critical point is a local minimum.     "
},
{
  "id": "sec-pca",
  "level": "1",
  "url": "sec-pca.html",
  "type": "Section",
  "number": "23.3",
  "title": "Principal Component Analysis",
  "body": " Principal Component Analysis   We are sometimes presented with a dataset having many data points that live in a high dimensional space. For instance, we looked at a dataset describing body fat index (BFI) in where each data point is six-dimensional. Developing an intuitive understanding of the data is hampered by the fact that it cannot be visualized.  This section explores a technique called principal component analysis , which enables us to reduce the dimension of a dataset so that it may be visualized or studied in a way so that interesting features more readily stand out. Our previous work with variance and the orthogonal diagonalization of symmetric matrices provides the key ideas.    We will begin by recalling our earlier discussion of variance. Suppose we have a dataset that leads to the covariance matrix    Suppose that is a unit eigenvector of with eigenvalue . What is the variance in the direction?    Find an orthogonal diagonalization of .     What is the total variance?    In which direction is the variance greatest and what is the variance in this direction? If we project the data onto this line, how much variance is lost?    In which direction is the variance smallest and how is this direction related to the direction of maximum variance?           .    We can write where     The total variance is the sum of the eigenvalues, .    The variance is greatest in the direction of the eigenvector associated to the largest eigenvalue. This direction is defined by , and the variance is 15 in this direction.    The variance is smallest in the direction defined by .       Here are some ideas we've seen previously that will be particularly useful for us in this section. Remember that the covariance matrix of a dataset is where is the matrix of demeaned data points.   When is a unit vector, the variance of the demeaned data after projecting onto the line defined by is given by the quadratic form .    In particular, if is a unit eigenvector of with associated eigenvalue , then .    Moreover, variance is additive, as we recorded in : if is a subspace having an orthonormal basis , then the variance        Principal Component Analysis  Let's begin by looking at an example that illustrates the central theme of this technique.    Suppose that we work with a dataset having 100 five-dimensional data points. The demeaned data matrix is therefore and leads to the covariance matrix , which is a matrix. Because is symmetric, the Spectral Theorem tells us it is orthogonally diagonalizable so suppose that where    What is , the variance in the direction?    Find the variance of the data projected onto the line defined by . What does this say about the data?    What is the total variance of the data?    Consider the 2-dimensional subspace spanned by and . If we project the data onto this subspace, what fraction of the total variance is represented by the variance of the projected data?    How does this question change if we project onto the 3-dimensional subspace spanned by , , and ?    What does this tell us about the data?                , which tells us there is no variance in the direction. Therefore, when we project onto the line defined by , every data point projects to so every data point is in the orthogonal complement of .     .    The variance of the data projected onto this subspace is , which represents of the variance.    Projecting onto this 3-dimensional subspace retains all of the variance.    All of the data lies in the -dimensional subspace spanned by , , and .                , which tells us every data point is in the orthogonal complement of .          of the variance     of the variance.    All of the data lies in the -dimensional subspace spanned by , , and .       This activity demonstrates how the eigenvalues of the covariance matrix can tell us when data are clustered around, or even wholly contained within, a smaller dimensional subspace. In particular, the original data is 5-dimensional, but we see that it actually lies in a 3-dimensional subspace of . Later in this section, we'll see how to use this observation to work with the data as if it were three-dimensional, an idea known as dimensional reduction .   principal components The eigenvectors of the covariance matrix are called principal components , and we will order them so that their associated eigenvalues decrease. Generally speaking, we hope that the first few principal components retain most of the variance, as the example in the activity demonstrates. In that example, we have the sequence of subspaces    , the 1-dimensional subspace spanned by , which retains of the total variance,     , the 2-dimensional subspace spanned by and , which retains of the variance, and     , the 3-dimensional subspace spanned by , , and , which retains all of the variance.     Notice how we retain more of the total variance as we increase the dimension of the subspace onto which the data are projected. Eventually, projecting the data onto retains all the variance, which tells us the data must lie in , a smaller dimensional subspace of .  In fact, these subspaces are the best possible. We know that the first principal component is the eigenvector of associated to the largest eigenvalue. This means that the variance is as large as possible in the direction. In other words, projecting onto any other line will retain a smaller amount of variance. Similarly, projecting onto any other 2-dimensional subspace besides will retain less variance than projecting onto . The principal components have the wonderful ability to pick out the best possible subspaces to retain as much variance as possible.  Of course, this is a contrived example. Typically, the presence of noise in a dataset means that we do not expect all the points to be wholly contained in a smaller dimensional subspace. In fact, the 2-dimensional subspace retains of the variance. Depending on the situation, we may want to write off the remaining of the variance as noise in exchange for the convenience of working with a smaller dimensional subspace. As we'll see later, we will seek a balance using a number of principal components large enough to retain most of the variance but small enough to be easy to work with.    We will work here with a dataset having 100 3-dimensional demeaned data points. Evaluating the following cell will plot those data points and define the demeaned data matrix A whose shape is . Notice that the data appears to cluster around a plane though it does not seem to be wholly contained within that plane.   Use the matrix A to construct the covariance matrix . Then determine the variance in the direction of ?     Find the eigenvalues of and determine the total variance. Notice that Sage does not necessarily sort the eigenvalues in decreasing order.    Use the right_eigenmatrix() command to find the eigenvectors of . Remembering that the Sage command B.column(1) retrieves the vector represented by the second column of B , define vectors u1 , u2 , and u3 representing the three principal components in order of decreasing eigenvalues. How can you check if these vectors are an orthonormal basis for ?    What fraction of the total variance is retained by projecting the data onto , the subspace spanned by ? What fraction of the total variance is retained by projecting onto , the subspace spanned by and ? What fraction of the total variance do we lose by projecting onto ?    If we project a data point onto , the Projection Formula tells us we obtain Rather than viewing the projected data in , we will record the coordinates of in the basis defined by and ; that is, we will record the coordinates Construct the matrix so that .     Since each column of represents a data point, the matrix represents the coordinates of the projected data points. Evaluating the following cell will plot those projected data points. Notice how this plot enables us to view the data as if it were two-dimensional. Why is this plot wider than it is tall?          After constructing the covariance matrix , we find that .    The total variance is the sum of the eigenvalues of so we obtain .    If we obtain , the matrix of eigenvectors, from Sage, computing evaluates the dot products between the columns. Since , the basis provided by Sage is orthonormal.    Projecting onto , we see that so retains about of the total variance. The subspace retains or of the total variance. If we project onto we lose less than of the variance.         The plot is wider because the variance in the direction, which corresponds to the horizontal coordinate, is greater than the variance in the direction.                    If is the matrix of eigenvectors, evaluate .     retains of the total variance, and retains .         Because the variance in the direction is greater than the variance in the direction.       This example is a more realistic illustration of principal component analysis. The plot of the 3-dimensional data appears to show that the data lies close to a plane, and the principal components will identify this plane. Starting with the matrix of demeaned data , we construct the covariance matrix and study its eigenvalues. Notice that the first two principal components account for more than 98% of the variance, which means we can expect the points to lie close to , the two-dimensional subspace spanned by and .  Since is a subspace of , projecting the data points onto gives a list of 100 points in . In order to visualize them more easily, we instead consider the coordinates of the projections in the basis defined by and . For instance, we know that the projection of a data point is which is a three-dimensional vector. Instead, we can record the coordinates and plot them in the two-dimensional coordinate plane, as illustrated in .       The projection of a data point onto is a three-dimensional vector, which may be represented by the two coordinates describing this vector as a linear combination of and .   If we form the matrix , then we have This means that the columns of represent the coordinates of the projected points, which may now be plotted in the plane.  In this plot, the first coordinate, represented by the horizontal coordinate, represents the projection of a data point onto the line defined by while the second coordinate represents the projection onto the line defined by . Since is the first principal component, the variance in the direction is greater than the variance in the direction. For this reason, the plot will be more spread out in the horizontal direction than in the vertical.    Using Principal Component Analysis  Now that we've explored the ideas behind principal component analysis, we will look at a few examples that illustrate its use.    The next cell will load a dataset describing the average consumption of various food groups for citizens in each of the four nations of the United Kingdom. The units for each entry are grams per person per week. We will view this as a dataset consisting of four points in . As such, it is impossible to visualize and studying the numbers themselves doesn't lead to much insight.  In addition to loading the data, evaluating the cell above created a vector data_mean , which is the mean of the four data points, and A , the matrix of demeaned data.   What is the average consumption of Beverages across the four nations?     Find the covariance matrix and its eigenvalues. Because there are four points in whose mean is zero, there are only three nonzero eigenvalues.    For what percentage of the total variance does the first principal component account?    Find the first principal component and project the four demeaned data points onto the line defined by . Plot those points on       A plot of the demeaned data projected onto the first principal component.     For what percentage of the total variance do the first two principal components account?    Find the coordinates of the demeaned data points projected onto , the two-dimensional subspace of spanned by the first two principal components.   Plot these coordinates in .      The coordinates of the demeaned data points projected onto the first two principal components.     What information do these plots reveal that is not clear from consideration of the original data points?    Study the first principal component and find the first component of , which corresponds to the dietary category Alcoholic Drinks. (To do this, you may wish to use N(u1, digits=2) for a result that's easier to read.) If a data point lies on the far right side of the plot in , what does it mean about that nation's consumption of Alcoholic Drinks?          Beverages is the second category so this would be the second component of the data_mean vector, which is .    The three nonzero eigenvalues are , , and .    The total variance is the sum of the eigenvalues so the first principal component accounts for of the total variance.    The coordinates are    Nation  Coordinate    England       Northern Ireland       Scotland       Wales           The first two principal components account for of the total variance.    The coordinates are    Nation  Coordinates    England       Northern Ireland       Scotland       England           Northern Ireland appears to be significantly different from the other three nations. There are several possible reasons for this, both historical and geographical, that we might explore.    The first component of is negative. Therefore, if a nation is on the right side of this plot, the average consumption of Alcoholic Drinks will be less than the mean. This can be confirmed by looking at the original data.                , , and          The coordinates are    Nation  Coordinate    England       Northern Ireland       Scotland       Wales                The coordinates are    Nation  Coordinates    England       Northern Ireland       Scotland       England           Northern Ireland appears to be significantly different from the other three nations.    The average consumption of Alcoholic Drinks will be less than the mean.       This activity demonstrates how principal component analysis enables us to extract information from a dataset that may not be easily obtained otherwise. As in our previous example, we see that the data points lie quite close to a two-dimensional subspace of . In fact, , the subspace spanned by the first two principal components, accounts for more than 96% of the variance. More importantly, when we project the data onto , it becomes apparent that Northern Ireland is fundamentally different from the other three nations.  With some additional thought, we can determine more specific ways in which Northern Ireland is different. On the -dimensional plot, Northern Ireland lies far to the right compared to the other three nations. Since the data has been demeaned, the origin in this plot corresponds to the average of the four nations. The coordinates of the point representing Northern Ireland are about , meaning that the projected data point differs from the mean by about .  Let's just focus on the contribution from . We see that the ninth component of , the one that describes Fresh Fruit, is about . This means that the ninth component of differs from the mean by about grams per person per week. So roughly speaking, people in Northern Ireland are eating about 300 fewer grams of Fresh Fruit than the average across the four nations. This is borne out by looking at the original data, which show that the consumption of Fresh Fruit in Northern Ireland is significantly less than the other nations. Examing the other components of shows other ways in which Northern Ireland differs from the other three nations.    In this activity, we'll look at a well-known dataset that describes 150 irises representing three species of iris: Iris setosa, Iris versicolor, and Iris virginica. For each flower, the length and width of its sepal and the length and width of its petal, all in centimeters, are recorded.      One of the three species, iris versicolor, represented in the dataset showing three shorter petals and three longer sepals. (Source: Wikipedia , License: GNU Free Documetation License )   Evaluating the following cell will load the dataset, which consists of 150 points in . In addition, we have a vector data_mean , a four-dimensional vector holding the mean of the data points, and A , the demeaned data matrix. Since the data is four-dimensional, we are not able to visualize it. Of course, we could forget about two of the measurements and plot the 150 points represented by their, say, sepal length and sepal width.    What is the mean sepal width?    Find the covariance matrix and its eigenvalues.     Find the fraction of variance for which the first two principal components account.    Construct the first two principal components and along with the matrix whose columns are and .     As we have seen, the columns of the matrix hold the coordinates of the demeaned data points after projecting onto , the subspace spanned by the first two principal components. Evaluating the following cell shows a plot of these coordinates. Suppose we have a flower whose coordinates in this plane are . To what species does this iris most likely belong? Find an estimate of the sepal length, sepal width, petal length, and petal width for this flower.    Suppose you have an iris, but you only know that its sepal length is 5.65 cm and its sepal width is 2.75 cm. Knowing only these two measurements, determine the coordinates in the plane where this iris lies. To what species does this iris most likely belong? Now estimate the petal length and petal width of this iris.     Suppose you find another iris whose sepal width is 3.2 cm and whose petal width is 2.2 cm. Find the coordinates of this iris and determine the species to which it most likely belongs. Also, estimate the sepal length and the petal length.           The second component of data_mean , which is the one corresponding to sepal width, is .    The eigenvalues are , , , and .    The first two principal components account for of the variance.    If is the matrix whose columns are an orthonormal basis of eigenvectors, then is formed from the first two columns of .    This would most likely belong to Iris setosa. To find its measurements, we evaluate where is the vector of means. This is the same as , which gives the vector of measurements .    Subtracting the mean sepal length and sepal width, we have . Then the first two components of . This gives . This looks like an Iris versicolor. As in the previous part, we can now find the petal length to be and the petal width to be .    Using the same approach as the last part, we find , which gives a sepal length of and a petal length of . Most likely, this flower belongs to Iris virginica.           .     , , , .         The columns of are for the first two principal components.    Iris setosa and the vector of measurements is .    The petal length is and the petal width is .    The sepal length is and the petal length is .         Summary  This section has explored principal component analysis as a technique to reduce the dimension of a dataset. From the demeaned data matrix , we form the covariance matrix , where is the number of data points.   The eigenvectors , of are called the principal components. We arrange them so that their corresponding eigenvalues are in decreasing order.    If is the subspace spanned by the first principal components, then the variance of the demeaned data projected onto is the sum of the first eigenvalues of . No other -dimensional subspace retains more variance when the data is projected onto it.    If is the matrix whose columns are the first principal components, then the columns of hold the coordinates, expressed in the basis , of the data once projected onto .    Our goal is to use a number of principal components that is large enough to retain most of the variance in the dataset but small enough to be manageable.        Suppose that and that we have two datasets, one whose covariance matrix is and one whose covariance matrix is . For each dataset, find   the total variance.    the fraction of variance represented by the first principal component.    a verbal description of how the demeaned data points appear when plotted in the plane.          for the first dataset and for the second     and     The points in the second set are clustered very close to a line.         For the first dataset, the total variance is while the second dataset has a total variance of .    In the first dataset, the first principal component represents of the variance compared to in the second dataset.    The points in the first dataset are scattered almost uniformly around the origin. In the second dataset, they are clustered very close to the line defined by the vector .       Suppose that a dataset has mean and that its associated covariance matrix is .    What fraction of the variance is represented by the first two principal components?    If is one of the data points, find the coordinates when the demeaned point is projected into the plane defined by the first two principal components.    If a projected data point has coordinates , find an estimate for the original data point.               .     .         The eigenvalues are , , and so the fraction of variance represented by the first two principal components is     Suppose is this data point and the vector of means. If is the matrix whose columns are the first two principal components, then we find .    We find .       Evaluating the following cell loads a demeaned data matrix A .    Find the principal components and and the variance in the direction of each principal component.    What is the total variance?    What can you conclude about this dataset?         The variances are and .         All of the data lies on a line         We find that and . The variances are and .    The total variance is .    All of the data lies on the line defined by .       Determine whether the following statements are true or false and explain your thinking.   If the eigenvalues of the covariance matrix are , , and , then is the variance of the demeaned data points when projected on the third principal component .    Principal component analysis always allows us to construct a smaller dimensional representation of a dataset without losing any information.    If the eigenvalues of the covariance matrix are 56, 32, and 0, then the demeaned data points all lie on a line in .         True    False    False         True. The variance of the projected data points is .    False. If we project onto a subspace that retains less than all the variance, then we are losing information about the data in the directions orthogonal to the subspace.    False. They will lie on the plane spanned by the first two principal components.       In , we looked at a dataset consisting of four measurements of 150 irises. These measurements are sepal length, sepal width, petal length, and petal width.   Find the first principal component and describe the meaning of its four components. Which component is most significant? What can you say about the relative importance of the four measurements?    When the dataset is plotted in the plane defined by and , the specimens from the species iris-setosa lie on the left side of the plot. What does this tell us about how iris-setosa differs from the other two species in the four measurements?    In general, which species is closest to the average iris ?         The petal length is the most signficant and the sepal width is the least.    Iris setosa has a below average petal length and appears overall smaller.    Iris versicolor         The first principal component is . If we move in the direction, each component describes how we move away from the mean of that measurement. The most important component is the third one, which corresponds to petal length, while the second component, corresponding to sepal width, is least signficant. Of the four measurements, there is the most variance in petal length.    To reach an Iris setosa sample, we multiply by a negative number. This says that the petal length, the most signficant component of will be below the mean for Iris setosa. The sepal length and petal width will also be below the mean. Basically, the Iris setosa would appear to be smaller overall.    Iris versicolor would appear to be closest to average since the points are closest to the origin in the - plane.       This problem explores a dataset describing 333 penguins. There are three species, Adelie, Chinstrap, and Gentoo, as illustrated on the left of , as well as both male and female penguins in the dataset.       Artwork by @allison_horst    Evaluating the next cell will load and display the data. The meaning of the culmen length and width is contained in the illustration on the right of . This dataset is a bit different from others that we've looked at because the scale of the measurements is significantly different. For instance, the measurements for the body mass are roughly 100 times as large as those for the culmen length. For this reason, we will standardize the data by first demeaning it, as usual, and then rescaling each measurement by the reciprocal of its standard deviation. The result is stored in the matrix A .    Find the covariance matrix and its eigenvalues.    What fraction of the total variance is explained by the first two principal components?    Construct the matrix whose columns are the coordinates of the demeaned data points projected onto the first two principal components. The following cell will create the plot.     Examine the components of the first two principal component vectors. How does the body mass of Gentoo penguins compare to that of the other two species?    What seems to be generally true about the culmen measurements for a Chinstrap penguin compared to a Adelie?    You can plot just the males or females using the following cell. What seems to be generally true about the body mass measurements for a male Gentoo compared to a female Gentoo?          , , , .          where the columns of are the first two principal components.    Gentoo penguins have a larger body mass.    These measurements seem larger for the Chinstrap penguins.    The male Gentoo have a higher body mass.         The eigenvalues are , , , and .    The first two principal components retain of the total variance.     where the columns of are the first two principal components.    Gentoo penguins have a larger body mass because the fourth component, which corresponds to body mass, of is negative and the Gentoo penguins are represented by where is negative.    These measurements seem larger for the Chinstrap penguins because the first two components of are positive.    The male Gentoo lie further to the left, which means they have a higher body mass.       "
},
{
  "id": "sec-pca-2-3",
  "level": "2",
  "url": "sec-pca.html#sec-pca-2-3",
  "type": "Preview Activity",
  "number": "23.3.1",
  "title": "",
  "body": "  We will begin by recalling our earlier discussion of variance. Suppose we have a dataset that leads to the covariance matrix    Suppose that is a unit eigenvector of with eigenvalue . What is the variance in the direction?    Find an orthogonal diagonalization of .     What is the total variance?    In which direction is the variance greatest and what is the variance in this direction? If we project the data onto this line, how much variance is lost?    In which direction is the variance smallest and how is this direction related to the direction of maximum variance?           .    We can write where     The total variance is the sum of the eigenvalues, .    The variance is greatest in the direction of the eigenvector associated to the largest eigenvalue. This direction is defined by , and the variance is 15 in this direction.    The variance is smallest in the direction defined by .      "
},
{
  "id": "sec-pca-3-3",
  "level": "2",
  "url": "sec-pca.html#sec-pca-3-3",
  "type": "Activity",
  "number": "23.3.2",
  "title": "",
  "body": "  Suppose that we work with a dataset having 100 five-dimensional data points. The demeaned data matrix is therefore and leads to the covariance matrix , which is a matrix. Because is symmetric, the Spectral Theorem tells us it is orthogonally diagonalizable so suppose that where    What is , the variance in the direction?    Find the variance of the data projected onto the line defined by . What does this say about the data?    What is the total variance of the data?    Consider the 2-dimensional subspace spanned by and . If we project the data onto this subspace, what fraction of the total variance is represented by the variance of the projected data?    How does this question change if we project onto the 3-dimensional subspace spanned by , , and ?    What does this tell us about the data?                , which tells us there is no variance in the direction. Therefore, when we project onto the line defined by , every data point projects to so every data point is in the orthogonal complement of .     .    The variance of the data projected onto this subspace is , which represents of the variance.    Projecting onto this 3-dimensional subspace retains all of the variance.    All of the data lies in the -dimensional subspace spanned by , , and .                , which tells us every data point is in the orthogonal complement of .          of the variance     of the variance.    All of the data lies in the -dimensional subspace spanned by , , and .      "
},
{
  "id": "sec-pca-3-9",
  "level": "2",
  "url": "sec-pca.html#sec-pca-3-9",
  "type": "Activity",
  "number": "23.3.3",
  "title": "",
  "body": "  We will work here with a dataset having 100 3-dimensional demeaned data points. Evaluating the following cell will plot those data points and define the demeaned data matrix A whose shape is . Notice that the data appears to cluster around a plane though it does not seem to be wholly contained within that plane.   Use the matrix A to construct the covariance matrix . Then determine the variance in the direction of ?     Find the eigenvalues of and determine the total variance. Notice that Sage does not necessarily sort the eigenvalues in decreasing order.    Use the right_eigenmatrix() command to find the eigenvectors of . Remembering that the Sage command B.column(1) retrieves the vector represented by the second column of B , define vectors u1 , u2 , and u3 representing the three principal components in order of decreasing eigenvalues. How can you check if these vectors are an orthonormal basis for ?    What fraction of the total variance is retained by projecting the data onto , the subspace spanned by ? What fraction of the total variance is retained by projecting onto , the subspace spanned by and ? What fraction of the total variance do we lose by projecting onto ?    If we project a data point onto , the Projection Formula tells us we obtain Rather than viewing the projected data in , we will record the coordinates of in the basis defined by and ; that is, we will record the coordinates Construct the matrix so that .     Since each column of represents a data point, the matrix represents the coordinates of the projected data points. Evaluating the following cell will plot those projected data points. Notice how this plot enables us to view the data as if it were two-dimensional. Why is this plot wider than it is tall?          After constructing the covariance matrix , we find that .    The total variance is the sum of the eigenvalues of so we obtain .    If we obtain , the matrix of eigenvectors, from Sage, computing evaluates the dot products between the columns. Since , the basis provided by Sage is orthonormal.    Projecting onto , we see that so retains about of the total variance. The subspace retains or of the total variance. If we project onto we lose less than of the variance.         The plot is wider because the variance in the direction, which corresponds to the horizontal coordinate, is greater than the variance in the direction.                    If is the matrix of eigenvectors, evaluate .     retains of the total variance, and retains .         Because the variance in the direction is greater than the variance in the direction.      "
},
{
  "id": "fig-pca-coords",
  "level": "2",
  "url": "sec-pca.html#fig-pca-coords",
  "type": "Figure",
  "number": "23.3.1",
  "title": "",
  "body": "     The projection of a data point onto is a three-dimensional vector, which may be represented by the two coordinates describing this vector as a linear combination of and .  "
},
{
  "id": "sec-pca-4-3",
  "level": "2",
  "url": "sec-pca.html#sec-pca-4-3",
  "type": "Activity",
  "number": "23.3.4",
  "title": "",
  "body": "  The next cell will load a dataset describing the average consumption of various food groups for citizens in each of the four nations of the United Kingdom. The units for each entry are grams per person per week. We will view this as a dataset consisting of four points in . As such, it is impossible to visualize and studying the numbers themselves doesn't lead to much insight.  In addition to loading the data, evaluating the cell above created a vector data_mean , which is the mean of the four data points, and A , the matrix of demeaned data.   What is the average consumption of Beverages across the four nations?     Find the covariance matrix and its eigenvalues. Because there are four points in whose mean is zero, there are only three nonzero eigenvalues.    For what percentage of the total variance does the first principal component account?    Find the first principal component and project the four demeaned data points onto the line defined by . Plot those points on       A plot of the demeaned data projected onto the first principal component.     For what percentage of the total variance do the first two principal components account?    Find the coordinates of the demeaned data points projected onto , the two-dimensional subspace of spanned by the first two principal components.   Plot these coordinates in .      The coordinates of the demeaned data points projected onto the first two principal components.     What information do these plots reveal that is not clear from consideration of the original data points?    Study the first principal component and find the first component of , which corresponds to the dietary category Alcoholic Drinks. (To do this, you may wish to use N(u1, digits=2) for a result that's easier to read.) If a data point lies on the far right side of the plot in , what does it mean about that nation's consumption of Alcoholic Drinks?          Beverages is the second category so this would be the second component of the data_mean vector, which is .    The three nonzero eigenvalues are , , and .    The total variance is the sum of the eigenvalues so the first principal component accounts for of the total variance.    The coordinates are    Nation  Coordinate    England       Northern Ireland       Scotland       Wales           The first two principal components account for of the total variance.    The coordinates are    Nation  Coordinates    England       Northern Ireland       Scotland       England           Northern Ireland appears to be significantly different from the other three nations. There are several possible reasons for this, both historical and geographical, that we might explore.    The first component of is negative. Therefore, if a nation is on the right side of this plot, the average consumption of Alcoholic Drinks will be less than the mean. This can be confirmed by looking at the original data.                , , and          The coordinates are    Nation  Coordinate    England       Northern Ireland       Scotland       Wales                The coordinates are    Nation  Coordinates    England       Northern Ireland       Scotland       England           Northern Ireland appears to be significantly different from the other three nations.    The average consumption of Alcoholic Drinks will be less than the mean.      "
},
{
  "id": "activity-pca-iris",
  "level": "2",
  "url": "sec-pca.html#activity-pca-iris",
  "type": "Activity",
  "number": "23.3.5",
  "title": "",
  "body": "  In this activity, we'll look at a well-known dataset that describes 150 irises representing three species of iris: Iris setosa, Iris versicolor, and Iris virginica. For each flower, the length and width of its sepal and the length and width of its petal, all in centimeters, are recorded.      One of the three species, iris versicolor, represented in the dataset showing three shorter petals and three longer sepals. (Source: Wikipedia , License: GNU Free Documetation License )   Evaluating the following cell will load the dataset, which consists of 150 points in . In addition, we have a vector data_mean , a four-dimensional vector holding the mean of the data points, and A , the demeaned data matrix. Since the data is four-dimensional, we are not able to visualize it. Of course, we could forget about two of the measurements and plot the 150 points represented by their, say, sepal length and sepal width.    What is the mean sepal width?    Find the covariance matrix and its eigenvalues.     Find the fraction of variance for which the first two principal components account.    Construct the first two principal components and along with the matrix whose columns are and .     As we have seen, the columns of the matrix hold the coordinates of the demeaned data points after projecting onto , the subspace spanned by the first two principal components. Evaluating the following cell shows a plot of these coordinates. Suppose we have a flower whose coordinates in this plane are . To what species does this iris most likely belong? Find an estimate of the sepal length, sepal width, petal length, and petal width for this flower.    Suppose you have an iris, but you only know that its sepal length is 5.65 cm and its sepal width is 2.75 cm. Knowing only these two measurements, determine the coordinates in the plane where this iris lies. To what species does this iris most likely belong? Now estimate the petal length and petal width of this iris.     Suppose you find another iris whose sepal width is 3.2 cm and whose petal width is 2.2 cm. Find the coordinates of this iris and determine the species to which it most likely belongs. Also, estimate the sepal length and the petal length.           The second component of data_mean , which is the one corresponding to sepal width, is .    The eigenvalues are , , , and .    The first two principal components account for of the variance.    If is the matrix whose columns are an orthonormal basis of eigenvectors, then is formed from the first two columns of .    This would most likely belong to Iris setosa. To find its measurements, we evaluate where is the vector of means. This is the same as , which gives the vector of measurements .    Subtracting the mean sepal length and sepal width, we have . Then the first two components of . This gives . This looks like an Iris versicolor. As in the previous part, we can now find the petal length to be and the petal width to be .    Using the same approach as the last part, we find , which gives a sepal length of and a petal length of . Most likely, this flower belongs to Iris virginica.           .     , , , .         The columns of are for the first two principal components.    Iris setosa and the vector of measurements is .    The petal length is and the petal width is .    The sepal length is and the petal length is .      "
},
{
  "id": "sec-pca-6-1",
  "level": "2",
  "url": "sec-pca.html#sec-pca-6-1",
  "type": "Exercise",
  "number": "23.3.4.1",
  "title": "",
  "body": " Suppose that and that we have two datasets, one whose covariance matrix is and one whose covariance matrix is . For each dataset, find   the total variance.    the fraction of variance represented by the first principal component.    a verbal description of how the demeaned data points appear when plotted in the plane.          for the first dataset and for the second     and     The points in the second set are clustered very close to a line.         For the first dataset, the total variance is while the second dataset has a total variance of .    In the first dataset, the first principal component represents of the variance compared to in the second dataset.    The points in the first dataset are scattered almost uniformly around the origin. In the second dataset, they are clustered very close to the line defined by the vector .     "
},
{
  "id": "sec-pca-6-2",
  "level": "2",
  "url": "sec-pca.html#sec-pca-6-2",
  "type": "Exercise",
  "number": "23.3.4.2",
  "title": "",
  "body": " Suppose that a dataset has mean and that its associated covariance matrix is .    What fraction of the variance is represented by the first two principal components?    If is one of the data points, find the coordinates when the demeaned point is projected into the plane defined by the first two principal components.    If a projected data point has coordinates , find an estimate for the original data point.               .     .         The eigenvalues are , , and so the fraction of variance represented by the first two principal components is     Suppose is this data point and the vector of means. If is the matrix whose columns are the first two principal components, then we find .    We find .     "
},
{
  "id": "sec-pca-6-3",
  "level": "2",
  "url": "sec-pca.html#sec-pca-6-3",
  "type": "Exercise",
  "number": "23.3.4.3",
  "title": "",
  "body": " Evaluating the following cell loads a demeaned data matrix A .    Find the principal components and and the variance in the direction of each principal component.    What is the total variance?    What can you conclude about this dataset?         The variances are and .         All of the data lies on a line         We find that and . The variances are and .    The total variance is .    All of the data lies on the line defined by .     "
},
{
  "id": "sec-pca-6-4",
  "level": "2",
  "url": "sec-pca.html#sec-pca-6-4",
  "type": "Exercise",
  "number": "23.3.4.4",
  "title": "",
  "body": " Determine whether the following statements are true or false and explain your thinking.   If the eigenvalues of the covariance matrix are , , and , then is the variance of the demeaned data points when projected on the third principal component .    Principal component analysis always allows us to construct a smaller dimensional representation of a dataset without losing any information.    If the eigenvalues of the covariance matrix are 56, 32, and 0, then the demeaned data points all lie on a line in .         True    False    False         True. The variance of the projected data points is .    False. If we project onto a subspace that retains less than all the variance, then we are losing information about the data in the directions orthogonal to the subspace.    False. They will lie on the plane spanned by the first two principal components.     "
},
{
  "id": "sec-pca-6-5",
  "level": "2",
  "url": "sec-pca.html#sec-pca-6-5",
  "type": "Exercise",
  "number": "23.3.4.5",
  "title": "",
  "body": " In , we looked at a dataset consisting of four measurements of 150 irises. These measurements are sepal length, sepal width, petal length, and petal width.   Find the first principal component and describe the meaning of its four components. Which component is most significant? What can you say about the relative importance of the four measurements?    When the dataset is plotted in the plane defined by and , the specimens from the species iris-setosa lie on the left side of the plot. What does this tell us about how iris-setosa differs from the other two species in the four measurements?    In general, which species is closest to the average iris ?         The petal length is the most signficant and the sepal width is the least.    Iris setosa has a below average petal length and appears overall smaller.    Iris versicolor         The first principal component is . If we move in the direction, each component describes how we move away from the mean of that measurement. The most important component is the third one, which corresponds to petal length, while the second component, corresponding to sepal width, is least signficant. Of the four measurements, there is the most variance in petal length.    To reach an Iris setosa sample, we multiply by a negative number. This says that the petal length, the most signficant component of will be below the mean for Iris setosa. The sepal length and petal width will also be below the mean. Basically, the Iris setosa would appear to be smaller overall.    Iris versicolor would appear to be closest to average since the points are closest to the origin in the - plane.     "
},
{
  "id": "sec-pca-6-6",
  "level": "2",
  "url": "sec-pca.html#sec-pca-6-6",
  "type": "Exercise",
  "number": "23.3.4.6",
  "title": "",
  "body": " This problem explores a dataset describing 333 penguins. There are three species, Adelie, Chinstrap, and Gentoo, as illustrated on the left of , as well as both male and female penguins in the dataset.       Artwork by @allison_horst    Evaluating the next cell will load and display the data. The meaning of the culmen length and width is contained in the illustration on the right of . This dataset is a bit different from others that we've looked at because the scale of the measurements is significantly different. For instance, the measurements for the body mass are roughly 100 times as large as those for the culmen length. For this reason, we will standardize the data by first demeaning it, as usual, and then rescaling each measurement by the reciprocal of its standard deviation. The result is stored in the matrix A .    Find the covariance matrix and its eigenvalues.    What fraction of the total variance is explained by the first two principal components?    Construct the matrix whose columns are the coordinates of the demeaned data points projected onto the first two principal components. The following cell will create the plot.     Examine the components of the first two principal component vectors. How does the body mass of Gentoo penguins compare to that of the other two species?    What seems to be generally true about the culmen measurements for a Chinstrap penguin compared to a Adelie?    You can plot just the males or females using the following cell. What seems to be generally true about the body mass measurements for a male Gentoo compared to a female Gentoo?          , , , .          where the columns of are the first two principal components.    Gentoo penguins have a larger body mass.    These measurements seem larger for the Chinstrap penguins.    The male Gentoo have a higher body mass.         The eigenvalues are , , , and .    The first two principal components retain of the total variance.     where the columns of are the first two principal components.    Gentoo penguins have a larger body mass because the fourth component, which corresponds to body mass, of is negative and the Gentoo penguins are represented by where is negative.    These measurements seem larger for the Chinstrap penguins because the first two components of are positive.    The male Gentoo lie further to the left, which means they have a higher body mass.     "
},
{
  "id": "sec-svd-intro",
  "level": "1",
  "url": "sec-svd-intro.html",
  "type": "Section",
  "number": "23.4",
  "title": "Singular Value Decompositions",
  "body": " Singular Value Decompositions   The Spectral Theorem has motivated the past few sections. In particular, we applied the fact that symmetric matrices can be orthogonally diagonalized to simplify quadratic forms, which enabled us to use principal component analysis to reduce the dimension of a dataset.  But what can we do with matrices that are not symmetric or even square? For instance, the following matrices are not diagonalizable, much less orthogonally so: In this section, we will develop a description of matrices called the singular value decomposition that is, in many ways, analogous to an orthogonal diagonalization. For example, we have seen that any symmetric matrix can be written in the form where is an orthogonal matrix and is diagonal. A singular value decomposition will have the form where and are orthogonal and is diagonal. Most notably, we will see that every matrix has a singular value decomposition whether it's symmetric or not.    Let's review orthogonal diagonalizations and quadratic forms as our understanding of singular value decompositions will rely on them.   Suppose that is any matrix. Explain why the matrix is symmetric.    Suppose that . Find the matrix and write out the quadratic form as a function of and .    What is the maximum value of over all unit vectors and in which direction does it occur?     What is the minimum value of over all unit vectors and in which direction does it occur?    What is the geometric relationship between the directions in which the maximum and minimum values occur?           .     leads to the quadratic form .    The maximum value of equals the largest eigenvalue of , which is . This maximum value occurs in the direction of the associated eigenvector .    The minimum value of equals the smallest eigenvalue of , which is . This minimum value occurs in the direction of the associated eigenvector .    These two directions are orthogonal to each other.         Finding singular value decompositions  We will begin by explaining what a singular value decomposition is and how we can find one for a given matrix .  Recall how the orthogonal diagonalization of a symmetric matrix is formed: if is symmetric, we write where the diagonal entries of are the eigenvalues of and the columns of are the associated eigenvectors. Moreover, the eigenvalues are related to the maximum and minimum values of the associated quadratic form among all unit vectors.  A general matrix, particularly a matrix that is not square, may not have eigenvalues and eigenvectors, but we can discover analogous features, called singular values and singular vectors , by studying a function somewhat similar to a quadratic form. More specifically, any matrix defines a function which measures the length of . For example, the diagonal matrix gives the function . The presence of the square root means that this function is not a quadratic form. We can, however, define the singular values and vectors by looking for the maximum and minimum of this function among all unit vectors .  While is not itself a quadratic form, it becomes one if we square it:  Gram matrix We call , the Gram matrix associated to and note that This is important in the next activity, which introduces singular values and singular vectors.    The following interactive figure will help us explore singular values and vectors geometrically before we begin a more algebraic approach.   Singular values, right singular vectors and left singular vectors    Select the matrix . As we vary the vector , we see the vector on the right in gray while the height of the blue bar to the right tells us .     The first singular value  is the maximum value of over all unit vectors and an associated right singular vector  is a unit vector describing a direction in which this maximum occurs.  Use the diagram to find the first singular value and an associated right singular vector .    The second singular value is the minimum value of over all unit vectors and an associated right singular vector is a unit vector describing a direction in which this minimum occurs.  Use the diagram to find the second singular value and an associated right singular vector .    Here's how we can find the right singular values and vectors without using the diagram. Remember that where is the Gram matrix associated to . Since is symmetric, it is orthogonally diagonalizable. Find and an orthogonal diagonalization of it. What is the maximum value of the quadratic form among all unit vectors and in which direction does it occur? What is the minimum value of and in which direction does it occur?    Because , the first singular value will be the square root of the maximum value of and the square root of the minimum. Verify that the singular values that you found from the diagram are the square roots of the maximum and minimum values of .    Verify that the right singular vectors and that you found from the diagram are the directions in which the maximum and minimum values occur.    Finally, we introduce the left singular vectors  and by requiring that and . Find the two left singular vectors.     Form the matrices and explain why .    Finally, explain why and verify that this relationship holds for this specific example.          The maximum value of is , which occurs at .    The minimum value of is , which occurs at .     where The maximum value of is therefore , which occurs in the direction . The minimum value of is , which occurs in the direction .    We see that and .    We also see that , the first right singular vector, agrees with the direction in which has its maximum value. The corresponding fact is true for .    We find that and .     .    Since is an orthogonal matrix, we have .           ,      ,     The maximum value of is , which occurs in the direction . The minimum value of is , which occurs in the direction .     and      agrees with the direction in which has its maximum value. The corresponding fact is true for .     and      .    Since is an orthogonal matrix, we have .       As this activity shows, the singular values of are the maximum and minimum values of among all unit vectors and the right singular vectors and are the directions in which they occur. The key to finding the singular values and vectors is to utilize the Gram matrix and its associated quadratic form . We will illustrate with some more examples.    We will find a singular value decomposition of the matrix . Notice that this matrix is not symmetric so it cannot be orthogonally diagonalized.  We begin by constructing the Gram matrix . Since is symmetric, it can be orthogonally diagonalized with   We now know that the maximum value of the quadratic form is 8, which occurs in the direction . Since , this tells us that the maximum value of , the first singular value, is and that this occurs in the direction of the first right singular vector .  In the same way, we also know that the second singular value with associated right singular vector .  The first left singular vector is defined by . Because , we have . Notice that is a unit vector because .  In the same way, the second left singular vector is defined by , which gives us .  We then construct   We now have because Because the right singular vectors, the columns of , are eigenvectors of the symmetric matrix , they form an orthonormal basis, which means that is orthogonal. Therefore, we have . This gives the singular value decomposition     To summarize, we find a singular value decomposition of a matrix in the following way:   Construct the Gram matrix and find an orthogonal diagonalization to obtain eigenvalues and an orthonormal basis of eigenvectors.    The singular values of are the squares roots of eigenvalues of ; that is, . By convention, the singular values are listed in decreasing order: . The right singular vectors are the associated eigenvectors of .    The left singular vectors are found by . Because , we know that will be a unit vector.  In fact, the left singular vectors will also form an orthonormal basis. To see this, suppose that the associated singular values are nonzero. We then have: since the right singular vectors are orthogonal.       Let's find a singular value decomposition for the symmetric matrix . The associated Gram matrix is which has an orthogonal diagonalization with This gives singular values and vectors and the singular value decomposition where   This example is special because is symmetric. With a little thought, it's possible to relate this singular value decomposition to an orthogonal diagonalization of using the fact that .      In this activity, we will construct the singular value decomposition of . Notice that this matrix is not square so there are no eigenvalues and eigenvectors associated to it.   Construct the Gram matrix and find an orthogonal diagonalization of it.     Identify the singular values of and the right singular vectors , , and . What is the dimension of these vectors? How many nonzero singular values are there?    Find the left singular vectors and using the fact that . What is the dimension of these vectors? What happens if you try to find a third left singular vector in this way?    As before, form the orthogonal matrices and from the left and right singular vectors. What are the shapes of and ? How do these shapes relate to the number of rows and columns of ?    Now form so that it has the same shape as : and verify that .     How can you use this singular value decomposition of to easily find a singular value decomposition of ?          Constructing the Gram matrix of gives the matrix which can be orthogonally diagaonalized with     This tells us that , , and . The three right singular vectors are the columns of . Since these vectors are 3-dimensional, it follows that the matrix will be .    We have showing that Notice that so it is not possible to find a vector in this way.  The left singular vectors are 2-dimensional so will be a matrix.    We have The matrix is since there are two rows in and is since there are three columns in     With , we see that .    If , then .           can be orthogonally diagaonalized with      , , and . The three right singular vectors are the columns of .     and     We have     With , we see that .     .         We will find a singular value decomposition of the matrix .  Finding an orthogonal diagonalization of gives which gives singular values , , and . The right singular vectors appear as the columns of so that .  We now find Notice that it's not possible to find a third left singular vector since . We therefore form the matrices which gives the singular value decomposition .  Notice that is a orthogonal matrix because has two rows, and is a orthogonal matrix because has three columns.    As we'll see in the next section, some additional work may be needed to construct the left singular vectors if more of the singular values are zero, but we won't worry about that now. For the time being, let's record our work in the following theorem.   The singular value decomposition  An matrix may be written as where is an orthogonal matrix, is an orthogonal matrix, and is an matrix whose entries are zero except for the singular values of which appear in decreasing order on the diagonal.   Notice that a singular value decomposition of gives us a singular value decomposition of . More specifically, if , then     If , then . In other words, and share the same singular values, and the left singular vectors of are the right singular vectors of and vice-versa.    As we said earlier, a singular value decomposition should be thought of a generalization of an orthogonal diagonalization. For instance, the Spectral Theorem tells us that a symmetric matrix can be written as . Many matrices, however, are not symmetric and so they are not orthogonally diagonalizable. However, every matrix has a singular value decomposition . The price of this generalization is that we usually have two sets of singular vectors that form the orthogonal matrices and whereas a symmetric matrix has a single set of eignevectors that form the orthogonal matrix .    The structure of singular value decompositions  Now that we have an understanding of what a singular value decomposition is and how to construct it, let's explore the ways in which a singular value decomposition reveals the underlying structure of the matrix. As we'll see, the matrices and in a singular value decomposition provide convenient bases for some important subspaces, such as the column and null spaces of the matrix. This observation will provide the key to some of our uses of these decompositions in the next section.    Let's suppose that a matrix has a singular value decomposition where    What is the shape of ; that is, how many rows and columns does have?    Suppose we write a three-dimensional vector as a linear combination of right singular vectors: We would like to find an expression for .  To begin, .  Now .  And finally, .  To summarize, we have .  What condition on , , and must be satisfied if is a solution to the equation ? Is there a unique solution or infinitely many?    Remembering that and are linearly independent, what condition on , , and must be satisfied if ?    How do the right singular vectors provide a basis for , the subspace of solutions to the equation ?    Remember that is in if the equation is consistent, which means that for some coefficients and . How do the left singular vectors provide an orthonormal basis for ?    Remember that is the dimension of the column space. What is and how do the number of nonzero singular values determine ?          The shape of is the same as so is ; that is, has rows and columns.    We have . This means that , , and could be anything. Since there is no condition on , there are infinitely many solutions.    We have , which says that , , and could be anything.    Any vector in satisfies and must have the form . Therefore, forms a basis for .    The vector is in only if is a linear combination of and . Therefore, and form a basis for .     , which is the number of nonzero singular values.           is      , , and there is no condition on      , , and there is no condition on      forms a basis for .     and form a basis for .     , which is the number of nonzero singular values.       This activity shows how a singular value decomposition of a matrix encodes important information about its null and column spaces. More specifically, the left and right singular vectors provide orthonormal bases for and . This is one of the reasons that singular value decompositions are so useful.    Suppose we have a singular value decomposition where . This means that has four rows and five columns just as does.  As in the activity, if , we have   If is in , then must have the form which says that is a linear combination of , , and . These three vectors therefore form a basis for . In fact, since they are columns in the orthogonal matrix , they form an orthonormal basis for .  Remembering that , we see that , which results from the three nonzero singular values. In general, the rank of a matrix equals the number of nonzero singular values, and form an orthonormal basis for .  Moreover, if satisfies , then which implies that , , and . Therefore, so and form an orthonormal basis for .  More generally, if is an matrix and if , the last right singular vectors form an orthonormal basis for .    Generally speaking, if the rank of an matrix is , then there are nonzero singular values and has the form The first columns of form an orthonormal basis for : and the last columns of form an orthonormal basis for :   Remember that says that and its transpose share the same singular values. Since the rank of a matrix equals its number of nonzero singular values, this means that , a fact that we cited back in .    For any matrix ,     If we have a singular value decomposition of an matrix , also tells us that the left singular vectors of are the right singular vectors of . Therefore, is the matrix whose columns are the right singular vectors of . This means that the last vectors form an orthonormal basis for . Therefore, the columns of provide orthonormal bases for and : This reflects the familiar fact that is the orthogonal complement of .   row space In the same way, is the matrix whose columns are the left singular vectors of , which means that the first vectors form an orthonormal basis for . Because the columns of are the rows of , this subspace is sometimes called the row space of and denoted . While we have yet to have an occasion to use , there are times when it is important to have an orthonormal basis for it, and a singular value decomposition provides just that. To summarize, the columns of provide orthonormal bases for and :   Considered altogether, the subspaces , , , and are called the four fundamental subspaces associated to . In addition to telling us the rank of a matrix, a singular value decomposition gives us orthonormal bases for all four fundamental subspaces.    Suppose is an matrix having a singular value decomposition . Then    is the number of nonzero singular values.    The columns form an orthonormal basis for .    The columns form an orthonormal basis for .    The columns form an orthonormal basis for .    The columns form an orthonormal basis for .       When we previously outlined a procedure for finding a singular decomposition of an matrix , we found the left singular vectors using the expression . This produces left singular vectors , where . If , however, we still need to find the left singular vectors . tells us how to do that: because those vectors form an orthonormal basis for , we can find them by solving to obtain a basis for and applying the Gram-Schmidt algorithm.  We won't worry about this issue too much, however, as we will frequently use software to find singular value decompositions for us.    Reduced singular value decompositions  As we'll see in the next section, there are times when it is helpful to express a singular value decomposition in a slightly different form.    Suppose we have a singular value decomposition where    What is the shape of ? What is ?    Identify bases for and .    Explain why     Explain why     If , explain why where the columns of are an orthonormal basis for , is a square, diagonal, invertible matrix, and the columns of form an orthonormal basis for .           is a matrix and because there are two nonzero singular values.     and form an orthonormal basis for , and and form an orthonormal basis for .    Notice that     Notice that     Put together the previous parts to see that where            is a matrix and      and form an orthonormal basis for , and and form an orthonormal basis for .    Notice that     Notice that     Put together the previous parts to see that where        We call this a reduced singular value decomposition .   Reduced singular value decomposition   If is an matrix having rank , then where    is an matrix whose columns form an orthonormal basis for ,     is an diagonal, invertible matrix, and     is an matrix whose columns form an orthonormal basis for .         In , we found the singular value decomposition Since there are two nonzero singular values, so that the reduced singular value decomposition is       Summary  This section has explored singular value decompositions, how to find them, and how they organize important information about a matrix.   A singular value decomposition of a matrix is a factorization where . The matrix has the same shape as , and its only nonzero entries are the singular values of , which appear in decreasing order on the diagonal. The matrices and are orthogonal and contain the left and right singular vectors, respectively, as their columns.    To find a singular value decomposition of a matrix, we construct the Gram matrix , which is symmetric. The singular values of are the square roots of the eigenvalues of , and the right singular vectors are the associated eigenvectors of . The left singular vectors are determined from the relationship .    A singular value decomposition reveals fundamental information about a matrix. For instance, the number of nonzero singular values is the rank of the matrix. The first left singular vectors form an orthonormal basis for with the remaining left singular vectors forming an orthonormal basis of . The first right singular vectors form an orthonormal basis for while the remaining right singular vectors form an orthonormal basis of .    If is a rank matrix, we can write a reduced singular value decomposition as where the columns of form an orthonormal basis for , the columns of form an orthonormal basis for , and is an diagonal, invertible matrix.        Consider the matrix .    Find the Gram matrix and use it to find the singular values and right singular vectors of .    Find the left singular vectors.    Form the matrices , , and and verify that .    What is and what does this say about ?    Determine an orthonormal basis for .          , which has    and . The right singular vectors are the columns of .     and .          and .               , which has   This says that and . The right singular vectors are the columns of .     and .          because there are two nonzero singular values. Therefore, is 2-dimensional and so .            Find singular value decompositions for the following matrices:    .     .                                                                   Consider the matrix .   Find a singular value decomposition of and verify that it is also an orthogonal diagonalization of .    If is a symmetric, positive semidefinite matrix, explain why a singular value decomposition of is an orthogonal diagonalization of .              The singular values , the eigenvalues of and the right singular vectors are eigenvectors of .            Since , this is an orthogonal diagonalization.    In this case, . If are the eigenvalues of and the associated eigenvectors, then is an eigenvalue of with associated eigenvector . Therefore, and the right singular vectors are .  The left singular vectors are so the left singular vectors are the same as the right singular vectors.       Suppose that the matrix has the singular value decomposition    What are the dimensions of ?    What is ?    Find orthonormal bases for , , , and .    Find the orthogonal projection of onto .                   A basis for is formed by the first two columns of . A basis for is formed by the last two columns of . A basis for is formed by the first two columns of . A basis for is formed by the last column of .     .          is          A basis for is formed by the first two columns of . A basis for is formed by the last two columns of . A basis for is formed by the first two columns of . A basis for is formed by the last column of .    Form and find .       Consider the matrix .    Construct the Gram matrix and use it to find the singular values and right singular vectors , , and of . What are the matrices and in a singular value decomposition?    What is ?    Find as many left singular vectors as you can using the relationship .    Find an orthonormal basis for and use it to construct the matrix so that .    State an orthonormal basis for and an orthonormal basis for .          and          and         The third column of forms a basis for while the first two columns of form a basis for .          and          and     so that     The third column of forms a basis for while the first two columns of form a basis for .       Consider the matrix and notice that where is the matrix in .    Use your result from to find a singular value decomposition of .    What is ? Determine a basis for and .    Suppose that . Use the bases you found in the previous part of this exercise to write , where is in and is in .    Find the least-squares approximate solution to the equation .         From the previous problem, we have                .         From the previous problem, we have      . A basis for consists of the first two columns of and a basis for consists of the third column of .    If is the matrix consisting of the first two columns of , then . Then .    We solve the equation to find .       Suppose that is a square matrix with singular value decomposition .   If is invertible, find a singular value decomposition of .    What condition on the singular values must hold for to be invertible?    How are the singular values of and the singular values of related to one another?    How are the right and left singular vectors of related to the right and left singular vectors of ?              All the singular values are nonzero.    They are reciprocals of one another.    The left singular vectors of are the right singular vectors of and vice versa.              We need so all the singular values are nonzero. This means that the square matrix is invertible.    If are the singular values for , then are the singular values for .    The left singular vectors of are the right singular vectors of and vice versa.          If is an orthogonal matrix, remember that . Explain why .    If is a singular value decomposition of a square matrix , explain why is the product of the singular values of .    What does this say about the singular values of if is invertible?                   All the singular values are nonzero.          so .     , which equals the product of the singular values.    If is invertible, then , which says that all the singular values are nonzero.       If is a matrix and its Gram matrix, remember that    For a general matrix , explain why the eigenvalues of are nonnegative.    Given a symmetric matrix having an eigenvalue , explain why is an eigenvalue of .    If is symmetric, explain why the singular values of equal the absolute value of its eigenvalues: .         If is an eigenvector of with eigenvalue , then     If is symmetric, then .    If is an eigenvalue of , then is an eigenvalue of .         If is an eigenvector of with eigenvalue , then so .    If is symmetric, then . Then if , it follows that .    If is an eigenvalue of , then is an eigenvalue of . Then .       Determine whether the following statements are true or false and explain your reasoning.   If is a singular value decomposition of , then is an orthogonal diagonalization of its Gram matrix.    If is a singular value decomposition of a rank 2 matrix , then and form an orthonormal basis for the column space .    If is a symmetric matrix, then its set of singular values is the same as its set of eigenvalues.    If is a matrix and , then the columns of are linearly independent.    The Gram matrix is always orthogonally diagonalizable.         True    False    False    True    True         True. Remembering that is orthogonal, we have . Since is diagonal and is orthogonal, this is an orthogonal diagonalization.    False. They form a basis for .    False. The singular values are always nonnegative whereas the eigenvalues can take on any sign.    True. This would say that , which implies that the columns are linearly independent.    True. The Gram matrix is always symmetric and hence orthogonally diagonalizable.       Suppose that is a singular value decomposition of the matrix . If are the nonzero singular values, the general form of the matrix is    If you know that the columns of are linearly independent, what more can you say about the form of ?    If you know that the columns of span , what more can you say about the form of ?    If you know that the columns of are linearly independent and span , what more can you say about the form of ?         Every column of has a nonzero singular value.    Every row of has a nonzero singular value.    Then is a square matrix and every singular value is nonzero.         If the columns are linearly independent, then , which means that the rank equals the number of columns. Therefore, every column of has a nonzero singular value.    In the columns span , then , which says that the rank equals the number of rows. Therefore, every row of has a nonzero singular value.    Then is a square matrix and every singular value is nonzero.       "
},
{
  "id": "sec-svd-intro-2-3",
  "level": "2",
  "url": "sec-svd-intro.html#sec-svd-intro-2-3",
  "type": "Preview Activity",
  "number": "23.4.1",
  "title": "",
  "body": "  Let's review orthogonal diagonalizations and quadratic forms as our understanding of singular value decompositions will rely on them.   Suppose that is any matrix. Explain why the matrix is symmetric.    Suppose that . Find the matrix and write out the quadratic form as a function of and .    What is the maximum value of over all unit vectors and in which direction does it occur?     What is the minimum value of over all unit vectors and in which direction does it occur?    What is the geometric relationship between the directions in which the maximum and minimum values occur?           .     leads to the quadratic form .    The maximum value of equals the largest eigenvalue of , which is . This maximum value occurs in the direction of the associated eigenvector .    The minimum value of equals the smallest eigenvalue of , which is . This minimum value occurs in the direction of the associated eigenvector .    These two directions are orthogonal to each other.      "
},
{
  "id": "sec-svd-intro-3-6",
  "level": "2",
  "url": "sec-svd-intro.html#sec-svd-intro-3-6",
  "type": "Activity",
  "number": "23.4.2",
  "title": "",
  "body": "  The following interactive figure will help us explore singular values and vectors geometrically before we begin a more algebraic approach.   Singular values, right singular vectors and left singular vectors    Select the matrix . As we vary the vector , we see the vector on the right in gray while the height of the blue bar to the right tells us .     The first singular value  is the maximum value of over all unit vectors and an associated right singular vector  is a unit vector describing a direction in which this maximum occurs.  Use the diagram to find the first singular value and an associated right singular vector .    The second singular value is the minimum value of over all unit vectors and an associated right singular vector is a unit vector describing a direction in which this minimum occurs.  Use the diagram to find the second singular value and an associated right singular vector .    Here's how we can find the right singular values and vectors without using the diagram. Remember that where is the Gram matrix associated to . Since is symmetric, it is orthogonally diagonalizable. Find and an orthogonal diagonalization of it. What is the maximum value of the quadratic form among all unit vectors and in which direction does it occur? What is the minimum value of and in which direction does it occur?    Because , the first singular value will be the square root of the maximum value of and the square root of the minimum. Verify that the singular values that you found from the diagram are the square roots of the maximum and minimum values of .    Verify that the right singular vectors and that you found from the diagram are the directions in which the maximum and minimum values occur.    Finally, we introduce the left singular vectors  and by requiring that and . Find the two left singular vectors.     Form the matrices and explain why .    Finally, explain why and verify that this relationship holds for this specific example.          The maximum value of is , which occurs at .    The minimum value of is , which occurs at .     where The maximum value of is therefore , which occurs in the direction . The minimum value of is , which occurs in the direction .    We see that and .    We also see that , the first right singular vector, agrees with the direction in which has its maximum value. The corresponding fact is true for .    We find that and .     .    Since is an orthogonal matrix, we have .           ,      ,     The maximum value of is , which occurs in the direction . The minimum value of is , which occurs in the direction .     and      agrees with the direction in which has its maximum value. The corresponding fact is true for .     and      .    Since is an orthogonal matrix, we have .      "
},
{
  "id": "sec-svd-intro-3-8",
  "level": "2",
  "url": "sec-svd-intro.html#sec-svd-intro-3-8",
  "type": "Example",
  "number": "23.4.2",
  "title": "",
  "body": "  We will find a singular value decomposition of the matrix . Notice that this matrix is not symmetric so it cannot be orthogonally diagonalized.  We begin by constructing the Gram matrix . Since is symmetric, it can be orthogonally diagonalized with   We now know that the maximum value of the quadratic form is 8, which occurs in the direction . Since , this tells us that the maximum value of , the first singular value, is and that this occurs in the direction of the first right singular vector .  In the same way, we also know that the second singular value with associated right singular vector .  The first left singular vector is defined by . Because , we have . Notice that is a unit vector because .  In the same way, the second left singular vector is defined by , which gives us .  We then construct   We now have because Because the right singular vectors, the columns of , are eigenvectors of the symmetric matrix , they form an orthonormal basis, which means that is orthogonal. Therefore, we have . This gives the singular value decomposition    "
},
{
  "id": "sec-svd-intro-3-10",
  "level": "2",
  "url": "sec-svd-intro.html#sec-svd-intro-3-10",
  "type": "Example",
  "number": "23.4.3",
  "title": "",
  "body": "  Let's find a singular value decomposition for the symmetric matrix . The associated Gram matrix is which has an orthogonal diagonalization with This gives singular values and vectors and the singular value decomposition where   This example is special because is symmetric. With a little thought, it's possible to relate this singular value decomposition to an orthogonal diagonalization of using the fact that .   "
},
{
  "id": "sec-svd-intro-3-11",
  "level": "2",
  "url": "sec-svd-intro.html#sec-svd-intro-3-11",
  "type": "Activity",
  "number": "23.4.3",
  "title": "",
  "body": "  In this activity, we will construct the singular value decomposition of . Notice that this matrix is not square so there are no eigenvalues and eigenvectors associated to it.   Construct the Gram matrix and find an orthogonal diagonalization of it.     Identify the singular values of and the right singular vectors , , and . What is the dimension of these vectors? How many nonzero singular values are there?    Find the left singular vectors and using the fact that . What is the dimension of these vectors? What happens if you try to find a third left singular vector in this way?    As before, form the orthogonal matrices and from the left and right singular vectors. What are the shapes of and ? How do these shapes relate to the number of rows and columns of ?    Now form so that it has the same shape as : and verify that .     How can you use this singular value decomposition of to easily find a singular value decomposition of ?          Constructing the Gram matrix of gives the matrix which can be orthogonally diagaonalized with     This tells us that , , and . The three right singular vectors are the columns of . Since these vectors are 3-dimensional, it follows that the matrix will be .    We have showing that Notice that so it is not possible to find a vector in this way.  The left singular vectors are 2-dimensional so will be a matrix.    We have The matrix is since there are two rows in and is since there are three columns in     With , we see that .    If , then .           can be orthogonally diagaonalized with      , , and . The three right singular vectors are the columns of .     and     We have     With , we see that .     .      "
},
{
  "id": "example-svd-nonsquare",
  "level": "2",
  "url": "sec-svd-intro.html#example-svd-nonsquare",
  "type": "Example",
  "number": "23.4.4",
  "title": "",
  "body": "  We will find a singular value decomposition of the matrix .  Finding an orthogonal diagonalization of gives which gives singular values , , and . The right singular vectors appear as the columns of so that .  We now find Notice that it's not possible to find a third left singular vector since . We therefore form the matrices which gives the singular value decomposition .  Notice that is a orthogonal matrix because has two rows, and is a orthogonal matrix because has three columns.   "
},
{
  "id": "theorem-svd",
  "level": "2",
  "url": "sec-svd-intro.html#theorem-svd",
  "type": "Theorem",
  "number": "23.4.5",
  "title": "The singular value decomposition.",
  "body": " The singular value decomposition  An matrix may be written as where is an orthogonal matrix, is an orthogonal matrix, and is an matrix whose entries are zero except for the singular values of which appear in decreasing order on the diagonal.  "
},
{
  "id": "prop-svd-transpose",
  "level": "2",
  "url": "sec-svd-intro.html#prop-svd-transpose",
  "type": "Proposition",
  "number": "23.4.6",
  "title": "",
  "body": "  If , then . In other words, and share the same singular values, and the left singular vectors of are the right singular vectors of and vice-versa.   "
},
{
  "id": "sec-svd-intro-4-3",
  "level": "2",
  "url": "sec-svd-intro.html#sec-svd-intro-4-3",
  "type": "Activity",
  "number": "23.4.4",
  "title": "",
  "body": "  Let's suppose that a matrix has a singular value decomposition where    What is the shape of ; that is, how many rows and columns does have?    Suppose we write a three-dimensional vector as a linear combination of right singular vectors: We would like to find an expression for .  To begin, .  Now .  And finally, .  To summarize, we have .  What condition on , , and must be satisfied if is a solution to the equation ? Is there a unique solution or infinitely many?    Remembering that and are linearly independent, what condition on , , and must be satisfied if ?    How do the right singular vectors provide a basis for , the subspace of solutions to the equation ?    Remember that is in if the equation is consistent, which means that for some coefficients and . How do the left singular vectors provide an orthonormal basis for ?    Remember that is the dimension of the column space. What is and how do the number of nonzero singular values determine ?          The shape of is the same as so is ; that is, has rows and columns.    We have . This means that , , and could be anything. Since there is no condition on , there are infinitely many solutions.    We have , which says that , , and could be anything.    Any vector in satisfies and must have the form . Therefore, forms a basis for .    The vector is in only if is a linear combination of and . Therefore, and form a basis for .     , which is the number of nonzero singular values.           is      , , and there is no condition on      , , and there is no condition on      forms a basis for .     and form a basis for .     , which is the number of nonzero singular values.      "
},
{
  "id": "sec-svd-intro-4-5",
  "level": "2",
  "url": "sec-svd-intro.html#sec-svd-intro-4-5",
  "type": "Example",
  "number": "23.4.7",
  "title": "",
  "body": "  Suppose we have a singular value decomposition where . This means that has four rows and five columns just as does.  As in the activity, if , we have   If is in , then must have the form which says that is a linear combination of , , and . These three vectors therefore form a basis for . In fact, since they are columns in the orthogonal matrix , they form an orthonormal basis for .  Remembering that , we see that , which results from the three nonzero singular values. In general, the rank of a matrix equals the number of nonzero singular values, and form an orthonormal basis for .  Moreover, if satisfies , then which implies that , , and . Therefore, so and form an orthonormal basis for .  More generally, if is an matrix and if , the last right singular vectors form an orthonormal basis for .   "
},
{
  "id": "prop-rank-transpose",
  "level": "2",
  "url": "sec-svd-intro.html#prop-rank-transpose",
  "type": "Proposition",
  "number": "23.4.8",
  "title": "",
  "body": "  For any matrix ,    "
},
{
  "id": "thm-four-subspaces",
  "level": "2",
  "url": "sec-svd-intro.html#thm-four-subspaces",
  "type": "Theorem",
  "number": "23.4.9",
  "title": "",
  "body": "  Suppose is an matrix having a singular value decomposition . Then    is the number of nonzero singular values.    The columns form an orthonormal basis for .    The columns form an orthonormal basis for .    The columns form an orthonormal basis for .    The columns form an orthonormal basis for .      "
},
{
  "id": "sec-svd-intro-5-3",
  "level": "2",
  "url": "sec-svd-intro.html#sec-svd-intro-5-3",
  "type": "Activity",
  "number": "23.4.5",
  "title": "",
  "body": "  Suppose we have a singular value decomposition where    What is the shape of ? What is ?    Identify bases for and .    Explain why     Explain why     If , explain why where the columns of are an orthonormal basis for , is a square, diagonal, invertible matrix, and the columns of form an orthonormal basis for .           is a matrix and because there are two nonzero singular values.     and form an orthonormal basis for , and and form an orthonormal basis for .    Notice that     Notice that     Put together the previous parts to see that where            is a matrix and      and form an orthonormal basis for , and and form an orthonormal basis for .    Notice that     Notice that     Put together the previous parts to see that where       "
},
{
  "id": "prop-reduced-svd",
  "level": "2",
  "url": "sec-svd-intro.html#prop-reduced-svd",
  "type": "Proposition",
  "number": "23.4.10",
  "title": "Reduced singular value decomposition.",
  "body": " Reduced singular value decomposition   If is an matrix having rank , then where    is an matrix whose columns form an orthonormal basis for ,     is an diagonal, invertible matrix, and     is an matrix whose columns form an orthonormal basis for .      "
},
{
  "id": "sec-svd-intro-5-6",
  "level": "2",
  "url": "sec-svd-intro.html#sec-svd-intro-5-6",
  "type": "Example",
  "number": "23.4.11",
  "title": "",
  "body": "  In , we found the singular value decomposition Since there are two nonzero singular values, so that the reduced singular value decomposition is    "
},
{
  "id": "ex-7-4-1",
  "level": "2",
  "url": "sec-svd-intro.html#ex-7-4-1",
  "type": "Exercise",
  "number": "23.4.5.1",
  "title": "",
  "body": " Consider the matrix .    Find the Gram matrix and use it to find the singular values and right singular vectors of .    Find the left singular vectors.    Form the matrices , , and and verify that .    What is and what does this say about ?    Determine an orthonormal basis for .          , which has    and . The right singular vectors are the columns of .     and .          and .               , which has   This says that and . The right singular vectors are the columns of .     and .          because there are two nonzero singular values. Therefore, is 2-dimensional and so .          "
},
{
  "id": "sec-svd-intro-7-2",
  "level": "2",
  "url": "sec-svd-intro.html#sec-svd-intro-7-2",
  "type": "Exercise",
  "number": "23.4.5.2",
  "title": "",
  "body": " Find singular value decompositions for the following matrices:    .     .                                                                 "
},
{
  "id": "sec-svd-intro-7-3",
  "level": "2",
  "url": "sec-svd-intro.html#sec-svd-intro-7-3",
  "type": "Exercise",
  "number": "23.4.5.3",
  "title": "",
  "body": " Consider the matrix .   Find a singular value decomposition of and verify that it is also an orthogonal diagonalization of .    If is a symmetric, positive semidefinite matrix, explain why a singular value decomposition of is an orthogonal diagonalization of .              The singular values , the eigenvalues of and the right singular vectors are eigenvectors of .            Since , this is an orthogonal diagonalization.    In this case, . If are the eigenvalues of and the associated eigenvectors, then is an eigenvalue of with associated eigenvector . Therefore, and the right singular vectors are .  The left singular vectors are so the left singular vectors are the same as the right singular vectors.     "
},
{
  "id": "sec-svd-intro-7-4",
  "level": "2",
  "url": "sec-svd-intro.html#sec-svd-intro-7-4",
  "type": "Exercise",
  "number": "23.4.5.4",
  "title": "",
  "body": " Suppose that the matrix has the singular value decomposition    What are the dimensions of ?    What is ?    Find orthonormal bases for , , , and .    Find the orthogonal projection of onto .                   A basis for is formed by the first two columns of . A basis for is formed by the last two columns of . A basis for is formed by the first two columns of . A basis for is formed by the last column of .     .          is          A basis for is formed by the first two columns of . A basis for is formed by the last two columns of . A basis for is formed by the first two columns of . A basis for is formed by the last column of .    Form and find .     "
},
{
  "id": "sec-svd-intro-7-5",
  "level": "2",
  "url": "sec-svd-intro.html#sec-svd-intro-7-5",
  "type": "Exercise",
  "number": "23.4.5.5",
  "title": "",
  "body": " Consider the matrix .    Construct the Gram matrix and use it to find the singular values and right singular vectors , , and of . What are the matrices and in a singular value decomposition?    What is ?    Find as many left singular vectors as you can using the relationship .    Find an orthonormal basis for and use it to construct the matrix so that .    State an orthonormal basis for and an orthonormal basis for .          and          and         The third column of forms a basis for while the first two columns of form a basis for .          and          and     so that     The third column of forms a basis for while the first two columns of form a basis for .     "
},
{
  "id": "sec-svd-intro-7-6",
  "level": "2",
  "url": "sec-svd-intro.html#sec-svd-intro-7-6",
  "type": "Exercise",
  "number": "23.4.5.6",
  "title": "",
  "body": " Consider the matrix and notice that where is the matrix in .    Use your result from to find a singular value decomposition of .    What is ? Determine a basis for and .    Suppose that . Use the bases you found in the previous part of this exercise to write , where is in and is in .    Find the least-squares approximate solution to the equation .         From the previous problem, we have                .         From the previous problem, we have      . A basis for consists of the first two columns of and a basis for consists of the third column of .    If is the matrix consisting of the first two columns of , then . Then .    We solve the equation to find .     "
},
{
  "id": "sec-svd-intro-7-7",
  "level": "2",
  "url": "sec-svd-intro.html#sec-svd-intro-7-7",
  "type": "Exercise",
  "number": "23.4.5.7",
  "title": "",
  "body": " Suppose that is a square matrix with singular value decomposition .   If is invertible, find a singular value decomposition of .    What condition on the singular values must hold for to be invertible?    How are the singular values of and the singular values of related to one another?    How are the right and left singular vectors of related to the right and left singular vectors of ?              All the singular values are nonzero.    They are reciprocals of one another.    The left singular vectors of are the right singular vectors of and vice versa.              We need so all the singular values are nonzero. This means that the square matrix is invertible.    If are the singular values for , then are the singular values for .    The left singular vectors of are the right singular vectors of and vice versa.     "
},
{
  "id": "sec-svd-intro-7-8",
  "level": "2",
  "url": "sec-svd-intro.html#sec-svd-intro-7-8",
  "type": "Exercise",
  "number": "23.4.5.8",
  "title": "",
  "body": "    If is an orthogonal matrix, remember that . Explain why .    If is a singular value decomposition of a square matrix , explain why is the product of the singular values of .    What does this say about the singular values of if is invertible?                   All the singular values are nonzero.          so .     , which equals the product of the singular values.    If is invertible, then , which says that all the singular values are nonzero.     "
},
{
  "id": "sec-svd-intro-7-9",
  "level": "2",
  "url": "sec-svd-intro.html#sec-svd-intro-7-9",
  "type": "Exercise",
  "number": "23.4.5.9",
  "title": "",
  "body": " If is a matrix and its Gram matrix, remember that    For a general matrix , explain why the eigenvalues of are nonnegative.    Given a symmetric matrix having an eigenvalue , explain why is an eigenvalue of .    If is symmetric, explain why the singular values of equal the absolute value of its eigenvalues: .         If is an eigenvector of with eigenvalue , then     If is symmetric, then .    If is an eigenvalue of , then is an eigenvalue of .         If is an eigenvector of with eigenvalue , then so .    If is symmetric, then . Then if , it follows that .    If is an eigenvalue of , then is an eigenvalue of . Then .     "
},
{
  "id": "sec-svd-intro-7-10",
  "level": "2",
  "url": "sec-svd-intro.html#sec-svd-intro-7-10",
  "type": "Exercise",
  "number": "23.4.5.10",
  "title": "",
  "body": " Determine whether the following statements are true or false and explain your reasoning.   If is a singular value decomposition of , then is an orthogonal diagonalization of its Gram matrix.    If is a singular value decomposition of a rank 2 matrix , then and form an orthonormal basis for the column space .    If is a symmetric matrix, then its set of singular values is the same as its set of eigenvalues.    If is a matrix and , then the columns of are linearly independent.    The Gram matrix is always orthogonally diagonalizable.         True    False    False    True    True         True. Remembering that is orthogonal, we have . Since is diagonal and is orthogonal, this is an orthogonal diagonalization.    False. They form a basis for .    False. The singular values are always nonnegative whereas the eigenvalues can take on any sign.    True. This would say that , which implies that the columns are linearly independent.    True. The Gram matrix is always symmetric and hence orthogonally diagonalizable.     "
},
{
  "id": "sec-svd-intro-7-11",
  "level": "2",
  "url": "sec-svd-intro.html#sec-svd-intro-7-11",
  "type": "Exercise",
  "number": "23.4.5.11",
  "title": "",
  "body": " Suppose that is a singular value decomposition of the matrix . If are the nonzero singular values, the general form of the matrix is    If you know that the columns of are linearly independent, what more can you say about the form of ?    If you know that the columns of span , what more can you say about the form of ?    If you know that the columns of are linearly independent and span , what more can you say about the form of ?         Every column of has a nonzero singular value.    Every row of has a nonzero singular value.    Then is a square matrix and every singular value is nonzero.         If the columns are linearly independent, then , which means that the rank equals the number of columns. Therefore, every column of has a nonzero singular value.    In the columns span , then , which says that the rank equals the number of rows. Therefore, every row of has a nonzero singular value.    Then is a square matrix and every singular value is nonzero.     "
},
{
  "id": "sec-svd-uses",
  "level": "1",
  "url": "sec-svd-uses.html",
  "type": "Section",
  "number": "23.5",
  "title": "Using Singular Value Decompositions",
  "body": " Using Singular Value Decompositions   We've now seen what singular value decompositions are, how to construct them, and how they provide important information about a matrix such as orthonormal bases for the four fundamental subspaces. This puts us in a good position to begin using singular value decompositions to solve a wide variety of problems.  Given the fact that singular value decompositions so immediately convey fundamental data about a matrix, it seems natural that some of our previous work can be reinterpreted in terms of singular value decompositions. Therefore, we'll take some time in this section to revisit some familiar issues, such as least-squares problems and principal component analysis, while also looking at some new applications.    Suppose that where vectors form the columns of , and vectors form the columns of .   What are the shapes of the matrices , , and ?    What is the rank of ?    Describe how to find an orthonormal basis for .    Describe how to find an orthonormal basis for .    If the columns of form an orthonormal basis for , what is ?    How would you form a matrix that projects vectors orthogonally onto ?           is , is , and is .     since there are three nonzero singular values.    The first three columns, , , and , of form an orthonormal basis for .    The last column of is a basis for .     since each entry is a dot product of two vectors in an orthonormal set.     projects vectors orthogonally onto .         Least-squares problems  Least-squares problems, which we explored in , arise when we are confronted with an inconsistent linear system . Since there is no solution to the system, we instead find the vector minimizing the distance between and . That is, we find the vector , the least-squares approximate solution, by solving where is the orthogonal projection of onto the column space of .  If we have a singular value decomposition , then the number of nonzero singular values tells us the rank of , and the first columns of form an orthonormal basis for . This basis may be used to project vectors onto and hence to solve least-squares problems.  Before exploring this connection further, we will introduce Sage as a tool for automating the construction of singular value decompositions. One new feature is that we need to declare our matrix to consist of floating point entries. We do this by including RDF inside the matrix definition, as illustrated in the following cell.     Consider the equation where    Find a singular value decomposition for using the Sage cell below. What are singular values of ?     What is , the rank of ? How can we identify an orthonormal basis for ?    Form the reduced singular value decomposition by constructing: the matrix , consisting of the first columns of ; the matrix , consisting of the first columns of ; and , a square diagonal matrix. Verify that .  You may find it convenient to remember that if B is a matrix defined in Sage, then B.matrix_from_columns( list ) and B.matrix_from_rows( list ) can be used to extract columns or rows from B . For instance, B.matrix_from_rows([0,1,2]) provides a matrix formed from the first three rows of B .     How does the reduced singular value decomposition provide a matrix whose columns are an orthonormal basis for ?    Explain why a least-squares approximate solution satisfies     What is the product and why does it have this form?    Explain why is the least-squares approximate solution, and use this expression to find .           The singular values are and .    There are two nonzero singular values so . The first two columns of form an orthonormal basis for .    We have     The columns of form an orthonormal basis for .    Since , we obtain the equation .     since this product computes the dot products of the columns.    We have the equation , which gives            and .     and the first two columns of form an orthonormal basis for .    We have     The columns of form an orthonormal basis for .                      This activity demonstrates the power of a singular value decomposition to find a least-squares approximate solution for an equation . Because it immediately provides an orthonormal basis for , something that we've had to construct using the Gram-Schmidt process in the past, we can easily project onto , which results in a simple expression for .    If is a reduced singular value decomposition of , then a least-squares approximate solution to is given by     If the columns of are linearly independent, then the equation has only one solution so there is a unique least-squares approximate solution . Otherwise, the expression in produces the solution to having the shortest length.   Moore-Penrose psuedoinverse The matrix is known as the Moore-Penrose psuedoinverse of . When is invertible, .    Rank approximations  If we have a singular value decomposition for a matrix , we can form a sequence of matrices that approximate with increasing accuracy. This may feel familiar to calculus students who have seen the way in which a function can be approximated by a linear function, a quadratic function, and so forth with increasing accuracy.  We'll begin with a singular value decomposition of a rank matrix so that . To create the approximating matrix , we keep the first singular values and set the others to zero. For instance, if , we can form matrices and define and . Because has nonzero singular values, we know that . In fact, there is a sense in which is the closest matrix to among all rank matrices.    Let's consider a matrix where Evaluating the following cell will create the matrices U , V , and Sigma . Notice how the diagonal_matrix command provides a convenient way to form the diagonal matrix .    Form the matrix . What is ?     Now form the approximating matrix . What is ?     Find the error in the approximation by finding .    Now find and the error . What is ?     Find and the error . What is ?    What would happen if we were to compute ?    What do you notice about the error as increases?          We find that is the rank matrix: .    Because it has one nonzero singular value, has rank and has the form: .          is the rank matrix and .     is the rank matrix with .     since is a rank matrix.    As increases, the entries in the get closer to . This means that our approximations are improving.           .     .          and .     and .         The entries get closer to .       In this activity, the approximating matrix has rank because its singular value decomposition has nonzero singular values. We then saw how the difference between and the approximations decreases as increases, which means that the sequence forms better approximations as increases.  Another way to represent is with a reduced singular value decomposition so that where Notice that the rank matrix then has the form and that we can similarly write:   Given two vectors and , the matrix is called the outer product of and . (The dot product is sometimes called the inner product .) An outer product will always be a rank matrix so we see above how is obtained by adding together rank matrices, each of which gets us one step closer to the original matrix .    Principal component analysis  In , we explored principal component analysis as a technique to reduce the dimension of a dataset. In particular, we constructed the covariance matrix from a demeaned data matrix and saw that the eigenvalues and eigenvectors of tell us about the variance of the dataset in different directions. We referred to the eigenvectors of as principal components and found that projecting the data onto a subspace defined by the first few principal components frequently gave us a way to visualize the dataset. As we added more principal components, we retained more information about the original dataset. This feels similar to the rank approximations we have just seen so let's explore the connection.  Suppose that we have a dataset with points, that represents the demeaned data matrix, that is a singular value decomposition, and that the singular values are are denoted as . It follows that the covariance matrix Notice that is a diagonal matrix whose diagonal entries are . Therefore, it follows that is an orthogonal diagonalization of showing that   the principal components of the dataset, which are the eigenvectors of , are given by the columns of . In other words, the left singular vectors of are the principal components of the dataset.    the variance in the direction of a principal component is the associated eigenvalue of and therefore        Let's revisit the iris dataset that we studied in . Remember that there are four measurements given for each of 150 irises and that each iris belongs to one of three species.  Evaluating the following cell will load the dataset and define the demeaned data matrix whose shape is .    Find the singular values of using the command A.singular_values() and use them to determine the variance in the direction of each of the four principal components. What is the fraction of variance retained by the first two principal components?     We will now write the matrix so that . Suppose that a demeaned data point, say, the 100th column of , is written as a linear combination of principal components: Explain why , the vector of coordinates of in the basis of principal components, appears as 100th column of .    Suppose that we now project this demeaned data point orthogonally onto the subspace spanned by the first two principal components and . What are the coordinates of the projected point in this basis and how can we find them in the matrix ?    Alternatively, consider the approximation of the demeaned data matrix . Explain why the 100th column of represents the projection of onto the two-dimensional subspace spanned by the first two principal components, and . Then explain why the coefficients in that projection, , form the two-dimensional vector that is the 100th column of .    Now we've seen that the columns of form the coordinates of the demeaned data points projected on to the two-dimensional subspace spanned by and . In the cell below, find a singular value decomposition of and use it to form the matrix Gamma2 . When you evaluate this cell, you will see a plot of the projected demeaned data plots, similar to the one we created in .           The singular values are , , and . Since , the variances are , , , and . The fraction of the total variance represented by the first two principal components is .    If is the column of , then is the column of .    The projected data point is since is orthogonal to the subspace spanned by the first two principal components. Therefore, the coordinates of the projected data point are the first two components of the corresponding column of . The coordinates of all the projected data points are given by the first two rows of .    Once again, suppose that is a column of and that . Then is the corresponding column of .    We can construct or just pull out the first two rows of .          The fraction of the total variance represented by the first two principal components is .    If , then is the corresponding column of .    We just need the first two components of the corresponding column of .     is the corresponding column of .    We can construct or just pull out the first two rows of .       In our first encounter with principal component analysis, we began with a demeaned data matrix , formed the covariance matrix , and used the eigenvalues and eigenvectors of to project the demeaned data onto a smaller dimensional subspace. In this section, we have seen that a singular value decomposition of provides a more direct route: the left singular vectors of form the principal components and the approximating matrix represents the data points projected onto the subspace spanned by the first principal components. The coordinates of a projected demeaned data point are given by the columns of .    Image compressing and denoising  In addition to principal component analysis, the approximations of a matrix obtained from a singular value decomposition can be used in image processing. Remember that we studied the JPEG compression algorithm, whose foundation is the change of basis defined by the Discrete Cosine Transform, in . We will now see how a singular value decomposition provides another tool for both compressing images and removing noise in them.    Evaluating the following cell loads some data that we'll use in this activity. To begin, it defines and displays a matrix .    If we interpret 0 as black and 1 as white, this matrix represents an image as shown below. We will explore how the singular value decomposition helps us to compress this image.   By inspecting the image represented by , identify a basis for and determine .    The following cell plots the singular values of . Explain how this plot verifies that the rank is what you found in the previous part.     There is a command approximate(A, k) that creates the approximation . Use the cell below to define and look at the images represented by the first few approximations. What is the smallest value of for which ?     Now we can see how the singular value decomposition allows us to compress images. Since this is a matrix, we need numbers to represent the image. However, we can also reconstruct the image using a small number of singular values and vectors: What are the dimensions of the singular vectors and ? Between the singular vectors and singular values, how many numbers do we need to reconstruct for the smallest for which ? This is the compressed size of the image.    The compression ratio is the ratio of the uncompressed size to the compressed size. What compression ratio does this represent?       Next we'll explore an example based on a photograph.   Consider the following image consisting of an array of pixels stored in the matrix .   Plot the singular values of .     Use the cell below to study the approximations for . Notice how the approximating image more closely approximates the original image as increases.  What is the compression ratio when ? What is the compression ratio when ? Notice how a higher compression ratio leads to a lower quality reconstruction of the image.       A second, related application of the singular value decomposition to image processing is called denoising . For example, consider the image represented by the matrix below. This image is similar to the image of the letter \"O\" we first studied in this activity, but there are splotchy regions in the background that result, perhaps, from scanning the image. We think of the splotchy regions as noise, and our goal is to improve the quality of the image by reducing the noise.   Plot the singular values below. How are the singular values of this matrix similar to those represented by the clean image that we considered earlier and how are they different?     There is a natural point where the singular values dramatically decrease so it makes sense to think of the noise as being formed by the small singular values. To denoise the image, we will therefore replace by its approximation , where is the point at which the singular values drop off. This has the effect of setting the small singular values to zero and hence eliminating the noise. Choose an appropriate value of below and notice that the new image appears to be somewhat cleaned up as a result of removing the noise.                  because there are three distinct columns represented by the first, third, and sixth columns.    There are three nonzero singular values so as we suspected.    The smallest value is since .    The left singular vectors are -dimensional and the right singular vectors are -dimensional. If we keep three singular values, left singular vectors, and right singular vectors, we have .    The compression ratio is so the data is compressed by a factor of .          The singular values fall off steeply but never reach .    When , the compression ratio is about . When , the compression ratio is about .          The singular values are similar, but they never reach .     seems like a natural approximation since that's the place where the singular values become almost .                                    The compression ratio is           The singular values fall off steeply but never reach .    When , the compression ratio is about . When , the compression ratio is about .          The singular values are similar, but they never reach .     seems like a natural approximation          Several examples illustrating how the singular value decomposition compresses images are available at this page from Tim Baumann.     Analyzing Supreme Court cases  As we've seen, a singular value decomposition concentrates the most important features of a matrix into the first singular values and singular vectors. We will now use this observation to extract meaning from a large dataset giving the voting records of Supreme Court justices. A similar analysis appears in the paper A pattern analysis of the second Rehnquist U.S. Supreme Court by Lawrence Sirovich.  The makeup of the Supreme Court was unusually stable during a period from 1994-2005 when it was led by Chief Justice William Rehnquist. This is sometimes called the second Rehnquist court . The justices during this period were:  William Rehnquist  Antonin Scalia  Clarence Thomas  Anthony Kennedy  Sandra Day O'Connor  John Paul Stevens  David Souter  Ruth Bader Ginsburg  Stephen Breyer    During this time, there were 911 cases in which all nine judges voted. We would like to understand patterns in their voting.    Evaluating the following cell loads and displays a dataset describing the votes of each justice in these 911 cases. More specifically, an entry of +1 means that the justice represented by the row voted with the majority in the case represented by the column. An entry of -1 means that justice was in the minority. This information is also stored in the matrix . The justices are listed, very roughly, in order from more conservative to more progressive.  In this activity, it will be helpful to visualize the entries in various matrices and vectors. The next cell displays the first 50 columns of the matrix with white representing an entry of +1, red representing -1, and black representing 0.    Plot the singular values of below. Describe the significance of this plot, including the relative contributions from the singular values as increases.     Form the singular value decomposition and the matrix of coefficients so that .     We will now study a particular case, the second case which appears as the column of indexed by 1 . There is a command display_column(A, k) that provides a visual display of the column of a matrix . Describe the justices' votes in the second case.     Also, display the first left singular vector , the column of indexed by , and the column of holding the coefficients that express the second case as a linear combination of left singular vectors. What does this tell us about how the second case is constructed as a linear combination of left singular vectors? What is the significance of the first left singular vector ?    Let's now study the case, which is represented by the column of indexed by 47 . Describe the voting pattern in this case.     Display the second left singular vector and the vector of coefficients that express the case as a linear combination of left singular vectors. Describe how this case is constructed as a linear combination of singular vectors. What is the significance of the second left singular vector ?    The data in describes the number of cases decided by each possible vote count.  Number of cases by vote count    Vote count  # of cases    9-0  405    8-1  89    7-2  111    6-3  118    5-4  188    How do the singular vectors and reflect this data? Would you characterize the court as leaning toward the conservatives or progressives? Use these singular vectors to explain your response.    Cases decided by a 5-4 vote are often the most impactful as they represent a sharp divide among the justices and, often, society at large. For that reason, we will now focus on the 5-4 decisions. Evaluating the next cell forms the matrix consisting of 5-4 decisions. Form the singular value decomposition of along with the matrix of coefficients so that and display the first left singular vector . Study how the case, indexed by 6 , is constructed as a linear combination of left singular vectors. What does this singular vector tell us about the make up of the court and whether it leans towards the conservatives or progressives?    Display the second left singular vector and study how the case, indexed by 5 , is constructed as a linear combination of left singular vectors. What does tell us about the relative importance of the justices' voting records?    By a swing vote , we mean a justice who is less inclined to vote with a particular bloc of justices but instead swings from one bloc to another with the potential to sway close decisions. What do the singular vectors and tell us about the presence of voting blocs on the court and the presence of a swing vote? Which justice represents the swing vote?          The first two singular values contribute most significantly.     is a matrix that expresses the cases as linear combinations of the left singular vectors.    This is a unanimous decision.    The unanimous decision is essentially represented as so represents a unanimous decision.    This is a 5-4 decision with the 5 conservative justices voting in the majority.    This 5-4 decision is essentially represented as , the second most important left singular vector.    We see that the most decisions are unanimous, which is why represents unanimous decisions. The second most frequently occurring decisions is a 5-4 decision, which is why represents a 5-4 decision that leans to the conservative justices.    The first singular vector represents a case where the five conservative justices are voting together. From this we conclude that the court leans toward the conservatives.    The second left singular vector essentially records the vote of Sandra Day O'Connor and shows how her vote has the power to swing a 5-4 decision from a conservative majority to a progressive majority.    Sandra Day O'Connor would be the swing vote.          The first two singular values contribute most significantly.     is a matrix that expresses the cases as linear combinations of the left singular vectors.    This is a unanimous decision.     represents a unanimous decision.    This is a 5-4 decision with the 5 conservative justices voting in the majority.     represents a 5-4 decision.    The most frequently occurring decisions are unanimous and the second most frequently occurring are 5-4.     represents a case where the five conservative justices are voting together.    The second left singular vector essentially records the vote of Sandra Day O'Connor.    Sandra Day O'Connor         Summary  This section has demonstrated some uses of the singular value decomposition. Because the singular values appear in decreasing order, the decomposition has the effect of concentrating the most important features of the matrix into the first singular values and singular vectors.   Because the first left singular vectors form an orthonormal basis for , a singular value decomposition provides a convenient way to project vectors onto and therefore to solve least-squares problems.    A singular value decomposition of a rank matrix leads to a series of approximations of where In each case, is the rank matrix that is closest to .    If is a demeaned data matrix, the left singular vectors give the principal components of , and the variance in the direction of a principal component can be simply expressed in terms of the corresponding singular value.    The singular value decomposition has many applications. In this section, we looked at how the decomposition is used in image processing through the techniques of compression and denoising.    Because the first few left singular vectors contain the most important features of a matrix, we can use a singular value decomposition to extract meaning from a large dataset as we did when analyzing the voting patterns of the second Rehnquist court.        Suppose that     Find the singular values of . What is ?    Find the sequence of matrices , , , and where is the rank approximation of .          .     , ,\\   ,          The singular values are , , , and so we have .     , ,   ,        Suppose we would like to find the best quadratic function fitting the points     Set up a linear system describing the coefficients .    Find the singular value decomposition of .    Use the singular value decomposition to find the least-squares approximate solution .         The linear system is      is a matrix, is , and is               The linear system is      is a matrix, is , and is     The rank of is 3 so we'll form the reduced singular value decomposition . Then        Remember that the outer product of two vectors and is the matrix .   Suppose that and . Evaluate the outer product . To get a clearer sense of how this works, perform this operation without using technology.  How is each of the columns of related to ?    Suppose and are general vectors. What is and what is a basis for its column space ?    Suppose that is a unit vector. What is the effect of multiplying a vector by the matrix ?              The rank is and forms a basis.    This matrix projects vectors orthogonally onto the line defined by .              Since each column of is a scalar multiple of , the rank of the matrix is and forms a basis.    This matrix projects vectors orthogonally onto the line defined by .       Evaluating the following cell loads in a dataset recording some features of 1057 houses. Notice how the lot size varies over a relatively small range compared to the other features. For this reason, in addition to demeaning the data, we'll scale each feature by dividing by its standard deviation so that the range of values is similar for each feature. The matrix holds the result.    Find the singular values of and use them to determine the variance in the direction of the principal components.     For what fraction of the variance do the first two principal components account?    Find a singular value decomposition of and construct the matrix whose entries are the coordinates of the demeaned data points projected on to the two-dimensional subspace spanned by the first two principal components. You can plot the projected data points using list_plot(B.columns()) .     Study the entries in the first two principal components and . Would a more expensive house lie on the left, right, top, or bottom of the plot you constructed?    In what ways does a house that lies on the far left of the plot you constructed differ from an average house? In what ways does a house that lies near the top of the plot you constructed differ from an average house?         The variances are , , , and .         The matrix is formed from the first two rows of the product .    Left    Houses on the left have larger living area, lot sizes, and prices and are newer. Houses near the top have larger lot sizes and ages.         The singular values are , , , and . Therefore, the variances in the direction of the principal components are given by and are , , , and .    The first two principal components retain of the total variance.    The matrix is formed from the first two rows of the product .    The fourth components, the ones that correspond to house prices, of and are and respectively. This shows us that doesn't contribute much to the house price. Moving in the negative direction causes the house price to go up so the more expensive homes are on the left.    The first principal component is . If we move to the left, this vector is multiplied by a negative number so the living area, lot size, and price go up while the age of the house decreases.  The second principal component is . If we move up, this is multiplied by a positive number so the lot size and the age of the house increase.       Let's revisit the voting records of justices on the second Rehnquist court. Evaluating the following cell will load the voting records of the justices in the 188 cases decided by a 5-4 vote and store them in the matrix .    The cell above also defined the 188-dimensional vector whose entries are all 1. What does the product represent? Use the following cell to evaluate this product.     How does the product tell us which justice voted in the majority most frequently? What does this say about the presence of a swing vote on the court?    How does this product tell us whether we should characterize this court as leaning conservative or progressive?    How does this product tell us about the presence of a second swing vote on the court?    Study the left singular vector and describe how it reinforces the fact that there was a second swing vote. Who was this second swing vote?         For each justice, it tells us the number of times that justice voted with the majority minus the number of times they voted in the minority.    The justice that most often votes with the majority could be a swing vote.    The first five justices usually vote in the majority.    A second justice voted with the majority 84 times more than with the minority.    Anthony Kennedy          is a 9-dimensional vector. For each justice, it tells us the number of times that justice voted with the majority minus the number of times they voted in the minority.    Out of 188 cases, the fifth justice Sandra Day O'Connor voted with the majority 94 more times than with the minority. Since a swing vote will often vote in the majority, this gives additional evidence that Sandra Day O'Connor is a swing vote.    The first five justices more often vote with the majority and the last four more often with the minority. This means the court leans toward the first five justices, who are the more conservative ones.    The fourth justice Anthony Kennedy voted with the majority 84 times more than with the minority. He is a potential swing vote.    The third singular vector effectively records his vote and not the votes of the other justices. This lends support to the claim that Anthony Kennedy is a second, somewhat less influential, swing vote.       The following cell loads a dataset that describes the percentages with which justices on the second Rehnquist court agreed with one another. For instance, the entry in the first row and second column is 72.78, which means that Justices Rehnquist and Scalia agreed with each other in 72.78% of the cases.    Examine the matrix . What special structure does this matrix have and why should we expect it to have this structure?    Plot the singular values of below. For what value of would the approximation be a reasonable approximation of ?     Find a singular value decomposition and examine the matrices and using, for instance, n(U, 3) . What do you notice about the relationship between and and why should we expect this relationship to hold?     The command approximate(A, k) will form the approximating matrix . Study the matrix using the display_matrix command. Which justice or justices seem to be most agreeable, that is, most likely to agree with other justices? Which justice is least agreeable?     Examine the difference and describe how this tells us about the presence of voting blocs and swing votes on the court.          The matrix is symmetric.     should be a good approximation to .         John Paul Stevens, is the least agreeable.    The fourth and fifth justices, Anthony Kennedy and Sandra Day O'Connor are less inclined to agree with the others.         The matrix is symmetric because the number of times Justice A votes with Justice B is the same as the number of times Justice B votes with Justice A.    The first two singular values appear to be most important so should be a good approximation to .     since is a positive definite, symmetric matrix.    The sixth justice, John Paul Stevens, is the least agreeable.    The fourth and fifth justices, Anthony Kennedy and Sandra Day O'Connor are less inclined to agree with the others so they form swing votes.       Suppose that is a reduced singular value decomposition of the matrix . The matrix is called the Moore-Penrose inverse of .   Explain why is an matrix.    If is an invertible, square matrix, explain why .    Explain why , the orthogonal projection of onto .    Explain why , the orthogonal projection of onto .         Find the number of columns of and the number of rows of .    If is invertible, then is an matrix whose rank is .                   The number of columns of is the number of columns of , which is , and the number of rows is the number of rows of , which is .    If is invertible, then is an matrix whose rank is . The reduced singular value decomposition is therefore so that .     , which projects vectors orthogonal onto since the columns of are an orthonormal basis for .     , which projects vectors orthogonal onto since the columns of are an orthonormal basis for .       In , we saw how some linear algebraic computations are sensitive to round off error made by a computer. A singular value decomposition can help us understand when this situation can occur.  For instance, consider the matrices The entries in these matrices are quite close to one another, but is invertible while is not. It seems like is almost singular. In fact, we can measure how close a matrix is to being singular by forming the condition number , , the ratio of the largest to smallest singular value. If were singular, the condition number would be undefined because the singular value . Therefore, we will think of matrices with large condition numbers as being close to singular.   Define the matrix and find a singular value decomposition. What is the condition number of ?     Define the left singular vectors and . Compare the results when    .     .   Notice how a small change in the vector leads to a small change in .    Now compare the results when    .     .   Notice now how a small change in leads to a large change in .    Previously, we saw that, if we write in terms of left singular vectors , then we have If we write , explain why is sensitive to small changes in .   Generally speaking, a square matrix with a large condition number will demonstrate this type of behavior so that the computation of is likely to be affected by round off error. We call such a matrix ill-conditioned .      The condition number is                                               The singular values of are and , which means the condition number is .                                    We have so . Since is so much smaller than , even small changes in can lead to relatively large changes in .       "
},
{
  "id": "sec-svd-uses-2-3",
  "level": "2",
  "url": "sec-svd-uses.html#sec-svd-uses-2-3",
  "type": "Preview Activity",
  "number": "23.5.1",
  "title": "",
  "body": "  Suppose that where vectors form the columns of , and vectors form the columns of .   What are the shapes of the matrices , , and ?    What is the rank of ?    Describe how to find an orthonormal basis for .    Describe how to find an orthonormal basis for .    If the columns of form an orthonormal basis for , what is ?    How would you form a matrix that projects vectors orthogonally onto ?           is , is , and is .     since there are three nonzero singular values.    The first three columns, , , and , of form an orthonormal basis for .    The last column of is a basis for .     since each entry is a dot product of two vectors in an orthonormal set.     projects vectors orthogonally onto .      "
},
{
  "id": "sec-svd-uses-3-5",
  "level": "2",
  "url": "sec-svd-uses.html#sec-svd-uses-3-5",
  "type": "Activity",
  "number": "23.5.2",
  "title": "",
  "body": "  Consider the equation where    Find a singular value decomposition for using the Sage cell below. What are singular values of ?     What is , the rank of ? How can we identify an orthonormal basis for ?    Form the reduced singular value decomposition by constructing: the matrix , consisting of the first columns of ; the matrix , consisting of the first columns of ; and , a square diagonal matrix. Verify that .  You may find it convenient to remember that if B is a matrix defined in Sage, then B.matrix_from_columns( list ) and B.matrix_from_rows( list ) can be used to extract columns or rows from B . For instance, B.matrix_from_rows([0,1,2]) provides a matrix formed from the first three rows of B .     How does the reduced singular value decomposition provide a matrix whose columns are an orthonormal basis for ?    Explain why a least-squares approximate solution satisfies     What is the product and why does it have this form?    Explain why is the least-squares approximate solution, and use this expression to find .           The singular values are and .    There are two nonzero singular values so . The first two columns of form an orthonormal basis for .    We have     The columns of form an orthonormal basis for .    Since , we obtain the equation .     since this product computes the dot products of the columns.    We have the equation , which gives            and .     and the first two columns of form an orthonormal basis for .    We have     The columns of form an orthonormal basis for .                     "
},
{
  "id": "prop-svd-ols",
  "level": "2",
  "url": "sec-svd-uses.html#prop-svd-ols",
  "type": "Proposition",
  "number": "23.5.1",
  "title": "",
  "body": "  If is a reduced singular value decomposition of , then a least-squares approximate solution to is given by    "
},
{
  "id": "sec-svd-uses-4-4",
  "level": "2",
  "url": "sec-svd-uses.html#sec-svd-uses-4-4",
  "type": "Activity",
  "number": "23.5.3",
  "title": "",
  "body": "  Let's consider a matrix where Evaluating the following cell will create the matrices U , V , and Sigma . Notice how the diagonal_matrix command provides a convenient way to form the diagonal matrix .    Form the matrix . What is ?     Now form the approximating matrix . What is ?     Find the error in the approximation by finding .    Now find and the error . What is ?     Find and the error . What is ?    What would happen if we were to compute ?    What do you notice about the error as increases?          We find that is the rank matrix: .    Because it has one nonzero singular value, has rank and has the form: .          is the rank matrix and .     is the rank matrix with .     since is a rank matrix.    As increases, the entries in the get closer to . This means that our approximations are improving.           .     .          and .     and .         The entries get closer to .      "
},
{
  "id": "sec-svd-uses-5-4",
  "level": "2",
  "url": "sec-svd-uses.html#sec-svd-uses-5-4",
  "type": "Activity",
  "number": "23.5.4",
  "title": "",
  "body": "  Let's revisit the iris dataset that we studied in . Remember that there are four measurements given for each of 150 irises and that each iris belongs to one of three species.  Evaluating the following cell will load the dataset and define the demeaned data matrix whose shape is .    Find the singular values of using the command A.singular_values() and use them to determine the variance in the direction of each of the four principal components. What is the fraction of variance retained by the first two principal components?     We will now write the matrix so that . Suppose that a demeaned data point, say, the 100th column of , is written as a linear combination of principal components: Explain why , the vector of coordinates of in the basis of principal components, appears as 100th column of .    Suppose that we now project this demeaned data point orthogonally onto the subspace spanned by the first two principal components and . What are the coordinates of the projected point in this basis and how can we find them in the matrix ?    Alternatively, consider the approximation of the demeaned data matrix . Explain why the 100th column of represents the projection of onto the two-dimensional subspace spanned by the first two principal components, and . Then explain why the coefficients in that projection, , form the two-dimensional vector that is the 100th column of .    Now we've seen that the columns of form the coordinates of the demeaned data points projected on to the two-dimensional subspace spanned by and . In the cell below, find a singular value decomposition of and use it to form the matrix Gamma2 . When you evaluate this cell, you will see a plot of the projected demeaned data plots, similar to the one we created in .           The singular values are , , and . Since , the variances are , , , and . The fraction of the total variance represented by the first two principal components is .    If is the column of , then is the column of .    The projected data point is since is orthogonal to the subspace spanned by the first two principal components. Therefore, the coordinates of the projected data point are the first two components of the corresponding column of . The coordinates of all the projected data points are given by the first two rows of .    Once again, suppose that is a column of and that . Then is the corresponding column of .    We can construct or just pull out the first two rows of .          The fraction of the total variance represented by the first two principal components is .    If , then is the corresponding column of .    We just need the first two components of the corresponding column of .     is the corresponding column of .    We can construct or just pull out the first two rows of .      "
},
{
  "id": "sec-svd-uses-6-3",
  "level": "2",
  "url": "sec-svd-uses.html#sec-svd-uses-6-3",
  "type": "Activity",
  "number": "23.5.5",
  "title": "",
  "body": "  Evaluating the following cell loads some data that we'll use in this activity. To begin, it defines and displays a matrix .    If we interpret 0 as black and 1 as white, this matrix represents an image as shown below. We will explore how the singular value decomposition helps us to compress this image.   By inspecting the image represented by , identify a basis for and determine .    The following cell plots the singular values of . Explain how this plot verifies that the rank is what you found in the previous part.     There is a command approximate(A, k) that creates the approximation . Use the cell below to define and look at the images represented by the first few approximations. What is the smallest value of for which ?     Now we can see how the singular value decomposition allows us to compress images. Since this is a matrix, we need numbers to represent the image. However, we can also reconstruct the image using a small number of singular values and vectors: What are the dimensions of the singular vectors and ? Between the singular vectors and singular values, how many numbers do we need to reconstruct for the smallest for which ? This is the compressed size of the image.    The compression ratio is the ratio of the uncompressed size to the compressed size. What compression ratio does this represent?       Next we'll explore an example based on a photograph.   Consider the following image consisting of an array of pixels stored in the matrix .   Plot the singular values of .     Use the cell below to study the approximations for . Notice how the approximating image more closely approximates the original image as increases.  What is the compression ratio when ? What is the compression ratio when ? Notice how a higher compression ratio leads to a lower quality reconstruction of the image.       A second, related application of the singular value decomposition to image processing is called denoising . For example, consider the image represented by the matrix below. This image is similar to the image of the letter \"O\" we first studied in this activity, but there are splotchy regions in the background that result, perhaps, from scanning the image. We think of the splotchy regions as noise, and our goal is to improve the quality of the image by reducing the noise.   Plot the singular values below. How are the singular values of this matrix similar to those represented by the clean image that we considered earlier and how are they different?     There is a natural point where the singular values dramatically decrease so it makes sense to think of the noise as being formed by the small singular values. To denoise the image, we will therefore replace by its approximation , where is the point at which the singular values drop off. This has the effect of setting the small singular values to zero and hence eliminating the noise. Choose an appropriate value of below and notice that the new image appears to be somewhat cleaned up as a result of removing the noise.                  because there are three distinct columns represented by the first, third, and sixth columns.    There are three nonzero singular values so as we suspected.    The smallest value is since .    The left singular vectors are -dimensional and the right singular vectors are -dimensional. If we keep three singular values, left singular vectors, and right singular vectors, we have .    The compression ratio is so the data is compressed by a factor of .          The singular values fall off steeply but never reach .    When , the compression ratio is about . When , the compression ratio is about .          The singular values are similar, but they never reach .     seems like a natural approximation since that's the place where the singular values become almost .                                    The compression ratio is           The singular values fall off steeply but never reach .    When , the compression ratio is about . When , the compression ratio is about .          The singular values are similar, but they never reach .     seems like a natural approximation         "
},
{
  "id": "sec-svd-uses-7-5",
  "level": "2",
  "url": "sec-svd-uses.html#sec-svd-uses-7-5",
  "type": "Activity",
  "number": "23.5.6",
  "title": "",
  "body": "  Evaluating the following cell loads and displays a dataset describing the votes of each justice in these 911 cases. More specifically, an entry of +1 means that the justice represented by the row voted with the majority in the case represented by the column. An entry of -1 means that justice was in the minority. This information is also stored in the matrix . The justices are listed, very roughly, in order from more conservative to more progressive.  In this activity, it will be helpful to visualize the entries in various matrices and vectors. The next cell displays the first 50 columns of the matrix with white representing an entry of +1, red representing -1, and black representing 0.    Plot the singular values of below. Describe the significance of this plot, including the relative contributions from the singular values as increases.     Form the singular value decomposition and the matrix of coefficients so that .     We will now study a particular case, the second case which appears as the column of indexed by 1 . There is a command display_column(A, k) that provides a visual display of the column of a matrix . Describe the justices' votes in the second case.     Also, display the first left singular vector , the column of indexed by , and the column of holding the coefficients that express the second case as a linear combination of left singular vectors. What does this tell us about how the second case is constructed as a linear combination of left singular vectors? What is the significance of the first left singular vector ?    Let's now study the case, which is represented by the column of indexed by 47 . Describe the voting pattern in this case.     Display the second left singular vector and the vector of coefficients that express the case as a linear combination of left singular vectors. Describe how this case is constructed as a linear combination of singular vectors. What is the significance of the second left singular vector ?    The data in describes the number of cases decided by each possible vote count.  Number of cases by vote count    Vote count  # of cases    9-0  405    8-1  89    7-2  111    6-3  118    5-4  188    How do the singular vectors and reflect this data? Would you characterize the court as leaning toward the conservatives or progressives? Use these singular vectors to explain your response.    Cases decided by a 5-4 vote are often the most impactful as they represent a sharp divide among the justices and, often, society at large. For that reason, we will now focus on the 5-4 decisions. Evaluating the next cell forms the matrix consisting of 5-4 decisions. Form the singular value decomposition of along with the matrix of coefficients so that and display the first left singular vector . Study how the case, indexed by 6 , is constructed as a linear combination of left singular vectors. What does this singular vector tell us about the make up of the court and whether it leans towards the conservatives or progressives?    Display the second left singular vector and study how the case, indexed by 5 , is constructed as a linear combination of left singular vectors. What does tell us about the relative importance of the justices' voting records?    By a swing vote , we mean a justice who is less inclined to vote with a particular bloc of justices but instead swings from one bloc to another with the potential to sway close decisions. What do the singular vectors and tell us about the presence of voting blocs on the court and the presence of a swing vote? Which justice represents the swing vote?          The first two singular values contribute most significantly.     is a matrix that expresses the cases as linear combinations of the left singular vectors.    This is a unanimous decision.    The unanimous decision is essentially represented as so represents a unanimous decision.    This is a 5-4 decision with the 5 conservative justices voting in the majority.    This 5-4 decision is essentially represented as , the second most important left singular vector.    We see that the most decisions are unanimous, which is why represents unanimous decisions. The second most frequently occurring decisions is a 5-4 decision, which is why represents a 5-4 decision that leans to the conservative justices.    The first singular vector represents a case where the five conservative justices are voting together. From this we conclude that the court leans toward the conservatives.    The second left singular vector essentially records the vote of Sandra Day O'Connor and shows how her vote has the power to swing a 5-4 decision from a conservative majority to a progressive majority.    Sandra Day O'Connor would be the swing vote.          The first two singular values contribute most significantly.     is a matrix that expresses the cases as linear combinations of the left singular vectors.    This is a unanimous decision.     represents a unanimous decision.    This is a 5-4 decision with the 5 conservative justices voting in the majority.     represents a 5-4 decision.    The most frequently occurring decisions are unanimous and the second most frequently occurring are 5-4.     represents a case where the five conservative justices are voting together.    The second left singular vector essentially records the vote of Sandra Day O'Connor.    Sandra Day O'Connor      "
},
{
  "id": "sec-svd-uses-9-1",
  "level": "2",
  "url": "sec-svd-uses.html#sec-svd-uses-9-1",
  "type": "Exercise",
  "number": "23.5.7.1",
  "title": "",
  "body": " Suppose that     Find the singular values of . What is ?    Find the sequence of matrices , , , and where is the rank approximation of .          .     , ,\\   ,          The singular values are , , , and so we have .     , ,   ,      "
},
{
  "id": "sec-svd-uses-9-2",
  "level": "2",
  "url": "sec-svd-uses.html#sec-svd-uses-9-2",
  "type": "Exercise",
  "number": "23.5.7.2",
  "title": "",
  "body": " Suppose we would like to find the best quadratic function fitting the points     Set up a linear system describing the coefficients .    Find the singular value decomposition of .    Use the singular value decomposition to find the least-squares approximate solution .         The linear system is      is a matrix, is , and is               The linear system is      is a matrix, is , and is     The rank of is 3 so we'll form the reduced singular value decomposition . Then      "
},
{
  "id": "sec-svd-uses-9-3",
  "level": "2",
  "url": "sec-svd-uses.html#sec-svd-uses-9-3",
  "type": "Exercise",
  "number": "23.5.7.3",
  "title": "",
  "body": " Remember that the outer product of two vectors and is the matrix .   Suppose that and . Evaluate the outer product . To get a clearer sense of how this works, perform this operation without using technology.  How is each of the columns of related to ?    Suppose and are general vectors. What is and what is a basis for its column space ?    Suppose that is a unit vector. What is the effect of multiplying a vector by the matrix ?              The rank is and forms a basis.    This matrix projects vectors orthogonally onto the line defined by .              Since each column of is a scalar multiple of , the rank of the matrix is and forms a basis.    This matrix projects vectors orthogonally onto the line defined by .     "
},
{
  "id": "sec-svd-uses-9-4",
  "level": "2",
  "url": "sec-svd-uses.html#sec-svd-uses-9-4",
  "type": "Exercise",
  "number": "23.5.7.4",
  "title": "",
  "body": " Evaluating the following cell loads in a dataset recording some features of 1057 houses. Notice how the lot size varies over a relatively small range compared to the other features. For this reason, in addition to demeaning the data, we'll scale each feature by dividing by its standard deviation so that the range of values is similar for each feature. The matrix holds the result.    Find the singular values of and use them to determine the variance in the direction of the principal components.     For what fraction of the variance do the first two principal components account?    Find a singular value decomposition of and construct the matrix whose entries are the coordinates of the demeaned data points projected on to the two-dimensional subspace spanned by the first two principal components. You can plot the projected data points using list_plot(B.columns()) .     Study the entries in the first two principal components and . Would a more expensive house lie on the left, right, top, or bottom of the plot you constructed?    In what ways does a house that lies on the far left of the plot you constructed differ from an average house? In what ways does a house that lies near the top of the plot you constructed differ from an average house?         The variances are , , , and .         The matrix is formed from the first two rows of the product .    Left    Houses on the left have larger living area, lot sizes, and prices and are newer. Houses near the top have larger lot sizes and ages.         The singular values are , , , and . Therefore, the variances in the direction of the principal components are given by and are , , , and .    The first two principal components retain of the total variance.    The matrix is formed from the first two rows of the product .    The fourth components, the ones that correspond to house prices, of and are and respectively. This shows us that doesn't contribute much to the house price. Moving in the negative direction causes the house price to go up so the more expensive homes are on the left.    The first principal component is . If we move to the left, this vector is multiplied by a negative number so the living area, lot size, and price go up while the age of the house decreases.  The second principal component is . If we move up, this is multiplied by a positive number so the lot size and the age of the house increase.     "
},
{
  "id": "sec-svd-uses-9-5",
  "level": "2",
  "url": "sec-svd-uses.html#sec-svd-uses-9-5",
  "type": "Exercise",
  "number": "23.5.7.5",
  "title": "",
  "body": " Let's revisit the voting records of justices on the second Rehnquist court. Evaluating the following cell will load the voting records of the justices in the 188 cases decided by a 5-4 vote and store them in the matrix .    The cell above also defined the 188-dimensional vector whose entries are all 1. What does the product represent? Use the following cell to evaluate this product.     How does the product tell us which justice voted in the majority most frequently? What does this say about the presence of a swing vote on the court?    How does this product tell us whether we should characterize this court as leaning conservative or progressive?    How does this product tell us about the presence of a second swing vote on the court?    Study the left singular vector and describe how it reinforces the fact that there was a second swing vote. Who was this second swing vote?         For each justice, it tells us the number of times that justice voted with the majority minus the number of times they voted in the minority.    The justice that most often votes with the majority could be a swing vote.    The first five justices usually vote in the majority.    A second justice voted with the majority 84 times more than with the minority.    Anthony Kennedy          is a 9-dimensional vector. For each justice, it tells us the number of times that justice voted with the majority minus the number of times they voted in the minority.    Out of 188 cases, the fifth justice Sandra Day O'Connor voted with the majority 94 more times than with the minority. Since a swing vote will often vote in the majority, this gives additional evidence that Sandra Day O'Connor is a swing vote.    The first five justices more often vote with the majority and the last four more often with the minority. This means the court leans toward the first five justices, who are the more conservative ones.    The fourth justice Anthony Kennedy voted with the majority 84 times more than with the minority. He is a potential swing vote.    The third singular vector effectively records his vote and not the votes of the other justices. This lends support to the claim that Anthony Kennedy is a second, somewhat less influential, swing vote.     "
},
{
  "id": "sec-svd-uses-9-6",
  "level": "2",
  "url": "sec-svd-uses.html#sec-svd-uses-9-6",
  "type": "Exercise",
  "number": "23.5.7.6",
  "title": "",
  "body": " The following cell loads a dataset that describes the percentages with which justices on the second Rehnquist court agreed with one another. For instance, the entry in the first row and second column is 72.78, which means that Justices Rehnquist and Scalia agreed with each other in 72.78% of the cases.    Examine the matrix . What special structure does this matrix have and why should we expect it to have this structure?    Plot the singular values of below. For what value of would the approximation be a reasonable approximation of ?     Find a singular value decomposition and examine the matrices and using, for instance, n(U, 3) . What do you notice about the relationship between and and why should we expect this relationship to hold?     The command approximate(A, k) will form the approximating matrix . Study the matrix using the display_matrix command. Which justice or justices seem to be most agreeable, that is, most likely to agree with other justices? Which justice is least agreeable?     Examine the difference and describe how this tells us about the presence of voting blocs and swing votes on the court.          The matrix is symmetric.     should be a good approximation to .         John Paul Stevens, is the least agreeable.    The fourth and fifth justices, Anthony Kennedy and Sandra Day O'Connor are less inclined to agree with the others.         The matrix is symmetric because the number of times Justice A votes with Justice B is the same as the number of times Justice B votes with Justice A.    The first two singular values appear to be most important so should be a good approximation to .     since is a positive definite, symmetric matrix.    The sixth justice, John Paul Stevens, is the least agreeable.    The fourth and fifth justices, Anthony Kennedy and Sandra Day O'Connor are less inclined to agree with the others so they form swing votes.     "
},
{
  "id": "sec-svd-uses-9-7",
  "level": "2",
  "url": "sec-svd-uses.html#sec-svd-uses-9-7",
  "type": "Exercise",
  "number": "23.5.7.7",
  "title": "",
  "body": " Suppose that is a reduced singular value decomposition of the matrix . The matrix is called the Moore-Penrose inverse of .   Explain why is an matrix.    If is an invertible, square matrix, explain why .    Explain why , the orthogonal projection of onto .    Explain why , the orthogonal projection of onto .         Find the number of columns of and the number of rows of .    If is invertible, then is an matrix whose rank is .                   The number of columns of is the number of columns of , which is , and the number of rows is the number of rows of , which is .    If is invertible, then is an matrix whose rank is . The reduced singular value decomposition is therefore so that .     , which projects vectors orthogonal onto since the columns of are an orthonormal basis for .     , which projects vectors orthogonal onto since the columns of are an orthonormal basis for .     "
},
{
  "id": "sec-svd-uses-9-8",
  "level": "2",
  "url": "sec-svd-uses.html#sec-svd-uses-9-8",
  "type": "Exercise",
  "number": "23.5.7.8",
  "title": "",
  "body": " In , we saw how some linear algebraic computations are sensitive to round off error made by a computer. A singular value decomposition can help us understand when this situation can occur.  For instance, consider the matrices The entries in these matrices are quite close to one another, but is invertible while is not. It seems like is almost singular. In fact, we can measure how close a matrix is to being singular by forming the condition number , , the ratio of the largest to smallest singular value. If were singular, the condition number would be undefined because the singular value . Therefore, we will think of matrices with large condition numbers as being close to singular.   Define the matrix and find a singular value decomposition. What is the condition number of ?     Define the left singular vectors and . Compare the results when    .     .   Notice how a small change in the vector leads to a small change in .    Now compare the results when    .     .   Notice now how a small change in leads to a large change in .    Previously, we saw that, if we write in terms of left singular vectors , then we have If we write , explain why is sensitive to small changes in .   Generally speaking, a square matrix with a large condition number will demonstrate this type of behavior so that the computation of is likely to be affected by round off error. We call such a matrix ill-conditioned .      The condition number is                                               The singular values of are and , which means the condition number is .                                    We have so . Since is so much smaller than , even small changes in can lead to relatively large changes in .     "
},
{
  "id": "app-alg1",
  "level": "1",
  "url": "app-alg1.html",
  "type": "Section",
  "number": "A.1",
  "title": "An Introduction to Algorithms",
  "body": "An Introduction to Algorithms  Most of the algorithms in this book will contain a combination of three kinds of steps: the assignment step, the conditional step, and the loop.  Assignments In order to assign a value to a variable, we use an assignment step, which takes the form: The equals sign in most languages is used for assignment but some languages may use variations such as := or a left pointing arrow. Logical equality, which produces a boolean result and would be used in conditional or looping steps, is most commonly expressed with a double-equals, == .  An example of an assignment is k = n - 1 which tells us to subtract 1 from the value of n and assign that value to variable k . During the execution of an algorithm, a variable may take on only one value at a time. Another example of an assignment is k = k - 1 . This is an instruction to subtract one from the value of k and then reassign that value to k .   Conditional steps  Frequently there are steps that must be performed in an algorithm if and only if a certain condition is met. The conditional or \"if ... then\" step is then employed. For example, suppose that in step 2 of an algorithm we want to assure that the values of variables x and y satisfy the condition x <= y . The following step would accomplish this objective.    2. If x > y: 2.1 t = x 2.2 x = y 2.3 y = t    Steps 2.1 through 2.3 would be bypassed if the condition x > y were false before step 2.  One slight variation is the \"if ... then ... else\" step, which allows us to prescribe a step to be taken if the condition is false. For example, if you wanted to exercise today, you might look out the window and execute the following algorithm.    1. If it is cold or raining: exercise indoors else: go outside and run 2. Rest     Loops  The conditional step tells us to do something once if a logical condition is true. A loop tells us to repeat one or more steps, called the body of the loop, while the logical condition is true. Before every execution of the body, the condition is tested. The following flow diagram serves to illustrate the steps in a While loop.   Flow diagram for a while loop   Flow diagram for a while loop    Suppose you wanted to solve the equation . The following initial assignment and loop could be employed.    1. c = your first guess 2. While f(c) != 0: c = another guess    Caution: One must always guard against the possibility that the condition of a While loop will never become false. Such \"infinite loops\" are the bane of beginning programmers. The loop above could very well be such a situation, particularly if the equation has no solution, or if the variable takes on real values  In cases where consecutive integer values are to be assigned to a variable, a different loop construction, a For loop , is often employed. For example, suppose we wanted to assign variable k each of the integer values from m to n and for each of these values perform some undefined steps. We could accomplish this with a While loop:    1. k := m 2. While k <= n: 2.1 execute some steps 2.2 k = k + l    Alternatively, we can perform these steps with a For loop.    For k = m to n: execute some steps    For loops such as this one have the advantage of being shorter than the equivalent While loop. The While loop construction has the advantage of being able to handle more different situations than the For loop.    Exercises  What are the inputs and outputs of the algorithms listed in the first sentence of this section?  What is wrong with this algorithm?   Input: a and b, integers Output: the value of c will be a - b (1) c = 0 (2) While a > b: (2.1) a := a - l (2.2) c := c + l    The algorithm only works when a > b .  Describe, in words, what the following algorithm does:   Input: k, a positive integer Output: s = ? (1) s = 0 (2) While k > 0: (2.1) s = s + k (2.2) k = k - 1     Write While loops to replace the For loops in the following partial algorithms:       S = 0  for k = 1 to 5: S = S + k^2     The floor of a number is the greatest integer less than or equal to that number.    m = a positive integer greater than 1  B = floor(sqrt(m))  for i = 2 to B: if i divides evenly into m, jump to step 5  print \"m is a prime\" and exit  print \"m is composite\" and exit      Describe in words what the following algorithm does:    Input: n, a positive integer Output: k? (1) f= 0 (2) k=n (3) While k is even: (3.1) f = f+ 1 (3.2) k = k div 2     Fix the algorithm in Exercise 2.    "
},
{
  "id": "ss-conditionalstep-3",
  "level": "2",
  "url": "app-alg1.html#ss-conditionalstep-3",
  "type": "Listing",
  "number": "A.1.1",
  "title": "",
  "body": "  2. If x > y: 2.1 t = x 2.2 x = y 2.3 y = t   "
},
{
  "id": "ss-conditionalstep-6",
  "level": "2",
  "url": "app-alg1.html#ss-conditionalstep-6",
  "type": "Listing",
  "number": "A.1.2",
  "title": "",
  "body": "  1. If it is cold or raining: exercise indoors else: go outside and run 2. Rest   "
},
{
  "id": "fig-while-flow",
  "level": "2",
  "url": "app-alg1.html#fig-while-flow",
  "type": "Figure",
  "number": "A.1.3",
  "title": "",
  "body": " Flow diagram for a while loop   Flow diagram for a while loop   "
},
{
  "id": "ss-loops-5",
  "level": "2",
  "url": "app-alg1.html#ss-loops-5",
  "type": "Listing",
  "number": "A.1.4",
  "title": "",
  "body": "  1. c = your first guess 2. While f(c) != 0: c = another guess   "
},
{
  "id": "ss-loops-8",
  "level": "2",
  "url": "app-alg1.html#ss-loops-8",
  "type": "Listing",
  "number": "A.1.5",
  "title": "",
  "body": "  1. k := m 2. While k <= n: 2.1 execute some steps 2.2 k = k + l   "
},
{
  "id": "ss-loops-10",
  "level": "2",
  "url": "app-alg1.html#ss-loops-10",
  "type": "Listing",
  "number": "A.1.6",
  "title": "",
  "body": "  For k = m to n: execute some steps   "
},
{
  "id": "exercises-app-alg-1-2",
  "level": "2",
  "url": "app-alg1.html#exercises-app-alg-1-2",
  "type": "Exercise",
  "number": "A.1.4.1",
  "title": "",
  "body": "What are the inputs and outputs of the algorithms listed in the first sentence of this section? "
},
{
  "id": "exercises-app-alg-1-3",
  "level": "2",
  "url": "app-alg1.html#exercises-app-alg-1-3",
  "type": "Exercise",
  "number": "A.1.4.2",
  "title": "",
  "body": "What is wrong with this algorithm?   Input: a and b, integers Output: the value of c will be a - b (1) c = 0 (2) While a > b: (2.1) a := a - l (2.2) c := c + l    The algorithm only works when a > b . "
},
{
  "id": "exercises-app-alg-1-4",
  "level": "2",
  "url": "app-alg1.html#exercises-app-alg-1-4",
  "type": "Exercise",
  "number": "A.1.4.3",
  "title": "",
  "body": "Describe, in words, what the following algorithm does:   Input: k, a positive integer Output: s = ? (1) s = 0 (2) While k > 0: (2.1) s = s + k (2.2) k = k - 1   "
},
{
  "id": "exercises-app-alg-1-5",
  "level": "2",
  "url": "app-alg1.html#exercises-app-alg-1-5",
  "type": "Exercise",
  "number": "A.1.4.4",
  "title": "",
  "body": " Write While loops to replace the For loops in the following partial algorithms:       S = 0  for k = 1 to 5: S = S + k^2     The floor of a number is the greatest integer less than or equal to that number.    m = a positive integer greater than 1  B = floor(sqrt(m))  for i = 2 to B: if i divides evenly into m, jump to step 5  print \"m is a prime\" and exit  print \"m is composite\" and exit     "
},
{
  "id": "exercises-app-alg-1-6",
  "level": "2",
  "url": "app-alg1.html#exercises-app-alg-1-6",
  "type": "Exercise",
  "number": "A.1.4.5",
  "title": "",
  "body": "Describe in words what the following algorithm does:    Input: n, a positive integer Output: k? (1) f= 0 (2) k=n (3) While k is even: (3.1) f = f+ 1 (3.2) k = k div 2    "
},
{
  "id": "exercises-app-alg-1-7",
  "level": "2",
  "url": "app-alg1.html#exercises-app-alg-1-7",
  "type": "Exercise",
  "number": "A.1.4.6",
  "title": "",
  "body": "Fix the algorithm in Exercise 2.  "
},
{
  "id": "app-alg2",
  "level": "1",
  "url": "app-alg2.html",
  "type": "Section",
  "number": "A.2",
  "title": "The Invariant Relation Theorem",
  "body": "The Invariant Relation Theorem  Two Exponentiation Algorithms  Consider the following algorithm implemented in Sage to compute , given an arbitrary integer , non-negative exponent , and a modulus , . The default sample evaluation computes , but you can edit the final line for other inputs.   It should be fairly clear that this algorithm will successfully compute since it mimics the basic definition of exponentiation. However, this algorithm is highly inefficient. The algorithm that is most commonly used for the task of exponentiation is the following one, also implemented in Sage.   The only difficulty with the \"fast algorithm\" is that it might not be so obvious that it always works. When implemented, it can be verified by example, but an even more rigorous verification can be done using the Invariant Relation Theorem. Before stating the theorem, we define some terminology.   Proving the correctness of the fast algorithm  Pre and Post Values pre and post values of a variable  Given a variable , the pre value of , denoted , is the value before an iteration of a loop. The post value, denoted , is the value after the iteration.  Pre and post values in the fast exponentiation algorithm In the fast exponentiation algorithm, the relationships between the pre and post values of the three variables are as follows.      Invariant Relation Given an algorithm's inputs and a set of variables that are used in the algorithm, an invariant relation is a set of one or more equations that are true prior to entering a loop and remain true in every iteration of the loop.   Invariant Relation for Fast Exponentiation We claim that the invariant relation in the fast algorithm is . We will prove that this is indeed true below.  The Invariant Relation Theorem   Given a loop within an algorithm, if is a relation with the properties   R is true before entering the loop  the truth of R is maintained in any iteration of the loop  the condition for exiting the loop will always be reached in a finite number of iterations.   then R will be true upon exiting the loop.   The condition that the loop ends in a finite number of iterations lets us apply mathematical induction with the induction variable being the number of iterations. We leave the details to the reader.  We can verify the correctness of the fast exponentiation algorithm using the Invariant Relation Theorem. First we note that prior to entering the loop, . Assuming the relation is true at the start of any iteration, that is , then Finally, the value of will decrease to zero in a finite number of steps because the number of binary digits of decreases by one with each iteration. At the end of the loop, which verifies the correctness of the algorithm.    Exercises  How are the pre and post values in the slow exponentiation algorithm related? What is the invariant relation between the variables in the slow algorithm?  Verify the correctness of the following algorithm to compute the greatest common divisor of two integers that are not both zero.   The invariant of this algorithm is .   Verify the correctness of the in Chapter 1.  A dragon has 100 heads. A knight can cut off 15, 17, 20, or 5 heads, respectively, with one blow of his sword. In each of these cases 24, 2, 14, or 17 new heads grow on its shoulders, respectively. If all heads are blown off, the dragon dies. Can the dragon ever die? (problem attributed to Biswaroop Roy)  "
},
{
  "id": "def-pre-post-values",
  "level": "2",
  "url": "app-alg2.html#def-pre-post-values",
  "type": "Definition",
  "number": "A.2.1",
  "title": "Pre and Post Values.",
  "body": "Pre and Post Values pre and post values of a variable  Given a variable , the pre value of , denoted , is the value before an iteration of a loop. The post value, denoted , is the value after the iteration. "
},
{
  "id": "ex-pre-post-fast",
  "level": "2",
  "url": "app-alg2.html#ex-pre-post-fast",
  "type": "Example",
  "number": "A.2.2",
  "title": "Pre and post values in the fast exponentiation algorithm.",
  "body": "Pre and post values in the fast exponentiation algorithm In the fast exponentiation algorithm, the relationships between the pre and post values of the three variables are as follows.     "
},
{
  "id": "def-invariant-relation",
  "level": "2",
  "url": "app-alg2.html#def-invariant-relation",
  "type": "Definition",
  "number": "A.2.3",
  "title": "Invariant Relation.",
  "body": "Invariant Relation Given an algorithm's inputs and a set of variables that are used in the algorithm, an invariant relation is a set of one or more equations that are true prior to entering a loop and remain true in every iteration of the loop.  "
},
{
  "id": "ex-invariant-fast-expo",
  "level": "2",
  "url": "app-alg2.html#ex-invariant-fast-expo",
  "type": "Example",
  "number": "A.2.4",
  "title": "Invariant Relation for Fast Exponentiation.",
  "body": "Invariant Relation for Fast Exponentiation We claim that the invariant relation in the fast algorithm is . We will prove that this is indeed true below. "
},
{
  "id": "th-invariant-relation-theorem",
  "level": "2",
  "url": "app-alg2.html#th-invariant-relation-theorem",
  "type": "Theorem",
  "number": "A.2.5",
  "title": "The Invariant Relation Theorem.",
  "body": "The Invariant Relation Theorem   Given a loop within an algorithm, if is a relation with the properties   R is true before entering the loop  the truth of R is maintained in any iteration of the loop  the condition for exiting the loop will always be reached in a finite number of iterations.   then R will be true upon exiting the loop.   The condition that the loop ends in a finite number of iterations lets us apply mathematical induction with the induction variable being the number of iterations. We leave the details to the reader. "
},
{
  "id": "exercises-app-alg1-2",
  "level": "2",
  "url": "app-alg2.html#exercises-app-alg1-2",
  "type": "Exercise",
  "number": "A.2.3.1",
  "title": "",
  "body": "How are the pre and post values in the slow exponentiation algorithm related? What is the invariant relation between the variables in the slow algorithm? "
},
{
  "id": "exercises-app-alg1-3",
  "level": "2",
  "url": "app-alg2.html#exercises-app-alg1-3",
  "type": "Exercise",
  "number": "A.2.3.2",
  "title": "",
  "body": "Verify the correctness of the following algorithm to compute the greatest common divisor of two integers that are not both zero.   The invariant of this algorithm is .  "
},
{
  "id": "exercises-app-alg1-4",
  "level": "2",
  "url": "app-alg2.html#exercises-app-alg1-4",
  "type": "Exercise",
  "number": "A.2.3.3",
  "title": "",
  "body": "Verify the correctness of the in Chapter 1. "
},
{
  "id": "exercises-app-alg1-5",
  "level": "2",
  "url": "app-alg2.html#exercises-app-alg1-5",
  "type": "Exercise",
  "number": "A.2.3.4",
  "title": "",
  "body": "A dragon has 100 heads. A knight can cut off 15, 17, 20, or 5 heads, respectively, with one blow of his sword. In each of these cases 24, 2, 14, or 17 new heads grow on its shoulders, respectively. If all heads are blown off, the dragon dies. Can the dragon ever die? (problem attributed to Biswaroop Roy) "
},
{
  "id": "app-python-iterators",
  "level": "1",
  "url": "app-python-iterators.html",
  "type": "Section",
  "number": "B.1",
  "title": "Python Iterators",
  "body": "Python Iterators  All programming languages allow for looping. A common form of loop is one in which a series of instructions are executed for each value of some index variable, commonly for values between two integers. Python allows a bit more generality by having structures called iterators over which looping can be done. An iterator can be as simple as a list, such as [0,1,2,3] , but also can be a power set of a finite set, as we see below, or the keys in a dictionary, which is described in the next section.  Counting Subsets  Suppose we want to count the number of subsets of that contain no adjacent elements. First, we will define our universe and its power set. The plan will be to define a function that determines whether a subset is \"valid\" in the sense that it contains no adjacent elements. Then we will iterate over the subsets, counting the valid ones. We know that the number of all subsets will be 2 raised to the number of elements in , which would be , but let's check.   The validity check in this case is very simple. For each element, , of a set, , we ask whether its successor, , is also in the set. If we never get an answer of \"True\" then we consider the set valid. This function could be edited to define validity in other ways to answer different counting questions. It's always a good idea to test your functions, so we try two tests, one with a valid set and one with an invalid one.   Finally we do the counting over our power set, incrementing the count variable with each valid set.    "
},
{
  "id": "app-pythonsage-dictionaries",
  "level": "1",
  "url": "app-pythonsage-dictionaries.html",
  "type": "Section",
  "number": "B.2",
  "title": "Dictionaries",
  "body": "Dictionaries  Colors of Fruits  In Python and SageMath, a dictionary is a convenient data structure for establishing a relationship between sets of data. From the point of view of this text, we can think of a dictionary as a concrete realization of a relation between two sets or on a single set. A dictionary resembles a function in that there is a set of data values called the keys , and for each key, there is a value . The value associated with a key can be almost anything, but it is most commonly a list.  To illustrate the use of dictionaries, we will define a relationship between colors and fruits. The keys will be a set of colors and values associated with each color will be a list of fruits that can take on that color. We will demonstrate how to initialize the dictionary and how to add to it. The following series of assignments have no output, so we add a print statement to verify that this cell is completely evaluated.   We distinguish a color from a fruit by capitalizing colors but not fruit. The keys of this dictionary are the colors. The keys() method returns an interator; so to get a list of keys we wrap the result with list() .   As an afterthought, we might add the information that a raspberry is red as follows. You have to be careful in that if 'Red' isn't already in the dictionary, it doesn't have a value. This is why we need an if statement.   A dictionary is iterable, with an iterator taking on values that are the keys. Here we iterate over our dictionary to output lists consisting of a color followed by a list of fruits that come in that color.   We can view a graph of this relation between colors and fruits, but the default view is a bit unconventional.   With a some additional coding we can line up the colors and fruits in their own column. First we set the positions of colors on the left with all -coordinates equal to -5 using another dictionary called vertex_pos .   Next, we place the fruit vertices in another column with -coordinates all equal to 5. In order to do this, we first collect all the fruit values into one set we call fruits .   Now the graph looks like a conventional graph for a relation between two different sets. Notice that it's not a function.    "
},
{
  "id": "app-determinants-3",
  "level": "1",
  "url": "app-determinants-3.html",
  "type": "Section",
  "number": "C.1",
  "title": "Definition",
  "body": " Definition  Associated with every square matrix is a number called its determinant. The most important information it provides us with is whether the matrix is invertible. A matrix has an inverse if and only if its determinant is nonzero. If is a square matrix, then the determinant of is commonly denoted either or . Strictly speaking, we only need to define the determinant of a matrix here and then define the higher ordered ones recursively, but for convenience we also recall the definition of the determinant of a matrix.   Determinant of and a matrices  Determinant and a cases      If is a matrix, then  If is a matrix, then      We now proceed to define the determinant of an matrix where . This definition requires two preliminary definitions those of minors and cofactors.   Matrix Minor  Minor    The minor of     Let be an matrix, . The determinant of the matrix formed by removing the row and column of is the minor denoted by .     Let then has nine minors, one of which is   For our purposes in computing , we only need minors corresponding to any one row or column. Completing the minors in the first row we have and    Cofactor  Cofactor    The cofactor of     Let be an matrix, . The row, column cofactor of , denoted , is defined by      Using the values of minors computed in , we have , , and .   Finally, we will define the determinant of a square matrix. Our definition is practical in that you can apply it easily to any matrix. It isn't the most general, nor is it the best definiton for the purposes of proving properties of determinants. The more general definition is beyond our current scope, but can be easily stated with background in permutation groups.   Determinant of a Square Matrix  Determinant    The determinant of     Let be an matrix, . The determinant of is equal to     Our definition of a determinant involves what is called expansion along the first row of the matrix A. It is certainly not obvious, but it is true, that the determinant of a matrix can be found by expanding along any row or any column.   We have computed the cofactors for row 1 of above and so the determinant is only a few operations away.    Characteristic Polynomial  Associated with any square matrix, , is a characteristic polynomial which is defined to be the . The roots of this polynomial are the eigenvalues of the matrix. Here, we compute the characteristic polynomial of .  To compute the determinant we expand along the first row.    "
},
{
  "id": "def-determinant-basis",
  "level": "2",
  "url": "app-determinants-3.html#def-determinant-basis",
  "type": "Definition",
  "number": "C.1.1",
  "title": "Determinant of <span xmlns:pf=\"https:\/\/prefigure.org\" class=\"process-math\">\\(1 \\times 1\\)<\/span> and a <span class=\"process-math\">\\(2 \\times 2\\)<\/span> matrices.",
  "body": " Determinant of and a matrices  Determinant and a cases      If is a matrix, then  If is a matrix, then     "
},
{
  "id": "def-minor",
  "level": "2",
  "url": "app-determinants-3.html#def-minor",
  "type": "Definition",
  "number": "C.1.2",
  "title": "Matrix Minor.",
  "body": " Matrix Minor  Minor    The minor of     Let be an matrix, . The determinant of the matrix formed by removing the row and column of is the minor denoted by .   "
},
{
  "id": "example-minor-3-by-3",
  "level": "2",
  "url": "app-determinants-3.html#example-minor-3-by-3",
  "type": "Example",
  "number": "C.1.3",
  "title": "",
  "body": " Let then has nine minors, one of which is   For our purposes in computing , we only need minors corresponding to any one row or column. Completing the minors in the first row we have and  "
},
{
  "id": "def-cofactor",
  "level": "2",
  "url": "app-determinants-3.html#def-cofactor",
  "type": "Definition",
  "number": "C.1.4",
  "title": "Cofactor.",
  "body": " Cofactor  Cofactor    The cofactor of     Let be an matrix, . The row, column cofactor of , denoted , is defined by    "
},
{
  "id": "example-cofactors-3-by-3",
  "level": "2",
  "url": "app-determinants-3.html#example-cofactors-3-by-3",
  "type": "Example",
  "number": "C.1.5",
  "title": "",
  "body": " Using the values of minors computed in , we have , , and .  "
},
{
  "id": "def-determinant-general",
  "level": "2",
  "url": "app-determinants-3.html#def-determinant-general",
  "type": "Definition",
  "number": "C.1.6",
  "title": "Determinant of a Square Matrix.",
  "body": " Determinant of a Square Matrix  Determinant    The determinant of     Let be an matrix, . The determinant of is equal to    "
},
{
  "id": "example-determinant-3-3",
  "level": "2",
  "url": "app-determinants-3.html#example-determinant-3-3",
  "type": "Example",
  "number": "C.1.7",
  "title": "",
  "body": " We have computed the cofactors for row 1 of above and so the determinant is only a few operations away.   "
},
{
  "id": "characteristic-polynomial",
  "level": "2",
  "url": "app-determinants-3.html#characteristic-polynomial",
  "type": "Example",
  "number": "C.1.8",
  "title": "",
  "body": "Characteristic Polynomial  Associated with any square matrix, , is a characteristic polynomial which is defined to be the . The roots of this polynomial are the eigenvalues of the matrix. Here, we compute the characteristic polynomial of .  To compute the determinant we expand along the first row.   "
},
{
  "id": "app-determinants-4",
  "level": "1",
  "url": "app-determinants-4.html",
  "type": "Section",
  "number": "C.2",
  "title": "Computation",
  "body": "Computation  Our definition of determinant can be applied to estimate the worst case for the time to evaluate an determinant. Let be the number of multiplications to evaluate an determinant. Then we have . To determine the value of we observe that this requires the computation of three minors, each a two by two matrix, and then a multiplication of each of them by the entries in row 1. Therefore, . Using the same logic in general, we have . The formula can be derived to be . For large this is approximately . Fortunately, there are ways to reduce the number of multiplications using properties of determinants, which we list here without proof.  Properties of Determinants  Let and be matrices, where .   can be found by expanding along any row or any column.  If two rows (or columns) of are interchanged, changes sign.  The value of a determinant is unchanged if a multiple of one row (or column) of is added to another row (or column) of .  If one row (or column) of a matrix is multiplied by a constant , then the value of is multiplied by .   .   where is the identity matrix.     Based on these properties, here are a few corollaries.  Further Properties  Let and be matrices, where .  If a row (or column) of consists entirely of zeros, then .  If a matrix has two equal rows (or columns) then .  If any row (or column) of is a scalar multiple of any other row (or column) of , then .   , if exists.    Computatation of a determinant by row reduction  We will apply some of these properties, most notably the first and third of , to compute a four by four determinant without doing as many multiplications as expected. We will use SageMath to do the calculations for us. In SageMath, as in Python, numbering starts at zero, so we will describe the process using that numbering system. Let  Our strategy will be to create a column that is mostly zero so that we can expand along that column and only need to compute one cofactor. That will be the 0th column. To do that we do the following row operations. We subtract row 0 from row 1, replacing row 1 with that result. Then we subtract six time row 0 from row 2, producing a new row 2. Finally, three times row 0 is subtracted from row 3 to produce a new row 3. The SageMath code below accomplishes this and produces a new matrix, , which has the same determinant.   Expanding this matrix along the column zero, we need only compute a single three by three cofactor. We will go one step further and do row operations to get a matrix with zeros in rows 2 and 3 of column 1. The SageMath code below tells what we are doing.   We are at a point where we can do the final calculation very easily. SageMath has a determinant function, det , that we can use to verify this calculation:    "
},
{
  "id": "determinant-properties",
  "level": "2",
  "url": "app-determinants-4.html#determinant-properties",
  "type": "Theorem",
  "number": "C.2.1",
  "title": "Properties of Determinants.",
  "body": "Properties of Determinants  Let and be matrices, where .   can be found by expanding along any row or any column.  If two rows (or columns) of are interchanged, changes sign.  The value of a determinant is unchanged if a multiple of one row (or column) of is added to another row (or column) of .  If one row (or column) of a matrix is multiplied by a constant , then the value of is multiplied by .   .   where is the identity matrix.    "
},
{
  "id": "determinant-corollaries",
  "level": "2",
  "url": "app-determinants-4.html#determinant-corollaries",
  "type": "Corollary",
  "number": "C.2.2",
  "title": "Further Properties.",
  "body": "Further Properties  Let and be matrices, where .  If a row (or column) of consists entirely of zeros, then .  If a matrix has two equal rows (or columns) then .  If any row (or column) of is a scalar multiple of any other row (or column) of , then .   , if exists.   "
},
{
  "id": "determinant-by-reduction",
  "level": "2",
  "url": "app-determinants-4.html#determinant-by-reduction",
  "type": "Example",
  "number": "C.2.3",
  "title": "Computatation of a determinant by row reduction.",
  "body": "Computatation of a determinant by row reduction  We will apply some of these properties, most notably the first and third of , to compute a four by four determinant without doing as many multiplications as expected. We will use SageMath to do the calculations for us. In SageMath, as in Python, numbering starts at zero, so we will describe the process using that numbering system. Let  Our strategy will be to create a column that is mostly zero so that we can expand along that column and only need to compute one cofactor. That will be the 0th column. To do that we do the following row operations. We subtract row 0 from row 1, replacing row 1 with that result. Then we subtract six time row 0 from row 2, producing a new row 2. Finally, three times row 0 is subtracted from row 3 to produce a new row 3. The SageMath code below accomplishes this and produces a new matrix, , which has the same determinant.   Expanding this matrix along the column zero, we need only compute a single three by three cofactor. We will go one step further and do row operations to get a matrix with zeros in rows 2 and 3 of column 1. The SageMath code below tells what we are doing.   We are at a point where we can do the final calculation very easily. SageMath has a determinant function, det , that we can use to verify this calculation:   "
},
{
  "id": "app-sage-reference",
  "level": "1",
  "url": "app-sage-reference.html",
  "type": "Appendix",
  "number": "D",
  "title": "Sage Reference",
  "body": " Sage Reference  We have introduced a number of Sage commands throughout the text, and the most important ones are summarized here in a single place.    Accessing Sage  In addition to the Sage cellls included throughout the book, there are a number of ways to access Sage.  There is a freely available Sage cell at .  You can save your Sage work by creating an account at and working in a Sage worksheet.  There is a page of Sage cells at . The results obtained from evaluating one cell are available in other cells on that page. However, you will lose any work once the page is reloaded.    Creating matrices  There are a couple of ways to create matrices. For instance, the matrix can be created in either of the two following ways.    matrix(3, 4, [-2, 3, 0, 4, 1,-2, 1,-3, 0, 2, 3, 0])      matrix([ [-2, 3, 0, 4], [ 1,-2, 1,-3], [ 0, 2, 3, 0] ])      Be aware that Sage can treat mathematically equivalent matrices in different ways depending on how they are entered. For instance, the matrix matrix([ [1, 2], [2, 1] ]) has integer entries while matrix([ [1.0, 2.0], [2.0, 1.0] ]) has floating point entries.  If you would like the entries to be considered as floating point numbers, you can include RDF in the definition of the matrix. matrix(RDF, [ [1, 2], [2, 1] ])     Special matrices  The identity matrix can be created with identity_matrix(4) A diagonal matrix can be created from a list of its diagonal entries. For instance, diagonal_matrix([3,-4,2])     Reduced row echelon form  The reduced row echelon form of a matrix can be obtained using the rref() function. For instance, A = matrix([ [1,2], [2,1] ]) A.rref()     Vectors  A vector is defined by listing its components. v = vector([3,-1,2])     Addition  The + operator performs vector and matrix addition. v = vector([2,1]) w = vector([-3,2]) print(v+w)  A = matrix([[2,-3],[1,2]]) B = matrix([[-4,1],[3,-1]]) print(A+B)     Multiplication  The * operator performs scalar multiplication of vectors and matrices. v = vector([2,1]) print(3*v) A = matrix([[2,1],[-3,2]]) print(3*A)   Similarly, the * is used for matrix-vector and matrix-matrix multiplication. A = matrix([[2,-3],[1,2]]) v = vector([2,1]) print(A*v) B = matrix([[-4,1],[3,-1]]) print(A*B)     Operations on vectors     The length of a vector v is found using v.norm() .    The dot product of two vectors v and w is v*w .       Operations on matrices    The transpose of a matrix A is obtained using either A.transpose() or A.T .    The inverse of a matrix A is obtained using either A.inverse() or A^-1 .    The determinant of A is A.det() .    A basis for the null space is found with A.right_kernel() .    Pull out a column of A using, for instance, A.column(0) , which returns the vector that is the first column of A .    The command A.matrix_from_columns([0,1,2]) returns the matrix formed by the first three columns of A .       Eigenvectors and eigenvalues     The eigenvalues of a matrix A can be found with A.eigenvalues() . The number of times that an eigenvalue appears in the list equals its multiplicity.    The eigenvectors of a matrix having rational entries can be found with A.eigenvectors_right() .    If can be diagonalized as , then D, P = A.right_eigenmatrix() provides the matrices D and P .    The characteristic polynomial of A is A.charpoly('x') and its factored form A.fcp('x') .       Matrix factorizations     The factorization of a matrix P, L, U = A.LU() gives matrices so that .    A singular value decomposition is obtained with U, Sigma, V = A.SVD() It's important to note that the matrix must be defined using RDF . For instance, A = matrix(RDF, 3,2,[1,0,-1,1,1,1]) .    The factorization of A is A.QR() provided that A is defined using RDF .        "
},
{
  "id": "solutions-backmatter",
  "level": "1",
  "url": "solutions-backmatter.html",
  "type": "Appendix",
  "number": "E",
  "title": "Hints and Solutions to Selected Exercises",
  "body": " Hints and Solutions to Selected Exercises  solutions   For the most part, solutions are provided here for odd-numbered exercises.   "
},
{
  "id": "backmatter-7",
  "level": "1",
  "url": "backmatter-7.html",
  "type": "Appendix",
  "number": "F",
  "title": "Notation",
  "body": "Notation  The following table defines the notation used in this book. Page numbers or references refer to the first appearance of each symbol.   "
},
{
  "id": "backmatter-8",
  "level": "1",
  "url": "backmatter-8.html",
  "type": "Appendix",
  "number": "G",
  "title": "Glossary",
  "body": "Glossary   An Informal Glossary of Terms  Glossary  Many of the words in this glossary are not formally defined in the book either because they are viewed as prerequisites to a course in discrete mathematics or are terms in computer science that some students may be unfamiliar with.   An  An  When referring to “an entity” we mean that the object can be any of the elements is some set. For example, if you say that is an integer, it could be any integer.    Bit Bit  The smallest unit of computer memory, normally represented as a 0 or 1.    Byte  Byte  A basic unit of computer memory containing eight s, normally modeled as a sequence of eight 0’s and 1’s.    Complex Number  Complex Number  A number of the form , where and are real numbers and .    Composite Integer Composite Integer  A positive integer is composite if it is greater than one and is the product of two positive integers greater than one. For example, 10 (equal to ) is composite. Any positive integer greater than one that is not composite is .    Constant  Constant  A numerical value that is unchanging . The value might be unknown and it still may be represented with a symbol. For example if we are discussing the process of sorting a file of numbers, is considered a constant with respect to the sorting algorithm. Constants can become variables though. If we have designed a sorting algorithm, and want to analyze its efficiency, we would consider to be a variable.    Creative Commons  Creative Commons  An organization which has created several open licenses for creative works such as Applied Discrete Structures .    Data Structure  Data Structure  A format for organizing, processing, retrieving and storing data.    Distinct Distinct  Two entities are distinct if they are not the same. For example, any two student ID numbers at a school should be distinct. If not, confusion could ensue. See also .    Even Integer  Even Integer  Any that is equal to two times an integer. That includes 0, since .    Factor Factor  If an algebraic expression is the product of several expressions, each of those expressions is a factor.    Iff  Iff  Shorthand for “if and only if”    Integer  Integer  Whole number, whether positive, negative or zero.    Irrational Number  Irrational Number  A number that is not equal to any fraction. is one we prove to be irrational in the book.    LaTeX LaTeX  A markup language used for books and papers with lots of mathematics, which is built on tex . PreTeXt uses latex as an intermediate format to produce PDF and print output.    Multiples Multiples  Multiples of a number are    Natural Numbers  Natural Numbers  In this book, its the numbers 0,1,2,3,4,… . There isn’t 100% agreement here. Some people say its the numbers 1,2,3,4, … . We call those numbers the positive integers. The symbol we use of the natural numbers is . There is no consistent definition of positive complex numbers.    Nonnegative Number Nonnegative Number  A number that is either positive or zero.    Odd Integer  Odd Integer  An integer is odd if there exists an integer so that . Any integer that is not even is odd.    Positive Number Positive Number  A positive number is a number that is greater than zero. Normally visualized as being to the right of zero on a conventional number line. The set of positive integers is denoted . The sets of positive rational and real numbers are denoted and , respectively    Powers  Powers  Powers of a nonzero number are . Recall that .    PreTeXt PreTeXt  An authoring and publishing system for authors of textbooks, research articles, and monographs, especially in STEM disciplines. Applied Discrete Structures is produced using PreTeXt.    Prime Prime  A positive integer that is divisible by exactly two positive integers, itself and . One is not prime, but is the oddest prime because it’s even. See also .    Queue  Queue  A conventional waiting line, with the first come-first serve service rule. A queue is a common in computer science. See also .    Rational Number Rational Number  Any real number that is equal to a quotient two integers, , with .    Real Number Real Number  For the purposes of this book, think of the numbers on a standard number line. All of the points make up the set of real numbers.    SageMath SageMath  An open source computer algebra system for a wide range of symbolic and numerical mathematical computations. Originally named simply Sage.    Stack Stack  A similar to a queue, but where the last come-first serve service rule is used. This wouldn’t be a fair waiting line rule, but it is a very useful data structure. See also    Subtraction Subtraction  Subtraction is really addition of the negation of a number: .    Term  Term  If an algebraic expression is the sum of several expressions, each of those expressions is a term. For example there are three terms in the expression . Note that subtraction is considered the same as addition here.    Unique  Unique  We say a mathematical entity is unique when there’s nothing else like it. For example, the solution, to the equation is unique. No other number solves the equation. See also .    Variable Variable  A quantity whose value that can vary within a specified set. Normally represented by an algebraic symbol. For discrete variables it is customary to use the letters in the range from i to n, but this isn’t a rigid rule. Letters at the end of the alphabet are traditionally used for continuous variables.    "
},
{
  "id": "backmatter-9",
  "level": "1",
  "url": "backmatter-9.html",
  "type": "Index",
  "number": "",
  "title": "Index",
  "body": " Index   "
},
{
  "id": "backmatter-10",
  "level": "1",
  "url": "backmatter-10.html",
  "type": "Colophon",
  "number": "",
  "title": "Colophon",
  "body": " This book was authored in PreTeXt .  "
}
]

var ptx_lunr_idx = lunr(function () {
  this.ref('id')
  this.field('title')
  this.field('body')
  this.metadataWhitelist = ['position']

  ptx_lunr_docs.forEach(function (doc) {
    this.add(doc)
  }, this)
})
